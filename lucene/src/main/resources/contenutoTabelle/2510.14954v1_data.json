{
    "S3.T1": {
        "caption": "Table 1: The quantitative results of text-to-motion on the HumanML3D subset of Motion-X dataset (Lin et al., 2023a), following the unified SMPL-X representation (Bian et al., 2025).",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Method</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">R Precision</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">FID <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">MM Dist<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Top-1 <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Top-2 <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Top-3 <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">GT</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"0.663^{\\pm{0.006}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m6\" intent=\":literal\"><semantics><msup><mn>0.663</mn><mrow><mo>&#177;</mo><mn>0.006</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.663^{\\pm{0.006}}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"0.807^{\\pm{0.002}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m7\" intent=\":literal\"><semantics><msup><mn>0.807</mn><mrow><mo>&#177;</mo><mn>0.002</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.807^{\\pm{0.002}}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"0.864^{\\pm{0.002}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m8\" intent=\":literal\"><semantics><msup><mn>0.864</mn><mrow><mo>&#177;</mo><mn>0.002</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.864^{\\pm{0.002}}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"0.000^{\\pm{0.000}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m9\" intent=\":literal\"><semantics><msup><mn>0.000</mn><mrow><mo>&#177;</mo><mn>0.000</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.000^{\\pm{0.000}}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"15.567^{\\pm{0.036}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m10\" intent=\":literal\"><semantics><msup><mn>15.567</mn><mrow><mo>&#177;</mo><mn>0.036</mn></mrow></msup><annotation encoding=\"application/x-tex\">15.567^{\\pm{0.036}}</annotation></semantics></math></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">T2M-GPT<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib65\" title=\"\">2023a</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"0.529^{\\pm{0.004}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m11\" intent=\":literal\"><semantics><msup><mn>0.529</mn><mrow><mo>&#177;</mo><mn>0.004</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.529^{\\pm{0.004}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"0.652^{\\pm{0.003}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m12\" intent=\":literal\"><semantics><msup><mn>0.652</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.652^{\\pm{0.003}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"0.732^{\\pm{0.003}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m13\" intent=\":literal\"><semantics><msup><mn>0.732</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.732^{\\pm{0.003}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"10.457^{\\pm{0.108}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m14\" intent=\":literal\"><semantics><msup><mn>10.457</mn><mrow><mo>&#177;</mo><mn>0.108</mn></mrow></msup><annotation encoding=\"application/x-tex\">10.457^{\\pm{0.108}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"17.029^{\\pm{0.039}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m15\" intent=\":literal\"><semantics><msup><mn>17.029</mn><mrow><mo>&#177;</mo><mn>0.039</mn></mrow></msup><annotation encoding=\"application/x-tex\">17.029^{\\pm{0.039}}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">MDM<cite class=\"ltx_cite ltx_citemacro_citep\">(Tevet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib52\" title=\"\">2023</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"0.383^{\\pm{0.010}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m16\" intent=\":literal\"><semantics><msup><mn>0.383</mn><mrow><mo>&#177;</mo><mn>0.010</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.383^{\\pm{0.010}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"0.527^{\\pm{0.012}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m17\" intent=\":literal\"><semantics><msup><mn>0.527</mn><mrow><mo>&#177;</mo><mn>0.012</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.527^{\\pm{0.012}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"0.604^{\\pm{0.009}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m18\" intent=\":literal\"><semantics><msup><mn>0.604</mn><mrow><mo>&#177;</mo><mn>0.009</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.604^{\\pm{0.009}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"18.671^{\\pm{0.370}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m19\" intent=\":literal\"><semantics><msup><mn>18.671</mn><mrow><mo>&#177;</mo><mn>0.370</mn></mrow></msup><annotation encoding=\"application/x-tex\">18.671^{\\pm{0.370}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"18.785^{\\pm{0.054}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m20\" intent=\":literal\"><semantics><msup><mn>18.785</mn><mrow><mo>&#177;</mo><mn>0.054</mn></mrow></msup><annotation encoding=\"application/x-tex\">18.785^{\\pm{0.054}}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">MotionDiffuse<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib68\" title=\"\">2024b</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"0.525^{\\pm{0.004}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m21\" intent=\":literal\"><semantics><msup><mn>0.525</mn><mrow><mo>&#177;</mo><mn>0.004</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.525^{\\pm{0.004}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"0.675^{\\pm{0.009}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m22\" intent=\":literal\"><semantics><msup><mn>0.675</mn><mrow><mo>&#177;</mo><mn>0.009</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.675^{\\pm{0.009}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"0.743^{\\pm{0.009}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m23\" intent=\":literal\"><semantics><msup><mn>0.743</mn><mrow><mo>&#177;</mo><mn>0.009</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.743^{\\pm{0.009}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"9.982^{\\pm{0.379}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m24\" intent=\":literal\"><semantics><msup><mn>9.982</mn><mrow><mo>&#177;</mo><mn>0.379</mn></mrow></msup><annotation encoding=\"application/x-tex\">9.982^{\\pm{0.379}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"17.314^{\\pm{0.066}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m25\" intent=\":literal\"><semantics><msup><mn>17.314</mn><mrow><mo>&#177;</mo><mn>0.066</mn></mrow></msup><annotation encoding=\"application/x-tex\">17.314^{\\pm{0.066}}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">FineMoGen<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib67\" title=\"\">2023c</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"0.565^{\\pm{0.001}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m26\" intent=\":literal\"><semantics><msup><mn>0.565</mn><mrow><mo>&#177;</mo><mn>0.001</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.565^{\\pm{0.001}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"0.710^{\\pm{0.004}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m27\" intent=\":literal\"><semantics><msup><mn>0.710</mn><mrow><mo>&#177;</mo><mn>0.004</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.710^{\\pm{0.004}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"0.775^{\\pm{0.004}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m28\" intent=\":literal\"><semantics><msup><mn>0.775</mn><mrow><mo>&#177;</mo><mn>0.004</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.775^{\\pm{0.004}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"7.323^{\\pm{0.143}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m29\" intent=\":literal\"><semantics><msup><mn>7.323</mn><mrow><mo>&#177;</mo><mn>0.143</mn></mrow></msup><annotation encoding=\"application/x-tex\">7.323^{\\pm{0.143}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"16.679^{\\pm{0.029}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m30\" intent=\":literal\"><semantics><msup><mn>16.679</mn><mrow><mo>&#177;</mo><mn>0.029</mn></mrow></msup><annotation encoding=\"application/x-tex\">16.679^{\\pm{0.029}}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">MCM<cite class=\"ltx_cite ltx_citemacro_citep\">(Ling et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib36\" title=\"\">2023</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"0.407^{\\pm{0.002}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m31\" intent=\":literal\"><semantics><msup><mn>0.407</mn><mrow><mo>&#177;</mo><mn>0.002</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.407^{\\pm{0.002}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"0.559^{\\pm{0.003}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m32\" intent=\":literal\"><semantics><msup><mn>0.559</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.559^{\\pm{0.003}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"0.636^{\\pm{0.001}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m33\" intent=\":literal\"><semantics><msup><mn>0.636</mn><mrow><mo>&#177;</mo><mn>0.001</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.636^{\\pm{0.001}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"15.540^{\\pm{0.443}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m34\" intent=\":literal\"><semantics><msup><mn>15.540</mn><mrow><mo>&#177;</mo><mn>0.443</mn></mrow></msup><annotation encoding=\"application/x-tex\">15.540^{\\pm{0.443}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"18.673^{\\pm{0.029}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m35\" intent=\":literal\"><semantics><msup><mn>18.673</mn><mrow><mo>&#177;</mo><mn>0.029</mn></mrow></msup><annotation encoding=\"application/x-tex\">18.673^{\\pm{0.029}}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"0.590^{\\pm{0.003}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m36\" intent=\":literal\"><semantics><msup><mn>0.590</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.590^{\\pm{0.003}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"0.743^{\\pm{0.004}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m37\" intent=\":literal\"><semantics><msup><mn>0.743</mn><mrow><mo>&#177;</mo><mn>0.004</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.743^{\\pm{0.004}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"0.804^{\\pm{0.004}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m38\" intent=\":literal\"><semantics><msup><mn>0.804</mn><mrow><mo>&#177;</mo><mn>0.004</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.804^{\\pm{0.004}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"8.477^{\\pm{0.102}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m39\" intent=\":literal\"><semantics><msup><mn>8.477</mn><mrow><mo>&#177;</mo><mn>0.102</mn></mrow></msup><annotation encoding=\"application/x-tex\">8.477^{\\pm{0.102}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"16.252^{\\pm{0.035}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m40\" intent=\":literal\"><semantics><msup><mn>16.252</mn><mrow><mo>&#177;</mo><mn>0.035</mn></mrow></msup><annotation encoding=\"application/x-tex\">16.252^{\\pm{0.035}}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Ours</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"\\mathbf{0.704}^{\\pm{0.003}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m41\" intent=\":literal\"><semantics><msup><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.704</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{0.704}^{\\pm{0.003}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"\\mathbf{0.843}^{\\pm{0.005}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m42\" intent=\":literal\"><semantics><msup><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.843</mn><mrow><mo>&#177;</mo><mn>0.005</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{0.843}^{\\pm{0.005}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"\\mathbf{0.898}^{\\pm{0.005}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m43\" intent=\":literal\"><semantics><msup><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.898</mn><mrow><mo>&#177;</mo><mn>0.005</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{0.898}^{\\pm{0.005}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"\\mathbf{4.838}^{\\pm{0.100}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m44\" intent=\":literal\"><semantics><msup><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">4.838</mn><mrow><mo>&#177;</mo><mn>0.100</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{4.838}^{\\pm{0.100}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"\\mathbf{15.871}^{\\pm{0.030}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m45\" intent=\":literal\"><semantics><msup><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">15.871</mn><mrow><mo>&#177;</mo><mn>0.030</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{15.871}^{\\pm{0.030}}</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "7323±01437323pm0143",
            "2023c",
            "smplx",
            "unified",
            "2024b",
            "top3",
            "0710±00040710pm0004",
            "0804±00040804pm0004",
            "0525±00040525pm0004",
            "0743±00090743pm0009",
            "bian",
            "16679±002916679pm0029",
            "0843±0005mathbf0843pm0005",
            "↓downarrow",
            "0407±00020407pm0002",
            "8477±01028477pm0102",
            "0604±00090604pm0009",
            "16252±003516252pm0035",
            "15871±0030mathbf15871pm0030",
            "10457±010810457pm0108",
            "0383±00100383pm0010",
            "0527±00120527pm0012",
            "0529±00040529pm0004",
            "t2mgptzhang",
            "0590±00030590pm0003",
            "0652±00030652pm0003",
            "17314±006617314pm0066",
            "mdmtevet",
            "0864±00020864pm0002",
            "lin",
            "15567±003615567pm0036",
            "0565±00010565pm0001",
            "17029±003917029pm0039",
            "following",
            "2023a",
            "top2",
            "0663±00060663pm0006",
            "0775±00040775pm0004",
            "0675±00090675pm0009",
            "9982±03799982pm0379",
            "results",
            "15540±044315540pm0443",
            "humanml3d",
            "0898±0005mathbf0898pm0005",
            "fid",
            "0636±00010636pm0001",
            "18673±002918673pm0029",
            "ours",
            "↑uparrow",
            "mcmling",
            "texttomotion",
            "0559±00030559pm0003",
            "motioncraft",
            "0807±00020807pm0002",
            "0732±00030732pm0003",
            "0704±0003mathbf0704pm0003",
            "representation",
            "motiondiffusezhang",
            "motionx",
            "quantitative",
            "18671±037018671pm0370",
            "finemogenzhang",
            "0743±00040743pm0004",
            "precision",
            "subset",
            "method",
            "0000±00000000pm0000",
            "dataset",
            "4838±0100mathbf4838pm0100",
            "top1",
            "18785±005418785pm0054",
            "dist↓downarrow"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-based motion generation. </span>\nWe conduct an evaluation of our model against prior text-to-motion approaches, including both discrete-domain and continuous-domain methods.\nAs summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S3.T1\" title=\"Table 1 &#8227; 3.5 Text-to-motion Pretraining and Multimodal Control Adaptation &#8227; 3 Method &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our method clearly surpasses prior techniques on the HumanML subset of Motion-X dataset. Remarkably, our model achieves improvements of 19.3%, 13.5%, and 11.7% in R-Precision for Top-1, 2, 3, respectively. Additionally, we enhance the FID score by 75.2% on this dataset, underscoring the exceptional fidelity of our generated motions. The qualitative results in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S3.F4\" title=\"Figure 4 &#8227; 3.5 Text-to-motion Pretraining and Multimodal Control Adaptation &#8227; 3 Method &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> further support these findings, showing that our approach yields whole-body motions that align closely with the input text.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Whole-body multi-modal human motion generation poses two primary challenges: creating an effective motion generation mechanism and integrating various modalities, such as text, speech, and music, into a cohesive framework.\nUnlike previous methods that usually employ discrete masked modeling or autoregressive modeling, we develop a continuous masked autoregressive motion transformer, where a causal attention is performed considering the sequential nature within the human motion.\nWithin this transformer, we introduce a gated linear attention and an RMSNorm module, which drive the transformer to pay attention to the key actions and suppress the instability caused by either the abnormal movements or the heterogeneous distributions within multi-modalities.\nTo further enhance both the motion generation and the multimodal generalization, we employ the DiT structure to diffuse the conditions from the transformer towards the targets.\nTo fuse different modalities, AdaLN and cross-attention are leveraged to inject the text, speech, and music signals.\nExperimental results demonstrate that our framework outperforms previous methods across all modalities, including text-to-motion, speech-to-gesture, and music-to-dance.\nThe code of our method will be made public.</p>\n\n",
                "matched_terms": [
                    "method",
                    "texttomotion",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To both leverage the advantages of autoregressive and masked modeling, and the benefits of the continuous motion representation, in this work we combine them together to propose a continuous masked autoregressive motion generation framework.\nWe apply a random masking on the sequential motion tokens, and employ a transformer to autoregressively predict the masked tokens.\nUnlike the visual MAR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>)</cite>, we sequentially predict the masked tokens with causal attention rather than performing random reordering, considering the sequential nature within human motion.\nTo enhance the MAR modeling in motion space, we introduce a gated linear mechanism and an RMSNorm module.\nThe gated linear mechanism serves as an adaptive feature selector, driving the transformer to not only pay more attention to the key actions, like gesture switching and large movement, but also disregard less relevant frames and suppress redundant actions, like stationary motions.\nThe RMSNorm is particularly advantageous in scenarios with features exhibiting a large dynamic range, e.g., our unified framework for multi-modalities, where the input distributions are highly heterogeneous.\nIn addition, RMSNorm helps relieve the gradient instability caused by the abnormally large motions, such as sudden jumping or turning back.\nAfter the masked autoregressive transformer, the calculated attention of the masked tokens is fed into a series of DiT blocks to diffuse towards the target tokens, which are decoded to the generated motions.</p>\n\n",
                "matched_terms": [
                    "2024b",
                    "unified",
                    "representation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous research on text-based motion generation can be generally categorized into two mainstream paths: continuous regression and discrete classification.\nIn the continuous regression domain, numerous strategies have leveraged the variational autoencoder (VAE) framework, integrating latent embeddings of encoded text with those of encoded poses, which are then decoded into motion predictions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib58\" title=\"\">2020</a>; Ahuja &amp; Morency, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib1\" title=\"\">2019</a>; Athanasiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib4\" title=\"\">2022</a>; Petrovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib45\" title=\"\">2022</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib15\" title=\"\">2022a</a>)</cite>.\nOther methods have investigated the potential of recurrent networks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib33\" title=\"\">2018</a>; Plappert et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib47\" title=\"\">2018</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib70\" title=\"\">2020</a>)</cite>, generative adversarial networks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib7\" title=\"\">2018</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib57\" title=\"\">2020</a>; Tulyakov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib54\" title=\"\">2018</a>)</cite>, or transformer networks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tevet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib51\" title=\"\">2022</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib35\" title=\"\">2023b</a>; Bhattacharya et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib5\" title=\"\">2021</a>; Petrovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib44\" title=\"\">2021</a>)</cite> to enhance the motion regression quality.\nBuilding on the success of diffusion models, recent approaches have begun to integrate the diffusion process into motion diffusion&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib22\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib12\" title=\"\">2023</a>; Tevet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib52\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib68\" title=\"\">2024b</a>; Dabral et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib13\" title=\"\">2023</a>; Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib40\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib66\" title=\"\">2023b</a>; Ribeiro-Gomes et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib48\" title=\"\">2024</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib62\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib56\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib67\" title=\"\">2023c</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib72\" title=\"\">2024d</a>; Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, yielding impressive results.\nIn the discrete classification domain, the input motion undergoes initial encoding via a VQ-VAE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Van Den&#160;Oord et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib55\" title=\"\">2017</a>)</cite>, producing motion tokens for subsequent prediction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib74\" title=\"\">2023</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib32\" title=\"\">2024e</a>)</cite>.\nDrawing inspiration from advancements in natural language processing, some methods utilize autoregressive modeling to predict tokens sequentially&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib16\" title=\"\">2022b</a>; Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib40\" title=\"\">2023</a>; Zou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib80\" title=\"\">2024</a>)</cite>.\nOthers employ generative masked modeling strategies, with tokens randomly masked during training for the model to predict&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib17\" title=\"\">2024</a>; Pinyoanuntapong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib46\" title=\"\">2024</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib61\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib28\" title=\"\">2023b</a>)</cite>.\nMore recently, large language models (LLMs) have been harnessed to help the prediction process, considering their large-scale pretraining&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib71\" title=\"\">2023d</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib77\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib31\" title=\"\">2024d</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib29\" title=\"\">2023c</a>)</cite>.\nIn this work, we seek to integrate the most effective elements from these two paths: the continuous diffusion and the masked autoregressive modeling.\nA previous attempt in this direction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib42\" title=\"\">2024</a>)</cite> directly transfers the MAR in image generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>)</cite> into motion generation without considering the difference between image and motion spaces, especially the temporal correlation. Also, its framework is only designed for body-only motion generation. Differently, we propose a new MAR mechanism that is especially designed for whole-body motion generation.</p>\n\n",
                "matched_terms": [
                    "lin",
                    "2023c",
                    "2024b",
                    "results",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to text, there are many other signals that various human motions are conditioned on, such as speech and music.\nIn the realm of speech-to-gesture generation, both continuous regression and discrete classification paths have been explored.\nIn the continuous domain, methods employ deep generative models like GANs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ahuja et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib2\" title=\"\">2022</a>)</cite>, normalizing flows&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib50\" title=\"\">2024</a>)</cite>, and diffusion models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Alexanderson et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib3\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib10\" title=\"\">2024a</a>; He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib20\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib59\" title=\"\">2023</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib78\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib11\" title=\"\">2024b</a>)</cite> to learn complex motion distributions in the speech data.\nIn the discrete domain, methods leverage either the autoregressive modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib60\" title=\"\">2023</a>)</cite> or the masked modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib37\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib38\" title=\"\">b</a>)</cite> to predict the discrete tokens quantified by the VQ-VAE.\nThe primary distinction among these methods lies in their specific handling of different parts of human motion, including body movements, hand gestures, and facial expressions.\nSimilarly, in the realm of music-to-dance generation, there are also methods in both the continuous domain&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhuang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib79\" title=\"\">2022</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib53\" title=\"\">2023</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib25\" title=\"\">2024a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib21\" title=\"\">2024</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib64\" title=\"\">2024a</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite> and the discrete domain&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Siyao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib49\" title=\"\">2022</a>)</cite>.\nThe discrete autoregressive model is leveraged after the motion quantization with VQ-VAE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Siyao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib49\" title=\"\">2022</a>)</cite>.\nMore methods harness the diffusion model to directly regress the target dancing motion in the continuous space&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib53\" title=\"\">2023</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib25\" title=\"\">2024a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib21\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite>.\nRecent methods also start to merge autoregressive and diffusion models, producing coherent and music-aligned dance sequences&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib64\" title=\"\">2024a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "2024b",
                    "2023a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent works have begun to seek multimodal solutions, i.e., designing one framework for motion generation from different input signals&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib41\" title=\"\">2024</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib69\" title=\"\">2024c</a>; Zhou &amp; Wang, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib75\" title=\"\">2023</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib76\" title=\"\">2023</a>; Ling et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib36\" title=\"\">2023</a>; Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib30\" title=\"\">2024c</a>)</cite>.\nSome methods in the discrete domain attempt to incorporate quantized condition tokens into the vocabulary of the generation model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib41\" title=\"\">2024</a>; Zhou &amp; Wang, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib75\" title=\"\">2023</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib76\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib73\" title=\"\">2025</a>)</cite>, while some methods in the continuous domain try to integrate the multimodal signals by designing the motion ControlNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ling et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib36\" title=\"\">2023</a>; Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, where the multimodal conditions guide the sampling of a pretrained text-to-motion diffusion model.\nHowever, most previous methods are restricted by the varying motion data of different modalities, limiting multi-modal frameworks primarily to body-only motion generation.\nTo overcome this, MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite> standardizes datasets across modalities into a unified whole-body format that includes body, hands, and face.\nIn this work, we follow this unified representation to build a whole-body multi-modal motion framework, taking advantage of continuous masked auto-regression.</p>\n\n",
                "matched_terms": [
                    "texttomotion",
                    "motioncraft",
                    "unified",
                    "representation",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To feed the human motion into the transformer, we start by encoding the original motion into motion tokens.\nGiven a motion sequence <math alttext=\"\\{\\mathbf{M}_{t}\\}_{t=1}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>&#119820;</mi><mi>t</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\{\\mathbf{M}_{t}\\}_{t=1}^{T}</annotation></semantics></math>, the objective of an autoencoder is to extract a latent code <math alttext=\"z_{\\text{AE}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>z</mi><mtext>AE</mtext></msub><annotation encoding=\"application/x-tex\">z_{\\text{AE}}</annotation></semantics></math> that optimally captures the essence of the original motion.\nDifferent from most previous motion generation methods with autoregressive transformers, we use a continuous autoencoder rather than the VQ-VAE to do the encoding, which avoids the precision loss associated with the quantization approximation.\nIn the encoder, we stack the 1D convolution networks with ReLU activation to do the feature processing. Following this, two down-sampling residual blocks are applied to reduce the motion feature size to one-fourth of its original dimensions.\nIn the decoder, the same structure in the reversed order is utilized to up-sample the motion feature back to the original size, producing <math alttext=\"\\{\\hat{\\mathbf{M}}_{t}\\}_{t=1}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mover accent=\"true\"><mi>&#119820;</mi><mo>^</mo></mover><mi>t</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\{\\hat{\\mathbf{M}}_{t}\\}_{t=1}^{T}</annotation></semantics></math>. Therefore, the loss for training the autoencoder is defined as</p>\n\n",
                "matched_terms": [
                    "precision",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the feature extraction, we utilize AdaLN to seamlessly integrate the text-derived control signals into the masked autoregressive transformer. AdaLN offers a dynamic approach to normalization, allowing the modulation of its parameters in response to the specific text input, thereby facilitating subsequent multimodal condition injection. By employing this method, we enhance the model&#8217;s ability to incorporate the guiding signals from the text and other signals into the motion generation process, ensuring that the transformer&#8217;s output features are better aligned with the intended motion generation goals. The features outputted by the transformer embody a strong directive capacity for motion generation. This enables the model not only to faithfully interpret the semantic content of the input text but also to produce motion sequences that are coherent with and reflective of the textual intent. The enriched output features contribute to achieving smoother transitions and logically consistent motion sequences in complex generation scenarios.</p>\n\n",
                "matched_terms": [
                    "method",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our objective is to construct a unified model that can simultaneously perform text-to-motion, speech-to-gesture, and music-to-dance tasks. Therefore, we aim to ensure that during the fine-tuning process across different datasets, the model does not suffer from instability that could lead to catastrophic failures. RMSNorm&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang &amp; Sennrich, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib63\" title=\"\">2019</a>)</cite> is particularly advantageous in scenarios with features exhibiting a large dynamic range, especially in tasks where the input distributions are highly heterogeneous. This characteristic enables the model to maintain stability when faced with diverse types of inputs or when observing uneven feature distributions. Additionally, RMSNorm has the potential to mitigate the gradient instability that may arise from significant motion variations, such as sudden jumps or rapid turns.</p>\n\n",
                "matched_terms": [
                    "unified",
                    "texttomotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, MLPs exhibit notable limitations when processing heterogeneous data. This incapacity results in suboptimal performance when confronted with diverse signal types, such as speech and music. Due to the relatively small number of parameters in MLPs, they are prone to overfitting on specific datasets (e.g., text-to-motion). This situation can be analogized to a dancer who is adept only in a single dance style; when asked to incorporate other styles, they appear clumsy and ineffective. Consequently, when we attempt to fine-tune MLPs on a different dataset, they are ill-equipped to adapt to the challenges posed by new signals, leading to failures in multimodal generation tasks.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "texttomotion",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation Details. </span>\nOur model is implemented on one NVIDIA V100 GPU using PyTorch. For our method, the autoencoder employs a ResNet-based&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib19\" title=\"\">2016</a>)</cite> three-layer encoder-decoder architecture with a hidden dimension of 512 and an overall downsampling rate of 4. For the generation process, we utilize a four-layer AdaLN-zero transformer encoder as the masked autoregressive transformer, featuring a hidden dimension of 1024 and 16 attention heads. The diffusion model consists of 4 layers of DiT, where each transformer block has a hidden dimension of 1792 and 8 attention heads.\nWe adopt the AdamW optimizer <math alttext=\"(\\beta_{1}=0.9,\\beta_{2}=0.99)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><mo>,</mo><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.99</mn></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\beta_{1}=0.9,\\beta_{2}=0.99)</annotation></semantics></math>. For training the autoencoder on the HumanML subset of Motion-X, we use a batch size of 256 and a maximum sequence length of 64 frames. For the text-to-motion task, the batch size is set to 50 with a maximum sequence length of 196 frames. The learning rate is initialized at 0.0002 with a linear warmup over 2000 steps. The autoencoder is trained for 50 epochs, while the text-to-motion task is trained for 1500 epochs.\nDuring multimodal generation, we first initialize the model with pretrained weights from the text-to-motion autoencoder and fine-tune it on task-specific datasets. Subsequently, we freeze the parameters of the text-to-motion DiT and only fine-tune the masked transformer along with newly incorporated cross-attention layers. The learning rate and training schedule remain consistent with the text-to-motion task.\nFor all three tasks, we employ exponential moving average (EMA) to update model parameters, ensuring training stability. During inference, the classifier-free guidance (CFG) scale is set to 4.5 for text-to-motion, while other tasks use a CFG scale of 6.5.</p>\n\n",
                "matched_terms": [
                    "subset",
                    "method",
                    "texttomotion",
                    "motionx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets and Metrics. </span>\nFor the evaluation, we utilize three datasets: HumanML3D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib15\" title=\"\">2022a</a>)</cite> for text-to-motion, BEAT2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib37\" title=\"\">2024a</a>)</cite> for speech-to-gesture, and FineDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite> for music-to-dance, all following the unified SMPL-X representation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>.\nRegarding the metrics, we use FID, R-Precision, and MM-Dist for text-based motion generation, use FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">H</span></sub>, FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">B</span></sub>, Face L2 loss, Beat Alignment Score, Diversity for speech-based motion generation, and use FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">H</span></sub>, FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">B</span></sub>, Diversity for music-based motion generation, respectively.\nFor the detailed explanation of the datasets and metrics, please refer to the appendix.</p>\n\n",
                "matched_terms": [
                    "texttomotion",
                    "following",
                    "smplx",
                    "unified",
                    "2023a",
                    "representation",
                    "bian",
                    "humanml3d",
                    "fid"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-based motion generation. </span>\nTo assess the speech-driven motion generation, we compare to previous speech-to-gesture methods.\nOur results, summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S3.T2\" title=\"Table 2 &#8227; 3.6 Inference &#8227; 3 Method &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, reveal that our method achieves good quality and diversity in both hand and body motion generation and excels in aligning\nwith the rhythm of first-person speech.\nThis demonstrates the effectiveness of our framework in motion generation when encompassing different modal signals.\nHowever, our method performs worse than single-modal methods.\nAs discussed in &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, this is attributed to the random or average expressions in the Motion-X dataset, which confuses the speech-to-gesture training.</p>\n\n",
                "matched_terms": [
                    "motionx",
                    "method",
                    "results",
                    "dataset",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Causal Attention. </span>\nTo verify the efficacy of the proposed framework, we initially establish a baseline model following the visual MAR setup&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>)</cite>, i.e, using the random pseudo reordering for the batched masked prediction, through the bidirectional attention computing.\nFrom the results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Ablation Study &#8227; 4 Experiments &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we see that the performance of this baseline in the motion area is limited. We attribute this to the difference between human motions and visual images, e.g., the human motion is in a strong temporal sequential structure, in which case a causal attention makes more sense.\nTherefore, changing the baseline to sequential masked prediction with causal attention improves the performance.</p>\n\n",
                "matched_terms": [
                    "results",
                    "2024b",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first pretrain a baseline autoencoder on the text-to-motion task. When fine-tuning it on the speech-to-gesture and music-to-dance tasks, the decoder fails to reconstruct valid motion sequences due to discrepancies in data distribution. However, fine-tuning the autoencoder using the reconstruction objective during the multi-modal training incurs high computational costs. Therefore, we independently fine-tune the baseline AE on each dataset using the reconstruction task before multi-modal generation, and employ the resulting models for downstream tasks.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "texttomotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following previous work&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Bian et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, we utilize SMPL-X formatted motion data with an input dimension of (frame length &#215; 322). The parameter structure is organized as follows: Root orientation (0:3): controls global body rotation; body pose (3:66): Governs major body joint rotations; hand articulation (66:156): controls finger movements; jaw pose (156:159): manages mouth opening/closing; facial expression (159:209): drives emotional expressions; facial shape (209: 309): determines static facial structure; translation (309:312): controls global body position; betas (312: 322): represents static body shape parameters. And the maximum motion length is 196.\nThe model&#8217;s output maintains identical dimensionality (frame length &#215; 322) to ensure full reconstruction capability. This comprehensive parameterization enables simultaneous control of body motion, facial animation, and global positioning within a unified framework.</p>\n\n",
                "matched_terms": [
                    "smplx",
                    "unified",
                    "following",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For text-based motion generation, we evaluate our method on the HumanML3D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib15\" title=\"\">2022a</a>)</cite> dataset, which consists of 14,616 high-quality human motions paired with 44,970 text descriptions.\nThe original body-only SMPL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Loper et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib39\" title=\"\">2023</a>)</cite> format of this dataset is extended to whole-body SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pavlakos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib43\" title=\"\">2019</a>)</cite> format in MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, which we follow in the experiments for evaluation.\nFor speech-based motion generation, we evaluate on the BEAT2 dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib37\" title=\"\">2024a</a>)</cite>, which collects 76 hours of data from 30 speakers, standardized into a mesh representation with paired audio and text lines.\nThe motion of the unified SMPL-X format is also extracted&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite> for multimodal evaluation.\nFor music-based motion generation, the largest dataset FineDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite> is utilized for evaluation. This dataset collects dances of 14.6 hours across 22 genres and provides detailed human motions using the SMPL-H format, which is then converted to the unified SMPL-X format and appended by text descriptions.</p>\n\n",
                "matched_terms": [
                    "motioncraft",
                    "smplx",
                    "unified",
                    "2023a",
                    "representation",
                    "method",
                    "bian",
                    "dataset",
                    "humanml3d"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable full-body, multimodal control over motion generation, we convert all datasets to the SMPL-X format. This involves filling in missing facial expressions in HumanML3D and FineDance using average expression coefficients from the training set, as well as transforming the SMPL-H Rot-6D representation in FineDance into axis-angle format via Gram-Schmidt orthogonalization. This conversion achieves better alignment with SMPL-X parameters and introduces minimal errors compared to the official body-retargeting method, while also offering improved computational efficiency.</p>\n\n",
                "matched_terms": [
                    "smplx",
                    "humanml3d",
                    "representation",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure consistency with MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, we utilize the pretrained motion encoder and text encode, enabling a unified evaluation of the SMPL-X motion representation across different modalities. For datasets that lack corresponding textual annotations&#8212;namely FineDance and BEAT2&#8212;we generate pseudo-captions such as &#8220;A dancer is performing a street dance in the Jazz style to the rhythm of the wildfire&#8221; and &#8220;A person is giving a speech, and the content is &#8230;&#8221;, respectively, to support cross-modal learning.</p>\n\n",
                "matched_terms": [
                    "motioncraft",
                    "smplx",
                    "unified",
                    "representation",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the same strategy as MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, we train two variants of our model: OmniMotion-Base and OmniMotion-Mix. OmniMotion-Base is a text-to-motion model pretrained solely on HumanML3D, while OmniMotion-Mix is trained on a combined dataset comprising HumanML3D, BEAT2, and FineDance to enable multimodal motion generation. Quantitative results on the text-to-motion task are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#A1.T7\" title=\"Table 7 &#8227; Text-based Motion Generation &#8227; A.3 Metrics &#8227; Appendix A Appendix &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "texttomotion",
                    "following",
                    "motioncraft",
                    "quantitative",
                    "results",
                    "bian",
                    "dataset",
                    "humanml3d"
                ]
            }
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Results of speech-based motion generation on the BEAT2 dataset (Liu et al., 2024a), following the unified SMPL-X representation (Bian et al., 2025).",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Method</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">FID<math alttext=\"{}_{H}\\downarrow\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S3.T2.m1\" intent=\":literal\"><semantics><mmultiscripts><mo stretchy=\"false\">&#8595;</mo><mprescripts/><mi>H</mi><mrow/></mmultiscripts><annotation encoding=\"application/x-tex\">{}_{H}\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">FID<math alttext=\"{}_{B}\\downarrow\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S3.T2.m2\" intent=\":literal\"><semantics><mmultiscripts><mo stretchy=\"false\">&#8595;</mo><mprescripts/><mi>B</mi><mrow/></mmultiscripts><annotation encoding=\"application/x-tex\">{}_{B}\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Face L2 Loss <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Beat Align Score <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Diversity <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Talkshow&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib60\" title=\"\">2023</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"26.713\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m6\" intent=\":literal\"><semantics><mn>26.713</mn><annotation encoding=\"application/x-tex\">26.713</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"74.824\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m7\" intent=\":literal\"><semantics><mn>74.824</mn><annotation encoding=\"application/x-tex\">74.824</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{7.791}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m8\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">7.791</mn><annotation encoding=\"application/x-tex\">\\mathbf{7.791}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"{6.947}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m9\" intent=\":literal\"><semantics><mn>6.947</mn><annotation encoding=\"application/x-tex\">{6.947}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"13.472\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m10\" intent=\":literal\"><semantics><mn>13.472</mn><annotation encoding=\"application/x-tex\">13.472</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">EMAGE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib37\" title=\"\">2024a</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"39.094\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m11\" intent=\":literal\"><semantics><mn>39.094</mn><annotation encoding=\"application/x-tex\">39.094</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"90.762\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m12\" intent=\":literal\"><semantics><mn>90.762</mn><annotation encoding=\"application/x-tex\">90.762</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"7.680\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m13\" intent=\":literal\"><semantics><mn>7.680</mn><annotation encoding=\"application/x-tex\">7.680</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"7.727\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m14\" intent=\":literal\"><semantics><mn>7.727</mn><annotation encoding=\"application/x-tex\">7.727</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"13.065\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m15\" intent=\":literal\"><semantics><mn>13.065</mn><annotation encoding=\"application/x-tex\">13.065</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">MCM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ling et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib36\" title=\"\">2023</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"23.946\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m16\" intent=\":literal\"><semantics><mn>23.946</mn><annotation encoding=\"application/x-tex\">23.946</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"71.241\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m17\" intent=\":literal\"><semantics><mn>71.241</mn><annotation encoding=\"application/x-tex\">71.241</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"16.983\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m18\" intent=\":literal\"><semantics><mn>16.983</mn><annotation encoding=\"application/x-tex\">16.983</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"7.993\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m19\" intent=\":literal\"><semantics><mn>7.993</mn><annotation encoding=\"application/x-tex\">7.993</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"13.167\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m20\" intent=\":literal\"><semantics><mn>13.167</mn><annotation encoding=\"application/x-tex\">13.167</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"18.486\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m21\" intent=\":literal\"><semantics><mn>18.486</mn><annotation encoding=\"application/x-tex\">18.486</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"27.023\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m22\" intent=\":literal\"><semantics><mn>27.023</mn><annotation encoding=\"application/x-tex\">27.023</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"10.097\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m23\" intent=\":literal\"><semantics><mn>10.097</mn><annotation encoding=\"application/x-tex\">10.097</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"8.098\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m24\" intent=\":literal\"><semantics><mn>8.098</mn><annotation encoding=\"application/x-tex\">8.098</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"10.334\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m25\" intent=\":literal\"><semantics><mn>10.334</mn><annotation encoding=\"application/x-tex\">10.334</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Ours</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{17.651}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m26\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">17.651</mn><annotation encoding=\"application/x-tex\">\\mathbf{17.651}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{25.923}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m27\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">25.923</mn><annotation encoding=\"application/x-tex\">\\mathbf{25.923}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"{9.883}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m28\" intent=\":literal\"><semantics><mn>9.883</mn><annotation encoding=\"application/x-tex\">{9.883}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{8.377}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m29\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">8.377</mn><annotation encoding=\"application/x-tex\">\\mathbf{8.377}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{14.703}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m30\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">14.703</mn><annotation encoding=\"application/x-tex\">\\mathbf{14.703}</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "motion",
            "8377mathbf8377",
            "smplx",
            "unified",
            "align",
            "bian",
            "↓downarrow",
            "mcm",
            "2024a",
            "face",
            "generation",
            "diversity",
            "loss",
            "fid↓hhdownarrow",
            "beat2",
            "speechbased",
            "beat",
            "ling",
            "talkshow",
            "following",
            "25923mathbf25923",
            "results",
            "14703mathbf14703",
            "ours",
            "score",
            "fid↓bbdownarrow",
            "↑uparrow",
            "7791mathbf7791",
            "emage",
            "motioncraft",
            "representation",
            "liu",
            "method",
            "dataset",
            "17651mathbf17651"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-based motion generation. </span>\nTo assess the speech-driven motion generation, we compare to previous speech-to-gesture methods.\nOur results, summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S3.T2\" title=\"Table 2 &#8227; 3.6 Inference &#8227; 3 Method &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, reveal that our method achieves good quality and diversity in both hand and body motion generation and excels in aligning\nwith the rhythm of first-person speech.\nThis demonstrates the effectiveness of our framework in motion generation when encompassing different modal signals.\nHowever, our method performs worse than single-modal methods.\nAs discussed in &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, this is attributed to the random or average expressions in the Motion-X dataset, which confuses the speech-to-gesture training.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Whole-body multi-modal human motion generation poses two primary challenges: creating an effective motion generation mechanism and integrating various modalities, such as text, speech, and music, into a cohesive framework.\nUnlike previous methods that usually employ discrete masked modeling or autoregressive modeling, we develop a continuous masked autoregressive motion transformer, where a causal attention is performed considering the sequential nature within the human motion.\nWithin this transformer, we introduce a gated linear attention and an RMSNorm module, which drive the transformer to pay attention to the key actions and suppress the instability caused by either the abnormal movements or the heterogeneous distributions within multi-modalities.\nTo further enhance both the motion generation and the multimodal generalization, we employ the DiT structure to diffuse the conditions from the transformer towards the targets.\nTo fuse different modalities, AdaLN and cross-attention are leveraged to inject the text, speech, and music signals.\nExperimental results demonstrate that our framework outperforms previous methods across all modalities, including text-to-motion, speech-to-gesture, and music-to-dance.\nThe code of our method will be made public.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "method",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Whole-body human motion generation represents an expanding frontier in computer vision, offering significant value across a variety of applications, including film production, gaming, virtual reality, robotics, and so on.\nBroadly speaking, motion generation could be conditioned on various signals, such as text, speech, music, and more.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Historically, approaches to whole-body motion generation usually focus on isolated tasks. Typically, they either address text-to-motion generation, or concentrate on speech-to-gesture translation, or engage in music-to-dance synthesis.\nDespite their successes in single task, their frameworks are exclusively designed for individual tasks and cannot be easily adapted to different tasks.\nIn addition, they tend to overlook the underlying commonalities that exist across different tasks.\nIn contrast, in this work we seek to address these motion generation challenges from various signals within an omni-framework.\nThis brings two advantages: 1) It allows each modality to benefit from the patterns present in other modalities, preventing single-mode solutions from becoming trapped in a local minimum;\n2) It enhances each task with data from other tasks, which is particularly relevant given the limited scale of data available for individual motion tasks.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous studies in motion generation generally proceed in two paths.\nThe first employs the vector quantization (VQ) technique to convert continuous motion to discrete tokens, and then performs autoregressive or masked modeling to predict the tokens&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib71\" title=\"\">2023d</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib65\" title=\"\">a</a>; Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib23\" title=\"\">2023</a>; Zhong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib74\" title=\"\">2023</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib17\" title=\"\">2024</a>)</cite>.\nWhile this path effectively utilizes the strengths of autoregressive and masked modeling, the quantization step inevitably introduces approximation errors, which impose undesirable limits on the quality of the generated motions.\nThe second directly regresses the continuous motions using techniques such as generative adversarial networks (GANs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tulyakov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib54\" title=\"\">2018</a>)</cite>, variational autoencoders (VAEs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib58\" title=\"\">2020</a>; Ahuja &amp; Morency, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib1\" title=\"\">2019</a>; Petrovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib45\" title=\"\">2022</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib15\" title=\"\">2022a</a>)</cite>, or recent diffusion models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib12\" title=\"\">2023</a>; Tevet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib52\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib68\" title=\"\">2024b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib66\" title=\"\">2023b</a>; Ribeiro-Gomes et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib48\" title=\"\">2024</a>)</cite>.\nDespite avoiding the approximation errors, they miss the autoregressive or masked modeling technologies, which have been shown to deliver superior performance in motion generation tasks.\nConsequently, the performance of the motion generated by this path is overall lower than that achieved by the first path.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To both leverage the advantages of autoregressive and masked modeling, and the benefits of the continuous motion representation, in this work we combine them together to propose a continuous masked autoregressive motion generation framework.\nWe apply a random masking on the sequential motion tokens, and employ a transformer to autoregressively predict the masked tokens.\nUnlike the visual MAR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>)</cite>, we sequentially predict the masked tokens with causal attention rather than performing random reordering, considering the sequential nature within human motion.\nTo enhance the MAR modeling in motion space, we introduce a gated linear mechanism and an RMSNorm module.\nThe gated linear mechanism serves as an adaptive feature selector, driving the transformer to not only pay more attention to the key actions, like gesture switching and large movement, but also disregard less relevant frames and suppress redundant actions, like stationary motions.\nThe RMSNorm is particularly advantageous in scenarios with features exhibiting a large dynamic range, e.g., our unified framework for multi-modalities, where the input distributions are highly heterogeneous.\nIn addition, RMSNorm helps relieve the gradient instability caused by the abnormally large motions, such as sudden jumping or turning back.\nAfter the masked autoregressive transformer, the calculated attention of the masked tokens is fed into a series of DiT blocks to diffuse towards the target tokens, which are decoded to the generated motions.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "unified",
                    "representation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the text-based motion generation, we further extend our framework to multimodal conditions.\nBuilding upon a similar structure, the multimodal signals are fused by AdaLN&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib18\" title=\"\">2022c</a>)</cite> and cross-attention modules.\nExtensive experiments across different datasets demonstrate our framework can work well with different modalities, including text, speech, and music, and outperform previous methods in whole-body text-to-motion, speech-to-gesture, and music-to-dance tasks.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We design an omni motion framework for whole-body human motion generation, where one framework encompasses multiple modalities.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We integrate the multimodal signals via AdaLN and cross-attention, obtaining superior performance than previous methods in text-based, speech-based, and music-based motion generation.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "speechbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous research on text-based motion generation can be generally categorized into two mainstream paths: continuous regression and discrete classification.\nIn the continuous regression domain, numerous strategies have leveraged the variational autoencoder (VAE) framework, integrating latent embeddings of encoded text with those of encoded poses, which are then decoded into motion predictions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib58\" title=\"\">2020</a>; Ahuja &amp; Morency, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib1\" title=\"\">2019</a>; Athanasiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib4\" title=\"\">2022</a>; Petrovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib45\" title=\"\">2022</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib15\" title=\"\">2022a</a>)</cite>.\nOther methods have investigated the potential of recurrent networks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib33\" title=\"\">2018</a>; Plappert et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib47\" title=\"\">2018</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib70\" title=\"\">2020</a>)</cite>, generative adversarial networks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib7\" title=\"\">2018</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib57\" title=\"\">2020</a>; Tulyakov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib54\" title=\"\">2018</a>)</cite>, or transformer networks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tevet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib51\" title=\"\">2022</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib35\" title=\"\">2023b</a>; Bhattacharya et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib5\" title=\"\">2021</a>; Petrovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib44\" title=\"\">2021</a>)</cite> to enhance the motion regression quality.\nBuilding on the success of diffusion models, recent approaches have begun to integrate the diffusion process into motion diffusion&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib22\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib12\" title=\"\">2023</a>; Tevet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib52\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib68\" title=\"\">2024b</a>; Dabral et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib13\" title=\"\">2023</a>; Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib40\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib66\" title=\"\">2023b</a>; Ribeiro-Gomes et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib48\" title=\"\">2024</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib62\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib56\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib67\" title=\"\">2023c</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib72\" title=\"\">2024d</a>; Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, yielding impressive results.\nIn the discrete classification domain, the input motion undergoes initial encoding via a VQ-VAE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Van Den&#160;Oord et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib55\" title=\"\">2017</a>)</cite>, producing motion tokens for subsequent prediction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib74\" title=\"\">2023</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib32\" title=\"\">2024e</a>)</cite>.\nDrawing inspiration from advancements in natural language processing, some methods utilize autoregressive modeling to predict tokens sequentially&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib16\" title=\"\">2022b</a>; Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib40\" title=\"\">2023</a>; Zou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib80\" title=\"\">2024</a>)</cite>.\nOthers employ generative masked modeling strategies, with tokens randomly masked during training for the model to predict&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib17\" title=\"\">2024</a>; Pinyoanuntapong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib46\" title=\"\">2024</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib61\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib28\" title=\"\">2023b</a>)</cite>.\nMore recently, large language models (LLMs) have been harnessed to help the prediction process, considering their large-scale pretraining&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib71\" title=\"\">2023d</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib77\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib31\" title=\"\">2024d</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib29\" title=\"\">2023c</a>)</cite>.\nIn this work, we seek to integrate the most effective elements from these two paths: the continuous diffusion and the masked autoregressive modeling.\nA previous attempt in this direction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib42\" title=\"\">2024</a>)</cite> directly transfers the MAR in image generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>)</cite> into motion generation without considering the difference between image and motion spaces, especially the temporal correlation. Also, its framework is only designed for body-only motion generation. Differently, we propose a new MAR mechanism that is especially designed for whole-body motion generation.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "results",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to text, there are many other signals that various human motions are conditioned on, such as speech and music.\nIn the realm of speech-to-gesture generation, both continuous regression and discrete classification paths have been explored.\nIn the continuous domain, methods employ deep generative models like GANs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ahuja et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib2\" title=\"\">2022</a>)</cite>, normalizing flows&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib50\" title=\"\">2024</a>)</cite>, and diffusion models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Alexanderson et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib3\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib10\" title=\"\">2024a</a>; He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib20\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib59\" title=\"\">2023</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib78\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib11\" title=\"\">2024b</a>)</cite> to learn complex motion distributions in the speech data.\nIn the discrete domain, methods leverage either the autoregressive modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib60\" title=\"\">2023</a>)</cite> or the masked modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib37\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib38\" title=\"\">b</a>)</cite> to predict the discrete tokens quantified by the VQ-VAE.\nThe primary distinction among these methods lies in their specific handling of different parts of human motion, including body movements, hand gestures, and facial expressions.\nSimilarly, in the realm of music-to-dance generation, there are also methods in both the continuous domain&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhuang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib79\" title=\"\">2022</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib53\" title=\"\">2023</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib25\" title=\"\">2024a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib21\" title=\"\">2024</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib64\" title=\"\">2024a</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite> and the discrete domain&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Siyao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib49\" title=\"\">2022</a>)</cite>.\nThe discrete autoregressive model is leveraged after the motion quantization with VQ-VAE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Siyao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib49\" title=\"\">2022</a>)</cite>.\nMore methods harness the diffusion model to directly regress the target dancing motion in the continuous space&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib53\" title=\"\">2023</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib25\" title=\"\">2024a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib21\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite>.\nRecent methods also start to merge autoregressive and diffusion models, producing coherent and music-aligned dance sequences&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib64\" title=\"\">2024a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "2024a",
                    "generation",
                    "liu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent works have begun to seek multimodal solutions, i.e., designing one framework for motion generation from different input signals&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib41\" title=\"\">2024</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib69\" title=\"\">2024c</a>; Zhou &amp; Wang, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib75\" title=\"\">2023</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib76\" title=\"\">2023</a>; Ling et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib36\" title=\"\">2023</a>; Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib30\" title=\"\">2024c</a>)</cite>.\nSome methods in the discrete domain attempt to incorporate quantized condition tokens into the vocabulary of the generation model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib41\" title=\"\">2024</a>; Zhou &amp; Wang, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib75\" title=\"\">2023</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib76\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib73\" title=\"\">2025</a>)</cite>, while some methods in the continuous domain try to integrate the multimodal signals by designing the motion ControlNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ling et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib36\" title=\"\">2023</a>; Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, where the multimodal conditions guide the sampling of a pretrained text-to-motion diffusion model.\nHowever, most previous methods are restricted by the varying motion data of different modalities, limiting multi-modal frameworks primarily to body-only motion generation.\nTo overcome this, MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite> standardizes datasets across modalities into a unified whole-body format that includes body, hands, and face.\nIn this work, we follow this unified representation to build a whole-body multi-modal motion framework, taking advantage of continuous masked auto-regression.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "ling",
                    "motioncraft",
                    "unified",
                    "representation",
                    "face",
                    "generation",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The overview of our framework is illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S2.F2\" title=\"Figure 2 &#8227; Multimodal Motion Generation. &#8227; 2 Related Work &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, which is divided into three main stages:\nIn the first stage, we encode the input motion with an autoencoder, which generates continuous motion tokens.\nIn the second stage, we focus on the text-based motion generation, utilizing our motion masked autoregressive framework to model the motion generation process.\nIn this stage, an autoregressive transformer is employed to predict the masked tokens, within which a gated linear mechanism is designed, and an RMSNorm module is employed.\nThe text information is integrated into the transformer via AdaLN after encoding.\nAfter the transformer, the generated embedding is fed into the DiT modules as the condition to diffuse towards the target token.\nIn the third stage, we extend the model learned in the previous stage to the multi-modal structure.\nThis involves merging the text embedding with multimodal signal embeddings&#8212;specifically speech or music&#8212;prior to their AdaLN input.\nFurthermore, we inject the multimodal embedding through a cross-attention module into the masked transformer.\nIn the multimodal learning stage, the DiT module is kept in the same structure and frozen.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To feed the human motion into the transformer, we start by encoding the original motion into motion tokens.\nGiven a motion sequence <math alttext=\"\\{\\mathbf{M}_{t}\\}_{t=1}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>&#119820;</mi><mi>t</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\{\\mathbf{M}_{t}\\}_{t=1}^{T}</annotation></semantics></math>, the objective of an autoencoder is to extract a latent code <math alttext=\"z_{\\text{AE}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>z</mi><mtext>AE</mtext></msub><annotation encoding=\"application/x-tex\">z_{\\text{AE}}</annotation></semantics></math> that optimally captures the essence of the original motion.\nDifferent from most previous motion generation methods with autoregressive transformers, we use a continuous autoencoder rather than the VQ-VAE to do the encoding, which avoids the precision loss associated with the quantization approximation.\nIn the encoder, we stack the 1D convolution networks with ReLU activation to do the feature processing. Following this, two down-sampling residual blocks are applied to reduce the motion feature size to one-fourth of its original dimensions.\nIn the decoder, the same structure in the reversed order is utilized to up-sample the motion feature back to the original size, producing <math alttext=\"\\{\\hat{\\mathbf{M}}_{t}\\}_{t=1}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mover accent=\"true\"><mi>&#119820;</mi><mo>^</mo></mover><mi>t</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\{\\hat{\\mathbf{M}}_{t}\\}_{t=1}^{T}</annotation></semantics></math>. Therefore, the loss for training the autoencoder is defined as</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "loss",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the continuous motion tokens, we design a masked autoregressive transformer to model the motion generation, effectively capturing the temporal relationship between different tokens and producing the rich contextual condition <math alttext=\"\\mathbf{z}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><msup><mi>&#119859;</mi><mi>i</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{z}^{i}</annotation></semantics></math> for the subsequent diffusion process.\nWe first randomly mask the motion tokens following language models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib14\" title=\"\">2018</a>)</cite>, obtaining some masked tokens <math alttext=\"\\{\\tilde{\\mathbf{x}}^{i}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msup><mover accent=\"true\"><mi>&#119857;</mi><mo>~</mo></mover><mi>i</mi></msup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\tilde{\\mathbf{x}}^{i}\\}</annotation></semantics></math>.\nThe temporal masking strategies adopt the same mask ratio schedule following&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib8\" title=\"\">2022</a>)</cite>, and are computed as</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the input text prompt <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>, we first employ the LaMP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib32\" title=\"\">2024e</a>)</cite> text transformer to extract textual features, leveraging its advanced capabilities to encode the linguistic nuances and semantic structure of the input prompt. This creates a high-dimensional feature representation that is crucial for guiding motion generation process.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "representation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the feature extraction, we utilize AdaLN to seamlessly integrate the text-derived control signals into the masked autoregressive transformer. AdaLN offers a dynamic approach to normalization, allowing the modulation of its parameters in response to the specific text input, thereby facilitating subsequent multimodal condition injection. By employing this method, we enhance the model&#8217;s ability to incorporate the guiding signals from the text and other signals into the motion generation process, ensuring that the transformer&#8217;s output features are better aligned with the intended motion generation goals. The features outputted by the transformer embody a strong directive capacity for motion generation. This enables the model not only to faithfully interpret the semantic content of the input text but also to produce motion sequences that are coherent with and reflective of the textual intent. The enriched output features contribute to achieving smoother transitions and logically consistent motion sequences in complex generation scenarios.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "method",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our objective is to construct a unified model that can simultaneously perform text-to-motion, speech-to-gesture, and music-to-dance tasks. Therefore, we aim to ensure that during the fine-tuning process across different datasets, the model does not suffer from instability that could lead to catastrophic failures. RMSNorm&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang &amp; Sennrich, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib63\" title=\"\">2019</a>)</cite> is particularly advantageous in scenarios with features exhibiting a large dynamic range, especially in tasks where the input distributions are highly heterogeneous. This characteristic enables the model to maintain stability when faced with diverse types of inputs or when observing uneven feature distributions. Additionally, RMSNorm has the potential to mitigate the gradient instability that may arise from significant motion variations, such as sudden jumps or rapid turns.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "unified"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to previous works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>; Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib42\" title=\"\">2024</a>)</cite>, we adopt Diffusion Transformer (DiT) as our diffusion model. While the use of DiT may incur additional time costs during training and inference (not much due to the compact structure of motion data), it significantly enhances the quality of generated outputs. Compared to MLPs, DiT provides greater convenience in injecting conditional control signals. During the training process of multimodal generation, we freeze the diffusion model and only fine-tune the masked transformer. The structural characteristics of DiT facilitate this approach, enabling it to better handle various types of conditional signals.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, MLPs exhibit notable limitations when processing heterogeneous data. This incapacity results in suboptimal performance when confronted with diverse signal types, such as speech and music. Due to the relatively small number of parameters in MLPs, they are prone to overfitting on specific datasets (e.g., text-to-motion). This situation can be analogized to a dancer who is adept only in a single dance style; when asked to incorporate other styles, they appear clumsy and ineffective. Consequently, when we attempt to fine-tune MLPs on a different dataset, they are ill-equipped to adapt to the challenges posed by new signals, leading to failures in multimodal generation tasks.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "generation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first pre-train the model on text-motion paired data in a text-to-motion generation setting. Owing to its strong semantic expressiveness and cross-modal alignment properties, we adopt text as a shared conditional signal across diverse unimodal datasets, enabling the model to learn sequence-level generation capabilities between text and motion, as well as a coarse-grained textual guidance mechanism for generative control.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hypothesize that the contextual features output by the masked transformer provide a more expressive control signal compared to raw text embeddings. Accordingly, within the DiT architecture, we inject the transformer&#8217;s output features by summing them with the time embeddings, thereby guiding the motion generation process as:</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation Details. </span>\nOur model is implemented on one NVIDIA V100 GPU using PyTorch. For our method, the autoencoder employs a ResNet-based&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib19\" title=\"\">2016</a>)</cite> three-layer encoder-decoder architecture with a hidden dimension of 512 and an overall downsampling rate of 4. For the generation process, we utilize a four-layer AdaLN-zero transformer encoder as the masked autoregressive transformer, featuring a hidden dimension of 1024 and 16 attention heads. The diffusion model consists of 4 layers of DiT, where each transformer block has a hidden dimension of 1792 and 8 attention heads.\nWe adopt the AdamW optimizer <math alttext=\"(\\beta_{1}=0.9,\\beta_{2}=0.99)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><mo>,</mo><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.99</mn></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\beta_{1}=0.9,\\beta_{2}=0.99)</annotation></semantics></math>. For training the autoencoder on the HumanML subset of Motion-X, we use a batch size of 256 and a maximum sequence length of 64 frames. For the text-to-motion task, the batch size is set to 50 with a maximum sequence length of 196 frames. The learning rate is initialized at 0.0002 with a linear warmup over 2000 steps. The autoencoder is trained for 50 epochs, while the text-to-motion task is trained for 1500 epochs.\nDuring multimodal generation, we first initialize the model with pretrained weights from the text-to-motion autoencoder and fine-tune it on task-specific datasets. Subsequently, we freeze the parameters of the text-to-motion DiT and only fine-tune the masked transformer along with newly incorporated cross-attention layers. The learning rate and training schedule remain consistent with the text-to-motion task.\nFor all three tasks, we employ exponential moving average (EMA) to update model parameters, ensuring training stability. During inference, the classifier-free guidance (CFG) scale is set to 4.5 for text-to-motion, while other tasks use a CFG scale of 6.5.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets and Metrics. </span>\nFor the evaluation, we utilize three datasets: HumanML3D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib15\" title=\"\">2022a</a>)</cite> for text-to-motion, BEAT2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib37\" title=\"\">2024a</a>)</cite> for speech-to-gesture, and FineDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite> for music-to-dance, all following the unified SMPL-X representation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>.\nRegarding the metrics, we use FID, R-Precision, and MM-Dist for text-based motion generation, use FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">H</span></sub>, FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">B</span></sub>, Face L2 loss, Beat Alignment Score, Diversity for speech-based motion generation, and use FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">H</span></sub>, FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">B</span></sub>, Diversity for music-based motion generation, respectively.\nFor the detailed explanation of the datasets and metrics, please refer to the appendix.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "beat",
                    "score",
                    "following",
                    "2024a",
                    "smplx",
                    "unified",
                    "representation",
                    "liu",
                    "face",
                    "generation",
                    "diversity",
                    "loss",
                    "bian",
                    "beat2",
                    "speechbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-based motion generation. </span>\nWe conduct an evaluation of our model against prior text-to-motion approaches, including both discrete-domain and continuous-domain methods.\nAs summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S3.T1\" title=\"Table 1 &#8227; 3.5 Text-to-motion Pretraining and Multimodal Control Adaptation &#8227; 3 Method &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our method clearly surpasses prior techniques on the HumanML subset of Motion-X dataset. Remarkably, our model achieves improvements of 19.3%, 13.5%, and 11.7% in R-Precision for Top-1, 2, 3, respectively. Additionally, we enhance the FID score by 75.2% on this dataset, underscoring the exceptional fidelity of our generated motions. The qualitative results in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S3.F4\" title=\"Figure 4 &#8227; 3.5 Text-to-motion Pretraining and Multimodal Control Adaptation &#8227; 3 Method &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> further support these findings, showing that our approach yields whole-body motions that align closely with the input text.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "score",
                    "align",
                    "generation",
                    "results",
                    "method",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Music-based motion generation. </span>\nWe further evaluate our framework on the music-to-dance task. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S4.T3\" title=\"Table 3 &#8227; 4.1 Experiment Settings &#8227; 4 Experiments &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our method achieves slightly improved performance over previous approaches, particularly in generating hand motions and body movements.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Causal Attention. </span>\nTo verify the efficacy of the proposed framework, we initially establish a baseline model following the visual MAR setup&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>)</cite>, i.e, using the random pseudo reordering for the batched masked prediction, through the bidirectional attention computing.\nFrom the results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Ablation Study &#8227; 4 Experiments &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we see that the performance of this baseline in the motion area is limited. We attribute this to the difference between human motions and visual images, e.g., the human motion is in a strong temporal sequential structure, in which case a causal attention makes more sense.\nTherefore, changing the baseline to sequential masked prediction with causal attention improves the performance.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "results",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DiT. </span>\nIn order to evaluate how the DiTs contribute to the motion quality, we further replace the MLPs in the baseline model with our DiTs.\nAs shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Ablation Study &#8227; 4 Experiments &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the model generates superior motions with DiTs compared to MLPs, especially in the context of multimodal motion generation.\nThis reveals the superior potential of DiTs in generating motion with complex multimodal contexts.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross Attention. </span>\nIn the baseline model, the multimodal signals are injected with only the AdaLN structure.\nWe then add the cross attention module and observe a significant improvement in multimodal motion generation, as depicted in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Ablation Study &#8227; 4 Experiments &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper proposes a new omni motion framework for multimodal whole-body human motion generation.\nWithin this one framework, text, speech, and music signals are all encompassed through AdaLN and cross-attention.\nThe motion generation process is modeled by a continuous masked autoregressive transformer with causal attention, as well as a DiT structure.\nExtensive experiments have been conducted to verify the efficacy of the proposed framework in different-modality tasks.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations.</span>\nDue to the restricted dataset, the naturalness and generalizability of the motion generation model are still limited, especially in speech and music-driven motion generation.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "dataset",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our speech and music encoders are designed to extract temporally aligned, high-level features from raw audio signals for effective speech-to-gesture and music-to-dance generation. The architecture builds upon a multi-layer 1D convolutional network with strided convolutions and leaky ReLU activations. Each convolutional block consists of a series of a block unit that progressively downsample the input waveform while increasing feature dimensionality. The input audio sequence is first processed through multiple stages of temporal aggregation and non-linear transformation, resulting in a sequence of compact and expressive latent representations, whereas the input music typically retains sufficient temporal structure and spectral richness in its raw form for effective motion synthesis. These latent codes capture prosodic, rhythmic, and semantic-like patterns in speech and music, which are then projected into the condition latent space of dimensionality. The final encoder output is transposed to align with the temporal structure expected by the diffusion model, enabling fine-grained cross-modal interaction between speech and motion sequences during generation.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "align"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first pretrain a baseline autoencoder on the text-to-motion task. When fine-tuning it on the speech-to-gesture and music-to-dance tasks, the decoder fails to reconstruct valid motion sequences due to discrepancies in data distribution. However, fine-tuning the autoencoder using the reconstruction objective during the multi-modal training incurs high computational costs. Therefore, we independently fine-tune the baseline AE on each dataset using the reconstruction task before multi-modal generation, and employ the resulting models for downstream tasks.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "dataset",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following previous work&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Bian et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, we utilize SMPL-X formatted motion data with an input dimension of (frame length &#215; 322). The parameter structure is organized as follows: Root orientation (0:3): controls global body rotation; body pose (3:66): Governs major body joint rotations; hand articulation (66:156): controls finger movements; jaw pose (156:159): manages mouth opening/closing; facial expression (159:209): drives emotional expressions; facial shape (209: 309): determines static facial structure; translation (309:312): controls global body position; betas (312: 322): represents static body shape parameters. And the maximum motion length is 196.\nThe model&#8217;s output maintains identical dimensionality (frame length &#215; 322) to ensure full reconstruction capability. This comprehensive parameterization enables simultaneous control of body motion, facial animation, and global positioning within a unified framework.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "following",
                    "smplx",
                    "unified",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For text-based motion generation, we evaluate our method on the HumanML3D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib15\" title=\"\">2022a</a>)</cite> dataset, which consists of 14,616 high-quality human motions paired with 44,970 text descriptions.\nThe original body-only SMPL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Loper et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib39\" title=\"\">2023</a>)</cite> format of this dataset is extended to whole-body SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pavlakos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib43\" title=\"\">2019</a>)</cite> format in MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, which we follow in the experiments for evaluation.\nFor speech-based motion generation, we evaluate on the BEAT2 dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib37\" title=\"\">2024a</a>)</cite>, which collects 76 hours of data from 30 speakers, standardized into a mesh representation with paired audio and text lines.\nThe motion of the unified SMPL-X format is also extracted&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite> for multimodal evaluation.\nFor music-based motion generation, the largest dataset FineDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite> is utilized for evaluation. This dataset collects dances of 14.6 hours across 22 genres and provides detailed human motions using the SMPL-H format, which is then converted to the unified SMPL-X format and appended by text descriptions.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "motioncraft",
                    "2024a",
                    "smplx",
                    "unified",
                    "representation",
                    "liu",
                    "generation",
                    "method",
                    "bian",
                    "dataset",
                    "beat2",
                    "speechbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable full-body, multimodal control over motion generation, we convert all datasets to the SMPL-X format. This involves filling in missing facial expressions in HumanML3D and FineDance using average expression coefficients from the training set, as well as transforming the SMPL-H Rot-6D representation in FineDance into axis-angle format via Gram-Schmidt orthogonalization. This conversion achieves better alignment with SMPL-X parameters and introduces minimal errors compared to the official body-retargeting method, while also offering improved computational efficiency.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "smplx",
                    "representation",
                    "generation",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure consistency with MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, we utilize the pretrained motion encoder and text encode, enabling a unified evaluation of the SMPL-X motion representation across different modalities. For datasets that lack corresponding textual annotations&#8212;namely FineDance and BEAT2&#8212;we generate pseudo-captions such as &#8220;A dancer is performing a street dance in the Jazz style to the rhythm of the wildfire&#8221; and &#8220;A person is giving a speech, and the content is &#8230;&#8221;, respectively, to support cross-modal learning.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "motioncraft",
                    "smplx",
                    "unified",
                    "representation",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the quality of the motions generated based on texts compared to the true data, we utilize the Frechet Inception Distance (FID) to evaluate the distribution differences between the generated motions and the ground truth.\nAdditionally, R-Precision is employed to determine how frequently the most relevant motions, identified as top-k closest matches, align with their respective captions within a batch of 32 samples. Lastly, Multi-Modal Distance (MM Dist) is employed to gauge the average Euclidean distance between motion representations and their corresponding textual features.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "align"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluating the quality and diversity of the motions generated based on speech, we employ FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">H</span></sub>, FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">B</span></sub>, and Diversity metrics. FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">H</span></sub> measures the difference between hand motion distribution and the true gesture distribution, whereas FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">B</span></sub> assesses the divergence between the whole-body motion distributions. The Beat Alignment Score&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib26\" title=\"\">2021</a>)</cite> is used to measure the synchronization between motions and speech beats. To quantify the difference between generated expressions and actual expressions, we use the L2 Loss.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "beat",
                    "score",
                    "diversity",
                    "loss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mirroring the approach used for speech-driven gesture generation, we apply FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">H</span></sub>, FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">B</span></sub>, and Diversity metrics to evaluate the quality and diversity of music-induced hand and whole-body movements. This approach ensures that the generated motions exhibit both high fidelity and variation.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "diversity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide a more comprehensive evaluation, we conduct additional comparisons on the original HumanML3D benchmark using the body-only H3D format, which contains redundant motion information.\nHere we mainly compare with the methods without VQ-VAE.\nAs shown in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#A1.T5\" title=\"Table 5 &#8227; Text-based Motion Generation &#8227; A.3 Metrics &#8227; Appendix A Appendix &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, OmniMotion consistently outperforms these baselines in terms of text-motion alignment, motion quality, and diversity, demonstrating its superior generalization capability across different motion representations.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "diversity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the same strategy as MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, we train two variants of our model: OmniMotion-Base and OmniMotion-Mix. OmniMotion-Base is a text-to-motion model pretrained solely on HumanML3D, while OmniMotion-Mix is trained on a combined dataset comprising HumanML3D, BEAT2, and FineDance to enable multimodal motion generation. Quantitative results on the text-to-motion task are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#A1.T7\" title=\"Table 7 &#8227; Text-based Motion Generation &#8227; A.3 Metrics &#8227; Appendix A Appendix &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "following",
                    "motioncraft",
                    "generation",
                    "results",
                    "bian",
                    "dataset",
                    "beat2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We display more visual results of speech-driven and music-driven motion generation in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#A1.F5\" title=\"Figure 5 &#8227; Text-based Motion Generation &#8227; A.3 Metrics &#8227; Appendix A Appendix &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#A1.F6\" title=\"Figure 6 &#8227; Text-based Motion Generation &#8227; A.3 Metrics &#8227; Appendix A Appendix &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, respectively.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "results"
                ]
            }
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Results of music-based motion generation on the FineDance  (Li et al., 2023a), following the unified SMPL-X representation (Bian et al., 2025).",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Method</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">FID<math alttext=\"{}_{H}\\downarrow\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.T3.m1\" intent=\":literal\"><semantics><mmultiscripts><mo stretchy=\"false\">&#8595;</mo><mprescripts/><mi>H</mi><mrow/></mmultiscripts><annotation encoding=\"application/x-tex\">{}_{H}\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">FID<math alttext=\"{}_{B}\\downarrow\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.T3.m2\" intent=\":literal\"><semantics><mmultiscripts><mo stretchy=\"false\">&#8595;</mo><mprescripts/><mi>B</mi><mrow/></mmultiscripts><annotation encoding=\"application/x-tex\">{}_{B}\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Div <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Edge&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib53\" title=\"\">2023</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"93.430\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m4\" intent=\":literal\"><semantics><mn>93.430</mn><annotation encoding=\"application/x-tex\">93.430</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"108.507\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m5\" intent=\":literal\"><semantics><mn>108.507</mn><annotation encoding=\"application/x-tex\">108.507</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"13.471\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m6\" intent=\":literal\"><semantics><mn>13.471</mn><annotation encoding=\"application/x-tex\">13.471</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Finedance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"10.747\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m7\" intent=\":literal\"><semantics><mn>10.747</mn><annotation encoding=\"application/x-tex\">10.747</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"72.229\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m8\" intent=\":literal\"><semantics><mn>72.229</mn><annotation encoding=\"application/x-tex\">72.229</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"13.813\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m9\" intent=\":literal\"><semantics><mn>13.813</mn><annotation encoding=\"application/x-tex\">13.813</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">MCM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ling et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib36\" title=\"\">2023</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"4.717\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m10\" intent=\":literal\"><semantics><mn>4.717</mn><annotation encoding=\"application/x-tex\">4.717</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"78.577\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m11\" intent=\":literal\"><semantics><mn>78.577</mn><annotation encoding=\"application/x-tex\">78.577</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math alttext=\"14.890\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.m12\" intent=\":literal\"><semantics><mn>14.890</mn><annotation encoding=\"application/x-tex\">14.890</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">3.858</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">76.248</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">16.667</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Ours</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">3.632</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">71.930</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">15.871</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "motion",
            "smplx",
            "unified",
            "finedance",
            "bian",
            "mcm",
            "generation",
            "fid↓hhdownarrow",
            "ling",
            "following",
            "2023a",
            "musicbased",
            "results",
            "tseng",
            "div",
            "ours",
            "↑uparrow",
            "fid↓bbdownarrow",
            "edge",
            "motioncraft",
            "representation",
            "method"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Music-based motion generation. </span>\nWe further evaluate our framework on the music-to-dance task. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S4.T3\" title=\"Table 3 &#8227; 4.1 Experiment Settings &#8227; 4 Experiments &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our method achieves slightly improved performance over previous approaches, particularly in generating hand motions and body movements.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Whole-body multi-modal human motion generation poses two primary challenges: creating an effective motion generation mechanism and integrating various modalities, such as text, speech, and music, into a cohesive framework.\nUnlike previous methods that usually employ discrete masked modeling or autoregressive modeling, we develop a continuous masked autoregressive motion transformer, where a causal attention is performed considering the sequential nature within the human motion.\nWithin this transformer, we introduce a gated linear attention and an RMSNorm module, which drive the transformer to pay attention to the key actions and suppress the instability caused by either the abnormal movements or the heterogeneous distributions within multi-modalities.\nTo further enhance both the motion generation and the multimodal generalization, we employ the DiT structure to diffuse the conditions from the transformer towards the targets.\nTo fuse different modalities, AdaLN and cross-attention are leveraged to inject the text, speech, and music signals.\nExperimental results demonstrate that our framework outperforms previous methods across all modalities, including text-to-motion, speech-to-gesture, and music-to-dance.\nThe code of our method will be made public.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "method",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Whole-body human motion generation represents an expanding frontier in computer vision, offering significant value across a variety of applications, including film production, gaming, virtual reality, robotics, and so on.\nBroadly speaking, motion generation could be conditioned on various signals, such as text, speech, music, and more.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Historically, approaches to whole-body motion generation usually focus on isolated tasks. Typically, they either address text-to-motion generation, or concentrate on speech-to-gesture translation, or engage in music-to-dance synthesis.\nDespite their successes in single task, their frameworks are exclusively designed for individual tasks and cannot be easily adapted to different tasks.\nIn addition, they tend to overlook the underlying commonalities that exist across different tasks.\nIn contrast, in this work we seek to address these motion generation challenges from various signals within an omni-framework.\nThis brings two advantages: 1) It allows each modality to benefit from the patterns present in other modalities, preventing single-mode solutions from becoming trapped in a local minimum;\n2) It enhances each task with data from other tasks, which is particularly relevant given the limited scale of data available for individual motion tasks.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous studies in motion generation generally proceed in two paths.\nThe first employs the vector quantization (VQ) technique to convert continuous motion to discrete tokens, and then performs autoregressive or masked modeling to predict the tokens&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib71\" title=\"\">2023d</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib65\" title=\"\">a</a>; Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib23\" title=\"\">2023</a>; Zhong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib74\" title=\"\">2023</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib17\" title=\"\">2024</a>)</cite>.\nWhile this path effectively utilizes the strengths of autoregressive and masked modeling, the quantization step inevitably introduces approximation errors, which impose undesirable limits on the quality of the generated motions.\nThe second directly regresses the continuous motions using techniques such as generative adversarial networks (GANs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tulyakov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib54\" title=\"\">2018</a>)</cite>, variational autoencoders (VAEs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib58\" title=\"\">2020</a>; Ahuja &amp; Morency, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib1\" title=\"\">2019</a>; Petrovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib45\" title=\"\">2022</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib15\" title=\"\">2022a</a>)</cite>, or recent diffusion models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib12\" title=\"\">2023</a>; Tevet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib52\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib68\" title=\"\">2024b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib66\" title=\"\">2023b</a>; Ribeiro-Gomes et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib48\" title=\"\">2024</a>)</cite>.\nDespite avoiding the approximation errors, they miss the autoregressive or masked modeling technologies, which have been shown to deliver superior performance in motion generation tasks.\nConsequently, the performance of the motion generated by this path is overall lower than that achieved by the first path.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To both leverage the advantages of autoregressive and masked modeling, and the benefits of the continuous motion representation, in this work we combine them together to propose a continuous masked autoregressive motion generation framework.\nWe apply a random masking on the sequential motion tokens, and employ a transformer to autoregressively predict the masked tokens.\nUnlike the visual MAR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>)</cite>, we sequentially predict the masked tokens with causal attention rather than performing random reordering, considering the sequential nature within human motion.\nTo enhance the MAR modeling in motion space, we introduce a gated linear mechanism and an RMSNorm module.\nThe gated linear mechanism serves as an adaptive feature selector, driving the transformer to not only pay more attention to the key actions, like gesture switching and large movement, but also disregard less relevant frames and suppress redundant actions, like stationary motions.\nThe RMSNorm is particularly advantageous in scenarios with features exhibiting a large dynamic range, e.g., our unified framework for multi-modalities, where the input distributions are highly heterogeneous.\nIn addition, RMSNorm helps relieve the gradient instability caused by the abnormally large motions, such as sudden jumping or turning back.\nAfter the masked autoregressive transformer, the calculated attention of the masked tokens is fed into a series of DiT blocks to diffuse towards the target tokens, which are decoded to the generated motions.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "unified",
                    "representation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the text-based motion generation, we further extend our framework to multimodal conditions.\nBuilding upon a similar structure, the multimodal signals are fused by AdaLN&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib18\" title=\"\">2022c</a>)</cite> and cross-attention modules.\nExtensive experiments across different datasets demonstrate our framework can work well with different modalities, including text, speech, and music, and outperform previous methods in whole-body text-to-motion, speech-to-gesture, and music-to-dance tasks.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We design an omni motion framework for whole-body human motion generation, where one framework encompasses multiple modalities.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We integrate the multimodal signals via AdaLN and cross-attention, obtaining superior performance than previous methods in text-based, speech-based, and music-based motion generation.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "musicbased",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous research on text-based motion generation can be generally categorized into two mainstream paths: continuous regression and discrete classification.\nIn the continuous regression domain, numerous strategies have leveraged the variational autoencoder (VAE) framework, integrating latent embeddings of encoded text with those of encoded poses, which are then decoded into motion predictions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib58\" title=\"\">2020</a>; Ahuja &amp; Morency, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib1\" title=\"\">2019</a>; Athanasiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib4\" title=\"\">2022</a>; Petrovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib45\" title=\"\">2022</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib15\" title=\"\">2022a</a>)</cite>.\nOther methods have investigated the potential of recurrent networks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib33\" title=\"\">2018</a>; Plappert et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib47\" title=\"\">2018</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib70\" title=\"\">2020</a>)</cite>, generative adversarial networks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib7\" title=\"\">2018</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib57\" title=\"\">2020</a>; Tulyakov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib54\" title=\"\">2018</a>)</cite>, or transformer networks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tevet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib51\" title=\"\">2022</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib35\" title=\"\">2023b</a>; Bhattacharya et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib5\" title=\"\">2021</a>; Petrovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib44\" title=\"\">2021</a>)</cite> to enhance the motion regression quality.\nBuilding on the success of diffusion models, recent approaches have begun to integrate the diffusion process into motion diffusion&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib22\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib12\" title=\"\">2023</a>; Tevet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib52\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib68\" title=\"\">2024b</a>; Dabral et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib13\" title=\"\">2023</a>; Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib40\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib66\" title=\"\">2023b</a>; Ribeiro-Gomes et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib48\" title=\"\">2024</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib62\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib56\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib67\" title=\"\">2023c</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib72\" title=\"\">2024d</a>; Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, yielding impressive results.\nIn the discrete classification domain, the input motion undergoes initial encoding via a VQ-VAE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Van Den&#160;Oord et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib55\" title=\"\">2017</a>)</cite>, producing motion tokens for subsequent prediction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib74\" title=\"\">2023</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib32\" title=\"\">2024e</a>)</cite>.\nDrawing inspiration from advancements in natural language processing, some methods utilize autoregressive modeling to predict tokens sequentially&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib16\" title=\"\">2022b</a>; Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib40\" title=\"\">2023</a>; Zou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib80\" title=\"\">2024</a>)</cite>.\nOthers employ generative masked modeling strategies, with tokens randomly masked during training for the model to predict&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib17\" title=\"\">2024</a>; Pinyoanuntapong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib46\" title=\"\">2024</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib61\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib28\" title=\"\">2023b</a>)</cite>.\nMore recently, large language models (LLMs) have been harnessed to help the prediction process, considering their large-scale pretraining&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib71\" title=\"\">2023d</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib77\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib31\" title=\"\">2024d</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib29\" title=\"\">2023c</a>)</cite>.\nIn this work, we seek to integrate the most effective elements from these two paths: the continuous diffusion and the masked autoregressive modeling.\nA previous attempt in this direction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib42\" title=\"\">2024</a>)</cite> directly transfers the MAR in image generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>)</cite> into motion generation without considering the difference between image and motion spaces, especially the temporal correlation. Also, its framework is only designed for body-only motion generation. Differently, we propose a new MAR mechanism that is especially designed for whole-body motion generation.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "results",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to text, there are many other signals that various human motions are conditioned on, such as speech and music.\nIn the realm of speech-to-gesture generation, both continuous regression and discrete classification paths have been explored.\nIn the continuous domain, methods employ deep generative models like GANs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ahuja et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib2\" title=\"\">2022</a>)</cite>, normalizing flows&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib50\" title=\"\">2024</a>)</cite>, and diffusion models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Alexanderson et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib3\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib10\" title=\"\">2024a</a>; He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib20\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib59\" title=\"\">2023</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib78\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib11\" title=\"\">2024b</a>)</cite> to learn complex motion distributions in the speech data.\nIn the discrete domain, methods leverage either the autoregressive modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib60\" title=\"\">2023</a>)</cite> or the masked modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib37\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib38\" title=\"\">b</a>)</cite> to predict the discrete tokens quantified by the VQ-VAE.\nThe primary distinction among these methods lies in their specific handling of different parts of human motion, including body movements, hand gestures, and facial expressions.\nSimilarly, in the realm of music-to-dance generation, there are also methods in both the continuous domain&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhuang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib79\" title=\"\">2022</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib53\" title=\"\">2023</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib25\" title=\"\">2024a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib21\" title=\"\">2024</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib64\" title=\"\">2024a</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite> and the discrete domain&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Siyao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib49\" title=\"\">2022</a>)</cite>.\nThe discrete autoregressive model is leveraged after the motion quantization with VQ-VAE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Siyao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib49\" title=\"\">2022</a>)</cite>.\nMore methods harness the diffusion model to directly regress the target dancing motion in the continuous space&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib53\" title=\"\">2023</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib25\" title=\"\">2024a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib21\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite>.\nRecent methods also start to merge autoregressive and diffusion models, producing coherent and music-aligned dance sequences&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib64\" title=\"\">2024a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "2023a",
                    "tseng"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent works have begun to seek multimodal solutions, i.e., designing one framework for motion generation from different input signals&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib41\" title=\"\">2024</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib69\" title=\"\">2024c</a>; Zhou &amp; Wang, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib75\" title=\"\">2023</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib76\" title=\"\">2023</a>; Ling et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib36\" title=\"\">2023</a>; Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib30\" title=\"\">2024c</a>)</cite>.\nSome methods in the discrete domain attempt to incorporate quantized condition tokens into the vocabulary of the generation model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib41\" title=\"\">2024</a>; Zhou &amp; Wang, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib75\" title=\"\">2023</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib76\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib73\" title=\"\">2025</a>)</cite>, while some methods in the continuous domain try to integrate the multimodal signals by designing the motion ControlNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ling et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib36\" title=\"\">2023</a>; Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, where the multimodal conditions guide the sampling of a pretrained text-to-motion diffusion model.\nHowever, most previous methods are restricted by the varying motion data of different modalities, limiting multi-modal frameworks primarily to body-only motion generation.\nTo overcome this, MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite> standardizes datasets across modalities into a unified whole-body format that includes body, hands, and face.\nIn this work, we follow this unified representation to build a whole-body multi-modal motion framework, taking advantage of continuous masked auto-regression.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "ling",
                    "motioncraft",
                    "unified",
                    "representation",
                    "generation",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The overview of our framework is illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S2.F2\" title=\"Figure 2 &#8227; Multimodal Motion Generation. &#8227; 2 Related Work &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, which is divided into three main stages:\nIn the first stage, we encode the input motion with an autoencoder, which generates continuous motion tokens.\nIn the second stage, we focus on the text-based motion generation, utilizing our motion masked autoregressive framework to model the motion generation process.\nIn this stage, an autoregressive transformer is employed to predict the masked tokens, within which a gated linear mechanism is designed, and an RMSNorm module is employed.\nThe text information is integrated into the transformer via AdaLN after encoding.\nAfter the transformer, the generated embedding is fed into the DiT modules as the condition to diffuse towards the target token.\nIn the third stage, we extend the model learned in the previous stage to the multi-modal structure.\nThis involves merging the text embedding with multimodal signal embeddings&#8212;specifically speech or music&#8212;prior to their AdaLN input.\nFurthermore, we inject the multimodal embedding through a cross-attention module into the masked transformer.\nIn the multimodal learning stage, the DiT module is kept in the same structure and frozen.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To feed the human motion into the transformer, we start by encoding the original motion into motion tokens.\nGiven a motion sequence <math alttext=\"\\{\\mathbf{M}_{t}\\}_{t=1}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>&#119820;</mi><mi>t</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\{\\mathbf{M}_{t}\\}_{t=1}^{T}</annotation></semantics></math>, the objective of an autoencoder is to extract a latent code <math alttext=\"z_{\\text{AE}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>z</mi><mtext>AE</mtext></msub><annotation encoding=\"application/x-tex\">z_{\\text{AE}}</annotation></semantics></math> that optimally captures the essence of the original motion.\nDifferent from most previous motion generation methods with autoregressive transformers, we use a continuous autoencoder rather than the VQ-VAE to do the encoding, which avoids the precision loss associated with the quantization approximation.\nIn the encoder, we stack the 1D convolution networks with ReLU activation to do the feature processing. Following this, two down-sampling residual blocks are applied to reduce the motion feature size to one-fourth of its original dimensions.\nIn the decoder, the same structure in the reversed order is utilized to up-sample the motion feature back to the original size, producing <math alttext=\"\\{\\hat{\\mathbf{M}}_{t}\\}_{t=1}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mover accent=\"true\"><mi>&#119820;</mi><mo>^</mo></mover><mi>t</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\{\\hat{\\mathbf{M}}_{t}\\}_{t=1}^{T}</annotation></semantics></math>. Therefore, the loss for training the autoencoder is defined as</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the continuous motion tokens, we design a masked autoregressive transformer to model the motion generation, effectively capturing the temporal relationship between different tokens and producing the rich contextual condition <math alttext=\"\\mathbf{z}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><msup><mi>&#119859;</mi><mi>i</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{z}^{i}</annotation></semantics></math> for the subsequent diffusion process.\nWe first randomly mask the motion tokens following language models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib14\" title=\"\">2018</a>)</cite>, obtaining some masked tokens <math alttext=\"\\{\\tilde{\\mathbf{x}}^{i}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msup><mover accent=\"true\"><mi>&#119857;</mi><mo>~</mo></mover><mi>i</mi></msup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\tilde{\\mathbf{x}}^{i}\\}</annotation></semantics></math>.\nThe temporal masking strategies adopt the same mask ratio schedule following&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib8\" title=\"\">2022</a>)</cite>, and are computed as</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the input text prompt <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>, we first employ the LaMP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib32\" title=\"\">2024e</a>)</cite> text transformer to extract textual features, leveraging its advanced capabilities to encode the linguistic nuances and semantic structure of the input prompt. This creates a high-dimensional feature representation that is crucial for guiding motion generation process.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "representation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the feature extraction, we utilize AdaLN to seamlessly integrate the text-derived control signals into the masked autoregressive transformer. AdaLN offers a dynamic approach to normalization, allowing the modulation of its parameters in response to the specific text input, thereby facilitating subsequent multimodal condition injection. By employing this method, we enhance the model&#8217;s ability to incorporate the guiding signals from the text and other signals into the motion generation process, ensuring that the transformer&#8217;s output features are better aligned with the intended motion generation goals. The features outputted by the transformer embody a strong directive capacity for motion generation. This enables the model not only to faithfully interpret the semantic content of the input text but also to produce motion sequences that are coherent with and reflective of the textual intent. The enriched output features contribute to achieving smoother transitions and logically consistent motion sequences in complex generation scenarios.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "method",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our objective is to construct a unified model that can simultaneously perform text-to-motion, speech-to-gesture, and music-to-dance tasks. Therefore, we aim to ensure that during the fine-tuning process across different datasets, the model does not suffer from instability that could lead to catastrophic failures. RMSNorm&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang &amp; Sennrich, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib63\" title=\"\">2019</a>)</cite> is particularly advantageous in scenarios with features exhibiting a large dynamic range, especially in tasks where the input distributions are highly heterogeneous. This characteristic enables the model to maintain stability when faced with diverse types of inputs or when observing uneven feature distributions. Additionally, RMSNorm has the potential to mitigate the gradient instability that may arise from significant motion variations, such as sudden jumps or rapid turns.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "unified"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to previous works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>; Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib42\" title=\"\">2024</a>)</cite>, we adopt Diffusion Transformer (DiT) as our diffusion model. While the use of DiT may incur additional time costs during training and inference (not much due to the compact structure of motion data), it significantly enhances the quality of generated outputs. Compared to MLPs, DiT provides greater convenience in injecting conditional control signals. During the training process of multimodal generation, we freeze the diffusion model and only fine-tune the masked transformer. The structural characteristics of DiT facilitate this approach, enabling it to better handle various types of conditional signals.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, MLPs exhibit notable limitations when processing heterogeneous data. This incapacity results in suboptimal performance when confronted with diverse signal types, such as speech and music. Due to the relatively small number of parameters in MLPs, they are prone to overfitting on specific datasets (e.g., text-to-motion). This situation can be analogized to a dancer who is adept only in a single dance style; when asked to incorporate other styles, they appear clumsy and ineffective. Consequently, when we attempt to fine-tune MLPs on a different dataset, they are ill-equipped to adapt to the challenges posed by new signals, leading to failures in multimodal generation tasks.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first pre-train the model on text-motion paired data in a text-to-motion generation setting. Owing to its strong semantic expressiveness and cross-modal alignment properties, we adopt text as a shared conditional signal across diverse unimodal datasets, enabling the model to learn sequence-level generation capabilities between text and motion, as well as a coarse-grained textual guidance mechanism for generative control.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hypothesize that the contextual features output by the masked transformer provide a more expressive control signal compared to raw text embeddings. Accordingly, within the DiT architecture, we inject the transformer&#8217;s output features by summing them with the time embeddings, thereby guiding the motion generation process as:</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation Details. </span>\nOur model is implemented on one NVIDIA V100 GPU using PyTorch. For our method, the autoencoder employs a ResNet-based&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib19\" title=\"\">2016</a>)</cite> three-layer encoder-decoder architecture with a hidden dimension of 512 and an overall downsampling rate of 4. For the generation process, we utilize a four-layer AdaLN-zero transformer encoder as the masked autoregressive transformer, featuring a hidden dimension of 1024 and 16 attention heads. The diffusion model consists of 4 layers of DiT, where each transformer block has a hidden dimension of 1792 and 8 attention heads.\nWe adopt the AdamW optimizer <math alttext=\"(\\beta_{1}=0.9,\\beta_{2}=0.99)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><mo>,</mo><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.99</mn></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\beta_{1}=0.9,\\beta_{2}=0.99)</annotation></semantics></math>. For training the autoencoder on the HumanML subset of Motion-X, we use a batch size of 256 and a maximum sequence length of 64 frames. For the text-to-motion task, the batch size is set to 50 with a maximum sequence length of 196 frames. The learning rate is initialized at 0.0002 with a linear warmup over 2000 steps. The autoencoder is trained for 50 epochs, while the text-to-motion task is trained for 1500 epochs.\nDuring multimodal generation, we first initialize the model with pretrained weights from the text-to-motion autoencoder and fine-tune it on task-specific datasets. Subsequently, we freeze the parameters of the text-to-motion DiT and only fine-tune the masked transformer along with newly incorporated cross-attention layers. The learning rate and training schedule remain consistent with the text-to-motion task.\nFor all three tasks, we employ exponential moving average (EMA) to update model parameters, ensuring training stability. During inference, the classifier-free guidance (CFG) scale is set to 4.5 for text-to-motion, while other tasks use a CFG scale of 6.5.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets and Metrics. </span>\nFor the evaluation, we utilize three datasets: HumanML3D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib15\" title=\"\">2022a</a>)</cite> for text-to-motion, BEAT2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib37\" title=\"\">2024a</a>)</cite> for speech-to-gesture, and FineDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite> for music-to-dance, all following the unified SMPL-X representation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>.\nRegarding the metrics, we use FID, R-Precision, and MM-Dist for text-based motion generation, use FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">H</span></sub>, FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">B</span></sub>, Face L2 loss, Beat Alignment Score, Diversity for speech-based motion generation, and use FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">H</span></sub>, FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">B</span></sub>, Diversity for music-based motion generation, respectively.\nFor the detailed explanation of the datasets and metrics, please refer to the appendix.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "following",
                    "smplx",
                    "unified",
                    "2023a",
                    "representation",
                    "finedance",
                    "musicbased",
                    "generation",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-based motion generation. </span>\nWe conduct an evaluation of our model against prior text-to-motion approaches, including both discrete-domain and continuous-domain methods.\nAs summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S3.T1\" title=\"Table 1 &#8227; 3.5 Text-to-motion Pretraining and Multimodal Control Adaptation &#8227; 3 Method &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our method clearly surpasses prior techniques on the HumanML subset of Motion-X dataset. Remarkably, our model achieves improvements of 19.3%, 13.5%, and 11.7% in R-Precision for Top-1, 2, 3, respectively. Additionally, we enhance the FID score by 75.2% on this dataset, underscoring the exceptional fidelity of our generated motions. The qualitative results in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S3.F4\" title=\"Figure 4 &#8227; 3.5 Text-to-motion Pretraining and Multimodal Control Adaptation &#8227; 3 Method &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> further support these findings, showing that our approach yields whole-body motions that align closely with the input text.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "method",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-based motion generation. </span>\nTo assess the speech-driven motion generation, we compare to previous speech-to-gesture methods.\nOur results, summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S3.T2\" title=\"Table 2 &#8227; 3.6 Inference &#8227; 3 Method &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, reveal that our method achieves good quality and diversity in both hand and body motion generation and excels in aligning\nwith the rhythm of first-person speech.\nThis demonstrates the effectiveness of our framework in motion generation when encompassing different modal signals.\nHowever, our method performs worse than single-modal methods.\nAs discussed in &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, this is attributed to the random or average expressions in the Motion-X dataset, which confuses the speech-to-gesture training.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "method",
                    "results",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Causal Attention. </span>\nTo verify the efficacy of the proposed framework, we initially establish a baseline model following the visual MAR setup&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>)</cite>, i.e, using the random pseudo reordering for the batched masked prediction, through the bidirectional attention computing.\nFrom the results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Ablation Study &#8227; 4 Experiments &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we see that the performance of this baseline in the motion area is limited. We attribute this to the difference between human motions and visual images, e.g., the human motion is in a strong temporal sequential structure, in which case a causal attention makes more sense.\nTherefore, changing the baseline to sequential masked prediction with causal attention improves the performance.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "results",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DiT. </span>\nIn order to evaluate how the DiTs contribute to the motion quality, we further replace the MLPs in the baseline model with our DiTs.\nAs shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Ablation Study &#8227; 4 Experiments &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the model generates superior motions with DiTs compared to MLPs, especially in the context of multimodal motion generation.\nThis reveals the superior potential of DiTs in generating motion with complex multimodal contexts.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross Attention. </span>\nIn the baseline model, the multimodal signals are injected with only the AdaLN structure.\nWe then add the cross attention module and observe a significant improvement in multimodal motion generation, as depicted in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Ablation Study &#8227; 4 Experiments &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper proposes a new omni motion framework for multimodal whole-body human motion generation.\nWithin this one framework, text, speech, and music signals are all encompassed through AdaLN and cross-attention.\nThe motion generation process is modeled by a continuous masked autoregressive transformer with causal attention, as well as a DiT structure.\nExtensive experiments have been conducted to verify the efficacy of the proposed framework in different-modality tasks.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations.</span>\nDue to the restricted dataset, the naturalness and generalizability of the motion generation model are still limited, especially in speech and music-driven motion generation.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our speech and music encoders are designed to extract temporally aligned, high-level features from raw audio signals for effective speech-to-gesture and music-to-dance generation. The architecture builds upon a multi-layer 1D convolutional network with strided convolutions and leaky ReLU activations. Each convolutional block consists of a series of a block unit that progressively downsample the input waveform while increasing feature dimensionality. The input audio sequence is first processed through multiple stages of temporal aggregation and non-linear transformation, resulting in a sequence of compact and expressive latent representations, whereas the input music typically retains sufficient temporal structure and spectral richness in its raw form for effective motion synthesis. These latent codes capture prosodic, rhythmic, and semantic-like patterns in speech and music, which are then projected into the condition latent space of dimensionality. The final encoder output is transposed to align with the temporal structure expected by the diffusion model, enabling fine-grained cross-modal interaction between speech and motion sequences during generation.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first pretrain a baseline autoencoder on the text-to-motion task. When fine-tuning it on the speech-to-gesture and music-to-dance tasks, the decoder fails to reconstruct valid motion sequences due to discrepancies in data distribution. However, fine-tuning the autoencoder using the reconstruction objective during the multi-modal training incurs high computational costs. Therefore, we independently fine-tune the baseline AE on each dataset using the reconstruction task before multi-modal generation, and employ the resulting models for downstream tasks.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following previous work&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Bian et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, we utilize SMPL-X formatted motion data with an input dimension of (frame length &#215; 322). The parameter structure is organized as follows: Root orientation (0:3): controls global body rotation; body pose (3:66): Governs major body joint rotations; hand articulation (66:156): controls finger movements; jaw pose (156:159): manages mouth opening/closing; facial expression (159:209): drives emotional expressions; facial shape (209: 309): determines static facial structure; translation (309:312): controls global body position; betas (312: 322): represents static body shape parameters. And the maximum motion length is 196.\nThe model&#8217;s output maintains identical dimensionality (frame length &#215; 322) to ensure full reconstruction capability. This comprehensive parameterization enables simultaneous control of body motion, facial animation, and global positioning within a unified framework.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "following",
                    "smplx",
                    "unified",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For text-based motion generation, we evaluate our method on the HumanML3D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib15\" title=\"\">2022a</a>)</cite> dataset, which consists of 14,616 high-quality human motions paired with 44,970 text descriptions.\nThe original body-only SMPL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Loper et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib39\" title=\"\">2023</a>)</cite> format of this dataset is extended to whole-body SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pavlakos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib43\" title=\"\">2019</a>)</cite> format in MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, which we follow in the experiments for evaluation.\nFor speech-based motion generation, we evaluate on the BEAT2 dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib37\" title=\"\">2024a</a>)</cite>, which collects 76 hours of data from 30 speakers, standardized into a mesh representation with paired audio and text lines.\nThe motion of the unified SMPL-X format is also extracted&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite> for multimodal evaluation.\nFor music-based motion generation, the largest dataset FineDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite> is utilized for evaluation. This dataset collects dances of 14.6 hours across 22 genres and provides detailed human motions using the SMPL-H format, which is then converted to the unified SMPL-X format and appended by text descriptions.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "motioncraft",
                    "smplx",
                    "unified",
                    "2023a",
                    "representation",
                    "finedance",
                    "musicbased",
                    "generation",
                    "method",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable full-body, multimodal control over motion generation, we convert all datasets to the SMPL-X format. This involves filling in missing facial expressions in HumanML3D and FineDance using average expression coefficients from the training set, as well as transforming the SMPL-H Rot-6D representation in FineDance into axis-angle format via Gram-Schmidt orthogonalization. This conversion achieves better alignment with SMPL-X parameters and introduces minimal errors compared to the official body-retargeting method, while also offering improved computational efficiency.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "smplx",
                    "representation",
                    "generation",
                    "method",
                    "finedance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure consistency with MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, we utilize the pretrained motion encoder and text encode, enabling a unified evaluation of the SMPL-X motion representation across different modalities. For datasets that lack corresponding textual annotations&#8212;namely FineDance and BEAT2&#8212;we generate pseudo-captions such as &#8220;A dancer is performing a street dance in the Jazz style to the rhythm of the wildfire&#8221; and &#8220;A person is giving a speech, and the content is &#8230;&#8221;, respectively, to support cross-modal learning.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "motioncraft",
                    "smplx",
                    "unified",
                    "representation",
                    "finedance",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the same strategy as MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, we train two variants of our model: OmniMotion-Base and OmniMotion-Mix. OmniMotion-Base is a text-to-motion model pretrained solely on HumanML3D, while OmniMotion-Mix is trained on a combined dataset comprising HumanML3D, BEAT2, and FineDance to enable multimodal motion generation. Quantitative results on the text-to-motion task are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#A1.T7\" title=\"Table 7 &#8227; Text-based Motion Generation &#8227; A.3 Metrics &#8227; Appendix A Appendix &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "following",
                    "motioncraft",
                    "finedance",
                    "generation",
                    "results",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We display more visual results of speech-driven and music-driven motion generation in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#A1.F5\" title=\"Figure 5 &#8227; Text-based Motion Generation &#8227; A.3 Metrics &#8227; Appendix A Appendix &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#A1.F6\" title=\"Figure 6 &#8227; Text-based Motion Generation &#8227; A.3 Metrics &#8227; Appendix A Appendix &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, respectively.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "results"
                ]
            }
        ]
    },
    "S4.T4": {
        "caption": "Table 4: The ablation study of different model components.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Text-based</td>\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_tt\" colspan=\"2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Speech-based</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Setting</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">R Precision</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">FID <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">H</span></sub> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">B</span></sub> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Top-1 <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Top-2 <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Top-3 <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Baseline</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.578^{\\pm{0.007}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m9\" intent=\":literal\"><semantics><msup><mn>0.578</mn><mrow><mo>&#177;</mo><mn>0.007</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.578^{\\pm{0.007}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.737^{\\pm{0.006}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m10\" intent=\":literal\"><semantics><msup><mn>0.737</mn><mrow><mo>&#177;</mo><mn>0.006</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.737^{\\pm{0.006}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.787^{\\pm{0.005}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m11\" intent=\":literal\"><semantics><msup><mn>0.787</mn><mrow><mo>&#177;</mo><mn>0.005</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.787^{\\pm{0.005}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"9.324^{\\pm{0.120}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m12\" intent=\":literal\"><semantics><msup><mn>9.324</mn><mrow><mo>&#177;</mo><mn>0.120</mn></mrow></msup><annotation encoding=\"application/x-tex\">9.324^{\\pm{0.120}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"37.732\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m13\" intent=\":literal\"><semantics><mn>37.732</mn><annotation encoding=\"application/x-tex\">37.732</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"40.419\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m14\" intent=\":literal\"><semantics><mn>40.419</mn><annotation encoding=\"application/x-tex\">40.419</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">+ Causal Attention</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.589^{\\pm{0.006}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m15\" intent=\":literal\"><semantics><msup><mn>0.589</mn><mrow><mo>&#177;</mo><mn>0.006</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.589^{\\pm{0.006}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.740^{\\pm{0.004}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m16\" intent=\":literal\"><semantics><msup><mn>0.740</mn><mrow><mo>&#177;</mo><mn>0.004</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.740^{\\pm{0.004}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.798^{\\pm{0.006}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m17\" intent=\":literal\"><semantics><msup><mn>0.798</mn><mrow><mo>&#177;</mo><mn>0.006</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.798^{\\pm{0.006}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"9.031^{\\pm{0.095}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m18\" intent=\":literal\"><semantics><msup><mn>9.031</mn><mrow><mo>&#177;</mo><mn>0.095</mn></mrow></msup><annotation encoding=\"application/x-tex\">9.031^{\\pm{0.095}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"36.815\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m19\" intent=\":literal\"><semantics><mn>36.815</mn><annotation encoding=\"application/x-tex\">36.815</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"38.674\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m20\" intent=\":literal\"><semantics><mn>38.674</mn><annotation encoding=\"application/x-tex\">38.674</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">+ DiT</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.688^{\\pm{0.005}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m21\" intent=\":literal\"><semantics><msup><mn>0.688</mn><mrow><mo>&#177;</mo><mn>0.005</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.688^{\\pm{0.005}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.828^{\\pm{0.007}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m22\" intent=\":literal\"><semantics><msup><mn>0.828</mn><mrow><mo>&#177;</mo><mn>0.007</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.828^{\\pm{0.007}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.851^{\\pm{0.004}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m23\" intent=\":literal\"><semantics><msup><mn>0.851</mn><mrow><mo>&#177;</mo><mn>0.004</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.851^{\\pm{0.004}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"5.562^{\\pm{0.085}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m24\" intent=\":literal\"><semantics><msup><mn>5.562</mn><mrow><mo>&#177;</mo><mn>0.085</mn></mrow></msup><annotation encoding=\"application/x-tex\">5.562^{\\pm{0.085}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"19.743\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m25\" intent=\":literal\"><semantics><mn>19.743</mn><annotation encoding=\"application/x-tex\">19.743</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"28.228\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m26\" intent=\":literal\"><semantics><mn>28.228</mn><annotation encoding=\"application/x-tex\">28.228</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">+ Gated Linear</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.692^{\\pm{0.004}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m27\" intent=\":literal\"><semantics><msup><mn>0.692</mn><mrow><mo>&#177;</mo><mn>0.004</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.692^{\\pm{0.004}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.834^{\\pm{0.005}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m28\" intent=\":literal\"><semantics><msup><mn>0.834</mn><mrow><mo>&#177;</mo><mn>0.005</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.834^{\\pm{0.005}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.877^{\\pm{0.006}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m29\" intent=\":literal\"><semantics><msup><mn>0.877</mn><mrow><mo>&#177;</mo><mn>0.006</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.877^{\\pm{0.006}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"4.844^{\\pm{0.078}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m30\" intent=\":literal\"><semantics><msup><mn>4.844</mn><mrow><mo>&#177;</mo><mn>0.078</mn></mrow></msup><annotation encoding=\"application/x-tex\">4.844^{\\pm{0.078}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"19.427\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m31\" intent=\":literal\"><semantics><mn>19.427</mn><annotation encoding=\"application/x-tex\">19.427</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"28.156\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m32\" intent=\":literal\"><semantics><mn>28.156</mn><annotation encoding=\"application/x-tex\">28.156</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">+ RMSNorm</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.704^{\\pm{0.003}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m33\" intent=\":literal\"><semantics><msup><mn>0.704</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.704^{\\pm{0.003}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.843^{\\pm{0.005}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m34\" intent=\":literal\"><semantics><msup><mn>0.843</mn><mrow><mo>&#177;</mo><mn>0.005</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.843^{\\pm{0.005}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.898^{\\pm{0.005}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m35\" intent=\":literal\"><semantics><msup><mn>0.898</mn><mrow><mo>&#177;</mo><mn>0.005</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.898^{\\pm{0.005}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"4.838^{\\pm{0.100}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m36\" intent=\":literal\"><semantics><msup><mn>4.838</mn><mrow><mo>&#177;</mo><mn>0.100</mn></mrow></msup><annotation encoding=\"application/x-tex\">4.838^{\\pm{0.100}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"18.329\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m37\" intent=\":literal\"><semantics><mn>18.329</mn><annotation encoding=\"application/x-tex\">18.329</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"27.741\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m38\" intent=\":literal\"><semantics><mn>27.741</mn><annotation encoding=\"application/x-tex\">27.741</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">+ Cross Attention</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.704^{\\pm{0.003}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m39\" intent=\":literal\"><semantics><msup><mn>0.704</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.704^{\\pm{0.003}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.843^{\\pm{0.005}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m40\" intent=\":literal\"><semantics><msup><mn>0.843</mn><mrow><mo>&#177;</mo><mn>0.005</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.843^{\\pm{0.005}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.898^{\\pm{0.005}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m41\" intent=\":literal\"><semantics><msup><mn>0.898</mn><mrow><mo>&#177;</mo><mn>0.005</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.898^{\\pm{0.005}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"4.838^{\\pm{0.100}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m42\" intent=\":literal\"><semantics><msup><mn>4.838</mn><mrow><mo>&#177;</mo><mn>0.100</mn></mrow></msup><annotation encoding=\"application/x-tex\">4.838^{\\pm{0.100}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"{17.651}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m43\" intent=\":literal\"><semantics><mn>17.651</mn><annotation encoding=\"application/x-tex\">{17.651}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"{25.823}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.m44\" intent=\":literal\"><semantics><mn>25.823</mn><annotation encoding=\"application/x-tex\">{25.823}</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "9031±00959031pm0095",
            "textbased",
            "0589±00060589pm0006",
            "0688±00050688pm0005",
            "setting",
            "top3",
            "attention",
            "fidb",
            "0898±00050898pm0005",
            "0834±00050834pm0005",
            "↓downarrow",
            "study",
            "fidh",
            "9324±01209324pm0120",
            "ablation",
            "rmsnorm",
            "cross",
            "speechbased",
            "0828±00070828pm0007",
            "top1",
            "causal",
            "0740±00040740pm0004",
            "0843±00050843pm0005",
            "0877±00060877pm0006",
            "top2",
            "gated",
            "0692±00040692pm0004",
            "4838±01004838pm0100",
            "0787±00050787pm0005",
            "5562±00855562pm0085",
            "0578±00070578pm0007",
            "fid",
            "0704±00030704pm0003",
            "components",
            "↑uparrow",
            "0851±00040851pm0004",
            "dit",
            "0737±00060737pm0006",
            "model",
            "4844±00784844pm0078",
            "linear",
            "precision",
            "baseline",
            "different",
            "0798±00060798pm0006"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Causal Attention. </span>\nTo verify the efficacy of the proposed framework, we initially establish a baseline model following the visual MAR setup&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>)</cite>, i.e, using the random pseudo reordering for the batched masked prediction, through the bidirectional attention computing.\nFrom the results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Ablation Study &#8227; 4 Experiments &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we see that the performance of this baseline in the motion area is limited. We attribute this to the difference between human motions and visual images, e.g., the human motion is in a strong temporal sequential structure, in which case a causal attention makes more sense.\nTherefore, changing the baseline to sequential masked prediction with causal attention improves the performance.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DiT. </span>\nIn order to evaluate how the DiTs contribute to the motion quality, we further replace the MLPs in the baseline model with our DiTs.\nAs shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Ablation Study &#8227; 4 Experiments &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the model generates superior motions with DiTs compared to MLPs, especially in the context of multimodal motion generation.\nThis reveals the superior potential of DiTs in generating motion with complex multimodal contexts.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Gated Linear Mechanism. </span>\nTo assess the function of the gated linear mechanism, we ablate this and report the results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Ablation Study &#8227; 4 Experiments &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, which indicates that the model outputs motions of higher quality with the inclusion of this mechanism. In the experiments, we observed that the output motions sometimes contain more detailed actions with this mechanism in place.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RMSNorm. </span>\nWe also conduct an ablation study to evaluate the function of the RMSNorm and report the results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Ablation Study &#8227; 4 Experiments &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. From the results, we see that the model produces better motions when utilizing RMSNorm. In experiments, we found that this module makes the output more stable.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross Attention. </span>\nIn the baseline model, the multimodal signals are injected with only the AdaLN structure.\nWe then add the cross attention module and observe a significant improvement in multimodal motion generation, as depicted in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Ablation Study &#8227; 4 Experiments &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Whole-body multi-modal human motion generation poses two primary challenges: creating an effective motion generation mechanism and integrating various modalities, such as text, speech, and music, into a cohesive framework.\nUnlike previous methods that usually employ discrete masked modeling or autoregressive modeling, we develop a continuous masked autoregressive motion transformer, where a causal attention is performed considering the sequential nature within the human motion.\nWithin this transformer, we introduce a gated linear attention and an RMSNorm module, which drive the transformer to pay attention to the key actions and suppress the instability caused by either the abnormal movements or the heterogeneous distributions within multi-modalities.\nTo further enhance both the motion generation and the multimodal generalization, we employ the DiT structure to diffuse the conditions from the transformer towards the targets.\nTo fuse different modalities, AdaLN and cross-attention are leveraged to inject the text, speech, and music signals.\nExperimental results demonstrate that our framework outperforms previous methods across all modalities, including text-to-motion, speech-to-gesture, and music-to-dance.\nThe code of our method will be made public.</p>\n\n",
                "matched_terms": [
                    "causal",
                    "dit",
                    "gated",
                    "attention",
                    "linear",
                    "rmsnorm",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To both leverage the advantages of autoregressive and masked modeling, and the benefits of the continuous motion representation, in this work we combine them together to propose a continuous masked autoregressive motion generation framework.\nWe apply a random masking on the sequential motion tokens, and employ a transformer to autoregressively predict the masked tokens.\nUnlike the visual MAR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>)</cite>, we sequentially predict the masked tokens with causal attention rather than performing random reordering, considering the sequential nature within human motion.\nTo enhance the MAR modeling in motion space, we introduce a gated linear mechanism and an RMSNorm module.\nThe gated linear mechanism serves as an adaptive feature selector, driving the transformer to not only pay more attention to the key actions, like gesture switching and large movement, but also disregard less relevant frames and suppress redundant actions, like stationary motions.\nThe RMSNorm is particularly advantageous in scenarios with features exhibiting a large dynamic range, e.g., our unified framework for multi-modalities, where the input distributions are highly heterogeneous.\nIn addition, RMSNorm helps relieve the gradient instability caused by the abnormally large motions, such as sudden jumping or turning back.\nAfter the masked autoregressive transformer, the calculated attention of the masked tokens is fed into a series of DiT blocks to diffuse towards the target tokens, which are decoded to the generated motions.</p>\n\n",
                "matched_terms": [
                    "causal",
                    "dit",
                    "gated",
                    "attention",
                    "linear",
                    "rmsnorm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the text-based motion generation, we further extend our framework to multimodal conditions.\nBuilding upon a similar structure, the multimodal signals are fused by AdaLN&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib18\" title=\"\">2022c</a>)</cite> and cross-attention modules.\nExtensive experiments across different datasets demonstrate our framework can work well with different modalities, including text, speech, and music, and outperform previous methods in whole-body text-to-motion, speech-to-gesture, and music-to-dance tasks.</p>\n\n",
                "matched_terms": [
                    "different",
                    "textbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a continuous autoregressive motion transformer with causal attention, where a gated linear mechanism and an RMSNorm module are developed to assist the motion modeling, and the DiT blocks are employed to improve the quality of the generated motions.</p>\n\n",
                "matched_terms": [
                    "causal",
                    "dit",
                    "gated",
                    "attention",
                    "linear",
                    "rmsnorm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We integrate the multimodal signals via AdaLN and cross-attention, obtaining superior performance than previous methods in text-based, speech-based, and music-based motion generation.</p>\n\n",
                "matched_terms": [
                    "speechbased",
                    "textbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous research on text-based motion generation can be generally categorized into two mainstream paths: continuous regression and discrete classification.\nIn the continuous regression domain, numerous strategies have leveraged the variational autoencoder (VAE) framework, integrating latent embeddings of encoded text with those of encoded poses, which are then decoded into motion predictions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib58\" title=\"\">2020</a>; Ahuja &amp; Morency, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib1\" title=\"\">2019</a>; Athanasiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib4\" title=\"\">2022</a>; Petrovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib45\" title=\"\">2022</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib15\" title=\"\">2022a</a>)</cite>.\nOther methods have investigated the potential of recurrent networks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib33\" title=\"\">2018</a>; Plappert et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib47\" title=\"\">2018</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib70\" title=\"\">2020</a>)</cite>, generative adversarial networks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib7\" title=\"\">2018</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib57\" title=\"\">2020</a>; Tulyakov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib54\" title=\"\">2018</a>)</cite>, or transformer networks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tevet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib51\" title=\"\">2022</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib35\" title=\"\">2023b</a>; Bhattacharya et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib5\" title=\"\">2021</a>; Petrovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib44\" title=\"\">2021</a>)</cite> to enhance the motion regression quality.\nBuilding on the success of diffusion models, recent approaches have begun to integrate the diffusion process into motion diffusion&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib22\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib12\" title=\"\">2023</a>; Tevet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib52\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib68\" title=\"\">2024b</a>; Dabral et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib13\" title=\"\">2023</a>; Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib40\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib66\" title=\"\">2023b</a>; Ribeiro-Gomes et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib48\" title=\"\">2024</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib62\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib56\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib67\" title=\"\">2023c</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib72\" title=\"\">2024d</a>; Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, yielding impressive results.\nIn the discrete classification domain, the input motion undergoes initial encoding via a VQ-VAE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Van Den&#160;Oord et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib55\" title=\"\">2017</a>)</cite>, producing motion tokens for subsequent prediction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib74\" title=\"\">2023</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib32\" title=\"\">2024e</a>)</cite>.\nDrawing inspiration from advancements in natural language processing, some methods utilize autoregressive modeling to predict tokens sequentially&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib16\" title=\"\">2022b</a>; Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib40\" title=\"\">2023</a>; Zou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib80\" title=\"\">2024</a>)</cite>.\nOthers employ generative masked modeling strategies, with tokens randomly masked during training for the model to predict&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib17\" title=\"\">2024</a>; Pinyoanuntapong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib46\" title=\"\">2024</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib61\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib28\" title=\"\">2023b</a>)</cite>.\nMore recently, large language models (LLMs) have been harnessed to help the prediction process, considering their large-scale pretraining&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib71\" title=\"\">2023d</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib77\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib31\" title=\"\">2024d</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib29\" title=\"\">2023c</a>)</cite>.\nIn this work, we seek to integrate the most effective elements from these two paths: the continuous diffusion and the masked autoregressive modeling.\nA previous attempt in this direction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib42\" title=\"\">2024</a>)</cite> directly transfers the MAR in image generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>)</cite> into motion generation without considering the difference between image and motion spaces, especially the temporal correlation. Also, its framework is only designed for body-only motion generation. Differently, we propose a new MAR mechanism that is especially designed for whole-body motion generation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "textbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to text, there are many other signals that various human motions are conditioned on, such as speech and music.\nIn the realm of speech-to-gesture generation, both continuous regression and discrete classification paths have been explored.\nIn the continuous domain, methods employ deep generative models like GANs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ahuja et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib2\" title=\"\">2022</a>)</cite>, normalizing flows&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib50\" title=\"\">2024</a>)</cite>, and diffusion models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Alexanderson et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib3\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib10\" title=\"\">2024a</a>; He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib20\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib59\" title=\"\">2023</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib78\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib11\" title=\"\">2024b</a>)</cite> to learn complex motion distributions in the speech data.\nIn the discrete domain, methods leverage either the autoregressive modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib60\" title=\"\">2023</a>)</cite> or the masked modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib37\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib38\" title=\"\">b</a>)</cite> to predict the discrete tokens quantified by the VQ-VAE.\nThe primary distinction among these methods lies in their specific handling of different parts of human motion, including body movements, hand gestures, and facial expressions.\nSimilarly, in the realm of music-to-dance generation, there are also methods in both the continuous domain&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhuang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib79\" title=\"\">2022</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib53\" title=\"\">2023</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib25\" title=\"\">2024a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib21\" title=\"\">2024</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib64\" title=\"\">2024a</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite> and the discrete domain&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Siyao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib49\" title=\"\">2022</a>)</cite>.\nThe discrete autoregressive model is leveraged after the motion quantization with VQ-VAE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Siyao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib49\" title=\"\">2022</a>)</cite>.\nMore methods harness the diffusion model to directly regress the target dancing motion in the continuous space&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib53\" title=\"\">2023</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib25\" title=\"\">2024a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib21\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite>.\nRecent methods also start to merge autoregressive and diffusion models, producing coherent and music-aligned dance sequences&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib64\" title=\"\">2024a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "different",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent works have begun to seek multimodal solutions, i.e., designing one framework for motion generation from different input signals&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib41\" title=\"\">2024</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib69\" title=\"\">2024c</a>; Zhou &amp; Wang, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib75\" title=\"\">2023</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib76\" title=\"\">2023</a>; Ling et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib36\" title=\"\">2023</a>; Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib30\" title=\"\">2024c</a>)</cite>.\nSome methods in the discrete domain attempt to incorporate quantized condition tokens into the vocabulary of the generation model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib41\" title=\"\">2024</a>; Zhou &amp; Wang, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib75\" title=\"\">2023</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib76\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib73\" title=\"\">2025</a>)</cite>, while some methods in the continuous domain try to integrate the multimodal signals by designing the motion ControlNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ling et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib36\" title=\"\">2023</a>; Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, where the multimodal conditions guide the sampling of a pretrained text-to-motion diffusion model.\nHowever, most previous methods are restricted by the varying motion data of different modalities, limiting multi-modal frameworks primarily to body-only motion generation.\nTo overcome this, MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite> standardizes datasets across modalities into a unified whole-body format that includes body, hands, and face.\nIn this work, we follow this unified representation to build a whole-body multi-modal motion framework, taking advantage of continuous masked auto-regression.</p>\n\n",
                "matched_terms": [
                    "different",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The overview of our framework is illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S2.F2\" title=\"Figure 2 &#8227; Multimodal Motion Generation. &#8227; 2 Related Work &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, which is divided into three main stages:\nIn the first stage, we encode the input motion with an autoencoder, which generates continuous motion tokens.\nIn the second stage, we focus on the text-based motion generation, utilizing our motion masked autoregressive framework to model the motion generation process.\nIn this stage, an autoregressive transformer is employed to predict the masked tokens, within which a gated linear mechanism is designed, and an RMSNorm module is employed.\nThe text information is integrated into the transformer via AdaLN after encoding.\nAfter the transformer, the generated embedding is fed into the DiT modules as the condition to diffuse towards the target token.\nIn the third stage, we extend the model learned in the previous stage to the multi-modal structure.\nThis involves merging the text embedding with multimodal signal embeddings&#8212;specifically speech or music&#8212;prior to their AdaLN input.\nFurthermore, we inject the multimodal embedding through a cross-attention module into the masked transformer.\nIn the multimodal learning stage, the DiT module is kept in the same structure and frozen.</p>\n\n",
                "matched_terms": [
                    "textbased",
                    "dit",
                    "model",
                    "gated",
                    "linear",
                    "rmsnorm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To feed the human motion into the transformer, we start by encoding the original motion into motion tokens.\nGiven a motion sequence <math alttext=\"\\{\\mathbf{M}_{t}\\}_{t=1}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>&#119820;</mi><mi>t</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\{\\mathbf{M}_{t}\\}_{t=1}^{T}</annotation></semantics></math>, the objective of an autoencoder is to extract a latent code <math alttext=\"z_{\\text{AE}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>z</mi><mtext>AE</mtext></msub><annotation encoding=\"application/x-tex\">z_{\\text{AE}}</annotation></semantics></math> that optimally captures the essence of the original motion.\nDifferent from most previous motion generation methods with autoregressive transformers, we use a continuous autoencoder rather than the VQ-VAE to do the encoding, which avoids the precision loss associated with the quantization approximation.\nIn the encoder, we stack the 1D convolution networks with ReLU activation to do the feature processing. Following this, two down-sampling residual blocks are applied to reduce the motion feature size to one-fourth of its original dimensions.\nIn the decoder, the same structure in the reversed order is utilized to up-sample the motion feature back to the original size, producing <math alttext=\"\\{\\hat{\\mathbf{M}}_{t}\\}_{t=1}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mover accent=\"true\"><mi>&#119820;</mi><mo>^</mo></mover><mi>t</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\{\\hat{\\mathbf{M}}_{t}\\}_{t=1}^{T}</annotation></semantics></math>. Therefore, the loss for training the autoencoder is defined as</p>\n\n",
                "matched_terms": [
                    "different",
                    "precision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the continuous motion tokens, we design a masked autoregressive transformer to model the motion generation, effectively capturing the temporal relationship between different tokens and producing the rich contextual condition <math alttext=\"\\mathbf{z}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><msup><mi>&#119859;</mi><mi>i</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{z}^{i}</annotation></semantics></math> for the subsequent diffusion process.\nWe first randomly mask the motion tokens following language models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib14\" title=\"\">2018</a>)</cite>, obtaining some masked tokens <math alttext=\"\\{\\tilde{\\mathbf{x}}^{i}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msup><mover accent=\"true\"><mi>&#119857;</mi><mo>~</mo></mover><mi>i</mi></msup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\tilde{\\mathbf{x}}^{i}\\}</annotation></semantics></math>.\nThe temporal masking strategies adopt the same mask ratio schedule following&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib8\" title=\"\">2022</a>)</cite>, and are computed as</p>\n\n",
                "matched_terms": [
                    "different",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After the masking, unlike previous MAR methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>; Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib42\" title=\"\">2024</a>)</cite>, our approach does not involve random rearranging of tokens or batch-tokens prediction. Also, we do not perform bidirectional attention.\nIn contrast, we maintain the temporal order of the original motion, and sequentially undertake autoregressive prediction, thus forming a causal attention manner, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S2.F2\" title=\"Figure 2 &#8227; Multimodal Motion Generation. &#8227; 2 Related Work &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "attention",
                    "causal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a gated linear attention mechanism within the transformer to regulate the attention weights at each time step. Specifically, we compute a gating signal by applying a linear transformation <math alttext=\"g_{o}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><msub><mi>g</mi><mi>o</mi></msub><annotation encoding=\"application/x-tex\">g_{o}</annotation></semantics></math> to the input <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> followed by a sigmoid activation function. This gating signal acts as a dynamic filter, adjusting the output of the attention module based on the relevance of the input features. Consequently, during the attention computation, the final output <math alttext=\"o\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mi>o</mi><annotation encoding=\"application/x-tex\">o</annotation></semantics></math> is modulated by this gating signal, enabling the model to selectively focus on the most pertinent action frames.</p>\n\n",
                "matched_terms": [
                    "attention",
                    "linear",
                    "model",
                    "gated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This mechanism effectively serves as an adaptive feature selector, allowing the model to disregard less relevant frames and suppress redundant action frames (such as stationary or repetitive motions), thereby enhancing its attention to key actions (e.g., gesture transitions and changes in motion direction). Furthermore, by dynamically adjusting the attention distribution through gating, the model is capable of selectively retaining historical frames or predicting future frames based on the current action state.</p>\n\n",
                "matched_terms": [
                    "attention",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our objective is to construct a unified model that can simultaneously perform text-to-motion, speech-to-gesture, and music-to-dance tasks. Therefore, we aim to ensure that during the fine-tuning process across different datasets, the model does not suffer from instability that could lead to catastrophic failures. RMSNorm&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang &amp; Sennrich, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib63\" title=\"\">2019</a>)</cite> is particularly advantageous in scenarios with features exhibiting a large dynamic range, especially in tasks where the input distributions are highly heterogeneous. This characteristic enables the model to maintain stability when faced with diverse types of inputs or when observing uneven feature distributions. Additionally, RMSNorm has the potential to mitigate the gradient instability that may arise from significant motion variations, such as sudden jumps or rapid turns.</p>\n\n",
                "matched_terms": [
                    "different",
                    "model",
                    "rmsnorm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to previous works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>; Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib42\" title=\"\">2024</a>)</cite>, we adopt Diffusion Transformer (DiT) as our diffusion model. While the use of DiT may incur additional time costs during training and inference (not much due to the compact structure of motion data), it significantly enhances the quality of generated outputs. Compared to MLPs, DiT provides greater convenience in injecting conditional control signals. During the training process of multimodal generation, we freeze the diffusion model and only fine-tune the masked transformer. The structural characteristics of DiT facilitate this approach, enabling it to better handle various types of conditional signals.</p>\n\n",
                "matched_terms": [
                    "dit",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, DiT demonstrates superior performance in complex multimodal generation contexts. Its enhanced generalization capabilities allow it to flexibly handle a variety of input signal types, rather than being confined to a single data format. This ensures that the model exhibits increased adaptability and reliability when exposed to diverse data, ultimately resulting in higher-quality outcomes.</p>\n\n",
                "matched_terms": [
                    "dit",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first pre-train the model on text-motion paired data in a text-to-motion generation setting. Owing to its strong semantic expressiveness and cross-modal alignment properties, we adopt text as a shared conditional signal across diverse unimodal datasets, enabling the model to learn sequence-level generation capabilities between text and motion, as well as a coarse-grained textual guidance mechanism for generative control.</p>\n\n",
                "matched_terms": [
                    "model",
                    "setting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This procedure yields the trained model <math alttext=\"\\mathcal{M}_{t2m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{M}_{t2m}</annotation></semantics></math>. When incorporating additional control signals, we initialize the entire model with the parameters of model <math alttext=\"\\mathcal{M}_{t2m}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>2</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{M}_{t2m}</annotation></semantics></math>, freeze the DiT, and fine-tune only the masked transformer. Crucially, in contrast to the original design, we introduce cross-attention layers within the transformer to explicitly model interactions between the control signals and the motion sequence. This modification aims to produce more precise, fine-grained control representations, thereby enhancing both the quality and controllability of the generated motions.</p>\n\n",
                "matched_terms": [
                    "dit",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When performing speech-to-gesture and music-to-dance tasks, the speech and music modalities are processed through cross-attention mechanisms within the transformer, interacting with the textual features. This allows the model to generate more fine-grained conditional signals that are temporally aligned with the text.\nThe predicted latents are then fed into the DiT to refine and generate the final motion sequence. The sampling process can be denoted as:</p>\n\n",
                "matched_terms": [
                    "dit",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation Details. </span>\nOur model is implemented on one NVIDIA V100 GPU using PyTorch. For our method, the autoencoder employs a ResNet-based&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib19\" title=\"\">2016</a>)</cite> three-layer encoder-decoder architecture with a hidden dimension of 512 and an overall downsampling rate of 4. For the generation process, we utilize a four-layer AdaLN-zero transformer encoder as the masked autoregressive transformer, featuring a hidden dimension of 1024 and 16 attention heads. The diffusion model consists of 4 layers of DiT, where each transformer block has a hidden dimension of 1792 and 8 attention heads.\nWe adopt the AdamW optimizer <math alttext=\"(\\beta_{1}=0.9,\\beta_{2}=0.99)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><mo>,</mo><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.99</mn></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\beta_{1}=0.9,\\beta_{2}=0.99)</annotation></semantics></math>. For training the autoencoder on the HumanML subset of Motion-X, we use a batch size of 256 and a maximum sequence length of 64 frames. For the text-to-motion task, the batch size is set to 50 with a maximum sequence length of 196 frames. The learning rate is initialized at 0.0002 with a linear warmup over 2000 steps. The autoencoder is trained for 50 epochs, while the text-to-motion task is trained for 1500 epochs.\nDuring multimodal generation, we first initialize the model with pretrained weights from the text-to-motion autoencoder and fine-tune it on task-specific datasets. Subsequently, we freeze the parameters of the text-to-motion DiT and only fine-tune the masked transformer along with newly incorporated cross-attention layers. The learning rate and training schedule remain consistent with the text-to-motion task.\nFor all three tasks, we employ exponential moving average (EMA) to update model parameters, ensuring training stability. During inference, the classifier-free guidance (CFG) scale is set to 4.5 for text-to-motion, while other tasks use a CFG scale of 6.5.</p>\n\n",
                "matched_terms": [
                    "attention",
                    "dit",
                    "linear",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets and Metrics. </span>\nFor the evaluation, we utilize three datasets: HumanML3D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib15\" title=\"\">2022a</a>)</cite> for text-to-motion, BEAT2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib37\" title=\"\">2024a</a>)</cite> for speech-to-gesture, and FineDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite> for music-to-dance, all following the unified SMPL-X representation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>.\nRegarding the metrics, we use FID, R-Precision, and MM-Dist for text-based motion generation, use FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">H</span></sub>, FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">B</span></sub>, Face L2 loss, Beat Alignment Score, Diversity for speech-based motion generation, and use FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">H</span></sub>, FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">B</span></sub>, Diversity for music-based motion generation, respectively.\nFor the detailed explanation of the datasets and metrics, please refer to the appendix.</p>\n\n",
                "matched_terms": [
                    "textbased",
                    "fidh",
                    "fidb",
                    "speechbased",
                    "fid"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-based motion generation. </span>\nWe conduct an evaluation of our model against prior text-to-motion approaches, including both discrete-domain and continuous-domain methods.\nAs summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S3.T1\" title=\"Table 1 &#8227; 3.5 Text-to-motion Pretraining and Multimodal Control Adaptation &#8227; 3 Method &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our method clearly surpasses prior techniques on the HumanML subset of Motion-X dataset. Remarkably, our model achieves improvements of 19.3%, 13.5%, and 11.7% in R-Precision for Top-1, 2, 3, respectively. Additionally, we enhance the FID score by 75.2% on this dataset, underscoring the exceptional fidelity of our generated motions. The qualitative results in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S3.F4\" title=\"Figure 4 &#8227; 3.5 Text-to-motion Pretraining and Multimodal Control Adaptation &#8227; 3 Method &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> further support these findings, showing that our approach yields whole-body motions that align closely with the input text.</p>\n\n",
                "matched_terms": [
                    "top1",
                    "model",
                    "fid",
                    "textbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-based motion generation. </span>\nTo assess the speech-driven motion generation, we compare to previous speech-to-gesture methods.\nOur results, summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S3.T2\" title=\"Table 2 &#8227; 3.6 Inference &#8227; 3 Method &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, reveal that our method achieves good quality and diversity in both hand and body motion generation and excels in aligning\nwith the rhythm of first-person speech.\nThis demonstrates the effectiveness of our framework in motion generation when encompassing different modal signals.\nHowever, our method performs worse than single-modal methods.\nAs discussed in &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, this is attributed to the random or average expressions in the Motion-X dataset, which confuses the speech-to-gesture training.</p>\n\n",
                "matched_terms": [
                    "different",
                    "speechbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper proposes a new omni motion framework for multimodal whole-body human motion generation.\nWithin this one framework, text, speech, and music signals are all encompassed through AdaLN and cross-attention.\nThe motion generation process is modeled by a continuous masked autoregressive transformer with causal attention, as well as a DiT structure.\nExtensive experiments have been conducted to verify the efficacy of the proposed framework in different-modality tasks.</p>\n\n",
                "matched_terms": [
                    "attention",
                    "dit",
                    "causal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For text-based motion generation, we evaluate our method on the HumanML3D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib15\" title=\"\">2022a</a>)</cite> dataset, which consists of 14,616 high-quality human motions paired with 44,970 text descriptions.\nThe original body-only SMPL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Loper et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib39\" title=\"\">2023</a>)</cite> format of this dataset is extended to whole-body SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pavlakos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib43\" title=\"\">2019</a>)</cite> format in MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, which we follow in the experiments for evaluation.\nFor speech-based motion generation, we evaluate on the BEAT2 dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib37\" title=\"\">2024a</a>)</cite>, which collects 76 hours of data from 30 speakers, standardized into a mesh representation with paired audio and text lines.\nThe motion of the unified SMPL-X format is also extracted&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite> for multimodal evaluation.\nFor music-based motion generation, the largest dataset FineDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite> is utilized for evaluation. This dataset collects dances of 14.6 hours across 22 genres and provides detailed human motions using the SMPL-H format, which is then converted to the unified SMPL-X format and appended by text descriptions.</p>\n\n",
                "matched_terms": [
                    "speechbased",
                    "textbased"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluating the quality and diversity of the motions generated based on speech, we employ FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">H</span></sub>, FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">B</span></sub>, and Diversity metrics. FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">H</span></sub> measures the difference between hand motion distribution and the true gesture distribution, whereas FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">B</span></sub> assesses the divergence between the whole-body motion distributions. The Beat Alignment Score&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib26\" title=\"\">2021</a>)</cite> is used to measure the synchronization between motions and speech beats. To quantify the difference between generated expressions and actual expressions, we use the L2 Loss.</p>\n\n",
                "matched_terms": [
                    "fidb",
                    "fidh"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mirroring the approach used for speech-driven gesture generation, we apply FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">H</span></sub>, FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">B</span></sub>, and Diversity metrics to evaluate the quality and diversity of music-induced hand and whole-body movements. This approach ensures that the generated motions exhibit both high fidelity and variation.</p>\n\n",
                "matched_terms": [
                    "fidb",
                    "fidh"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a comprehensive evaluation on the final model (after fine-tuning on both speech-to-gesture and music-to-dance datasets) and present the results below. Since textual conditioning participates throughout the entire training pipeline, our model does not suffer from catastrophic forgetting after fine-tuning. This confirms the robustness of our architecture&#8217;s knowledge retention capabilities under different training paradigms.</p>\n\n",
                "matched_terms": [
                    "different",
                    "model"
                ]
            }
        ]
    },
    "A1.T5": {
        "caption": "Table 5: Results of text-to-motion on the original HumanML3D benchmark.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Method</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">R Precision</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">FID <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">MM Dist<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Div <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Top-1 <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Top-2 <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Top-3 <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">GT</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.511^{\\pm{0.003}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m7\" intent=\":literal\"><semantics><msup><mn>0.511</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.511^{\\pm{0.003}}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.703^{\\pm{0.003}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m8\" intent=\":literal\"><semantics><msup><mn>0.703</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.703^{\\pm{0.003}}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.797^{\\pm{0.002}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m9\" intent=\":literal\"><semantics><msup><mn>0.797</mn><mrow><mo>&#177;</mo><mn>0.002</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.797^{\\pm{0.002}}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.002^{\\pm{0.000}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m10\" intent=\":literal\"><semantics><msup><mn>0.002</mn><mrow><mo>&#177;</mo><mn>0.000</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.002^{\\pm{0.000}}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"9.503^{\\pm{0.065}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m11\" intent=\":literal\"><semantics><msup><mn>9.503</mn><mrow><mo>&#177;</mo><mn>0.065</mn></mrow></msup><annotation encoding=\"application/x-tex\">9.503^{\\pm{0.065}}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"2.974^{\\pm{0.008}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m12\" intent=\":literal\"><semantics><msup><mn>2.974</mn><mrow><mo>&#177;</mo><mn>0.008</mn></mrow></msup><annotation encoding=\"application/x-tex\">2.974^{\\pm{0.008}}</annotation></semantics></math></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">MDM<cite class=\"ltx_cite ltx_citemacro_citep\">(Tevet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib52\" title=\"\">2023</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.418^{\\pm{0.005}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m13\" intent=\":literal\"><semantics><msup><mn>0.418</mn><mrow><mo>&#177;</mo><mn>0.005</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.418^{\\pm{0.005}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.604^{\\pm{0.005}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m14\" intent=\":literal\"><semantics><msup><mn>0.604</mn><mrow><mo>&#177;</mo><mn>0.005</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.604^{\\pm{0.005}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.707^{\\pm{0.004}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m15\" intent=\":literal\"><semantics><msup><mn>0.707</mn><mrow><mo>&#177;</mo><mn>0.004</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.707^{\\pm{0.004}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.489^{\\pm{0.025}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m16\" intent=\":literal\"><semantics><msup><mn>0.489</mn><mrow><mo>&#177;</mo><mn>0.025</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.489^{\\pm{0.025}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"9.450^{\\pm{0.066}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m17\" intent=\":literal\"><semantics><msup><mn>9.450</mn><mrow><mo>&#177;</mo><mn>0.066</mn></mrow></msup><annotation encoding=\"application/x-tex\">9.450^{\\pm{0.066}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"3.630^{\\pm{0.023}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m18\" intent=\":literal\"><semantics><msup><mn>3.630</mn><mrow><mo>&#177;</mo><mn>0.023</mn></mrow></msup><annotation encoding=\"application/x-tex\">3.630^{\\pm{0.023}}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">MotionDiffuse<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib68\" title=\"\">2024b</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.491^{\\pm{0.001}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m19\" intent=\":literal\"><semantics><msup><mn>0.491</mn><mrow><mo>&#177;</mo><mn>0.001</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.491^{\\pm{0.001}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.681^{\\pm{0.001}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m20\" intent=\":literal\"><semantics><msup><mn>0.681</mn><mrow><mo>&#177;</mo><mn>0.001</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.681^{\\pm{0.001}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.782^{\\pm{0.001}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m21\" intent=\":literal\"><semantics><msup><mn>0.782</mn><mrow><mo>&#177;</mo><mn>0.001</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.782^{\\pm{0.001}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.630^{\\pm{0.001}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m22\" intent=\":literal\"><semantics><msup><mn>0.630</mn><mrow><mo>&#177;</mo><mn>0.001</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.630^{\\pm{0.001}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"9.410^{\\pm{0.049}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m23\" intent=\":literal\"><semantics><msup><mn>9.410</mn><mrow><mo>&#177;</mo><mn>0.049</mn></mrow></msup><annotation encoding=\"application/x-tex\">9.410^{\\pm{0.049}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"3.113^{\\pm{0.001}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m24\" intent=\":literal\"><semantics><msup><mn>3.113</mn><mrow><mo>&#177;</mo><mn>0.001</mn></mrow></msup><annotation encoding=\"application/x-tex\">3.113^{\\pm{0.001}}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">FineMoGen<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib67\" title=\"\">2023c</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.504^{\\pm{0.002}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m25\" intent=\":literal\"><semantics><msup><mn>0.504</mn><mrow><mo>&#177;</mo><mn>0.002</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.504^{\\pm{0.002}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.690^{\\pm{0.002}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m26\" intent=\":literal\"><semantics><msup><mn>0.690</mn><mrow><mo>&#177;</mo><mn>0.002</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.690^{\\pm{0.002}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.784^{\\pm{0.002}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m27\" intent=\":literal\"><semantics><msup><mn>0.784</mn><mrow><mo>&#177;</mo><mn>0.002</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.784^{\\pm{0.002}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.151^{\\pm{0.008}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m28\" intent=\":literal\"><semantics><msup><mn>0.151</mn><mrow><mo>&#177;</mo><mn>0.008</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.151^{\\pm{0.008}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"9.263^{\\pm{0.094}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m29\" intent=\":literal\"><semantics><msup><mn>9.263</mn><mrow><mo>&#177;</mo><mn>0.094</mn></mrow></msup><annotation encoding=\"application/x-tex\">9.263^{\\pm{0.094}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"2.998^{\\pm{0.008}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m30\" intent=\":literal\"><semantics><msup><mn>2.998</mn><mrow><mo>&#177;</mo><mn>0.008</mn></mrow></msup><annotation encoding=\"application/x-tex\">2.998^{\\pm{0.008}}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Motion-Verse<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib69\" title=\"\">2024c</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.496^{\\pm{0.002}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m31\" intent=\":literal\"><semantics><msup><mn>0.496</mn><mrow><mo>&#177;</mo><mn>0.002</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.496^{\\pm{0.002}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.685^{\\pm{0.002}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m32\" intent=\":literal\"><semantics><msup><mn>0.685</mn><mrow><mo>&#177;</mo><mn>0.002</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.685^{\\pm{0.002}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.785^{\\pm{0.002}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m33\" intent=\":literal\"><semantics><msup><mn>0.785</mn><mrow><mo>&#177;</mo><mn>0.002</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.785^{\\pm{0.002}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.415^{\\pm{0.002}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m34\" intent=\":literal\"><semantics><msup><mn>0.415</mn><mrow><mo>&#177;</mo><mn>0.002</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.415^{\\pm{0.002}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"9.176^{\\pm{0.074}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m35\" intent=\":literal\"><semantics><msup><mn>9.176</mn><mrow><mo>&#177;</mo><mn>0.074</mn></mrow></msup><annotation encoding=\"application/x-tex\">9.176^{\\pm{0.074}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"3.087^{\\pm{0.012}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m36\" intent=\":literal\"><semantics><msup><mn>3.087</mn><mrow><mo>&#177;</mo><mn>0.012</mn></mrow></msup><annotation encoding=\"application/x-tex\">3.087^{\\pm{0.012}}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">MCM<cite class=\"ltx_cite ltx_citemacro_citep\">(Ling et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib36\" title=\"\">2023</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.494^{\\pm{0.003}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m37\" intent=\":literal\"><semantics><msup><mn>0.494</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.494^{\\pm{0.003}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.682^{\\pm{0.005}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m38\" intent=\":literal\"><semantics><msup><mn>0.682</mn><mrow><mo>&#177;</mo><mn>0.005</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.682^{\\pm{0.005}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.777^{\\pm{0.003}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m39\" intent=\":literal\"><semantics><msup><mn>0.777</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.777^{\\pm{0.003}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.075^{\\pm{0.003}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m40\" intent=\":literal\"><semantics><msup><mn>0.075</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.075^{\\pm{0.003}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"9.484^{\\pm{0.074}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m41\" intent=\":literal\"><semantics><msup><mn>9.484</mn><mrow><mo>&#177;</mo><mn>0.074</mn></mrow></msup><annotation encoding=\"application/x-tex\">9.484^{\\pm{0.074}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"3.086^{\\pm{0.011}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m42\" intent=\":literal\"><semantics><msup><mn>3.086</mn><mrow><mo>&#177;</mo><mn>0.011</mn></mrow></msup><annotation encoding=\"application/x-tex\">3.086^{\\pm{0.011}}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.501^{\\pm{0.003}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m43\" intent=\":literal\"><semantics><msup><mn>0.501</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.501^{\\pm{0.003}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.697^{\\pm{0.003}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m44\" intent=\":literal\"><semantics><msup><mn>0.697</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.697^{\\pm{0.003}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.796^{\\pm{0.002}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m45\" intent=\":literal\"><semantics><msup><mn>0.796</mn><mrow><mo>&#177;</mo><mn>0.002</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.796^{\\pm{0.002}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.173^{\\pm{0.002}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m46\" intent=\":literal\"><semantics><msup><mn>0.173</mn><mrow><mo>&#177;</mo><mn>0.002</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.173^{\\pm{0.002}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"9.543^{\\pm{0.098}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m47\" intent=\":literal\"><semantics><msup><mn>9.543</mn><mrow><mo>&#177;</mo><mn>0.098</mn></mrow></msup><annotation encoding=\"application/x-tex\">9.543^{\\pm{0.098}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"3.025^{\\pm{0.008}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m48\" intent=\":literal\"><semantics><msup><mn>3.025</mn><mrow><mo>&#177;</mo><mn>0.008</mn></mrow></msup><annotation encoding=\"application/x-tex\">3.025^{\\pm{0.008}}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">MARDM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib42\" title=\"\">2024</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.502^{\\pm{0.003}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m49\" intent=\":literal\"><semantics><msup><mn>0.502</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.502^{\\pm{0.003}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.691^{\\pm{0.003}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m50\" intent=\":literal\"><semantics><msup><mn>0.691</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.691^{\\pm{0.003}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.787^{\\pm{0.002}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m51\" intent=\":literal\"><semantics><msup><mn>0.787</mn><mrow><mo>&#177;</mo><mn>0.002</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.787^{\\pm{0.002}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.286^{\\pm{0.003}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m52\" intent=\":literal\"><semantics><msup><mn>0.286</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.286^{\\pm{0.003}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"9.470^{\\pm{0.081}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m53\" intent=\":literal\"><semantics><msup><mn>9.470</mn><mrow><mo>&#177;</mo><mn>0.081</mn></mrow></msup><annotation encoding=\"application/x-tex\">9.470^{\\pm{0.081}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"3.346^{\\pm{0.007}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m54\" intent=\":literal\"><semantics><msup><mn>3.346</mn><mrow><mo>&#177;</mo><mn>0.007</mn></mrow></msup><annotation encoding=\"application/x-tex\">3.346^{\\pm{0.007}}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Ours</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"{0.548}^{\\pm{0.003}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m55\" intent=\":literal\"><semantics><msup><mn>0.548</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">{0.548}^{\\pm{0.003}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"{0.743}^{\\pm{0.003}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m56\" intent=\":literal\"><semantics><msup><mn>0.743</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">{0.743}^{\\pm{0.003}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"{0.837}^{\\pm{0.002}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m57\" intent=\":literal\"><semantics><msup><mn>0.837</mn><mrow><mo>&#177;</mo><mn>0.002</mn></mrow></msup><annotation encoding=\"application/x-tex\">{0.837}^{\\pm{0.002}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"{0.141}^{\\pm{0.003}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m58\" intent=\":literal\"><semantics><msup><mn>0.141</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">{0.141}^{\\pm{0.003}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"{9.537}^{\\pm{0.087}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m59\" intent=\":literal\"><semantics><msup><mn>9.537</mn><mrow><mo>&#177;</mo><mn>0.087</mn></mrow></msup><annotation encoding=\"application/x-tex\">{9.537}^{\\pm{0.087}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"{2.856}^{\\pm{0.008}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T5.m60\" intent=\":literal\"><semantics><msup><mn>2.856</mn><mrow><mo>&#177;</mo><mn>0.008</mn></mrow></msup><annotation encoding=\"application/x-tex\">{2.856}^{\\pm{0.008}}</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "2023c",
            "0707±00040707pm0004",
            "0797±00020797pm0002",
            "2024c",
            "0496±00020496pm0002",
            "mardm",
            "0501±00030501pm0003",
            "3025±00083025pm0008",
            "0502±00030502pm0003",
            "0777±00030777pm0003",
            "3087±00123087pm0012",
            "0796±00020796pm0002",
            "top2",
            "2856±00082856pm0008",
            "0494±00030494pm0003",
            "results",
            "0837±00020837pm0002",
            "3113±00013113pm0001",
            "ours",
            "texttomotion",
            "0504±00020504pm0002",
            "precision",
            "method",
            "3086±00113086pm0011",
            "dist↓downarrow",
            "0681±00010681pm0001",
            "2024b",
            "0286±00030286pm0003",
            "9537±00879537pm0087",
            "0173±00020173pm0002",
            "0690±00020690pm0002",
            "→rightarrow",
            "0151±00080151pm0008",
            "9543±00989543pm0098",
            "9470±00819470pm0081",
            "0630±00010630pm0001",
            "mdmtevet",
            "0697±00030697pm0003",
            "0511±00030511pm0003",
            "0489±00250489pm0025",
            "0782±00010782pm0001",
            "3346±00073346pm0007",
            "div",
            "0785±00020785pm0002",
            "benchmark",
            "mcmling",
            "finemogenzhang",
            "0682±00050682pm0005",
            "9484±00749484pm0074",
            "0548±00030548pm0003",
            "0141±00030141pm0003",
            "top3",
            "0691±00030691pm0003",
            "9503±00659503pm0065",
            "bian",
            "↓downarrow",
            "3630±00233630pm0023",
            "0002±00000002pm0000",
            "0787±00020787pm0002",
            "9263±00949263pm0094",
            "9450±00669450pm0066",
            "0491±00010491pm0001",
            "0075±00030075pm0003",
            "9410±00499410pm0049",
            "↑uparrow",
            "motionversezhang",
            "motioncraft",
            "0604±00050604pm0005",
            "0743±00030743pm0003",
            "top1",
            "original",
            "meng",
            "0418±00050418pm0005",
            "0415±00020415pm0002",
            "2998±00082998pm0008",
            "0784±00020784pm0002",
            "0703±00030703pm0003",
            "9176±00749176pm0074",
            "humanml3d",
            "fid",
            "0685±00020685pm0002",
            "motiondiffusezhang",
            "2974±00082974pm0008"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To provide a more comprehensive evaluation, we conduct additional comparisons on the original HumanML3D benchmark using the body-only H3D format, which contains redundant motion information.\nHere we mainly compare with the methods without VQ-VAE.\nAs shown in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#A1.T5\" title=\"Table 5 &#8227; Text-based Motion Generation &#8227; A.3 Metrics &#8227; Appendix A Appendix &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, OmniMotion consistently outperforms these baselines in terms of text-motion alignment, motion quality, and diversity, demonstrating its superior generalization capability across different motion representations.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Whole-body multi-modal human motion generation poses two primary challenges: creating an effective motion generation mechanism and integrating various modalities, such as text, speech, and music, into a cohesive framework.\nUnlike previous methods that usually employ discrete masked modeling or autoregressive modeling, we develop a continuous masked autoregressive motion transformer, where a causal attention is performed considering the sequential nature within the human motion.\nWithin this transformer, we introduce a gated linear attention and an RMSNorm module, which drive the transformer to pay attention to the key actions and suppress the instability caused by either the abnormal movements or the heterogeneous distributions within multi-modalities.\nTo further enhance both the motion generation and the multimodal generalization, we employ the DiT structure to diffuse the conditions from the transformer towards the targets.\nTo fuse different modalities, AdaLN and cross-attention are leveraged to inject the text, speech, and music signals.\nExperimental results demonstrate that our framework outperforms previous methods across all modalities, including text-to-motion, speech-to-gesture, and music-to-dance.\nThe code of our method will be made public.</p>\n\n",
                "matched_terms": [
                    "method",
                    "texttomotion",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous research on text-based motion generation can be generally categorized into two mainstream paths: continuous regression and discrete classification.\nIn the continuous regression domain, numerous strategies have leveraged the variational autoencoder (VAE) framework, integrating latent embeddings of encoded text with those of encoded poses, which are then decoded into motion predictions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib58\" title=\"\">2020</a>; Ahuja &amp; Morency, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib1\" title=\"\">2019</a>; Athanasiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib4\" title=\"\">2022</a>; Petrovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib45\" title=\"\">2022</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib15\" title=\"\">2022a</a>)</cite>.\nOther methods have investigated the potential of recurrent networks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib33\" title=\"\">2018</a>; Plappert et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib47\" title=\"\">2018</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib70\" title=\"\">2020</a>)</cite>, generative adversarial networks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib7\" title=\"\">2018</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib57\" title=\"\">2020</a>; Tulyakov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib54\" title=\"\">2018</a>)</cite>, or transformer networks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tevet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib51\" title=\"\">2022</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib35\" title=\"\">2023b</a>; Bhattacharya et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib5\" title=\"\">2021</a>; Petrovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib44\" title=\"\">2021</a>)</cite> to enhance the motion regression quality.\nBuilding on the success of diffusion models, recent approaches have begun to integrate the diffusion process into motion diffusion&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib22\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib12\" title=\"\">2023</a>; Tevet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib52\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib68\" title=\"\">2024b</a>; Dabral et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib13\" title=\"\">2023</a>; Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib40\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib66\" title=\"\">2023b</a>; Ribeiro-Gomes et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib48\" title=\"\">2024</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib62\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib56\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib67\" title=\"\">2023c</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib72\" title=\"\">2024d</a>; Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, yielding impressive results.\nIn the discrete classification domain, the input motion undergoes initial encoding via a VQ-VAE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Van Den&#160;Oord et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib55\" title=\"\">2017</a>)</cite>, producing motion tokens for subsequent prediction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib74\" title=\"\">2023</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib32\" title=\"\">2024e</a>)</cite>.\nDrawing inspiration from advancements in natural language processing, some methods utilize autoregressive modeling to predict tokens sequentially&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib16\" title=\"\">2022b</a>; Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib40\" title=\"\">2023</a>; Zou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib80\" title=\"\">2024</a>)</cite>.\nOthers employ generative masked modeling strategies, with tokens randomly masked during training for the model to predict&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib17\" title=\"\">2024</a>; Pinyoanuntapong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib46\" title=\"\">2024</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib61\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib28\" title=\"\">2023b</a>)</cite>.\nMore recently, large language models (LLMs) have been harnessed to help the prediction process, considering their large-scale pretraining&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib71\" title=\"\">2023d</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib77\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib31\" title=\"\">2024d</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib29\" title=\"\">2023c</a>)</cite>.\nIn this work, we seek to integrate the most effective elements from these two paths: the continuous diffusion and the masked autoregressive modeling.\nA previous attempt in this direction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib42\" title=\"\">2024</a>)</cite> directly transfers the MAR in image generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>)</cite> into motion generation without considering the difference between image and motion spaces, especially the temporal correlation. Also, its framework is only designed for body-only motion generation. Differently, we propose a new MAR mechanism that is especially designed for whole-body motion generation.</p>\n\n",
                "matched_terms": [
                    "2023c",
                    "2024b",
                    "meng",
                    "results",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent works have begun to seek multimodal solutions, i.e., designing one framework for motion generation from different input signals&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib41\" title=\"\">2024</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib69\" title=\"\">2024c</a>; Zhou &amp; Wang, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib75\" title=\"\">2023</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib76\" title=\"\">2023</a>; Ling et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib36\" title=\"\">2023</a>; Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib30\" title=\"\">2024c</a>)</cite>.\nSome methods in the discrete domain attempt to incorporate quantized condition tokens into the vocabulary of the generation model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib41\" title=\"\">2024</a>; Zhou &amp; Wang, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib75\" title=\"\">2023</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib76\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib73\" title=\"\">2025</a>)</cite>, while some methods in the continuous domain try to integrate the multimodal signals by designing the motion ControlNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ling et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib36\" title=\"\">2023</a>; Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, where the multimodal conditions guide the sampling of a pretrained text-to-motion diffusion model.\nHowever, most previous methods are restricted by the varying motion data of different modalities, limiting multi-modal frameworks primarily to body-only motion generation.\nTo overcome this, MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite> standardizes datasets across modalities into a unified whole-body format that includes body, hands, and face.\nIn this work, we follow this unified representation to build a whole-body multi-modal motion framework, taking advantage of continuous masked auto-regression.</p>\n\n",
                "matched_terms": [
                    "texttomotion",
                    "motioncraft",
                    "2024c",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To feed the human motion into the transformer, we start by encoding the original motion into motion tokens.\nGiven a motion sequence <math alttext=\"\\{\\mathbf{M}_{t}\\}_{t=1}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>&#119820;</mi><mi>t</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\{\\mathbf{M}_{t}\\}_{t=1}^{T}</annotation></semantics></math>, the objective of an autoencoder is to extract a latent code <math alttext=\"z_{\\text{AE}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>z</mi><mtext>AE</mtext></msub><annotation encoding=\"application/x-tex\">z_{\\text{AE}}</annotation></semantics></math> that optimally captures the essence of the original motion.\nDifferent from most previous motion generation methods with autoregressive transformers, we use a continuous autoencoder rather than the VQ-VAE to do the encoding, which avoids the precision loss associated with the quantization approximation.\nIn the encoder, we stack the 1D convolution networks with ReLU activation to do the feature processing. Following this, two down-sampling residual blocks are applied to reduce the motion feature size to one-fourth of its original dimensions.\nIn the decoder, the same structure in the reversed order is utilized to up-sample the motion feature back to the original size, producing <math alttext=\"\\{\\hat{\\mathbf{M}}_{t}\\}_{t=1}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mover accent=\"true\"><mi>&#119820;</mi><mo>^</mo></mover><mi>t</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\{\\hat{\\mathbf{M}}_{t}\\}_{t=1}^{T}</annotation></semantics></math>. Therefore, the loss for training the autoencoder is defined as</p>\n\n",
                "matched_terms": [
                    "original",
                    "precision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After the masking, unlike previous MAR methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>; Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib42\" title=\"\">2024</a>)</cite>, our approach does not involve random rearranging of tokens or batch-tokens prediction. Also, we do not perform bidirectional attention.\nIn contrast, we maintain the temporal order of the original motion, and sequentially undertake autoregressive prediction, thus forming a causal attention manner, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S2.F2\" title=\"Figure 2 &#8227; Multimodal Motion Generation. &#8227; 2 Related Work &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "2024b",
                    "original",
                    "meng"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to previous works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>; Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib42\" title=\"\">2024</a>)</cite>, we adopt Diffusion Transformer (DiT) as our diffusion model. While the use of DiT may incur additional time costs during training and inference (not much due to the compact structure of motion data), it significantly enhances the quality of generated outputs. Compared to MLPs, DiT provides greater convenience in injecting conditional control signals. During the training process of multimodal generation, we freeze the diffusion model and only fine-tune the masked transformer. The structural characteristics of DiT facilitate this approach, enabling it to better handle various types of conditional signals.</p>\n\n",
                "matched_terms": [
                    "2024b",
                    "meng"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, MLPs exhibit notable limitations when processing heterogeneous data. This incapacity results in suboptimal performance when confronted with diverse signal types, such as speech and music. Due to the relatively small number of parameters in MLPs, they are prone to overfitting on specific datasets (e.g., text-to-motion). This situation can be analogized to a dancer who is adept only in a single dance style; when asked to incorporate other styles, they appear clumsy and ineffective. Consequently, when we attempt to fine-tune MLPs on a different dataset, they are ill-equipped to adapt to the challenges posed by new signals, leading to failures in multimodal generation tasks.</p>\n\n",
                "matched_terms": [
                    "texttomotion",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation Details. </span>\nOur model is implemented on one NVIDIA V100 GPU using PyTorch. For our method, the autoencoder employs a ResNet-based&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib19\" title=\"\">2016</a>)</cite> three-layer encoder-decoder architecture with a hidden dimension of 512 and an overall downsampling rate of 4. For the generation process, we utilize a four-layer AdaLN-zero transformer encoder as the masked autoregressive transformer, featuring a hidden dimension of 1024 and 16 attention heads. The diffusion model consists of 4 layers of DiT, where each transformer block has a hidden dimension of 1792 and 8 attention heads.\nWe adopt the AdamW optimizer <math alttext=\"(\\beta_{1}=0.9,\\beta_{2}=0.99)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><mo>,</mo><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.99</mn></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\beta_{1}=0.9,\\beta_{2}=0.99)</annotation></semantics></math>. For training the autoencoder on the HumanML subset of Motion-X, we use a batch size of 256 and a maximum sequence length of 64 frames. For the text-to-motion task, the batch size is set to 50 with a maximum sequence length of 196 frames. The learning rate is initialized at 0.0002 with a linear warmup over 2000 steps. The autoencoder is trained for 50 epochs, while the text-to-motion task is trained for 1500 epochs.\nDuring multimodal generation, we first initialize the model with pretrained weights from the text-to-motion autoencoder and fine-tune it on task-specific datasets. Subsequently, we freeze the parameters of the text-to-motion DiT and only fine-tune the masked transformer along with newly incorporated cross-attention layers. The learning rate and training schedule remain consistent with the text-to-motion task.\nFor all three tasks, we employ exponential moving average (EMA) to update model parameters, ensuring training stability. During inference, the classifier-free guidance (CFG) scale is set to 4.5 for text-to-motion, while other tasks use a CFG scale of 6.5.</p>\n\n",
                "matched_terms": [
                    "texttomotion",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets and Metrics. </span>\nFor the evaluation, we utilize three datasets: HumanML3D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib15\" title=\"\">2022a</a>)</cite> for text-to-motion, BEAT2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib37\" title=\"\">2024a</a>)</cite> for speech-to-gesture, and FineDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite> for music-to-dance, all following the unified SMPL-X representation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>.\nRegarding the metrics, we use FID, R-Precision, and MM-Dist for text-based motion generation, use FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">H</span></sub>, FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">B</span></sub>, Face L2 loss, Beat Alignment Score, Diversity for speech-based motion generation, and use FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">H</span></sub>, FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">B</span></sub>, Diversity for music-based motion generation, respectively.\nFor the detailed explanation of the datasets and metrics, please refer to the appendix.</p>\n\n",
                "matched_terms": [
                    "humanml3d",
                    "texttomotion",
                    "fid",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-based motion generation. </span>\nWe conduct an evaluation of our model against prior text-to-motion approaches, including both discrete-domain and continuous-domain methods.\nAs summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S3.T1\" title=\"Table 1 &#8227; 3.5 Text-to-motion Pretraining and Multimodal Control Adaptation &#8227; 3 Method &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our method clearly surpasses prior techniques on the HumanML subset of Motion-X dataset. Remarkably, our model achieves improvements of 19.3%, 13.5%, and 11.7% in R-Precision for Top-1, 2, 3, respectively. Additionally, we enhance the FID score by 75.2% on this dataset, underscoring the exceptional fidelity of our generated motions. The qualitative results in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S3.F4\" title=\"Figure 4 &#8227; 3.5 Text-to-motion Pretraining and Multimodal Control Adaptation &#8227; 3 Method &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> further support these findings, showing that our approach yields whole-body motions that align closely with the input text.</p>\n\n",
                "matched_terms": [
                    "texttomotion",
                    "method",
                    "results",
                    "top1",
                    "fid"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-based motion generation. </span>\nTo assess the speech-driven motion generation, we compare to previous speech-to-gesture methods.\nOur results, summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S3.T2\" title=\"Table 2 &#8227; 3.6 Inference &#8227; 3 Method &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, reveal that our method achieves good quality and diversity in both hand and body motion generation and excels in aligning\nwith the rhythm of first-person speech.\nThis demonstrates the effectiveness of our framework in motion generation when encompassing different modal signals.\nHowever, our method performs worse than single-modal methods.\nAs discussed in &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, this is attributed to the random or average expressions in the Motion-X dataset, which confuses the speech-to-gesture training.</p>\n\n",
                "matched_terms": [
                    "method",
                    "results",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Causal Attention. </span>\nTo verify the efficacy of the proposed framework, we initially establish a baseline model following the visual MAR setup&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>)</cite>, i.e, using the random pseudo reordering for the batched masked prediction, through the bidirectional attention computing.\nFrom the results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Ablation Study &#8227; 4 Experiments &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we see that the performance of this baseline in the motion area is limited. We attribute this to the difference between human motions and visual images, e.g., the human motion is in a strong temporal sequential structure, in which case a causal attention makes more sense.\nTherefore, changing the baseline to sequential masked prediction with causal attention improves the performance.</p>\n\n",
                "matched_terms": [
                    "2024b",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For text-based motion generation, we evaluate our method on the HumanML3D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib15\" title=\"\">2022a</a>)</cite> dataset, which consists of 14,616 high-quality human motions paired with 44,970 text descriptions.\nThe original body-only SMPL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Loper et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib39\" title=\"\">2023</a>)</cite> format of this dataset is extended to whole-body SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pavlakos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib43\" title=\"\">2019</a>)</cite> format in MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, which we follow in the experiments for evaluation.\nFor speech-based motion generation, we evaluate on the BEAT2 dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib37\" title=\"\">2024a</a>)</cite>, which collects 76 hours of data from 30 speakers, standardized into a mesh representation with paired audio and text lines.\nThe motion of the unified SMPL-X format is also extracted&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite> for multimodal evaluation.\nFor music-based motion generation, the largest dataset FineDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite> is utilized for evaluation. This dataset collects dances of 14.6 hours across 22 genres and provides detailed human motions using the SMPL-H format, which is then converted to the unified SMPL-X format and appended by text descriptions.</p>\n\n",
                "matched_terms": [
                    "original",
                    "motioncraft",
                    "method",
                    "bian",
                    "humanml3d"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable full-body, multimodal control over motion generation, we convert all datasets to the SMPL-X format. This involves filling in missing facial expressions in HumanML3D and FineDance using average expression coefficients from the training set, as well as transforming the SMPL-H Rot-6D representation in FineDance into axis-angle format via Gram-Schmidt orthogonalization. This conversion achieves better alignment with SMPL-X parameters and introduces minimal errors compared to the official body-retargeting method, while also offering improved computational efficiency.</p>\n\n",
                "matched_terms": [
                    "humanml3d",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure consistency with MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, we utilize the pretrained motion encoder and text encode, enabling a unified evaluation of the SMPL-X motion representation across different modalities. For datasets that lack corresponding textual annotations&#8212;namely FineDance and BEAT2&#8212;we generate pseudo-captions such as &#8220;A dancer is performing a street dance in the Jazz style to the rhythm of the wildfire&#8221; and &#8220;A person is giving a speech, and the content is &#8230;&#8221;, respectively, to support cross-modal learning.</p>\n\n",
                "matched_terms": [
                    "motioncraft",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the same strategy as MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, we train two variants of our model: OmniMotion-Base and OmniMotion-Mix. OmniMotion-Base is a text-to-motion model pretrained solely on HumanML3D, while OmniMotion-Mix is trained on a combined dataset comprising HumanML3D, BEAT2, and FineDance to enable multimodal motion generation. Quantitative results on the text-to-motion task are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#A1.T7\" title=\"Table 7 &#8227; Text-based Motion Generation &#8227; A.3 Metrics &#8227; Appendix A Appendix &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "texttomotion",
                    "motioncraft",
                    "results",
                    "bian",
                    "humanml3d"
                ]
            }
        ]
    },
    "A1.T6": {
        "caption": "Table 6: Results of text-to-motion after fine-tuning. (On the HumanML3D subset of Motion-X dataset, following the unified SMPL-X representation.)",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Method</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">R Precision</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">FID <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T6.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">MM Dist<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T6.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Top-1 <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T6.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Top-2 <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T6.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Top-3 <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T6.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Ours</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{0.704}^{\\pm{0.003}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T6.m6\" intent=\":literal\"><semantics><msup><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.704</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{0.704}^{\\pm{0.003}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.843^{\\pm{0.005}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T6.m7\" intent=\":literal\"><semantics><msup><mn>0.843</mn><mrow><mo>&#177;</mo><mn>0.005</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.843^{\\pm{0.005}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{0.898}^{\\pm{0.005}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T6.m8\" intent=\":literal\"><semantics><msup><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.898</mn><mrow><mo>&#177;</mo><mn>0.005</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{0.898}^{\\pm{0.005}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{4.838}^{\\pm{0.100}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T6.m9\" intent=\":literal\"><semantics><msup><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">4.838</mn><mrow><mo>&#177;</mo><mn>0.100</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{4.838}^{\\pm{0.100}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"15.871^{\\pm{0.030}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T6.m10\" intent=\":literal\"><semantics><msup><mn>15.871</mn><mrow><mo>&#177;</mo><mn>0.030</mn></mrow></msup><annotation encoding=\"application/x-tex\">15.871^{\\pm{0.030}}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Ours-Finetuned</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.701^{\\pm{0.002}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T6.m11\" intent=\":literal\"><semantics><msup><mn>0.701</mn><mrow><mo>&#177;</mo><mn>0.002</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.701^{\\pm{0.002}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{0.846}^{\\pm{0.005}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T6.m12\" intent=\":literal\"><semantics><msup><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.846</mn><mrow><mo>&#177;</mo><mn>0.005</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{0.846}^{\\pm{0.005}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{0.898}^{\\pm{0.005}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T6.m13\" intent=\":literal\"><semantics><msup><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.898</mn><mrow><mo>&#177;</mo><mn>0.005</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{0.898}^{\\pm{0.005}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"4.843^{\\pm{0.102}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T6.m14\" intent=\":literal\"><semantics><msup><mn>4.843</mn><mrow><mo>&#177;</mo><mn>0.102</mn></mrow></msup><annotation encoding=\"application/x-tex\">4.843^{\\pm{0.102}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{15.868}^{\\pm{0.027}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T6.m15\" intent=\":literal\"><semantics><msup><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">15.868</mn><mrow><mo>&#177;</mo><mn>0.027</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{15.868}^{\\pm{0.027}}</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "15868±0027mathbf15868pm0027",
            "smplx",
            "unified",
            "top3",
            "0846±0005mathbf0846pm0005",
            "↓downarrow",
            "15871±003015871pm0030",
            "4843±01024843pm0102",
            "after",
            "finetuning",
            "following",
            "0843±00050843pm0005",
            "top2",
            "results",
            "humanml3d",
            "0898±0005mathbf0898pm0005",
            "fid",
            "ours",
            "↑uparrow",
            "texttomotion",
            "0704±0003mathbf0704pm0003",
            "representation",
            "oursfinetuned",
            "motionx",
            "0701±00020701pm0002",
            "precision",
            "subset",
            "method",
            "4838±0100mathbf4838pm0100",
            "dataset",
            "top1",
            "dist↓downarrow"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Whole-body multi-modal human motion generation poses two primary challenges: creating an effective motion generation mechanism and integrating various modalities, such as text, speech, and music, into a cohesive framework.\nUnlike previous methods that usually employ discrete masked modeling or autoregressive modeling, we develop a continuous masked autoregressive motion transformer, where a causal attention is performed considering the sequential nature within the human motion.\nWithin this transformer, we introduce a gated linear attention and an RMSNorm module, which drive the transformer to pay attention to the key actions and suppress the instability caused by either the abnormal movements or the heterogeneous distributions within multi-modalities.\nTo further enhance both the motion generation and the multimodal generalization, we employ the DiT structure to diffuse the conditions from the transformer towards the targets.\nTo fuse different modalities, AdaLN and cross-attention are leveraged to inject the text, speech, and music signals.\nExperimental results demonstrate that our framework outperforms previous methods across all modalities, including text-to-motion, speech-to-gesture, and music-to-dance.\nThe code of our method will be made public.</p>\n\n",
                "matched_terms": [
                    "method",
                    "texttomotion",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To both leverage the advantages of autoregressive and masked modeling, and the benefits of the continuous motion representation, in this work we combine them together to propose a continuous masked autoregressive motion generation framework.\nWe apply a random masking on the sequential motion tokens, and employ a transformer to autoregressively predict the masked tokens.\nUnlike the visual MAR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>)</cite>, we sequentially predict the masked tokens with causal attention rather than performing random reordering, considering the sequential nature within human motion.\nTo enhance the MAR modeling in motion space, we introduce a gated linear mechanism and an RMSNorm module.\nThe gated linear mechanism serves as an adaptive feature selector, driving the transformer to not only pay more attention to the key actions, like gesture switching and large movement, but also disregard less relevant frames and suppress redundant actions, like stationary motions.\nThe RMSNorm is particularly advantageous in scenarios with features exhibiting a large dynamic range, e.g., our unified framework for multi-modalities, where the input distributions are highly heterogeneous.\nIn addition, RMSNorm helps relieve the gradient instability caused by the abnormally large motions, such as sudden jumping or turning back.\nAfter the masked autoregressive transformer, the calculated attention of the masked tokens is fed into a series of DiT blocks to diffuse towards the target tokens, which are decoded to the generated motions.</p>\n\n",
                "matched_terms": [
                    "unified",
                    "representation",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent works have begun to seek multimodal solutions, i.e., designing one framework for motion generation from different input signals&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib41\" title=\"\">2024</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib69\" title=\"\">2024c</a>; Zhou &amp; Wang, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib75\" title=\"\">2023</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib76\" title=\"\">2023</a>; Ling et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib36\" title=\"\">2023</a>; Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib30\" title=\"\">2024c</a>)</cite>.\nSome methods in the discrete domain attempt to incorporate quantized condition tokens into the vocabulary of the generation model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib41\" title=\"\">2024</a>; Zhou &amp; Wang, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib75\" title=\"\">2023</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib76\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib73\" title=\"\">2025</a>)</cite>, while some methods in the continuous domain try to integrate the multimodal signals by designing the motion ControlNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ling et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib36\" title=\"\">2023</a>; Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, where the multimodal conditions guide the sampling of a pretrained text-to-motion diffusion model.\nHowever, most previous methods are restricted by the varying motion data of different modalities, limiting multi-modal frameworks primarily to body-only motion generation.\nTo overcome this, MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite> standardizes datasets across modalities into a unified whole-body format that includes body, hands, and face.\nIn this work, we follow this unified representation to build a whole-body multi-modal motion framework, taking advantage of continuous masked auto-regression.</p>\n\n",
                "matched_terms": [
                    "representation",
                    "unified",
                    "texttomotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To feed the human motion into the transformer, we start by encoding the original motion into motion tokens.\nGiven a motion sequence <math alttext=\"\\{\\mathbf{M}_{t}\\}_{t=1}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>&#119820;</mi><mi>t</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\{\\mathbf{M}_{t}\\}_{t=1}^{T}</annotation></semantics></math>, the objective of an autoencoder is to extract a latent code <math alttext=\"z_{\\text{AE}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>z</mi><mtext>AE</mtext></msub><annotation encoding=\"application/x-tex\">z_{\\text{AE}}</annotation></semantics></math> that optimally captures the essence of the original motion.\nDifferent from most previous motion generation methods with autoregressive transformers, we use a continuous autoencoder rather than the VQ-VAE to do the encoding, which avoids the precision loss associated with the quantization approximation.\nIn the encoder, we stack the 1D convolution networks with ReLU activation to do the feature processing. Following this, two down-sampling residual blocks are applied to reduce the motion feature size to one-fourth of its original dimensions.\nIn the decoder, the same structure in the reversed order is utilized to up-sample the motion feature back to the original size, producing <math alttext=\"\\{\\hat{\\mathbf{M}}_{t}\\}_{t=1}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mover accent=\"true\"><mi>&#119820;</mi><mo>^</mo></mover><mi>t</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\{\\hat{\\mathbf{M}}_{t}\\}_{t=1}^{T}</annotation></semantics></math>. Therefore, the loss for training the autoencoder is defined as</p>\n\n",
                "matched_terms": [
                    "precision",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the feature extraction, we utilize AdaLN to seamlessly integrate the text-derived control signals into the masked autoregressive transformer. AdaLN offers a dynamic approach to normalization, allowing the modulation of its parameters in response to the specific text input, thereby facilitating subsequent multimodal condition injection. By employing this method, we enhance the model&#8217;s ability to incorporate the guiding signals from the text and other signals into the motion generation process, ensuring that the transformer&#8217;s output features are better aligned with the intended motion generation goals. The features outputted by the transformer embody a strong directive capacity for motion generation. This enables the model not only to faithfully interpret the semantic content of the input text but also to produce motion sequences that are coherent with and reflective of the textual intent. The enriched output features contribute to achieving smoother transitions and logically consistent motion sequences in complex generation scenarios.</p>\n\n",
                "matched_terms": [
                    "method",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our objective is to construct a unified model that can simultaneously perform text-to-motion, speech-to-gesture, and music-to-dance tasks. Therefore, we aim to ensure that during the fine-tuning process across different datasets, the model does not suffer from instability that could lead to catastrophic failures. RMSNorm&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang &amp; Sennrich, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib63\" title=\"\">2019</a>)</cite> is particularly advantageous in scenarios with features exhibiting a large dynamic range, especially in tasks where the input distributions are highly heterogeneous. This characteristic enables the model to maintain stability when faced with diverse types of inputs or when observing uneven feature distributions. Additionally, RMSNorm has the potential to mitigate the gradient instability that may arise from significant motion variations, such as sudden jumps or rapid turns.</p>\n\n",
                "matched_terms": [
                    "unified",
                    "texttomotion",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, MLPs exhibit notable limitations when processing heterogeneous data. This incapacity results in suboptimal performance when confronted with diverse signal types, such as speech and music. Due to the relatively small number of parameters in MLPs, they are prone to overfitting on specific datasets (e.g., text-to-motion). This situation can be analogized to a dancer who is adept only in a single dance style; when asked to incorporate other styles, they appear clumsy and ineffective. Consequently, when we attempt to fine-tune MLPs on a different dataset, they are ill-equipped to adapt to the challenges posed by new signals, leading to failures in multimodal generation tasks.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "texttomotion",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation Details. </span>\nOur model is implemented on one NVIDIA V100 GPU using PyTorch. For our method, the autoencoder employs a ResNet-based&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib19\" title=\"\">2016</a>)</cite> three-layer encoder-decoder architecture with a hidden dimension of 512 and an overall downsampling rate of 4. For the generation process, we utilize a four-layer AdaLN-zero transformer encoder as the masked autoregressive transformer, featuring a hidden dimension of 1024 and 16 attention heads. The diffusion model consists of 4 layers of DiT, where each transformer block has a hidden dimension of 1792 and 8 attention heads.\nWe adopt the AdamW optimizer <math alttext=\"(\\beta_{1}=0.9,\\beta_{2}=0.99)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><mo>,</mo><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.99</mn></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\beta_{1}=0.9,\\beta_{2}=0.99)</annotation></semantics></math>. For training the autoencoder on the HumanML subset of Motion-X, we use a batch size of 256 and a maximum sequence length of 64 frames. For the text-to-motion task, the batch size is set to 50 with a maximum sequence length of 196 frames. The learning rate is initialized at 0.0002 with a linear warmup over 2000 steps. The autoencoder is trained for 50 epochs, while the text-to-motion task is trained for 1500 epochs.\nDuring multimodal generation, we first initialize the model with pretrained weights from the text-to-motion autoencoder and fine-tune it on task-specific datasets. Subsequently, we freeze the parameters of the text-to-motion DiT and only fine-tune the masked transformer along with newly incorporated cross-attention layers. The learning rate and training schedule remain consistent with the text-to-motion task.\nFor all three tasks, we employ exponential moving average (EMA) to update model parameters, ensuring training stability. During inference, the classifier-free guidance (CFG) scale is set to 4.5 for text-to-motion, while other tasks use a CFG scale of 6.5.</p>\n\n",
                "matched_terms": [
                    "subset",
                    "method",
                    "texttomotion",
                    "motionx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets and Metrics. </span>\nFor the evaluation, we utilize three datasets: HumanML3D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib15\" title=\"\">2022a</a>)</cite> for text-to-motion, BEAT2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib37\" title=\"\">2024a</a>)</cite> for speech-to-gesture, and FineDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite> for music-to-dance, all following the unified SMPL-X representation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>.\nRegarding the metrics, we use FID, R-Precision, and MM-Dist for text-based motion generation, use FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">H</span></sub>, FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">B</span></sub>, Face L2 loss, Beat Alignment Score, Diversity for speech-based motion generation, and use FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">H</span></sub>, FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">B</span></sub>, Diversity for music-based motion generation, respectively.\nFor the detailed explanation of the datasets and metrics, please refer to the appendix.</p>\n\n",
                "matched_terms": [
                    "texttomotion",
                    "following",
                    "smplx",
                    "unified",
                    "representation",
                    "humanml3d",
                    "fid"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-based motion generation. </span>\nWe conduct an evaluation of our model against prior text-to-motion approaches, including both discrete-domain and continuous-domain methods.\nAs summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S3.T1\" title=\"Table 1 &#8227; 3.5 Text-to-motion Pretraining and Multimodal Control Adaptation &#8227; 3 Method &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our method clearly surpasses prior techniques on the HumanML subset of Motion-X dataset. Remarkably, our model achieves improvements of 19.3%, 13.5%, and 11.7% in R-Precision for Top-1, 2, 3, respectively. Additionally, we enhance the FID score by 75.2% on this dataset, underscoring the exceptional fidelity of our generated motions. The qualitative results in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S3.F4\" title=\"Figure 4 &#8227; 3.5 Text-to-motion Pretraining and Multimodal Control Adaptation &#8227; 3 Method &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> further support these findings, showing that our approach yields whole-body motions that align closely with the input text.</p>\n\n",
                "matched_terms": [
                    "texttomotion",
                    "motionx",
                    "subset",
                    "results",
                    "method",
                    "dataset",
                    "top1",
                    "fid"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-based motion generation. </span>\nTo assess the speech-driven motion generation, we compare to previous speech-to-gesture methods.\nOur results, summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S3.T2\" title=\"Table 2 &#8227; 3.6 Inference &#8227; 3 Method &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, reveal that our method achieves good quality and diversity in both hand and body motion generation and excels in aligning\nwith the rhythm of first-person speech.\nThis demonstrates the effectiveness of our framework in motion generation when encompassing different modal signals.\nHowever, our method performs worse than single-modal methods.\nAs discussed in &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, this is attributed to the random or average expressions in the Motion-X dataset, which confuses the speech-to-gesture training.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "method",
                    "results",
                    "motionx"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Causal Attention. </span>\nTo verify the efficacy of the proposed framework, we initially establish a baseline model following the visual MAR setup&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>)</cite>, i.e, using the random pseudo reordering for the batched masked prediction, through the bidirectional attention computing.\nFrom the results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Ablation Study &#8227; 4 Experiments &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we see that the performance of this baseline in the motion area is limited. We attribute this to the difference between human motions and visual images, e.g., the human motion is in a strong temporal sequential structure, in which case a causal attention makes more sense.\nTherefore, changing the baseline to sequential masked prediction with causal attention improves the performance.</p>\n\n",
                "matched_terms": [
                    "results",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first pretrain a baseline autoencoder on the text-to-motion task. When fine-tuning it on the speech-to-gesture and music-to-dance tasks, the decoder fails to reconstruct valid motion sequences due to discrepancies in data distribution. However, fine-tuning the autoencoder using the reconstruction objective during the multi-modal training incurs high computational costs. Therefore, we independently fine-tune the baseline AE on each dataset using the reconstruction task before multi-modal generation, and employ the resulting models for downstream tasks.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "texttomotion",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following previous work&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Bian et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, we utilize SMPL-X formatted motion data with an input dimension of (frame length &#215; 322). The parameter structure is organized as follows: Root orientation (0:3): controls global body rotation; body pose (3:66): Governs major body joint rotations; hand articulation (66:156): controls finger movements; jaw pose (156:159): manages mouth opening/closing; facial expression (159:209): drives emotional expressions; facial shape (209: 309): determines static facial structure; translation (309:312): controls global body position; betas (312: 322): represents static body shape parameters. And the maximum motion length is 196.\nThe model&#8217;s output maintains identical dimensionality (frame length &#215; 322) to ensure full reconstruction capability. This comprehensive parameterization enables simultaneous control of body motion, facial animation, and global positioning within a unified framework.</p>\n\n",
                "matched_terms": [
                    "smplx",
                    "unified",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For text-based motion generation, we evaluate our method on the HumanML3D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib15\" title=\"\">2022a</a>)</cite> dataset, which consists of 14,616 high-quality human motions paired with 44,970 text descriptions.\nThe original body-only SMPL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Loper et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib39\" title=\"\">2023</a>)</cite> format of this dataset is extended to whole-body SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pavlakos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib43\" title=\"\">2019</a>)</cite> format in MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, which we follow in the experiments for evaluation.\nFor speech-based motion generation, we evaluate on the BEAT2 dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib37\" title=\"\">2024a</a>)</cite>, which collects 76 hours of data from 30 speakers, standardized into a mesh representation with paired audio and text lines.\nThe motion of the unified SMPL-X format is also extracted&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite> for multimodal evaluation.\nFor music-based motion generation, the largest dataset FineDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite> is utilized for evaluation. This dataset collects dances of 14.6 hours across 22 genres and provides detailed human motions using the SMPL-H format, which is then converted to the unified SMPL-X format and appended by text descriptions.</p>\n\n",
                "matched_terms": [
                    "smplx",
                    "unified",
                    "representation",
                    "method",
                    "dataset",
                    "humanml3d"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable full-body, multimodal control over motion generation, we convert all datasets to the SMPL-X format. This involves filling in missing facial expressions in HumanML3D and FineDance using average expression coefficients from the training set, as well as transforming the SMPL-H Rot-6D representation in FineDance into axis-angle format via Gram-Schmidt orthogonalization. This conversion achieves better alignment with SMPL-X parameters and introduces minimal errors compared to the official body-retargeting method, while also offering improved computational efficiency.</p>\n\n",
                "matched_terms": [
                    "smplx",
                    "humanml3d",
                    "representation",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure consistency with MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, we utilize the pretrained motion encoder and text encode, enabling a unified evaluation of the SMPL-X motion representation across different modalities. For datasets that lack corresponding textual annotations&#8212;namely FineDance and BEAT2&#8212;we generate pseudo-captions such as &#8220;A dancer is performing a street dance in the Jazz style to the rhythm of the wildfire&#8221; and &#8220;A person is giving a speech, and the content is &#8230;&#8221;, respectively, to support cross-modal learning.</p>\n\n",
                "matched_terms": [
                    "smplx",
                    "unified",
                    "representation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a comprehensive evaluation on the final model (after fine-tuning on both speech-to-gesture and music-to-dance datasets) and present the results below. Since textual conditioning participates throughout the entire training pipeline, our model does not suffer from catastrophic forgetting after fine-tuning. This confirms the robustness of our architecture&#8217;s knowledge retention capabilities under different training paradigms.</p>\n\n",
                "matched_terms": [
                    "results",
                    "finetuning",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the same strategy as MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, we train two variants of our model: OmniMotion-Base and OmniMotion-Mix. OmniMotion-Base is a text-to-motion model pretrained solely on HumanML3D, while OmniMotion-Mix is trained on a combined dataset comprising HumanML3D, BEAT2, and FineDance to enable multimodal motion generation. Quantitative results on the text-to-motion task are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#A1.T7\" title=\"Table 7 &#8227; Text-based Motion Generation &#8227; A.3 Metrics &#8227; Appendix A Appendix &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "texttomotion",
                    "following",
                    "results",
                    "dataset",
                    "humanml3d"
                ]
            }
        ]
    },
    "A1.T7": {
        "caption": "Table 7: Results of text-driven motion generation on the HumanML3D dataset following the mix training setup (Bian et al., 2025).",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Method</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">R Precision</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">FID <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">MM Dist<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Top-1 <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Top-2 <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Top-3 <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">GT</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.663^{\\pm{0.006}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m6\" intent=\":literal\"><semantics><msup><mn>0.663</mn><mrow><mo>&#177;</mo><mn>0.006</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.663^{\\pm{0.006}}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.807^{\\pm{0.002}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m7\" intent=\":literal\"><semantics><msup><mn>0.807</mn><mrow><mo>&#177;</mo><mn>0.002</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.807^{\\pm{0.002}}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.864^{\\pm{0.002}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m8\" intent=\":literal\"><semantics><msup><mn>0.864</mn><mrow><mo>&#177;</mo><mn>0.002</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.864^{\\pm{0.002}}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.000^{\\pm{0.000}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m9\" intent=\":literal\"><semantics><msup><mn>0.000</mn><mrow><mo>&#177;</mo><mn>0.000</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.000^{\\pm{0.000}}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"15.567^{\\pm{0.036}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m10\" intent=\":literal\"><semantics><msup><mn>15.567</mn><mrow><mo>&#177;</mo><mn>0.036</mn></mrow></msup><annotation encoding=\"application/x-tex\">15.567^{\\pm{0.036}}</annotation></semantics></math></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">MotionCraft-Basic&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.590^{\\pm{0.003}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m11\" intent=\":literal\"><semantics><msup><mn>0.590</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.590^{\\pm{0.003}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.743^{\\pm{0.004}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m12\" intent=\":literal\"><semantics><msup><mn>0.743</mn><mrow><mo>&#177;</mo><mn>0.004</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.743^{\\pm{0.004}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.804^{\\pm{0.004}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m13\" intent=\":literal\"><semantics><msup><mn>0.804</mn><mrow><mo>&#177;</mo><mn>0.004</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.804^{\\pm{0.004}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"8.477^{\\pm{0.102}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m14\" intent=\":literal\"><semantics><msup><mn>8.477</mn><mrow><mo>&#177;</mo><mn>0.102</mn></mrow></msup><annotation encoding=\"application/x-tex\">8.477^{\\pm{0.102}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"16.252^{\\pm{0.035}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m15\" intent=\":literal\"><semantics><msup><mn>16.252</mn><mrow><mo>&#177;</mo><mn>0.035</mn></mrow></msup><annotation encoding=\"application/x-tex\">16.252^{\\pm{0.035}}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">MotionCraft-Mix&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.600^{\\pm{0.003}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m16\" intent=\":literal\"><semantics><msup><mn>0.600</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.600^{\\pm{0.003}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.747^{\\pm{0.004}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m17\" intent=\":literal\"><semantics><msup><mn>0.747</mn><mrow><mo>&#177;</mo><mn>0.004</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.747^{\\pm{0.004}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.812^{\\pm{0.006}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m18\" intent=\":literal\"><semantics><msup><mn>0.812</mn><mrow><mo>&#177;</mo><mn>0.006</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.812^{\\pm{0.006}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"6.707^{\\pm{0.081}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m19\" intent=\":literal\"><semantics><msup><mn>6.707</mn><mrow><mo>&#177;</mo><mn>0.081</mn></mrow></msup><annotation encoding=\"application/x-tex\">6.707^{\\pm{0.081}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"16.334^{\\pm{0.059}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m20\" intent=\":literal\"><semantics><msup><mn>16.334</mn><mrow><mo>&#177;</mo><mn>0.059</mn></mrow></msup><annotation encoding=\"application/x-tex\">16.334^{\\pm{0.059}}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Ours-Basic</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.704^{\\pm{0.003}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m21\" intent=\":literal\"><semantics><msup><mn>0.704</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.704^{\\pm{0.003}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.843^{\\pm{0.005}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m22\" intent=\":literal\"><semantics><msup><mn>0.843</mn><mrow><mo>&#177;</mo><mn>0.005</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.843^{\\pm{0.005}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"0.898^{\\pm{0.005}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m23\" intent=\":literal\"><semantics><msup><mn>0.898</mn><mrow><mo>&#177;</mo><mn>0.005</mn></mrow></msup><annotation encoding=\"application/x-tex\">0.898^{\\pm{0.005}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"4.838^{\\pm{0.100}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m24\" intent=\":literal\"><semantics><msup><mn>4.838</mn><mrow><mo>&#177;</mo><mn>0.100</mn></mrow></msup><annotation encoding=\"application/x-tex\">4.838^{\\pm{0.100}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"15.871^{\\pm{0.030}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m25\" intent=\":literal\"><semantics><msup><mn>15.871</mn><mrow><mo>&#177;</mo><mn>0.030</mn></mrow></msup><annotation encoding=\"application/x-tex\">15.871^{\\pm{0.030}}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Ours-Mix</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{0.712}^{\\pm{0.003}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m26\" intent=\":literal\"><semantics><msup><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.712</mn><mrow><mo>&#177;</mo><mn>0.003</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{0.712}^{\\pm{0.003}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{0.849}^{\\pm{0.005}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m27\" intent=\":literal\"><semantics><msup><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.849</mn><mrow><mo>&#177;</mo><mn>0.005</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{0.849}^{\\pm{0.005}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{0.904}^{\\pm{0.004}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m28\" intent=\":literal\"><semantics><msup><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.904</mn><mrow><mo>&#177;</mo><mn>0.004</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{0.904}^{\\pm{0.004}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{4.759}^{\\pm{0.102}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m29\" intent=\":literal\"><semantics><msup><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">4.759</mn><mrow><mo>&#177;</mo><mn>0.102</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{4.759}^{\\pm{0.102}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{15.765}^{\\pm{0.026}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T7.m30\" intent=\":literal\"><semantics><msup><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">15.765</mn><mrow><mo>&#177;</mo><mn>0.026</mn></mrow></msup><annotation encoding=\"application/x-tex\">\\mathbf{15.765}^{\\pm{0.026}}</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "motion",
            "0712±0003mathbf0712pm0003",
            "setup",
            "top3",
            "0804±00040804pm0004",
            "0898±00050898pm0005",
            "bian",
            "0904±0004mathbf0904pm0004",
            "↓downarrow",
            "8477±01028477pm0102",
            "16252±003516252pm0035",
            "motioncraftmix",
            "16334±005916334pm0059",
            "15871±003015871pm0030",
            "0590±00030590pm0003",
            "generation",
            "mix",
            "15765±0026mathbf15765pm0026",
            "training",
            "4759±0102mathbf4759pm0102",
            "0864±00020864pm0002",
            "15567±003615567pm0036",
            "following",
            "0843±00050843pm0005",
            "0747±00040747pm0004",
            "top2",
            "0663±00060663pm0006",
            "4838±01004838pm0100",
            "results",
            "0812±00060812pm0006",
            "humanml3d",
            "fid",
            "0849±0005mathbf0849pm0005",
            "0600±00030600pm0003",
            "0704±00030704pm0003",
            "↑uparrow",
            "6707±00816707pm0081",
            "oursmix",
            "0807±00020807pm0002",
            "textdriven",
            "0743±00040743pm0004",
            "precision",
            "method",
            "0000±00000000pm0000",
            "dataset",
            "oursbasic",
            "top1",
            "motioncraftbasic",
            "dist↓downarrow"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Following the same strategy as MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, we train two variants of our model: OmniMotion-Base and OmniMotion-Mix. OmniMotion-Base is a text-to-motion model pretrained solely on HumanML3D, while OmniMotion-Mix is trained on a combined dataset comprising HumanML3D, BEAT2, and FineDance to enable multimodal motion generation. Quantitative results on the text-to-motion task are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#A1.T7\" title=\"Table 7 &#8227; Text-based Motion Generation &#8227; A.3 Metrics &#8227; Appendix A Appendix &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Whole-body multi-modal human motion generation poses two primary challenges: creating an effective motion generation mechanism and integrating various modalities, such as text, speech, and music, into a cohesive framework.\nUnlike previous methods that usually employ discrete masked modeling or autoregressive modeling, we develop a continuous masked autoregressive motion transformer, where a causal attention is performed considering the sequential nature within the human motion.\nWithin this transformer, we introduce a gated linear attention and an RMSNorm module, which drive the transformer to pay attention to the key actions and suppress the instability caused by either the abnormal movements or the heterogeneous distributions within multi-modalities.\nTo further enhance both the motion generation and the multimodal generalization, we employ the DiT structure to diffuse the conditions from the transformer towards the targets.\nTo fuse different modalities, AdaLN and cross-attention are leveraged to inject the text, speech, and music signals.\nExperimental results demonstrate that our framework outperforms previous methods across all modalities, including text-to-motion, speech-to-gesture, and music-to-dance.\nThe code of our method will be made public.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "method",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Whole-body human motion generation represents an expanding frontier in computer vision, offering significant value across a variety of applications, including film production, gaming, virtual reality, robotics, and so on.\nBroadly speaking, motion generation could be conditioned on various signals, such as text, speech, music, and more.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Historically, approaches to whole-body motion generation usually focus on isolated tasks. Typically, they either address text-to-motion generation, or concentrate on speech-to-gesture translation, or engage in music-to-dance synthesis.\nDespite their successes in single task, their frameworks are exclusively designed for individual tasks and cannot be easily adapted to different tasks.\nIn addition, they tend to overlook the underlying commonalities that exist across different tasks.\nIn contrast, in this work we seek to address these motion generation challenges from various signals within an omni-framework.\nThis brings two advantages: 1) It allows each modality to benefit from the patterns present in other modalities, preventing single-mode solutions from becoming trapped in a local minimum;\n2) It enhances each task with data from other tasks, which is particularly relevant given the limited scale of data available for individual motion tasks.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous studies in motion generation generally proceed in two paths.\nThe first employs the vector quantization (VQ) technique to convert continuous motion to discrete tokens, and then performs autoregressive or masked modeling to predict the tokens&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib71\" title=\"\">2023d</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib65\" title=\"\">a</a>; Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib23\" title=\"\">2023</a>; Zhong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib74\" title=\"\">2023</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib17\" title=\"\">2024</a>)</cite>.\nWhile this path effectively utilizes the strengths of autoregressive and masked modeling, the quantization step inevitably introduces approximation errors, which impose undesirable limits on the quality of the generated motions.\nThe second directly regresses the continuous motions using techniques such as generative adversarial networks (GANs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tulyakov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib54\" title=\"\">2018</a>)</cite>, variational autoencoders (VAEs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib58\" title=\"\">2020</a>; Ahuja &amp; Morency, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib1\" title=\"\">2019</a>; Petrovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib45\" title=\"\">2022</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib15\" title=\"\">2022a</a>)</cite>, or recent diffusion models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib12\" title=\"\">2023</a>; Tevet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib52\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib68\" title=\"\">2024b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib66\" title=\"\">2023b</a>; Ribeiro-Gomes et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib48\" title=\"\">2024</a>)</cite>.\nDespite avoiding the approximation errors, they miss the autoregressive or masked modeling technologies, which have been shown to deliver superior performance in motion generation tasks.\nConsequently, the performance of the motion generated by this path is overall lower than that achieved by the first path.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To both leverage the advantages of autoregressive and masked modeling, and the benefits of the continuous motion representation, in this work we combine them together to propose a continuous masked autoregressive motion generation framework.\nWe apply a random masking on the sequential motion tokens, and employ a transformer to autoregressively predict the masked tokens.\nUnlike the visual MAR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>)</cite>, we sequentially predict the masked tokens with causal attention rather than performing random reordering, considering the sequential nature within human motion.\nTo enhance the MAR modeling in motion space, we introduce a gated linear mechanism and an RMSNorm module.\nThe gated linear mechanism serves as an adaptive feature selector, driving the transformer to not only pay more attention to the key actions, like gesture switching and large movement, but also disregard less relevant frames and suppress redundant actions, like stationary motions.\nThe RMSNorm is particularly advantageous in scenarios with features exhibiting a large dynamic range, e.g., our unified framework for multi-modalities, where the input distributions are highly heterogeneous.\nIn addition, RMSNorm helps relieve the gradient instability caused by the abnormally large motions, such as sudden jumping or turning back.\nAfter the masked autoregressive transformer, the calculated attention of the masked tokens is fed into a series of DiT blocks to diffuse towards the target tokens, which are decoded to the generated motions.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the text-based motion generation, we further extend our framework to multimodal conditions.\nBuilding upon a similar structure, the multimodal signals are fused by AdaLN&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib18\" title=\"\">2022c</a>)</cite> and cross-attention modules.\nExtensive experiments across different datasets demonstrate our framework can work well with different modalities, including text, speech, and music, and outperform previous methods in whole-body text-to-motion, speech-to-gesture, and music-to-dance tasks.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We design an omni motion framework for whole-body human motion generation, where one framework encompasses multiple modalities.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We integrate the multimodal signals via AdaLN and cross-attention, obtaining superior performance than previous methods in text-based, speech-based, and music-based motion generation.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous research on text-based motion generation can be generally categorized into two mainstream paths: continuous regression and discrete classification.\nIn the continuous regression domain, numerous strategies have leveraged the variational autoencoder (VAE) framework, integrating latent embeddings of encoded text with those of encoded poses, which are then decoded into motion predictions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib58\" title=\"\">2020</a>; Ahuja &amp; Morency, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib1\" title=\"\">2019</a>; Athanasiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib4\" title=\"\">2022</a>; Petrovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib45\" title=\"\">2022</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib15\" title=\"\">2022a</a>)</cite>.\nOther methods have investigated the potential of recurrent networks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib33\" title=\"\">2018</a>; Plappert et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib47\" title=\"\">2018</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib70\" title=\"\">2020</a>)</cite>, generative adversarial networks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib7\" title=\"\">2018</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib57\" title=\"\">2020</a>; Tulyakov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib54\" title=\"\">2018</a>)</cite>, or transformer networks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tevet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib51\" title=\"\">2022</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib35\" title=\"\">2023b</a>; Bhattacharya et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib5\" title=\"\">2021</a>; Petrovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib44\" title=\"\">2021</a>)</cite> to enhance the motion regression quality.\nBuilding on the success of diffusion models, recent approaches have begun to integrate the diffusion process into motion diffusion&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib22\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib12\" title=\"\">2023</a>; Tevet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib52\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib68\" title=\"\">2024b</a>; Dabral et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib13\" title=\"\">2023</a>; Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib40\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib66\" title=\"\">2023b</a>; Ribeiro-Gomes et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib48\" title=\"\">2024</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib62\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib56\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib67\" title=\"\">2023c</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib72\" title=\"\">2024d</a>; Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, yielding impressive results.\nIn the discrete classification domain, the input motion undergoes initial encoding via a VQ-VAE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Van Den&#160;Oord et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib55\" title=\"\">2017</a>)</cite>, producing motion tokens for subsequent prediction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib74\" title=\"\">2023</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib32\" title=\"\">2024e</a>)</cite>.\nDrawing inspiration from advancements in natural language processing, some methods utilize autoregressive modeling to predict tokens sequentially&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib16\" title=\"\">2022b</a>; Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib40\" title=\"\">2023</a>; Zou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib80\" title=\"\">2024</a>)</cite>.\nOthers employ generative masked modeling strategies, with tokens randomly masked during training for the model to predict&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib17\" title=\"\">2024</a>; Pinyoanuntapong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib46\" title=\"\">2024</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib61\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib28\" title=\"\">2023b</a>)</cite>.\nMore recently, large language models (LLMs) have been harnessed to help the prediction process, considering their large-scale pretraining&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib71\" title=\"\">2023d</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib77\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib31\" title=\"\">2024d</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib29\" title=\"\">2023c</a>)</cite>.\nIn this work, we seek to integrate the most effective elements from these two paths: the continuous diffusion and the masked autoregressive modeling.\nA previous attempt in this direction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib42\" title=\"\">2024</a>)</cite> directly transfers the MAR in image generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>)</cite> into motion generation without considering the difference between image and motion spaces, especially the temporal correlation. Also, its framework is only designed for body-only motion generation. Differently, we propose a new MAR mechanism that is especially designed for whole-body motion generation.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "training",
                    "results",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to text, there are many other signals that various human motions are conditioned on, such as speech and music.\nIn the realm of speech-to-gesture generation, both continuous regression and discrete classification paths have been explored.\nIn the continuous domain, methods employ deep generative models like GANs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ahuja et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib2\" title=\"\">2022</a>)</cite>, normalizing flows&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib50\" title=\"\">2024</a>)</cite>, and diffusion models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Alexanderson et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib3\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib10\" title=\"\">2024a</a>; He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib20\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib59\" title=\"\">2023</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib78\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib11\" title=\"\">2024b</a>)</cite> to learn complex motion distributions in the speech data.\nIn the discrete domain, methods leverage either the autoregressive modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib60\" title=\"\">2023</a>)</cite> or the masked modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib37\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib38\" title=\"\">b</a>)</cite> to predict the discrete tokens quantified by the VQ-VAE.\nThe primary distinction among these methods lies in their specific handling of different parts of human motion, including body movements, hand gestures, and facial expressions.\nSimilarly, in the realm of music-to-dance generation, there are also methods in both the continuous domain&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhuang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib79\" title=\"\">2022</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib53\" title=\"\">2023</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib25\" title=\"\">2024a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib21\" title=\"\">2024</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib64\" title=\"\">2024a</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite> and the discrete domain&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Siyao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib49\" title=\"\">2022</a>)</cite>.\nThe discrete autoregressive model is leveraged after the motion quantization with VQ-VAE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Siyao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib49\" title=\"\">2022</a>)</cite>.\nMore methods harness the diffusion model to directly regress the target dancing motion in the continuous space&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib53\" title=\"\">2023</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib25\" title=\"\">2024a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib21\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite>.\nRecent methods also start to merge autoregressive and diffusion models, producing coherent and music-aligned dance sequences&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib64\" title=\"\">2024a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent works have begun to seek multimodal solutions, i.e., designing one framework for motion generation from different input signals&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib41\" title=\"\">2024</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib69\" title=\"\">2024c</a>; Zhou &amp; Wang, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib75\" title=\"\">2023</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib76\" title=\"\">2023</a>; Ling et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib36\" title=\"\">2023</a>; Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib30\" title=\"\">2024c</a>)</cite>.\nSome methods in the discrete domain attempt to incorporate quantized condition tokens into the vocabulary of the generation model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib41\" title=\"\">2024</a>; Zhou &amp; Wang, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib75\" title=\"\">2023</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib76\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib73\" title=\"\">2025</a>)</cite>, while some methods in the continuous domain try to integrate the multimodal signals by designing the motion ControlNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ling et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib36\" title=\"\">2023</a>; Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, where the multimodal conditions guide the sampling of a pretrained text-to-motion diffusion model.\nHowever, most previous methods are restricted by the varying motion data of different modalities, limiting multi-modal frameworks primarily to body-only motion generation.\nTo overcome this, MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite> standardizes datasets across modalities into a unified whole-body format that includes body, hands, and face.\nIn this work, we follow this unified representation to build a whole-body multi-modal motion framework, taking advantage of continuous masked auto-regression.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The overview of our framework is illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S2.F2\" title=\"Figure 2 &#8227; Multimodal Motion Generation. &#8227; 2 Related Work &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, which is divided into three main stages:\nIn the first stage, we encode the input motion with an autoencoder, which generates continuous motion tokens.\nIn the second stage, we focus on the text-based motion generation, utilizing our motion masked autoregressive framework to model the motion generation process.\nIn this stage, an autoregressive transformer is employed to predict the masked tokens, within which a gated linear mechanism is designed, and an RMSNorm module is employed.\nThe text information is integrated into the transformer via AdaLN after encoding.\nAfter the transformer, the generated embedding is fed into the DiT modules as the condition to diffuse towards the target token.\nIn the third stage, we extend the model learned in the previous stage to the multi-modal structure.\nThis involves merging the text embedding with multimodal signal embeddings&#8212;specifically speech or music&#8212;prior to their AdaLN input.\nFurthermore, we inject the multimodal embedding through a cross-attention module into the masked transformer.\nIn the multimodal learning stage, the DiT module is kept in the same structure and frozen.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To feed the human motion into the transformer, we start by encoding the original motion into motion tokens.\nGiven a motion sequence <math alttext=\"\\{\\mathbf{M}_{t}\\}_{t=1}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>&#119820;</mi><mi>t</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\{\\mathbf{M}_{t}\\}_{t=1}^{T}</annotation></semantics></math>, the objective of an autoencoder is to extract a latent code <math alttext=\"z_{\\text{AE}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>z</mi><mtext>AE</mtext></msub><annotation encoding=\"application/x-tex\">z_{\\text{AE}}</annotation></semantics></math> that optimally captures the essence of the original motion.\nDifferent from most previous motion generation methods with autoregressive transformers, we use a continuous autoencoder rather than the VQ-VAE to do the encoding, which avoids the precision loss associated with the quantization approximation.\nIn the encoder, we stack the 1D convolution networks with ReLU activation to do the feature processing. Following this, two down-sampling residual blocks are applied to reduce the motion feature size to one-fourth of its original dimensions.\nIn the decoder, the same structure in the reversed order is utilized to up-sample the motion feature back to the original size, producing <math alttext=\"\\{\\hat{\\mathbf{M}}_{t}\\}_{t=1}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mover accent=\"true\"><mi>&#119820;</mi><mo>^</mo></mover><mi>t</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\{\\hat{\\mathbf{M}}_{t}\\}_{t=1}^{T}</annotation></semantics></math>. Therefore, the loss for training the autoencoder is defined as</p>\n\n",
                "matched_terms": [
                    "motion",
                    "following",
                    "generation",
                    "training",
                    "precision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the continuous motion tokens, we design a masked autoregressive transformer to model the motion generation, effectively capturing the temporal relationship between different tokens and producing the rich contextual condition <math alttext=\"\\mathbf{z}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><msup><mi>&#119859;</mi><mi>i</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{z}^{i}</annotation></semantics></math> for the subsequent diffusion process.\nWe first randomly mask the motion tokens following language models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib14\" title=\"\">2018</a>)</cite>, obtaining some masked tokens <math alttext=\"\\{\\tilde{\\mathbf{x}}^{i}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msup><mover accent=\"true\"><mi>&#119857;</mi><mo>~</mo></mover><mi>i</mi></msup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\tilde{\\mathbf{x}}^{i}\\}</annotation></semantics></math>.\nThe temporal masking strategies adopt the same mask ratio schedule following&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib8\" title=\"\">2022</a>)</cite>, and are computed as</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the input text prompt <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>, we first employ the LaMP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib32\" title=\"\">2024e</a>)</cite> text transformer to extract textual features, leveraging its advanced capabilities to encode the linguistic nuances and semantic structure of the input prompt. This creates a high-dimensional feature representation that is crucial for guiding motion generation process.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the feature extraction, we utilize AdaLN to seamlessly integrate the text-derived control signals into the masked autoregressive transformer. AdaLN offers a dynamic approach to normalization, allowing the modulation of its parameters in response to the specific text input, thereby facilitating subsequent multimodal condition injection. By employing this method, we enhance the model&#8217;s ability to incorporate the guiding signals from the text and other signals into the motion generation process, ensuring that the transformer&#8217;s output features are better aligned with the intended motion generation goals. The features outputted by the transformer embody a strong directive capacity for motion generation. This enables the model not only to faithfully interpret the semantic content of the input text but also to produce motion sequences that are coherent with and reflective of the textual intent. The enriched output features contribute to achieving smoother transitions and logically consistent motion sequences in complex generation scenarios.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "method",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to previous works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>; Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib42\" title=\"\">2024</a>)</cite>, we adopt Diffusion Transformer (DiT) as our diffusion model. While the use of DiT may incur additional time costs during training and inference (not much due to the compact structure of motion data), it significantly enhances the quality of generated outputs. Compared to MLPs, DiT provides greater convenience in injecting conditional control signals. During the training process of multimodal generation, we freeze the diffusion model and only fine-tune the masked transformer. The structural characteristics of DiT facilitate this approach, enabling it to better handle various types of conditional signals.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, MLPs exhibit notable limitations when processing heterogeneous data. This incapacity results in suboptimal performance when confronted with diverse signal types, such as speech and music. Due to the relatively small number of parameters in MLPs, they are prone to overfitting on specific datasets (e.g., text-to-motion). This situation can be analogized to a dancer who is adept only in a single dance style; when asked to incorporate other styles, they appear clumsy and ineffective. Consequently, when we attempt to fine-tune MLPs on a different dataset, they are ill-equipped to adapt to the challenges posed by new signals, leading to failures in multimodal generation tasks.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "generation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first pre-train the model on text-motion paired data in a text-to-motion generation setting. Owing to its strong semantic expressiveness and cross-modal alignment properties, we adopt text as a shared conditional signal across diverse unimodal datasets, enabling the model to learn sequence-level generation capabilities between text and motion, as well as a coarse-grained textual guidance mechanism for generative control.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hypothesize that the contextual features output by the masked transformer provide a more expressive control signal compared to raw text embeddings. Accordingly, within the DiT architecture, we inject the transformer&#8217;s output features by summing them with the time embeddings, thereby guiding the motion generation process as:</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation Details. </span>\nOur model is implemented on one NVIDIA V100 GPU using PyTorch. For our method, the autoencoder employs a ResNet-based&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib19\" title=\"\">2016</a>)</cite> three-layer encoder-decoder architecture with a hidden dimension of 512 and an overall downsampling rate of 4. For the generation process, we utilize a four-layer AdaLN-zero transformer encoder as the masked autoregressive transformer, featuring a hidden dimension of 1024 and 16 attention heads. The diffusion model consists of 4 layers of DiT, where each transformer block has a hidden dimension of 1792 and 8 attention heads.\nWe adopt the AdamW optimizer <math alttext=\"(\\beta_{1}=0.9,\\beta_{2}=0.99)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><mo>,</mo><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.99</mn></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\beta_{1}=0.9,\\beta_{2}=0.99)</annotation></semantics></math>. For training the autoencoder on the HumanML subset of Motion-X, we use a batch size of 256 and a maximum sequence length of 64 frames. For the text-to-motion task, the batch size is set to 50 with a maximum sequence length of 196 frames. The learning rate is initialized at 0.0002 with a linear warmup over 2000 steps. The autoencoder is trained for 50 epochs, while the text-to-motion task is trained for 1500 epochs.\nDuring multimodal generation, we first initialize the model with pretrained weights from the text-to-motion autoencoder and fine-tune it on task-specific datasets. Subsequently, we freeze the parameters of the text-to-motion DiT and only fine-tune the masked transformer along with newly incorporated cross-attention layers. The learning rate and training schedule remain consistent with the text-to-motion task.\nFor all three tasks, we employ exponential moving average (EMA) to update model parameters, ensuring training stability. During inference, the classifier-free guidance (CFG) scale is set to 4.5 for text-to-motion, while other tasks use a CFG scale of 6.5.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "training",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets and Metrics. </span>\nFor the evaluation, we utilize three datasets: HumanML3D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib15\" title=\"\">2022a</a>)</cite> for text-to-motion, BEAT2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib37\" title=\"\">2024a</a>)</cite> for speech-to-gesture, and FineDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite> for music-to-dance, all following the unified SMPL-X representation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>.\nRegarding the metrics, we use FID, R-Precision, and MM-Dist for text-based motion generation, use FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">H</span></sub>, FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">B</span></sub>, Face L2 loss, Beat Alignment Score, Diversity for speech-based motion generation, and use FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">H</span></sub>, FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">B</span></sub>, Diversity for music-based motion generation, respectively.\nFor the detailed explanation of the datasets and metrics, please refer to the appendix.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "following",
                    "generation",
                    "bian",
                    "humanml3d",
                    "fid"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-based motion generation. </span>\nWe conduct an evaluation of our model against prior text-to-motion approaches, including both discrete-domain and continuous-domain methods.\nAs summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S3.T1\" title=\"Table 1 &#8227; 3.5 Text-to-motion Pretraining and Multimodal Control Adaptation &#8227; 3 Method &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our method clearly surpasses prior techniques on the HumanML subset of Motion-X dataset. Remarkably, our model achieves improvements of 19.3%, 13.5%, and 11.7% in R-Precision for Top-1, 2, 3, respectively. Additionally, we enhance the FID score by 75.2% on this dataset, underscoring the exceptional fidelity of our generated motions. The qualitative results in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S3.F4\" title=\"Figure 4 &#8227; 3.5 Text-to-motion Pretraining and Multimodal Control Adaptation &#8227; 3 Method &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> further support these findings, showing that our approach yields whole-body motions that align closely with the input text.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "method",
                    "results",
                    "dataset",
                    "top1",
                    "fid"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-based motion generation. </span>\nTo assess the speech-driven motion generation, we compare to previous speech-to-gesture methods.\nOur results, summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S3.T2\" title=\"Table 2 &#8227; 3.6 Inference &#8227; 3 Method &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, reveal that our method achieves good quality and diversity in both hand and body motion generation and excels in aligning\nwith the rhythm of first-person speech.\nThis demonstrates the effectiveness of our framework in motion generation when encompassing different modal signals.\nHowever, our method performs worse than single-modal methods.\nAs discussed in &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, this is attributed to the random or average expressions in the Motion-X dataset, which confuses the speech-to-gesture training.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "training",
                    "results",
                    "bian",
                    "method",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Music-based motion generation. </span>\nWe further evaluate our framework on the music-to-dance task. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S4.T3\" title=\"Table 3 &#8227; 4.1 Experiment Settings &#8227; 4 Experiments &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our method achieves slightly improved performance over previous approaches, particularly in generating hand motions and body movements.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Causal Attention. </span>\nTo verify the efficacy of the proposed framework, we initially establish a baseline model following the visual MAR setup&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>)</cite>, i.e, using the random pseudo reordering for the batched masked prediction, through the bidirectional attention computing.\nFrom the results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Ablation Study &#8227; 4 Experiments &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we see that the performance of this baseline in the motion area is limited. We attribute this to the difference between human motions and visual images, e.g., the human motion is in a strong temporal sequential structure, in which case a causal attention makes more sense.\nTherefore, changing the baseline to sequential masked prediction with causal attention improves the performance.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "results",
                    "setup",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DiT. </span>\nIn order to evaluate how the DiTs contribute to the motion quality, we further replace the MLPs in the baseline model with our DiTs.\nAs shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Ablation Study &#8227; 4 Experiments &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the model generates superior motions with DiTs compared to MLPs, especially in the context of multimodal motion generation.\nThis reveals the superior potential of DiTs in generating motion with complex multimodal contexts.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross Attention. </span>\nIn the baseline model, the multimodal signals are injected with only the AdaLN structure.\nWe then add the cross attention module and observe a significant improvement in multimodal motion generation, as depicted in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Ablation Study &#8227; 4 Experiments &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper proposes a new omni motion framework for multimodal whole-body human motion generation.\nWithin this one framework, text, speech, and music signals are all encompassed through AdaLN and cross-attention.\nThe motion generation process is modeled by a continuous masked autoregressive transformer with causal attention, as well as a DiT structure.\nExtensive experiments have been conducted to verify the efficacy of the proposed framework in different-modality tasks.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations.</span>\nDue to the restricted dataset, the naturalness and generalizability of the motion generation model are still limited, especially in speech and music-driven motion generation.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "dataset",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our speech and music encoders are designed to extract temporally aligned, high-level features from raw audio signals for effective speech-to-gesture and music-to-dance generation. The architecture builds upon a multi-layer 1D convolutional network with strided convolutions and leaky ReLU activations. Each convolutional block consists of a series of a block unit that progressively downsample the input waveform while increasing feature dimensionality. The input audio sequence is first processed through multiple stages of temporal aggregation and non-linear transformation, resulting in a sequence of compact and expressive latent representations, whereas the input music typically retains sufficient temporal structure and spectral richness in its raw form for effective motion synthesis. These latent codes capture prosodic, rhythmic, and semantic-like patterns in speech and music, which are then projected into the condition latent space of dimensionality. The final encoder output is transposed to align with the temporal structure expected by the diffusion model, enabling fine-grained cross-modal interaction between speech and motion sequences during generation.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first pretrain a baseline autoencoder on the text-to-motion task. When fine-tuning it on the speech-to-gesture and music-to-dance tasks, the decoder fails to reconstruct valid motion sequences due to discrepancies in data distribution. However, fine-tuning the autoencoder using the reconstruction objective during the multi-modal training incurs high computational costs. Therefore, we independently fine-tune the baseline AE on each dataset using the reconstruction task before multi-modal generation, and employ the resulting models for downstream tasks.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "dataset",
                    "generation",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following previous work&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Bian et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, we utilize SMPL-X formatted motion data with an input dimension of (frame length &#215; 322). The parameter structure is organized as follows: Root orientation (0:3): controls global body rotation; body pose (3:66): Governs major body joint rotations; hand articulation (66:156): controls finger movements; jaw pose (156:159): manages mouth opening/closing; facial expression (159:209): drives emotional expressions; facial shape (209: 309): determines static facial structure; translation (309:312): controls global body position; betas (312: 322): represents static body shape parameters. And the maximum motion length is 196.\nThe model&#8217;s output maintains identical dimensionality (frame length &#215; 322) to ensure full reconstruction capability. This comprehensive parameterization enables simultaneous control of body motion, facial animation, and global positioning within a unified framework.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "following",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For text-based motion generation, we evaluate our method on the HumanML3D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib15\" title=\"\">2022a</a>)</cite> dataset, which consists of 14,616 high-quality human motions paired with 44,970 text descriptions.\nThe original body-only SMPL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Loper et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib39\" title=\"\">2023</a>)</cite> format of this dataset is extended to whole-body SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pavlakos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib43\" title=\"\">2019</a>)</cite> format in MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, which we follow in the experiments for evaluation.\nFor speech-based motion generation, we evaluate on the BEAT2 dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib37\" title=\"\">2024a</a>)</cite>, which collects 76 hours of data from 30 speakers, standardized into a mesh representation with paired audio and text lines.\nThe motion of the unified SMPL-X format is also extracted&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite> for multimodal evaluation.\nFor music-based motion generation, the largest dataset FineDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite> is utilized for evaluation. This dataset collects dances of 14.6 hours across 22 genres and provides detailed human motions using the SMPL-H format, which is then converted to the unified SMPL-X format and appended by text descriptions.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "method",
                    "bian",
                    "dataset",
                    "humanml3d"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable full-body, multimodal control over motion generation, we convert all datasets to the SMPL-X format. This involves filling in missing facial expressions in HumanML3D and FineDance using average expression coefficients from the training set, as well as transforming the SMPL-H Rot-6D representation in FineDance into axis-angle format via Gram-Schmidt orthogonalization. This conversion achieves better alignment with SMPL-X parameters and introduces minimal errors compared to the official body-retargeting method, while also offering improved computational efficiency.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "training",
                    "method",
                    "humanml3d"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure consistency with MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, we utilize the pretrained motion encoder and text encode, enabling a unified evaluation of the SMPL-X motion representation across different modalities. For datasets that lack corresponding textual annotations&#8212;namely FineDance and BEAT2&#8212;we generate pseudo-captions such as &#8220;A dancer is performing a street dance in the Jazz style to the rhythm of the wildfire&#8221; and &#8220;A person is giving a speech, and the content is &#8230;&#8221;, respectively, to support cross-modal learning.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the quality of the motions generated based on texts compared to the true data, we utilize the Frechet Inception Distance (FID) to evaluate the distribution differences between the generated motions and the ground truth.\nAdditionally, R-Precision is employed to determine how frequently the most relevant motions, identified as top-k closest matches, align with their respective captions within a batch of 32 samples. Lastly, Multi-Modal Distance (MM Dist) is employed to gauge the average Euclidean distance between motion representations and their corresponding textual features.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "fid"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide a more comprehensive evaluation, we conduct additional comparisons on the original HumanML3D benchmark using the body-only H3D format, which contains redundant motion information.\nHere we mainly compare with the methods without VQ-VAE.\nAs shown in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#A1.T5\" title=\"Table 5 &#8227; Text-based Motion Generation &#8227; A.3 Metrics &#8227; Appendix A Appendix &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, OmniMotion consistently outperforms these baselines in terms of text-motion alignment, motion quality, and diversity, demonstrating its superior generalization capability across different motion representations.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "humanml3d"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a comprehensive evaluation on the final model (after fine-tuning on both speech-to-gesture and music-to-dance datasets) and present the results below. Since textual conditioning participates throughout the entire training pipeline, our model does not suffer from catastrophic forgetting after fine-tuning. This confirms the robustness of our architecture&#8217;s knowledge retention capabilities under different training paradigms.</p>\n\n",
                "matched_terms": [
                    "training",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We display more visual results of speech-driven and music-driven motion generation in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#A1.F5\" title=\"Figure 5 &#8227; Text-based Motion Generation &#8227; A.3 Metrics &#8227; Appendix A Appendix &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#A1.F6\" title=\"Figure 6 &#8227; Text-based Motion Generation &#8227; A.3 Metrics &#8227; Appendix A Appendix &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, respectively.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "results"
                ]
            }
        ]
    },
    "A1.T8": {
        "caption": "Table 8: Results of speech-driven motion generation on the BEAT2 dataset (Liu et al., 2024a) following the mix training setup (Bian et al., 2025).",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Method</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">FID<math alttext=\"{}_{H}\\downarrow\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"A1.T8.m1\" intent=\":literal\"><semantics><mmultiscripts><mo stretchy=\"false\">&#8595;</mo><mprescripts/><mi>H</mi><mrow/></mmultiscripts><annotation encoding=\"application/x-tex\">{}_{H}\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">FID<math alttext=\"{}_{B}\\downarrow\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"A1.T8.m2\" intent=\":literal\"><semantics><mmultiscripts><mo stretchy=\"false\">&#8595;</mo><mprescripts/><mi>B</mi><mrow/></mmultiscripts><annotation encoding=\"application/x-tex\">{}_{B}\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Face L2 Loss <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Beat Align Score <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Diversity <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">MotionCraft-Basic&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"18.486\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m6\" intent=\":literal\"><semantics><mn>18.486</mn><annotation encoding=\"application/x-tex\">18.486</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"27.023\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m7\" intent=\":literal\"><semantics><mn>27.023</mn><annotation encoding=\"application/x-tex\">27.023</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"10.097\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m8\" intent=\":literal\"><semantics><mn>10.097</mn><annotation encoding=\"application/x-tex\">10.097</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"8.098\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m9\" intent=\":literal\"><semantics><mn>8.098</mn><annotation encoding=\"application/x-tex\">8.098</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"10.334\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m10\" intent=\":literal\"><semantics><mn>10.334</mn><annotation encoding=\"application/x-tex\">10.334</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">MotionCraft-Mix&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"12.882\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m11\" intent=\":literal\"><semantics><mn>12.882</mn><annotation encoding=\"application/x-tex\">12.882</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"25.187\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m12\" intent=\":literal\"><semantics><mn>25.187</mn><annotation encoding=\"application/x-tex\">25.187</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{8.906}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m13\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">8.906</mn><annotation encoding=\"application/x-tex\">\\mathbf{8.906}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"8.226\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m14\" intent=\":literal\"><semantics><mn>8.226</mn><annotation encoding=\"application/x-tex\">8.226</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"12.595\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m15\" intent=\":literal\"><semantics><mn>12.595</mn><annotation encoding=\"application/x-tex\">12.595</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Ours-Basic</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"17.651\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m16\" intent=\":literal\"><semantics><mn>17.651</mn><annotation encoding=\"application/x-tex\">17.651</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"25.923\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m17\" intent=\":literal\"><semantics><mn>25.923</mn><annotation encoding=\"application/x-tex\">25.923</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"{9.883}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m18\" intent=\":literal\"><semantics><mn>9.883</mn><annotation encoding=\"application/x-tex\">{9.883}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"8.377\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m19\" intent=\":literal\"><semantics><mn>8.377</mn><annotation encoding=\"application/x-tex\">8.377</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"14.703\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m20\" intent=\":literal\"><semantics><mn>14.703</mn><annotation encoding=\"application/x-tex\">14.703</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Ours-Mix</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{12.201}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m21\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">12.201</mn><annotation encoding=\"application/x-tex\">\\mathbf{12.201}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{25.644}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m22\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">25.644</mn><annotation encoding=\"application/x-tex\">\\mathbf{25.644}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"{8.947}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m23\" intent=\":literal\"><semantics><mn>8.947</mn><annotation encoding=\"application/x-tex\">{8.947}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{8.430}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m24\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">8.430</mn><annotation encoding=\"application/x-tex\">\\mathbf{8.430}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><math alttext=\"\\mathbf{15.003}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T8.m25\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">15.003</mn><annotation encoding=\"application/x-tex\">\\mathbf{15.003}</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "motion",
            "align",
            "setup",
            "bian",
            "↓downarrow",
            "motioncraftmix",
            "speechdriven",
            "2024a",
            "15003mathbf15003",
            "face",
            "generation",
            "mix",
            "loss",
            "training",
            "diversity",
            "fid↓hhdownarrow",
            "beat2",
            "beat",
            "following",
            "12201mathbf12201",
            "results",
            "25644mathbf25644",
            "score",
            "fid↓bbdownarrow",
            "↑uparrow",
            "8906mathbf8906",
            "oursmix",
            "liu",
            "method",
            "8430mathbf8430",
            "dataset",
            "oursbasic",
            "motioncraftbasic"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We further evaluate OmniMotion-Mix across all three modalities. For the speech-to-gesture and music-to-dance tasks, we fine-tune the model on the respective target datasets. The corresponding results are reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#A1.T8\" title=\"Table 8 &#8227; Text-based Motion Generation &#8227; A.3 Metrics &#8227; Appendix A Appendix &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#A1.T9\" title=\"Table 9 &#8227; Evaluation of OmniMotion Variants &#8227; A.4 Additional Experiment Results &#8227; Appendix A Appendix &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, respectively.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Whole-body multi-modal human motion generation poses two primary challenges: creating an effective motion generation mechanism and integrating various modalities, such as text, speech, and music, into a cohesive framework.\nUnlike previous methods that usually employ discrete masked modeling or autoregressive modeling, we develop a continuous masked autoregressive motion transformer, where a causal attention is performed considering the sequential nature within the human motion.\nWithin this transformer, we introduce a gated linear attention and an RMSNorm module, which drive the transformer to pay attention to the key actions and suppress the instability caused by either the abnormal movements or the heterogeneous distributions within multi-modalities.\nTo further enhance both the motion generation and the multimodal generalization, we employ the DiT structure to diffuse the conditions from the transformer towards the targets.\nTo fuse different modalities, AdaLN and cross-attention are leveraged to inject the text, speech, and music signals.\nExperimental results demonstrate that our framework outperforms previous methods across all modalities, including text-to-motion, speech-to-gesture, and music-to-dance.\nThe code of our method will be made public.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "method",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Whole-body human motion generation represents an expanding frontier in computer vision, offering significant value across a variety of applications, including film production, gaming, virtual reality, robotics, and so on.\nBroadly speaking, motion generation could be conditioned on various signals, such as text, speech, music, and more.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Historically, approaches to whole-body motion generation usually focus on isolated tasks. Typically, they either address text-to-motion generation, or concentrate on speech-to-gesture translation, or engage in music-to-dance synthesis.\nDespite their successes in single task, their frameworks are exclusively designed for individual tasks and cannot be easily adapted to different tasks.\nIn addition, they tend to overlook the underlying commonalities that exist across different tasks.\nIn contrast, in this work we seek to address these motion generation challenges from various signals within an omni-framework.\nThis brings two advantages: 1) It allows each modality to benefit from the patterns present in other modalities, preventing single-mode solutions from becoming trapped in a local minimum;\n2) It enhances each task with data from other tasks, which is particularly relevant given the limited scale of data available for individual motion tasks.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous studies in motion generation generally proceed in two paths.\nThe first employs the vector quantization (VQ) technique to convert continuous motion to discrete tokens, and then performs autoregressive or masked modeling to predict the tokens&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib71\" title=\"\">2023d</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib65\" title=\"\">a</a>; Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib23\" title=\"\">2023</a>; Zhong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib74\" title=\"\">2023</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib17\" title=\"\">2024</a>)</cite>.\nWhile this path effectively utilizes the strengths of autoregressive and masked modeling, the quantization step inevitably introduces approximation errors, which impose undesirable limits on the quality of the generated motions.\nThe second directly regresses the continuous motions using techniques such as generative adversarial networks (GANs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tulyakov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib54\" title=\"\">2018</a>)</cite>, variational autoencoders (VAEs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib58\" title=\"\">2020</a>; Ahuja &amp; Morency, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib1\" title=\"\">2019</a>; Petrovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib45\" title=\"\">2022</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib15\" title=\"\">2022a</a>)</cite>, or recent diffusion models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib12\" title=\"\">2023</a>; Tevet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib52\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib68\" title=\"\">2024b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib66\" title=\"\">2023b</a>; Ribeiro-Gomes et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib48\" title=\"\">2024</a>)</cite>.\nDespite avoiding the approximation errors, they miss the autoregressive or masked modeling technologies, which have been shown to deliver superior performance in motion generation tasks.\nConsequently, the performance of the motion generated by this path is overall lower than that achieved by the first path.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To both leverage the advantages of autoregressive and masked modeling, and the benefits of the continuous motion representation, in this work we combine them together to propose a continuous masked autoregressive motion generation framework.\nWe apply a random masking on the sequential motion tokens, and employ a transformer to autoregressively predict the masked tokens.\nUnlike the visual MAR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>)</cite>, we sequentially predict the masked tokens with causal attention rather than performing random reordering, considering the sequential nature within human motion.\nTo enhance the MAR modeling in motion space, we introduce a gated linear mechanism and an RMSNorm module.\nThe gated linear mechanism serves as an adaptive feature selector, driving the transformer to not only pay more attention to the key actions, like gesture switching and large movement, but also disregard less relevant frames and suppress redundant actions, like stationary motions.\nThe RMSNorm is particularly advantageous in scenarios with features exhibiting a large dynamic range, e.g., our unified framework for multi-modalities, where the input distributions are highly heterogeneous.\nIn addition, RMSNorm helps relieve the gradient instability caused by the abnormally large motions, such as sudden jumping or turning back.\nAfter the masked autoregressive transformer, the calculated attention of the masked tokens is fed into a series of DiT blocks to diffuse towards the target tokens, which are decoded to the generated motions.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the text-based motion generation, we further extend our framework to multimodal conditions.\nBuilding upon a similar structure, the multimodal signals are fused by AdaLN&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib18\" title=\"\">2022c</a>)</cite> and cross-attention modules.\nExtensive experiments across different datasets demonstrate our framework can work well with different modalities, including text, speech, and music, and outperform previous methods in whole-body text-to-motion, speech-to-gesture, and music-to-dance tasks.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We design an omni motion framework for whole-body human motion generation, where one framework encompasses multiple modalities.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We integrate the multimodal signals via AdaLN and cross-attention, obtaining superior performance than previous methods in text-based, speech-based, and music-based motion generation.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous research on text-based motion generation can be generally categorized into two mainstream paths: continuous regression and discrete classification.\nIn the continuous regression domain, numerous strategies have leveraged the variational autoencoder (VAE) framework, integrating latent embeddings of encoded text with those of encoded poses, which are then decoded into motion predictions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib58\" title=\"\">2020</a>; Ahuja &amp; Morency, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib1\" title=\"\">2019</a>; Athanasiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib4\" title=\"\">2022</a>; Petrovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib45\" title=\"\">2022</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib15\" title=\"\">2022a</a>)</cite>.\nOther methods have investigated the potential of recurrent networks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib33\" title=\"\">2018</a>; Plappert et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib47\" title=\"\">2018</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib70\" title=\"\">2020</a>)</cite>, generative adversarial networks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib7\" title=\"\">2018</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib57\" title=\"\">2020</a>; Tulyakov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib54\" title=\"\">2018</a>)</cite>, or transformer networks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tevet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib51\" title=\"\">2022</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib35\" title=\"\">2023b</a>; Bhattacharya et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib5\" title=\"\">2021</a>; Petrovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib44\" title=\"\">2021</a>)</cite> to enhance the motion regression quality.\nBuilding on the success of diffusion models, recent approaches have begun to integrate the diffusion process into motion diffusion&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib22\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib12\" title=\"\">2023</a>; Tevet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib52\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib68\" title=\"\">2024b</a>; Dabral et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib13\" title=\"\">2023</a>; Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib40\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib66\" title=\"\">2023b</a>; Ribeiro-Gomes et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib48\" title=\"\">2024</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib62\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib56\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib67\" title=\"\">2023c</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib72\" title=\"\">2024d</a>; Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, yielding impressive results.\nIn the discrete classification domain, the input motion undergoes initial encoding via a VQ-VAE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Van Den&#160;Oord et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib55\" title=\"\">2017</a>)</cite>, producing motion tokens for subsequent prediction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib74\" title=\"\">2023</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib32\" title=\"\">2024e</a>)</cite>.\nDrawing inspiration from advancements in natural language processing, some methods utilize autoregressive modeling to predict tokens sequentially&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib16\" title=\"\">2022b</a>; Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib40\" title=\"\">2023</a>; Zou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib80\" title=\"\">2024</a>)</cite>.\nOthers employ generative masked modeling strategies, with tokens randomly masked during training for the model to predict&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib17\" title=\"\">2024</a>; Pinyoanuntapong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib46\" title=\"\">2024</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib61\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib28\" title=\"\">2023b</a>)</cite>.\nMore recently, large language models (LLMs) have been harnessed to help the prediction process, considering their large-scale pretraining&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib71\" title=\"\">2023d</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib77\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib31\" title=\"\">2024d</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib29\" title=\"\">2023c</a>)</cite>.\nIn this work, we seek to integrate the most effective elements from these two paths: the continuous diffusion and the masked autoregressive modeling.\nA previous attempt in this direction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib42\" title=\"\">2024</a>)</cite> directly transfers the MAR in image generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>)</cite> into motion generation without considering the difference between image and motion spaces, especially the temporal correlation. Also, its framework is only designed for body-only motion generation. Differently, we propose a new MAR mechanism that is especially designed for whole-body motion generation.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "training",
                    "results",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to text, there are many other signals that various human motions are conditioned on, such as speech and music.\nIn the realm of speech-to-gesture generation, both continuous regression and discrete classification paths have been explored.\nIn the continuous domain, methods employ deep generative models like GANs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ahuja et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib2\" title=\"\">2022</a>)</cite>, normalizing flows&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib50\" title=\"\">2024</a>)</cite>, and diffusion models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Alexanderson et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib3\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib10\" title=\"\">2024a</a>; He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib20\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib59\" title=\"\">2023</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib78\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib11\" title=\"\">2024b</a>)</cite> to learn complex motion distributions in the speech data.\nIn the discrete domain, methods leverage either the autoregressive modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib60\" title=\"\">2023</a>)</cite> or the masked modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib37\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib38\" title=\"\">b</a>)</cite> to predict the discrete tokens quantified by the VQ-VAE.\nThe primary distinction among these methods lies in their specific handling of different parts of human motion, including body movements, hand gestures, and facial expressions.\nSimilarly, in the realm of music-to-dance generation, there are also methods in both the continuous domain&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhuang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib79\" title=\"\">2022</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib53\" title=\"\">2023</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib25\" title=\"\">2024a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib21\" title=\"\">2024</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib64\" title=\"\">2024a</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite> and the discrete domain&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Siyao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib49\" title=\"\">2022</a>)</cite>.\nThe discrete autoregressive model is leveraged after the motion quantization with VQ-VAE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Siyao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib49\" title=\"\">2022</a>)</cite>.\nMore methods harness the diffusion model to directly regress the target dancing motion in the continuous space&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib53\" title=\"\">2023</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib25\" title=\"\">2024a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib21\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite>.\nRecent methods also start to merge autoregressive and diffusion models, producing coherent and music-aligned dance sequences&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib64\" title=\"\">2024a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "2024a",
                    "generation",
                    "liu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent works have begun to seek multimodal solutions, i.e., designing one framework for motion generation from different input signals&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib41\" title=\"\">2024</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib69\" title=\"\">2024c</a>; Zhou &amp; Wang, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib75\" title=\"\">2023</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib76\" title=\"\">2023</a>; Ling et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib36\" title=\"\">2023</a>; Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib30\" title=\"\">2024c</a>)</cite>.\nSome methods in the discrete domain attempt to incorporate quantized condition tokens into the vocabulary of the generation model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib41\" title=\"\">2024</a>; Zhou &amp; Wang, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib75\" title=\"\">2023</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib76\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib73\" title=\"\">2025</a>)</cite>, while some methods in the continuous domain try to integrate the multimodal signals by designing the motion ControlNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ling et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib36\" title=\"\">2023</a>; Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, where the multimodal conditions guide the sampling of a pretrained text-to-motion diffusion model.\nHowever, most previous methods are restricted by the varying motion data of different modalities, limiting multi-modal frameworks primarily to body-only motion generation.\nTo overcome this, MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite> standardizes datasets across modalities into a unified whole-body format that includes body, hands, and face.\nIn this work, we follow this unified representation to build a whole-body multi-modal motion framework, taking advantage of continuous masked auto-regression.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "face",
                    "generation",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The overview of our framework is illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S2.F2\" title=\"Figure 2 &#8227; Multimodal Motion Generation. &#8227; 2 Related Work &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, which is divided into three main stages:\nIn the first stage, we encode the input motion with an autoencoder, which generates continuous motion tokens.\nIn the second stage, we focus on the text-based motion generation, utilizing our motion masked autoregressive framework to model the motion generation process.\nIn this stage, an autoregressive transformer is employed to predict the masked tokens, within which a gated linear mechanism is designed, and an RMSNorm module is employed.\nThe text information is integrated into the transformer via AdaLN after encoding.\nAfter the transformer, the generated embedding is fed into the DiT modules as the condition to diffuse towards the target token.\nIn the third stage, we extend the model learned in the previous stage to the multi-modal structure.\nThis involves merging the text embedding with multimodal signal embeddings&#8212;specifically speech or music&#8212;prior to their AdaLN input.\nFurthermore, we inject the multimodal embedding through a cross-attention module into the masked transformer.\nIn the multimodal learning stage, the DiT module is kept in the same structure and frozen.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To feed the human motion into the transformer, we start by encoding the original motion into motion tokens.\nGiven a motion sequence <math alttext=\"\\{\\mathbf{M}_{t}\\}_{t=1}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>&#119820;</mi><mi>t</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\{\\mathbf{M}_{t}\\}_{t=1}^{T}</annotation></semantics></math>, the objective of an autoencoder is to extract a latent code <math alttext=\"z_{\\text{AE}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>z</mi><mtext>AE</mtext></msub><annotation encoding=\"application/x-tex\">z_{\\text{AE}}</annotation></semantics></math> that optimally captures the essence of the original motion.\nDifferent from most previous motion generation methods with autoregressive transformers, we use a continuous autoencoder rather than the VQ-VAE to do the encoding, which avoids the precision loss associated with the quantization approximation.\nIn the encoder, we stack the 1D convolution networks with ReLU activation to do the feature processing. Following this, two down-sampling residual blocks are applied to reduce the motion feature size to one-fourth of its original dimensions.\nIn the decoder, the same structure in the reversed order is utilized to up-sample the motion feature back to the original size, producing <math alttext=\"\\{\\hat{\\mathbf{M}}_{t}\\}_{t=1}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mover accent=\"true\"><mi>&#119820;</mi><mo>^</mo></mover><mi>t</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\{\\hat{\\mathbf{M}}_{t}\\}_{t=1}^{T}</annotation></semantics></math>. Therefore, the loss for training the autoencoder is defined as</p>\n\n",
                "matched_terms": [
                    "motion",
                    "following",
                    "generation",
                    "loss",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the continuous motion tokens, we design a masked autoregressive transformer to model the motion generation, effectively capturing the temporal relationship between different tokens and producing the rich contextual condition <math alttext=\"\\mathbf{z}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><msup><mi>&#119859;</mi><mi>i</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{z}^{i}</annotation></semantics></math> for the subsequent diffusion process.\nWe first randomly mask the motion tokens following language models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib14\" title=\"\">2018</a>)</cite>, obtaining some masked tokens <math alttext=\"\\{\\tilde{\\mathbf{x}}^{i}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msup><mover accent=\"true\"><mi>&#119857;</mi><mo>~</mo></mover><mi>i</mi></msup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\tilde{\\mathbf{x}}^{i}\\}</annotation></semantics></math>.\nThe temporal masking strategies adopt the same mask ratio schedule following&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib8\" title=\"\">2022</a>)</cite>, and are computed as</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the input text prompt <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>, we first employ the LaMP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib32\" title=\"\">2024e</a>)</cite> text transformer to extract textual features, leveraging its advanced capabilities to encode the linguistic nuances and semantic structure of the input prompt. This creates a high-dimensional feature representation that is crucial for guiding motion generation process.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the feature extraction, we utilize AdaLN to seamlessly integrate the text-derived control signals into the masked autoregressive transformer. AdaLN offers a dynamic approach to normalization, allowing the modulation of its parameters in response to the specific text input, thereby facilitating subsequent multimodal condition injection. By employing this method, we enhance the model&#8217;s ability to incorporate the guiding signals from the text and other signals into the motion generation process, ensuring that the transformer&#8217;s output features are better aligned with the intended motion generation goals. The features outputted by the transformer embody a strong directive capacity for motion generation. This enables the model not only to faithfully interpret the semantic content of the input text but also to produce motion sequences that are coherent with and reflective of the textual intent. The enriched output features contribute to achieving smoother transitions and logically consistent motion sequences in complex generation scenarios.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "method",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to previous works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>; Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib42\" title=\"\">2024</a>)</cite>, we adopt Diffusion Transformer (DiT) as our diffusion model. While the use of DiT may incur additional time costs during training and inference (not much due to the compact structure of motion data), it significantly enhances the quality of generated outputs. Compared to MLPs, DiT provides greater convenience in injecting conditional control signals. During the training process of multimodal generation, we freeze the diffusion model and only fine-tune the masked transformer. The structural characteristics of DiT facilitate this approach, enabling it to better handle various types of conditional signals.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, MLPs exhibit notable limitations when processing heterogeneous data. This incapacity results in suboptimal performance when confronted with diverse signal types, such as speech and music. Due to the relatively small number of parameters in MLPs, they are prone to overfitting on specific datasets (e.g., text-to-motion). This situation can be analogized to a dancer who is adept only in a single dance style; when asked to incorporate other styles, they appear clumsy and ineffective. Consequently, when we attempt to fine-tune MLPs on a different dataset, they are ill-equipped to adapt to the challenges posed by new signals, leading to failures in multimodal generation tasks.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "generation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first pre-train the model on text-motion paired data in a text-to-motion generation setting. Owing to its strong semantic expressiveness and cross-modal alignment properties, we adopt text as a shared conditional signal across diverse unimodal datasets, enabling the model to learn sequence-level generation capabilities between text and motion, as well as a coarse-grained textual guidance mechanism for generative control.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hypothesize that the contextual features output by the masked transformer provide a more expressive control signal compared to raw text embeddings. Accordingly, within the DiT architecture, we inject the transformer&#8217;s output features by summing them with the time embeddings, thereby guiding the motion generation process as:</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation Details. </span>\nOur model is implemented on one NVIDIA V100 GPU using PyTorch. For our method, the autoencoder employs a ResNet-based&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib19\" title=\"\">2016</a>)</cite> three-layer encoder-decoder architecture with a hidden dimension of 512 and an overall downsampling rate of 4. For the generation process, we utilize a four-layer AdaLN-zero transformer encoder as the masked autoregressive transformer, featuring a hidden dimension of 1024 and 16 attention heads. The diffusion model consists of 4 layers of DiT, where each transformer block has a hidden dimension of 1792 and 8 attention heads.\nWe adopt the AdamW optimizer <math alttext=\"(\\beta_{1}=0.9,\\beta_{2}=0.99)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><mo>,</mo><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.99</mn></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\beta_{1}=0.9,\\beta_{2}=0.99)</annotation></semantics></math>. For training the autoencoder on the HumanML subset of Motion-X, we use a batch size of 256 and a maximum sequence length of 64 frames. For the text-to-motion task, the batch size is set to 50 with a maximum sequence length of 196 frames. The learning rate is initialized at 0.0002 with a linear warmup over 2000 steps. The autoencoder is trained for 50 epochs, while the text-to-motion task is trained for 1500 epochs.\nDuring multimodal generation, we first initialize the model with pretrained weights from the text-to-motion autoencoder and fine-tune it on task-specific datasets. Subsequently, we freeze the parameters of the text-to-motion DiT and only fine-tune the masked transformer along with newly incorporated cross-attention layers. The learning rate and training schedule remain consistent with the text-to-motion task.\nFor all three tasks, we employ exponential moving average (EMA) to update model parameters, ensuring training stability. During inference, the classifier-free guidance (CFG) scale is set to 4.5 for text-to-motion, while other tasks use a CFG scale of 6.5.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "training",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets and Metrics. </span>\nFor the evaluation, we utilize three datasets: HumanML3D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib15\" title=\"\">2022a</a>)</cite> for text-to-motion, BEAT2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib37\" title=\"\">2024a</a>)</cite> for speech-to-gesture, and FineDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite> for music-to-dance, all following the unified SMPL-X representation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>.\nRegarding the metrics, we use FID, R-Precision, and MM-Dist for text-based motion generation, use FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">H</span></sub>, FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">B</span></sub>, Face L2 loss, Beat Alignment Score, Diversity for speech-based motion generation, and use FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">H</span></sub>, FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">B</span></sub>, Diversity for music-based motion generation, respectively.\nFor the detailed explanation of the datasets and metrics, please refer to the appendix.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "beat",
                    "score",
                    "following",
                    "2024a",
                    "liu",
                    "face",
                    "generation",
                    "diversity",
                    "loss",
                    "bian",
                    "beat2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-based motion generation. </span>\nWe conduct an evaluation of our model against prior text-to-motion approaches, including both discrete-domain and continuous-domain methods.\nAs summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S3.T1\" title=\"Table 1 &#8227; 3.5 Text-to-motion Pretraining and Multimodal Control Adaptation &#8227; 3 Method &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our method clearly surpasses prior techniques on the HumanML subset of Motion-X dataset. Remarkably, our model achieves improvements of 19.3%, 13.5%, and 11.7% in R-Precision for Top-1, 2, 3, respectively. Additionally, we enhance the FID score by 75.2% on this dataset, underscoring the exceptional fidelity of our generated motions. The qualitative results in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S3.F4\" title=\"Figure 4 &#8227; 3.5 Text-to-motion Pretraining and Multimodal Control Adaptation &#8227; 3 Method &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> further support these findings, showing that our approach yields whole-body motions that align closely with the input text.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "score",
                    "align",
                    "generation",
                    "results",
                    "method",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-based motion generation. </span>\nTo assess the speech-driven motion generation, we compare to previous speech-to-gesture methods.\nOur results, summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S3.T2\" title=\"Table 2 &#8227; 3.6 Inference &#8227; 3 Method &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, reveal that our method achieves good quality and diversity in both hand and body motion generation and excels in aligning\nwith the rhythm of first-person speech.\nThis demonstrates the effectiveness of our framework in motion generation when encompassing different modal signals.\nHowever, our method performs worse than single-modal methods.\nAs discussed in &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, this is attributed to the random or average expressions in the Motion-X dataset, which confuses the speech-to-gesture training.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "speechdriven",
                    "generation",
                    "diversity",
                    "training",
                    "results",
                    "bian",
                    "method",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Music-based motion generation. </span>\nWe further evaluate our framework on the music-to-dance task. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S4.T3\" title=\"Table 3 &#8227; 4.1 Experiment Settings &#8227; 4 Experiments &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our method achieves slightly improved performance over previous approaches, particularly in generating hand motions and body movements.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Causal Attention. </span>\nTo verify the efficacy of the proposed framework, we initially establish a baseline model following the visual MAR setup&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>)</cite>, i.e, using the random pseudo reordering for the batched masked prediction, through the bidirectional attention computing.\nFrom the results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Ablation Study &#8227; 4 Experiments &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we see that the performance of this baseline in the motion area is limited. We attribute this to the difference between human motions and visual images, e.g., the human motion is in a strong temporal sequential structure, in which case a causal attention makes more sense.\nTherefore, changing the baseline to sequential masked prediction with causal attention improves the performance.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "results",
                    "setup",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DiT. </span>\nIn order to evaluate how the DiTs contribute to the motion quality, we further replace the MLPs in the baseline model with our DiTs.\nAs shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Ablation Study &#8227; 4 Experiments &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the model generates superior motions with DiTs compared to MLPs, especially in the context of multimodal motion generation.\nThis reveals the superior potential of DiTs in generating motion with complex multimodal contexts.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross Attention. </span>\nIn the baseline model, the multimodal signals are injected with only the AdaLN structure.\nWe then add the cross attention module and observe a significant improvement in multimodal motion generation, as depicted in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Ablation Study &#8227; 4 Experiments &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper proposes a new omni motion framework for multimodal whole-body human motion generation.\nWithin this one framework, text, speech, and music signals are all encompassed through AdaLN and cross-attention.\nThe motion generation process is modeled by a continuous masked autoregressive transformer with causal attention, as well as a DiT structure.\nExtensive experiments have been conducted to verify the efficacy of the proposed framework in different-modality tasks.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations.</span>\nDue to the restricted dataset, the naturalness and generalizability of the motion generation model are still limited, especially in speech and music-driven motion generation.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "dataset",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our speech and music encoders are designed to extract temporally aligned, high-level features from raw audio signals for effective speech-to-gesture and music-to-dance generation. The architecture builds upon a multi-layer 1D convolutional network with strided convolutions and leaky ReLU activations. Each convolutional block consists of a series of a block unit that progressively downsample the input waveform while increasing feature dimensionality. The input audio sequence is first processed through multiple stages of temporal aggregation and non-linear transformation, resulting in a sequence of compact and expressive latent representations, whereas the input music typically retains sufficient temporal structure and spectral richness in its raw form for effective motion synthesis. These latent codes capture prosodic, rhythmic, and semantic-like patterns in speech and music, which are then projected into the condition latent space of dimensionality. The final encoder output is transposed to align with the temporal structure expected by the diffusion model, enabling fine-grained cross-modal interaction between speech and motion sequences during generation.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "align"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first pretrain a baseline autoencoder on the text-to-motion task. When fine-tuning it on the speech-to-gesture and music-to-dance tasks, the decoder fails to reconstruct valid motion sequences due to discrepancies in data distribution. However, fine-tuning the autoencoder using the reconstruction objective during the multi-modal training incurs high computational costs. Therefore, we independently fine-tune the baseline AE on each dataset using the reconstruction task before multi-modal generation, and employ the resulting models for downstream tasks.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "dataset",
                    "generation",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following previous work&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Bian et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, we utilize SMPL-X formatted motion data with an input dimension of (frame length &#215; 322). The parameter structure is organized as follows: Root orientation (0:3): controls global body rotation; body pose (3:66): Governs major body joint rotations; hand articulation (66:156): controls finger movements; jaw pose (156:159): manages mouth opening/closing; facial expression (159:209): drives emotional expressions; facial shape (209: 309): determines static facial structure; translation (309:312): controls global body position; betas (312: 322): represents static body shape parameters. And the maximum motion length is 196.\nThe model&#8217;s output maintains identical dimensionality (frame length &#215; 322) to ensure full reconstruction capability. This comprehensive parameterization enables simultaneous control of body motion, facial animation, and global positioning within a unified framework.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "following",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For text-based motion generation, we evaluate our method on the HumanML3D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib15\" title=\"\">2022a</a>)</cite> dataset, which consists of 14,616 high-quality human motions paired with 44,970 text descriptions.\nThe original body-only SMPL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Loper et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib39\" title=\"\">2023</a>)</cite> format of this dataset is extended to whole-body SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pavlakos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib43\" title=\"\">2019</a>)</cite> format in MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, which we follow in the experiments for evaluation.\nFor speech-based motion generation, we evaluate on the BEAT2 dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib37\" title=\"\">2024a</a>)</cite>, which collects 76 hours of data from 30 speakers, standardized into a mesh representation with paired audio and text lines.\nThe motion of the unified SMPL-X format is also extracted&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite> for multimodal evaluation.\nFor music-based motion generation, the largest dataset FineDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite> is utilized for evaluation. This dataset collects dances of 14.6 hours across 22 genres and provides detailed human motions using the SMPL-H format, which is then converted to the unified SMPL-X format and appended by text descriptions.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "2024a",
                    "liu",
                    "generation",
                    "method",
                    "bian",
                    "dataset",
                    "beat2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable full-body, multimodal control over motion generation, we convert all datasets to the SMPL-X format. This involves filling in missing facial expressions in HumanML3D and FineDance using average expression coefficients from the training set, as well as transforming the SMPL-H Rot-6D representation in FineDance into axis-angle format via Gram-Schmidt orthogonalization. This conversion achieves better alignment with SMPL-X parameters and introduces minimal errors compared to the official body-retargeting method, while also offering improved computational efficiency.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "training",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure consistency with MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, we utilize the pretrained motion encoder and text encode, enabling a unified evaluation of the SMPL-X motion representation across different modalities. For datasets that lack corresponding textual annotations&#8212;namely FineDance and BEAT2&#8212;we generate pseudo-captions such as &#8220;A dancer is performing a street dance in the Jazz style to the rhythm of the wildfire&#8221; and &#8220;A person is giving a speech, and the content is &#8230;&#8221;, respectively, to support cross-modal learning.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the quality of the motions generated based on texts compared to the true data, we utilize the Frechet Inception Distance (FID) to evaluate the distribution differences between the generated motions and the ground truth.\nAdditionally, R-Precision is employed to determine how frequently the most relevant motions, identified as top-k closest matches, align with their respective captions within a batch of 32 samples. Lastly, Multi-Modal Distance (MM Dist) is employed to gauge the average Euclidean distance between motion representations and their corresponding textual features.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "align"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluating the quality and diversity of the motions generated based on speech, we employ FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">H</span></sub>, FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">B</span></sub>, and Diversity metrics. FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">H</span></sub> measures the difference between hand motion distribution and the true gesture distribution, whereas FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">B</span></sub> assesses the divergence between the whole-body motion distributions. The Beat Alignment Score&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib26\" title=\"\">2021</a>)</cite> is used to measure the synchronization between motions and speech beats. To quantify the difference between generated expressions and actual expressions, we use the L2 Loss.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "beat",
                    "score",
                    "diversity",
                    "loss"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Mirroring the approach used for speech-driven gesture generation, we apply FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">H</span></sub>, FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">B</span></sub>, and Diversity metrics to evaluate the quality and diversity of music-induced hand and whole-body movements. This approach ensures that the generated motions exhibit both high fidelity and variation.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "speechdriven",
                    "diversity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide a more comprehensive evaluation, we conduct additional comparisons on the original HumanML3D benchmark using the body-only H3D format, which contains redundant motion information.\nHere we mainly compare with the methods without VQ-VAE.\nAs shown in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#A1.T5\" title=\"Table 5 &#8227; Text-based Motion Generation &#8227; A.3 Metrics &#8227; Appendix A Appendix &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, OmniMotion consistently outperforms these baselines in terms of text-motion alignment, motion quality, and diversity, demonstrating its superior generalization capability across different motion representations.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "diversity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a comprehensive evaluation on the final model (after fine-tuning on both speech-to-gesture and music-to-dance datasets) and present the results below. Since textual conditioning participates throughout the entire training pipeline, our model does not suffer from catastrophic forgetting after fine-tuning. This confirms the robustness of our architecture&#8217;s knowledge retention capabilities under different training paradigms.</p>\n\n",
                "matched_terms": [
                    "training",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the same strategy as MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, we train two variants of our model: OmniMotion-Base and OmniMotion-Mix. OmniMotion-Base is a text-to-motion model pretrained solely on HumanML3D, while OmniMotion-Mix is trained on a combined dataset comprising HumanML3D, BEAT2, and FineDance to enable multimodal motion generation. Quantitative results on the text-to-motion task are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#A1.T7\" title=\"Table 7 &#8227; Text-based Motion Generation &#8227; A.3 Metrics &#8227; Appendix A Appendix &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "following",
                    "generation",
                    "results",
                    "bian",
                    "dataset",
                    "beat2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We display more visual results of speech-driven and music-driven motion generation in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#A1.F5\" title=\"Figure 5 &#8227; Text-based Motion Generation &#8227; A.3 Metrics &#8227; Appendix A Appendix &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#A1.F6\" title=\"Figure 6 &#8227; Text-based Motion Generation &#8227; A.3 Metrics &#8227; Appendix A Appendix &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, respectively.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "results",
                    "speechdriven",
                    "generation"
                ]
            }
        ]
    },
    "A1.T9": {
        "caption": "Table 9: Results of music-driven motion generation on the FineDance dataset (Li et al., 2023a) following the mix training setup (Bian et al., 2025).",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Method</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">FID<math alttext=\"{}_{H}\\downarrow\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"A1.T9.m1\" intent=\":literal\"><semantics><mmultiscripts><mo stretchy=\"false\">&#8595;</mo><mprescripts/><mi>H</mi><mrow/></mmultiscripts><annotation encoding=\"application/x-tex\">{}_{H}\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">FID<math alttext=\"{}_{B}\\downarrow\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"A1.T9.m2\" intent=\":literal\"><semantics><mmultiscripts><mo stretchy=\"false\">&#8595;</mo><mprescripts/><mi>B</mi><mrow/></mmultiscripts><annotation encoding=\"application/x-tex\">{}_{B}\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Div <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T9.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">MotionCraft-Basic&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">3.858</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">76.248</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">16.667</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">MotionCraft-Mix&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">2.849</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">67.159</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">18.483</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Ours-Basic</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">3.632</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">71.930</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">15.871</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Ours-Mix</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">2.781</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\">64.380</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">17.605</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "motion",
            "setup",
            "finedance",
            "bian",
            "motioncraftmix",
            "generation",
            "mix",
            "training",
            "fid↓hhdownarrow",
            "musicdriven",
            "following",
            "2023a",
            "results",
            "div",
            "↑uparrow",
            "fid↓bbdownarrow",
            "oursmix",
            "method",
            "dataset",
            "oursbasic",
            "motioncraftbasic"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We further evaluate OmniMotion-Mix across all three modalities. For the speech-to-gesture and music-to-dance tasks, we fine-tune the model on the respective target datasets. The corresponding results are reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#A1.T8\" title=\"Table 8 &#8227; Text-based Motion Generation &#8227; A.3 Metrics &#8227; Appendix A Appendix &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> and Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#A1.T9\" title=\"Table 9 &#8227; Evaluation of OmniMotion Variants &#8227; A.4 Additional Experiment Results &#8227; Appendix A Appendix &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, respectively.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Whole-body multi-modal human motion generation poses two primary challenges: creating an effective motion generation mechanism and integrating various modalities, such as text, speech, and music, into a cohesive framework.\nUnlike previous methods that usually employ discrete masked modeling or autoregressive modeling, we develop a continuous masked autoregressive motion transformer, where a causal attention is performed considering the sequential nature within the human motion.\nWithin this transformer, we introduce a gated linear attention and an RMSNorm module, which drive the transformer to pay attention to the key actions and suppress the instability caused by either the abnormal movements or the heterogeneous distributions within multi-modalities.\nTo further enhance both the motion generation and the multimodal generalization, we employ the DiT structure to diffuse the conditions from the transformer towards the targets.\nTo fuse different modalities, AdaLN and cross-attention are leveraged to inject the text, speech, and music signals.\nExperimental results demonstrate that our framework outperforms previous methods across all modalities, including text-to-motion, speech-to-gesture, and music-to-dance.\nThe code of our method will be made public.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "method",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Whole-body human motion generation represents an expanding frontier in computer vision, offering significant value across a variety of applications, including film production, gaming, virtual reality, robotics, and so on.\nBroadly speaking, motion generation could be conditioned on various signals, such as text, speech, music, and more.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Historically, approaches to whole-body motion generation usually focus on isolated tasks. Typically, they either address text-to-motion generation, or concentrate on speech-to-gesture translation, or engage in music-to-dance synthesis.\nDespite their successes in single task, their frameworks are exclusively designed for individual tasks and cannot be easily adapted to different tasks.\nIn addition, they tend to overlook the underlying commonalities that exist across different tasks.\nIn contrast, in this work we seek to address these motion generation challenges from various signals within an omni-framework.\nThis brings two advantages: 1) It allows each modality to benefit from the patterns present in other modalities, preventing single-mode solutions from becoming trapped in a local minimum;\n2) It enhances each task with data from other tasks, which is particularly relevant given the limited scale of data available for individual motion tasks.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous studies in motion generation generally proceed in two paths.\nThe first employs the vector quantization (VQ) technique to convert continuous motion to discrete tokens, and then performs autoregressive or masked modeling to predict the tokens&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib71\" title=\"\">2023d</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib65\" title=\"\">a</a>; Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib23\" title=\"\">2023</a>; Zhong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib74\" title=\"\">2023</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib17\" title=\"\">2024</a>)</cite>.\nWhile this path effectively utilizes the strengths of autoregressive and masked modeling, the quantization step inevitably introduces approximation errors, which impose undesirable limits on the quality of the generated motions.\nThe second directly regresses the continuous motions using techniques such as generative adversarial networks (GANs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tulyakov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib54\" title=\"\">2018</a>)</cite>, variational autoencoders (VAEs)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib58\" title=\"\">2020</a>; Ahuja &amp; Morency, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib1\" title=\"\">2019</a>; Petrovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib45\" title=\"\">2022</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib15\" title=\"\">2022a</a>)</cite>, or recent diffusion models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib12\" title=\"\">2023</a>; Tevet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib52\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib68\" title=\"\">2024b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib66\" title=\"\">2023b</a>; Ribeiro-Gomes et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib48\" title=\"\">2024</a>)</cite>.\nDespite avoiding the approximation errors, they miss the autoregressive or masked modeling technologies, which have been shown to deliver superior performance in motion generation tasks.\nConsequently, the performance of the motion generated by this path is overall lower than that achieved by the first path.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To both leverage the advantages of autoregressive and masked modeling, and the benefits of the continuous motion representation, in this work we combine them together to propose a continuous masked autoregressive motion generation framework.\nWe apply a random masking on the sequential motion tokens, and employ a transformer to autoregressively predict the masked tokens.\nUnlike the visual MAR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>)</cite>, we sequentially predict the masked tokens with causal attention rather than performing random reordering, considering the sequential nature within human motion.\nTo enhance the MAR modeling in motion space, we introduce a gated linear mechanism and an RMSNorm module.\nThe gated linear mechanism serves as an adaptive feature selector, driving the transformer to not only pay more attention to the key actions, like gesture switching and large movement, but also disregard less relevant frames and suppress redundant actions, like stationary motions.\nThe RMSNorm is particularly advantageous in scenarios with features exhibiting a large dynamic range, e.g., our unified framework for multi-modalities, where the input distributions are highly heterogeneous.\nIn addition, RMSNorm helps relieve the gradient instability caused by the abnormally large motions, such as sudden jumping or turning back.\nAfter the masked autoregressive transformer, the calculated attention of the masked tokens is fed into a series of DiT blocks to diffuse towards the target tokens, which are decoded to the generated motions.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to the text-based motion generation, we further extend our framework to multimodal conditions.\nBuilding upon a similar structure, the multimodal signals are fused by AdaLN&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib18\" title=\"\">2022c</a>)</cite> and cross-attention modules.\nExtensive experiments across different datasets demonstrate our framework can work well with different modalities, including text, speech, and music, and outperform previous methods in whole-body text-to-motion, speech-to-gesture, and music-to-dance tasks.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We design an omni motion framework for whole-body human motion generation, where one framework encompasses multiple modalities.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We integrate the multimodal signals via AdaLN and cross-attention, obtaining superior performance than previous methods in text-based, speech-based, and music-based motion generation.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous research on text-based motion generation can be generally categorized into two mainstream paths: continuous regression and discrete classification.\nIn the continuous regression domain, numerous strategies have leveraged the variational autoencoder (VAE) framework, integrating latent embeddings of encoded text with those of encoded poses, which are then decoded into motion predictions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib58\" title=\"\">2020</a>; Ahuja &amp; Morency, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib1\" title=\"\">2019</a>; Athanasiou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib4\" title=\"\">2022</a>; Petrovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib45\" title=\"\">2022</a>; Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib15\" title=\"\">2022a</a>)</cite>.\nOther methods have investigated the potential of recurrent networks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib33\" title=\"\">2018</a>; Plappert et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib47\" title=\"\">2018</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib70\" title=\"\">2020</a>)</cite>, generative adversarial networks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Cai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib7\" title=\"\">2018</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib57\" title=\"\">2020</a>; Tulyakov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib54\" title=\"\">2018</a>)</cite>, or transformer networks&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tevet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib51\" title=\"\">2022</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib35\" title=\"\">2023b</a>; Bhattacharya et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib5\" title=\"\">2021</a>; Petrovich et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib44\" title=\"\">2021</a>)</cite> to enhance the motion regression quality.\nBuilding on the success of diffusion models, recent approaches have begun to integrate the diffusion process into motion diffusion&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib22\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib12\" title=\"\">2023</a>; Tevet et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib52\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib68\" title=\"\">2024b</a>; Dabral et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib13\" title=\"\">2023</a>; Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib40\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib66\" title=\"\">2023b</a>; Ribeiro-Gomes et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib48\" title=\"\">2024</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib62\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib56\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib67\" title=\"\">2023c</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib72\" title=\"\">2024d</a>; Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, yielding impressive results.\nIn the discrete classification domain, the input motion undergoes initial encoding via a VQ-VAE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Van Den&#160;Oord et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib55\" title=\"\">2017</a>)</cite>, producing motion tokens for subsequent prediction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib74\" title=\"\">2023</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib32\" title=\"\">2024e</a>)</cite>.\nDrawing inspiration from advancements in natural language processing, some methods utilize autoregressive modeling to predict tokens sequentially&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib16\" title=\"\">2022b</a>; Lou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib40\" title=\"\">2023</a>; Zou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib80\" title=\"\">2024</a>)</cite>.\nOthers employ generative masked modeling strategies, with tokens randomly masked during training for the model to predict&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib17\" title=\"\">2024</a>; Pinyoanuntapong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib46\" title=\"\">2024</a>; Yuan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib61\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib28\" title=\"\">2023b</a>)</cite>.\nMore recently, large language models (LLMs) have been harnessed to help the prediction process, considering their large-scale pretraining&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib71\" title=\"\">2023d</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib77\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib31\" title=\"\">2024d</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib29\" title=\"\">2023c</a>)</cite>.\nIn this work, we seek to integrate the most effective elements from these two paths: the continuous diffusion and the masked autoregressive modeling.\nA previous attempt in this direction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib42\" title=\"\">2024</a>)</cite> directly transfers the MAR in image generation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>)</cite> into motion generation without considering the difference between image and motion spaces, especially the temporal correlation. Also, its framework is only designed for body-only motion generation. Differently, we propose a new MAR mechanism that is especially designed for whole-body motion generation.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "training",
                    "results",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to text, there are many other signals that various human motions are conditioned on, such as speech and music.\nIn the realm of speech-to-gesture generation, both continuous regression and discrete classification paths have been explored.\nIn the continuous domain, methods employ deep generative models like GANs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ahuja et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib2\" title=\"\">2022</a>)</cite>, normalizing flows&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib50\" title=\"\">2024</a>)</cite>, and diffusion models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Alexanderson et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib3\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib10\" title=\"\">2024a</a>; He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib20\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib59\" title=\"\">2023</a>; Zhu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib78\" title=\"\">2023</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib11\" title=\"\">2024b</a>)</cite> to learn complex motion distributions in the speech data.\nIn the discrete domain, methods leverage either the autoregressive modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib60\" title=\"\">2023</a>)</cite> or the masked modeling&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib37\" title=\"\">2024a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib38\" title=\"\">b</a>)</cite> to predict the discrete tokens quantified by the VQ-VAE.\nThe primary distinction among these methods lies in their specific handling of different parts of human motion, including body movements, hand gestures, and facial expressions.\nSimilarly, in the realm of music-to-dance generation, there are also methods in both the continuous domain&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhuang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib79\" title=\"\">2022</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib53\" title=\"\">2023</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib25\" title=\"\">2024a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib21\" title=\"\">2024</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib64\" title=\"\">2024a</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite> and the discrete domain&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Siyao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib49\" title=\"\">2022</a>)</cite>.\nThe discrete autoregressive model is leveraged after the motion quantization with VQ-VAE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Siyao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib49\" title=\"\">2022</a>)</cite>.\nMore methods harness the diffusion model to directly regress the target dancing motion in the continuous space&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib53\" title=\"\">2023</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib25\" title=\"\">2024a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib21\" title=\"\">2024</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite>.\nRecent methods also start to merge autoregressive and diffusion models, producing coherent and music-aligned dance sequences&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib64\" title=\"\">2024a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "2023a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent works have begun to seek multimodal solutions, i.e., designing one framework for motion generation from different input signals&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib41\" title=\"\">2024</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib69\" title=\"\">2024c</a>; Zhou &amp; Wang, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib75\" title=\"\">2023</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib76\" title=\"\">2023</a>; Ling et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib36\" title=\"\">2023</a>; Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>; Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib30\" title=\"\">2024c</a>)</cite>.\nSome methods in the discrete domain attempt to incorporate quantized condition tokens into the vocabulary of the generation model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Luo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib41\" title=\"\">2024</a>; Zhou &amp; Wang, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib75\" title=\"\">2023</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib76\" title=\"\">2023</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib73\" title=\"\">2025</a>)</cite>, while some methods in the continuous domain try to integrate the multimodal signals by designing the motion ControlNet&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ling et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib36\" title=\"\">2023</a>; Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, where the multimodal conditions guide the sampling of a pretrained text-to-motion diffusion model.\nHowever, most previous methods are restricted by the varying motion data of different modalities, limiting multi-modal frameworks primarily to body-only motion generation.\nTo overcome this, MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite> standardizes datasets across modalities into a unified whole-body format that includes body, hands, and face.\nIn this work, we follow this unified representation to build a whole-body multi-modal motion framework, taking advantage of continuous masked auto-regression.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The overview of our framework is illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S2.F2\" title=\"Figure 2 &#8227; Multimodal Motion Generation. &#8227; 2 Related Work &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, which is divided into three main stages:\nIn the first stage, we encode the input motion with an autoencoder, which generates continuous motion tokens.\nIn the second stage, we focus on the text-based motion generation, utilizing our motion masked autoregressive framework to model the motion generation process.\nIn this stage, an autoregressive transformer is employed to predict the masked tokens, within which a gated linear mechanism is designed, and an RMSNorm module is employed.\nThe text information is integrated into the transformer via AdaLN after encoding.\nAfter the transformer, the generated embedding is fed into the DiT modules as the condition to diffuse towards the target token.\nIn the third stage, we extend the model learned in the previous stage to the multi-modal structure.\nThis involves merging the text embedding with multimodal signal embeddings&#8212;specifically speech or music&#8212;prior to their AdaLN input.\nFurthermore, we inject the multimodal embedding through a cross-attention module into the masked transformer.\nIn the multimodal learning stage, the DiT module is kept in the same structure and frozen.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To feed the human motion into the transformer, we start by encoding the original motion into motion tokens.\nGiven a motion sequence <math alttext=\"\\{\\mathbf{M}_{t}\\}_{t=1}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>&#119820;</mi><mi>t</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\{\\mathbf{M}_{t}\\}_{t=1}^{T}</annotation></semantics></math>, the objective of an autoencoder is to extract a latent code <math alttext=\"z_{\\text{AE}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>z</mi><mtext>AE</mtext></msub><annotation encoding=\"application/x-tex\">z_{\\text{AE}}</annotation></semantics></math> that optimally captures the essence of the original motion.\nDifferent from most previous motion generation methods with autoregressive transformers, we use a continuous autoencoder rather than the VQ-VAE to do the encoding, which avoids the precision loss associated with the quantization approximation.\nIn the encoder, we stack the 1D convolution networks with ReLU activation to do the feature processing. Following this, two down-sampling residual blocks are applied to reduce the motion feature size to one-fourth of its original dimensions.\nIn the decoder, the same structure in the reversed order is utilized to up-sample the motion feature back to the original size, producing <math alttext=\"\\{\\hat{\\mathbf{M}}_{t}\\}_{t=1}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mover accent=\"true\"><mi>&#119820;</mi><mo>^</mo></mover><mi>t</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\{\\hat{\\mathbf{M}}_{t}\\}_{t=1}^{T}</annotation></semantics></math>. Therefore, the loss for training the autoencoder is defined as</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "training",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the continuous motion tokens, we design a masked autoregressive transformer to model the motion generation, effectively capturing the temporal relationship between different tokens and producing the rich contextual condition <math alttext=\"\\mathbf{z}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><msup><mi>&#119859;</mi><mi>i</mi></msup><annotation encoding=\"application/x-tex\">\\mathbf{z}^{i}</annotation></semantics></math> for the subsequent diffusion process.\nWe first randomly mask the motion tokens following language models&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Devlin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib14\" title=\"\">2018</a>)</cite>, obtaining some masked tokens <math alttext=\"\\{\\tilde{\\mathbf{x}}^{i}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msup><mover accent=\"true\"><mi>&#119857;</mi><mo>~</mo></mover><mi>i</mi></msup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\tilde{\\mathbf{x}}^{i}\\}</annotation></semantics></math>.\nThe temporal masking strategies adopt the same mask ratio schedule following&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib8\" title=\"\">2022</a>)</cite>, and are computed as</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the input text prompt <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>, we first employ the LaMP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib32\" title=\"\">2024e</a>)</cite> text transformer to extract textual features, leveraging its advanced capabilities to encode the linguistic nuances and semantic structure of the input prompt. This creates a high-dimensional feature representation that is crucial for guiding motion generation process.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the feature extraction, we utilize AdaLN to seamlessly integrate the text-derived control signals into the masked autoregressive transformer. AdaLN offers a dynamic approach to normalization, allowing the modulation of its parameters in response to the specific text input, thereby facilitating subsequent multimodal condition injection. By employing this method, we enhance the model&#8217;s ability to incorporate the guiding signals from the text and other signals into the motion generation process, ensuring that the transformer&#8217;s output features are better aligned with the intended motion generation goals. The features outputted by the transformer embody a strong directive capacity for motion generation. This enables the model not only to faithfully interpret the semantic content of the input text but also to produce motion sequences that are coherent with and reflective of the textual intent. The enriched output features contribute to achieving smoother transitions and logically consistent motion sequences in complex generation scenarios.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "method",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to previous works&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>; Meng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib42\" title=\"\">2024</a>)</cite>, we adopt Diffusion Transformer (DiT) as our diffusion model. While the use of DiT may incur additional time costs during training and inference (not much due to the compact structure of motion data), it significantly enhances the quality of generated outputs. Compared to MLPs, DiT provides greater convenience in injecting conditional control signals. During the training process of multimodal generation, we freeze the diffusion model and only fine-tune the masked transformer. The structural characteristics of DiT facilitate this approach, enabling it to better handle various types of conditional signals.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moreover, MLPs exhibit notable limitations when processing heterogeneous data. This incapacity results in suboptimal performance when confronted with diverse signal types, such as speech and music. Due to the relatively small number of parameters in MLPs, they are prone to overfitting on specific datasets (e.g., text-to-motion). This situation can be analogized to a dancer who is adept only in a single dance style; when asked to incorporate other styles, they appear clumsy and ineffective. Consequently, when we attempt to fine-tune MLPs on a different dataset, they are ill-equipped to adapt to the challenges posed by new signals, leading to failures in multimodal generation tasks.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "generation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first pre-train the model on text-motion paired data in a text-to-motion generation setting. Owing to its strong semantic expressiveness and cross-modal alignment properties, we adopt text as a shared conditional signal across diverse unimodal datasets, enabling the model to learn sequence-level generation capabilities between text and motion, as well as a coarse-grained textual guidance mechanism for generative control.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We hypothesize that the contextual features output by the masked transformer provide a more expressive control signal compared to raw text embeddings. Accordingly, within the DiT architecture, we inject the transformer&#8217;s output features by summing them with the time embeddings, thereby guiding the motion generation process as:</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Implementation Details. </span>\nOur model is implemented on one NVIDIA V100 GPU using PyTorch. For our method, the autoencoder employs a ResNet-based&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib19\" title=\"\">2016</a>)</cite> three-layer encoder-decoder architecture with a hidden dimension of 512 and an overall downsampling rate of 4. For the generation process, we utilize a four-layer AdaLN-zero transformer encoder as the masked autoregressive transformer, featuring a hidden dimension of 1024 and 16 attention heads. The diffusion model consists of 4 layers of DiT, where each transformer block has a hidden dimension of 1792 and 8 attention heads.\nWe adopt the AdamW optimizer <math alttext=\"(\\beta_{1}=0.9,\\beta_{2}=0.99)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mrow><mrow><msub><mi>&#946;</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><mo>,</mo><mrow><msub><mi>&#946;</mi><mn>2</mn></msub><mo>=</mo><mn>0.99</mn></mrow></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\beta_{1}=0.9,\\beta_{2}=0.99)</annotation></semantics></math>. For training the autoencoder on the HumanML subset of Motion-X, we use a batch size of 256 and a maximum sequence length of 64 frames. For the text-to-motion task, the batch size is set to 50 with a maximum sequence length of 196 frames. The learning rate is initialized at 0.0002 with a linear warmup over 2000 steps. The autoencoder is trained for 50 epochs, while the text-to-motion task is trained for 1500 epochs.\nDuring multimodal generation, we first initialize the model with pretrained weights from the text-to-motion autoencoder and fine-tune it on task-specific datasets. Subsequently, we freeze the parameters of the text-to-motion DiT and only fine-tune the masked transformer along with newly incorporated cross-attention layers. The learning rate and training schedule remain consistent with the text-to-motion task.\nFor all three tasks, we employ exponential moving average (EMA) to update model parameters, ensuring training stability. During inference, the classifier-free guidance (CFG) scale is set to 4.5 for text-to-motion, while other tasks use a CFG scale of 6.5.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "training",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets and Metrics. </span>\nFor the evaluation, we utilize three datasets: HumanML3D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib15\" title=\"\">2022a</a>)</cite> for text-to-motion, BEAT2&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib37\" title=\"\">2024a</a>)</cite> for speech-to-gesture, and FineDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite> for music-to-dance, all following the unified SMPL-X representation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>.\nRegarding the metrics, we use FID, R-Precision, and MM-Dist for text-based motion generation, use FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">H</span></sub>, FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">B</span></sub>, Face L2 loss, Beat Alignment Score, Diversity for speech-based motion generation, and use FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">H</span></sub>, FID<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">B</span></sub>, Diversity for music-based motion generation, respectively.\nFor the detailed explanation of the datasets and metrics, please refer to the appendix.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "following",
                    "2023a",
                    "finedance",
                    "generation",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-based motion generation. </span>\nWe conduct an evaluation of our model against prior text-to-motion approaches, including both discrete-domain and continuous-domain methods.\nAs summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S3.T1\" title=\"Table 1 &#8227; 3.5 Text-to-motion Pretraining and Multimodal Control Adaptation &#8227; 3 Method &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our method clearly surpasses prior techniques on the HumanML subset of Motion-X dataset. Remarkably, our model achieves improvements of 19.3%, 13.5%, and 11.7% in R-Precision for Top-1, 2, 3, respectively. Additionally, we enhance the FID score by 75.2% on this dataset, underscoring the exceptional fidelity of our generated motions. The qualitative results in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S3.F4\" title=\"Figure 4 &#8227; 3.5 Text-to-motion Pretraining and Multimodal Control Adaptation &#8227; 3 Method &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> further support these findings, showing that our approach yields whole-body motions that align closely with the input text.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "method",
                    "results",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-based motion generation. </span>\nTo assess the speech-driven motion generation, we compare to previous speech-to-gesture methods.\nOur results, summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S3.T2\" title=\"Table 2 &#8227; 3.6 Inference &#8227; 3 Method &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, reveal that our method achieves good quality and diversity in both hand and body motion generation and excels in aligning\nwith the rhythm of first-person speech.\nThis demonstrates the effectiveness of our framework in motion generation when encompassing different modal signals.\nHowever, our method performs worse than single-modal methods.\nAs discussed in &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, this is attributed to the random or average expressions in the Motion-X dataset, which confuses the speech-to-gesture training.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "training",
                    "results",
                    "bian",
                    "method",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Music-based motion generation. </span>\nWe further evaluate our framework on the music-to-dance task. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S4.T3\" title=\"Table 3 &#8227; 4.1 Experiment Settings &#8227; 4 Experiments &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, our method achieves slightly improved performance over previous approaches, particularly in generating hand motions and body movements.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Causal Attention. </span>\nTo verify the efficacy of the proposed framework, we initially establish a baseline model following the visual MAR setup&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib27\" title=\"\">2024b</a>)</cite>, i.e, using the random pseudo reordering for the batched masked prediction, through the bidirectional attention computing.\nFrom the results presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Ablation Study &#8227; 4 Experiments &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we see that the performance of this baseline in the motion area is limited. We attribute this to the difference between human motions and visual images, e.g., the human motion is in a strong temporal sequential structure, in which case a causal attention makes more sense.\nTherefore, changing the baseline to sequential masked prediction with causal attention improves the performance.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "results",
                    "setup",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">DiT. </span>\nIn order to evaluate how the DiTs contribute to the motion quality, we further replace the MLPs in the baseline model with our DiTs.\nAs shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Ablation Study &#8227; 4 Experiments &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, the model generates superior motions with DiTs compared to MLPs, especially in the context of multimodal motion generation.\nThis reveals the superior potential of DiTs in generating motion with complex multimodal contexts.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross Attention. </span>\nIn the baseline model, the multimodal signals are injected with only the AdaLN structure.\nWe then add the cross attention module and observe a significant improvement in multimodal motion generation, as depicted in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#S4.T4\" title=\"Table 4 &#8227; 4.3 Ablation Study &#8227; 4 Experiments &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper proposes a new omni motion framework for multimodal whole-body human motion generation.\nWithin this one framework, text, speech, and music signals are all encompassed through AdaLN and cross-attention.\nThe motion generation process is modeled by a continuous masked autoregressive transformer with causal attention, as well as a DiT structure.\nExtensive experiments have been conducted to verify the efficacy of the proposed framework in different-modality tasks.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limitations.</span>\nDue to the restricted dataset, the naturalness and generalizability of the motion generation model are still limited, especially in speech and music-driven motion generation.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "dataset",
                    "musicdriven",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our speech and music encoders are designed to extract temporally aligned, high-level features from raw audio signals for effective speech-to-gesture and music-to-dance generation. The architecture builds upon a multi-layer 1D convolutional network with strided convolutions and leaky ReLU activations. Each convolutional block consists of a series of a block unit that progressively downsample the input waveform while increasing feature dimensionality. The input audio sequence is first processed through multiple stages of temporal aggregation and non-linear transformation, resulting in a sequence of compact and expressive latent representations, whereas the input music typically retains sufficient temporal structure and spectral richness in its raw form for effective motion synthesis. These latent codes capture prosodic, rhythmic, and semantic-like patterns in speech and music, which are then projected into the condition latent space of dimensionality. The final encoder output is transposed to align with the temporal structure expected by the diffusion model, enabling fine-grained cross-modal interaction between speech and motion sequences during generation.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first pretrain a baseline autoencoder on the text-to-motion task. When fine-tuning it on the speech-to-gesture and music-to-dance tasks, the decoder fails to reconstruct valid motion sequences due to discrepancies in data distribution. However, fine-tuning the autoencoder using the reconstruction objective during the multi-modal training incurs high computational costs. Therefore, we independently fine-tune the baseline AE on each dataset using the reconstruction task before multi-modal generation, and employ the resulting models for downstream tasks.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "dataset",
                    "generation",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following previous work&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Bian et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, we utilize SMPL-X formatted motion data with an input dimension of (frame length &#215; 322). The parameter structure is organized as follows: Root orientation (0:3): controls global body rotation; body pose (3:66): Governs major body joint rotations; hand articulation (66:156): controls finger movements; jaw pose (156:159): manages mouth opening/closing; facial expression (159:209): drives emotional expressions; facial shape (209: 309): determines static facial structure; translation (309:312): controls global body position; betas (312: 322): represents static body shape parameters. And the maximum motion length is 196.\nThe model&#8217;s output maintains identical dimensionality (frame length &#215; 322) to ensure full reconstruction capability. This comprehensive parameterization enables simultaneous control of body motion, facial animation, and global positioning within a unified framework.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "following",
                    "bian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For text-based motion generation, we evaluate our method on the HumanML3D&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib15\" title=\"\">2022a</a>)</cite> dataset, which consists of 14,616 high-quality human motions paired with 44,970 text descriptions.\nThe original body-only SMPL&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Loper et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib39\" title=\"\">2023</a>)</cite> format of this dataset is extended to whole-body SMPL-X&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Pavlakos et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib43\" title=\"\">2019</a>)</cite> format in MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, which we follow in the experiments for evaluation.\nFor speech-based motion generation, we evaluate on the BEAT2 dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib37\" title=\"\">2024a</a>)</cite>, which collects 76 hours of data from 30 speakers, standardized into a mesh representation with paired audio and text lines.\nThe motion of the unified SMPL-X format is also extracted&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite> for multimodal evaluation.\nFor music-based motion generation, the largest dataset FineDance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib24\" title=\"\">2023a</a>)</cite> is utilized for evaluation. This dataset collects dances of 14.6 hours across 22 genres and provides detailed human motions using the SMPL-H format, which is then converted to the unified SMPL-X format and appended by text descriptions.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "2023a",
                    "finedance",
                    "generation",
                    "method",
                    "bian",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enable full-body, multimodal control over motion generation, we convert all datasets to the SMPL-X format. This involves filling in missing facial expressions in HumanML3D and FineDance using average expression coefficients from the training set, as well as transforming the SMPL-H Rot-6D representation in FineDance into axis-angle format via Gram-Schmidt orthogonalization. This conversion achieves better alignment with SMPL-X parameters and introduces minimal errors compared to the official body-retargeting method, while also offering improved computational efficiency.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "training",
                    "method",
                    "finedance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure consistency with MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, we utilize the pretrained motion encoder and text encode, enabling a unified evaluation of the SMPL-X motion representation across different modalities. For datasets that lack corresponding textual annotations&#8212;namely FineDance and BEAT2&#8212;we generate pseudo-captions such as &#8220;A dancer is performing a street dance in the Jazz style to the rhythm of the wildfire&#8221; and &#8220;A person is giving a speech, and the content is &#8230;&#8221;, respectively, to support cross-modal learning.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "bian",
                    "finedance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conduct a comprehensive evaluation on the final model (after fine-tuning on both speech-to-gesture and music-to-dance datasets) and present the results below. Since textual conditioning participates throughout the entire training pipeline, our model does not suffer from catastrophic forgetting after fine-tuning. This confirms the robustness of our architecture&#8217;s knowledge retention capabilities under different training paradigms.</p>\n\n",
                "matched_terms": [
                    "training",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the same strategy as MotionCraft&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#bib.bib6\" title=\"\">2025</a>)</cite>, we train two variants of our model: OmniMotion-Base and OmniMotion-Mix. OmniMotion-Base is a text-to-motion model pretrained solely on HumanML3D, while OmniMotion-Mix is trained on a combined dataset comprising HumanML3D, BEAT2, and FineDance to enable multimodal motion generation. Quantitative results on the text-to-motion task are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#A1.T7\" title=\"Table 7 &#8227; Text-based Motion Generation &#8227; A.3 Metrics &#8227; Appendix A Appendix &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "following",
                    "finedance",
                    "generation",
                    "results",
                    "bian",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We display more visual results of speech-driven and music-driven motion generation in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#A1.F5\" title=\"Figure 5 &#8227; Text-based Motion Generation &#8227; A.3 Metrics &#8227; Appendix A Appendix &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.14954v1#A1.F6\" title=\"Figure 6 &#8227; Text-based Motion Generation &#8227; A.3 Metrics &#8227; Appendix A Appendix &#8227; OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, respectively.</p>\n\n",
                "matched_terms": [
                    "motion",
                    "generation",
                    "musicdriven",
                    "results"
                ]
            }
        ]
    }
}