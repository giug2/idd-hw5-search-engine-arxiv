{
    "S4.T1": {
        "source_file": "No Verifiable Reward for Prosody: Toward Preference-Guided Prosody Learning in TTS",
        "caption": "Table 1: Results on KoCC-TTS. CER (%, lower is better) and ELO-based human preference (higher is better). Rows under Ours are internal models; the shaded entry marks the best DPO round (R2).",
        "body": "Model\n\nCER ↓\\downarrow (%)\n\nELO\n\n\n\nElevenLabs (Multilingual v2)111https://elevenlabs.io/blog/eleven-multilingual-v2\n\n4.744.74\n955.1\n\n\n\nSupertone222https://www.supertone.ai/ko\n\n2.982.98\n1046.9\n\n\n\nGPT-4o-mini-tts (sage)\n\n2.912.91\n848.9\n\n\nLlasa-8B\n3.243.24\n–\n\n\nLlasa-3B\n3.473.47\n–\n\n\nLlasa-1B\n10.4510.45\n–\n\n\nOurs\n\n\nchannel-base\n2.902.90\n1150.1\n\n\nGRPO (clean)\n2.20\n753.7\n\n\n\nGRPO-sim extension\n\n42.6342.63\n878.7\n\n\nchannel-base-dpo-v1\n5.805.80\n1096.5\n\n\nchannel-base-dpo-v2\n3.603.60\n1190.1\n\n\nchannel-base-dpo-v3\n3.303.30\n1064.2",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">CER</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> (%)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ELO</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">ElevenLabs (Multilingual v2)</span><span class=\"ltx_note ltx_role_footnote\" id=\"footnotex1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://elevenlabs.io/blog/eleven-multilingual-v2\" title=\"\">https://elevenlabs.io/blog/eleven-multilingual-v2</a></span></span></span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><math alttext=\"4.74\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m2\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">4.74</mn><annotation encoding=\"application/x-tex\">4.74</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">955.1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Supertone</span><span class=\"ltx_note ltx_role_footnote\" id=\"footnotex2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.supertone.ai/ko\" title=\"\">https://www.supertone.ai/ko</a></span></span></span>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><math alttext=\"2.98\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m3\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">2.98</mn><annotation encoding=\"application/x-tex\">2.98</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1046.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">GPT</span><span class=\"ltx_text\" style=\"font-size:90%;\">-</span><span class=\"ltx_text\" style=\"font-size:90%;\">4o</span><span class=\"ltx_text\" style=\"font-size:90%;\">-</span><span class=\"ltx_text\" style=\"font-size:90%;\">mini</span><span class=\"ltx_text\" style=\"font-size:90%;\">-</span><span class=\"ltx_text\" style=\"font-size:90%;\">tts (sage)</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><math alttext=\"2.91\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m4\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">2.91</mn><annotation encoding=\"application/x-tex\">2.91</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">848.9</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Llasa-8B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><math alttext=\"3.24\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m5\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">3.24</mn><annotation encoding=\"application/x-tex\">3.24</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Llasa-3B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><math alttext=\"3.47\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m6\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">3.47</mn><annotation encoding=\"application/x-tex\">3.47</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Llasa-1B</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><math alttext=\"10.45\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m7\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">10.45</mn><annotation encoding=\"application/x-tex\">10.45</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8211;</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" colspan=\"3\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Ours</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">channel-base</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><math alttext=\"2.90\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m8\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">2.90</mn><annotation encoding=\"application/x-tex\">2.90</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1150.1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">GRPO (clean)</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2.20</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">753.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">GRPO</span><span class=\"ltx_text\" style=\"font-size:90%;\">-</span><span class=\"ltx_text\" style=\"font-size:90%;\">sim extension</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><math alttext=\"42.63\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m9\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">42.63</mn><annotation encoding=\"application/x-tex\">42.63</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">878.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">channel-base-dpo-v1</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><math alttext=\"5.80\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m10\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">5.80</mn><annotation encoding=\"application/x-tex\">5.80</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1096.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">channel-base-dpo-v2</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><math alttext=\"3.60\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m11\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">3.60</mn><annotation encoding=\"application/x-tex\">3.60</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#D8E3FB;padding-top:0.55pt;padding-bottom:0.55pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#D8E3FB;\">1190.1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">channel-base-dpo-v3</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><math alttext=\"3.30\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m12\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">3.30</mn><annotation encoding=\"application/x-tex\">3.30</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.55pt;padding-bottom:0.55pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1064.2</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "channelbasedpov1",
            "elevenlabs",
            "lower",
            "preference",
            "multilingual",
            "llasa1b",
            "grpo",
            "kocctts",
            "elo",
            "clean",
            "best",
            "internal",
            "results",
            "extension",
            "channelbase",
            "channelbasedpov3",
            "↓downarrow",
            "shaded",
            "under",
            "entry",
            "llasa3b",
            "rows",
            "channelbasedpov2",
            "higher",
            "supertone222httpswwwsupertoneaiko",
            "gpt4ominitts",
            "ours",
            "sage",
            "round",
            "models",
            "human",
            "better",
            "dpo",
            "marks",
            "grposim",
            "v2111httpselevenlabsioblogelevenmultilingualv2",
            "cer",
            "model",
            "llasa8b",
            "elobased"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We evaluate 12 systems on the KoCC-TTS dataset, including 3 production-grade external services, 3 open-source models, and 6 internal variants. We intentionally exclude open-source TTS baselines from the main comparison. A preliminary screening indicated that most off-the-shelf OSS voices exhibited inadequate prosodic fluency in Korean, and their inclusion would likely result in floor effects rather than provide a meaningful basis for comparison.\nFor external systems, we adopt vendors&#8217; strongest Korean voices and default synthesis settings unless otherwise noted: ElevenLabs Multilingual v2 (&#8220;Anna&#8221;)<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://elevenlabs.io/blog/eleven-multilingual-v2\" title=\"\">https://elevenlabs.io/blog/eleven-multilingual-v2</a></span></span></span>, Supertone (&#8220;<span class=\"ltx_text ltx_font_typewriter\">sona_speech_1</span>&#8221;)<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://docs.supertoneapi.com/ko/user-guide/quickstart\" title=\"\">https://docs.supertoneapi.com/ko/user-guide/quickstart</a></span></span></span>, and GPT-4o-mini-tts (&#8220;sage&#8221;). To ensure fairness, all systems synthesize from the same prompts with identical text normalization rules; speaking rate and punctuation handling are held fixed, and outputs are evaluated at vendors&#8217; native sampling configurations. We report (i) character error rate (CER) computed from Whisper-large-v3 transcriptions and (ii) human preference aggregated into ELO scores. This ELO ranking is derived from 596 total votes from 27 participants, who each provided pairwise blind judgments. Aggregate results are summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18531v1#S4.T1\" title=\"Table 1 &#8227; 4 Experiments &#8227; No Verifiable Reward for Prosody: Toward Preference-Guided Prosody Learning in TTS\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, with ELO rankings visualized in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18531v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; No Verifiable Reward for Prosody: Toward Preference-Guided Prosody Learning in TTS\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Outcomes</span>\nAs summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18531v1#S4.T1\" title=\"Table 1 &#8227; 4 Experiments &#8227; No Verifiable Reward for Prosody: Toward Preference-Guided Prosody Learning in TTS\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, starting from <span class=\"ltx_text ltx_font_typewriter\">channel-base</span> (CER = 2.90%, ELO = 1150.1),\nGRPO attains the lowest CER (2.20%) but the lowest preference (ELO = 753.7) due to monotone prosody.\nIterative DPO reverses this trade-off:</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Recent work reports gains in neural text-to-speech (TTS) with Group Relative Policy Optimization (GRPO). However, in the absence of a verifiable reward for <span class=\"ltx_text ltx_font_italic\">prosody</span>, GRPO trained on transcription-oriented signals (CER/NLL) lowers error rates yet collapses prosody into monotone, unnatural speech; adding speaker-similarity further destabilizes training and degrades CER. We address this with an <span class=\"ltx_text ltx_font_italic\">iterative Direct Preference Optimization (DPO)</span> scheme that uses only a few hundred human-labeled preference pairs per round to directly optimize prosodic naturalness while regularizing to the current model. On <span class=\"ltx_text ltx_font_bold\">KoCC-TTS</span>, a curated dataset of authentic Korean call center interactions capturing task-oriented dialogues, our method attains the highest human preference (ELO) with competitive CER, outperforming GRPO and strong commercial baselines. These results suggest that when prosody cannot be rewarded automatically, <span class=\"ltx_text ltx_font_italic\">human preference optimization</span> offers a practical and data-efficient path to natural and robust TTS. The demo page is available at <a class=\"ltx_ref ltx_href\" href=\"https://tts.ch.dev\" title=\"\">https://tts.ch.dev</a>.</p>\n\n",
                "matched_terms": [
                    "elo",
                    "round",
                    "cer",
                    "human",
                    "results",
                    "preference",
                    "grpo",
                    "dpo",
                    "kocctts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We argue that the lack of naturalness stems from a gap in reward design. Reliable automatic metrics for prosody remain limited, making it difficult to provide reinforcement signals that align with natural speech patterns. Optimizing GRPO on CER or NLL improves intelligibility but suppresses prosodic variation, often resulting in near-monotone speech. However incorporating speaker-similarity rewards further introduces instability and degrades performance, inflating CER. In this paper, we contend that the bottleneck lies in the reward formulation rather than in the choice of optimizer.</p>\n\n",
                "matched_terms": [
                    "cer",
                    "grpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To close this reward gap, we adopt iterative Direct Preference Optimization (DPO) with small human-in-the-loop batches (200 preference pairs per round). Across rounds, DPO supplies a directly verifiable signal for prosodic naturalness while regularizing to the current model, yielding both improved human preference and competitive CER (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18531v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; No Verifiable Reward for Prosody: Toward Preference-Guided Prosody Learning in TTS\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Our contributions are as follows:</p>\n\n",
                "matched_terms": [
                    "round",
                    "human",
                    "preference",
                    "dpo",
                    "cer",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We demonstrate that applying <em class=\"ltx_emph ltx_font_italic\">iterative Direct Preference Optimization (DPO)</em> with <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S1.I1.i2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>200 human preference pairs per round restores conversational prosody while keeping CER competitive.</p>\n\n",
                "matched_terms": [
                    "round",
                    "human",
                    "preference",
                    "dpo",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">GRPO for TTS and reward design</span>\nRecent TTS studies that adopt group-relative policy optimization (GRPO) primarily target intelligibility and identity preservation by rewarding ASR-derived errors and speaker similarity, sometimes adding non-intrusive quality predictors.\nFor instance, F5R-TTS couples WER with speaker-similarity (SIM) under GRPO <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18531v1#bib.bib9\" title=\"\">9</a>]</cite>, DMOSpeech2 optimizes a duration policy with SIM+WER via GRPO <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18531v1#bib.bib6\" title=\"\">6</a>]</cite>, and the TTS-1 technical report describes a composite GRPO reward that blends WER/SIM/DNSMOS for RL alignment <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18531v1#bib.bib7\" title=\"\">7</a>]</cite>.\nWhile these report lower error rates and stronger speaker faithfulness, they largely omit explicit prosody-sensitive rewards (e.g., pitch movement, phrasing, boundary control).\nIn practice, we also observed that CER/NLL-oriented GRPO can collapse prosodic variation into near-monotone renderings, consistent with reports of punctuation- and phrasing-related failures in state-of-the-art systems <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18531v1#bib.bib5\" title=\"\">5</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "lower",
                    "under",
                    "grpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Preference-based objectives for prosody</span>\nDirect Preference Optimization (DPO) offers a complementary route by optimizing pairwise human (or proxy) preferences without training a separate reward model <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18531v1#bib.bib8\" title=\"\">8</a>]</cite>.\nIn TTS, Emo-DPO applies DPO to better capture subtle emotional/prosodic nuances with an LLM-TTS backbone, improving both prosody similarity and perceived naturalness <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18531v1#bib.bib10\" title=\"\">10</a>]</cite>.\nBeyond single-shot DPO, iterative preference optimization for speech generation has also been explored: SpeechAlign constructs codec-token preference pairs and refines a speech LM in multiple rounds, demonstrating iterative self-improvement <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18531v1#bib.bib11\" title=\"\">11</a>]</cite>.\nConcurrently, differentiable or multi-dimensional preference objectives have been proposed to move past coarse ASR metrics&#8212;e.g., DiffRO directly optimizes differentiable rewards over codec tokens <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18531v1#bib.bib12\" title=\"\">12</a>]</cite>, and MPO considers multi-criteria screening of preference pairs in speech synthesis <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18531v1#bib.bib13\" title=\"\">13</a>]</cite>.\nThese studies collectively suggest that <em class=\"ltx_emph ltx_font_italic\">preference-based post-training</em> is a promising way to recover communicative prosody without sacrificing robustness to transcription errors.</p>\n\n",
                "matched_terms": [
                    "human",
                    "better",
                    "preference",
                    "dpo",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt an architecture based on Llasa, which uses a Transformer (initialized from LLaMA) to generate discrete speech tokens decoded into waveforms via XCodec2<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18531v1#bib.bib4\" title=\"\">4</a>]</cite>. Starting from the Llasa-1B checkpoint,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/HKUSTAudio/Llasa-1B\" title=\"\">https://huggingface.co/HKUSTAudio/Llasa-1B</a></span></span></span> we perform continual training on a 36k hours Korean corpus to instill language-specific competence. Then we fine-tune on an 18-hour proprietary single-speaker dataset to adapt prosody toward a natural conversational style. We refer to this model as <span class=\"ltx_text ltx_font_typewriter\">channel-base</span>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "llasa1b",
                    "channelbase"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To restore prosodic variation while preserving transcription robustness, we perform <em class=\"ltx_emph ltx_font_italic\">round-based</em> preference learning with Direct Preference Optimization (DPO)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18531v1#bib.bib8\" title=\"\">8</a>]</cite>.\nAt round <math alttext=\"r\\in\\{1,2,3\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">r\\in\\{1,2,3\\}</annotation></semantics></math>, the policy is initialized from the previous checkpoint <math alttext=\"\\pi_{\\theta_{r-1}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><msub><mi>&#952;</mi><mrow><mi>r</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub></msub><annotation encoding=\"application/x-tex\">\\pi_{\\theta_{r-1}}</annotation></semantics></math>, which also serves as the moving reference <math alttext=\"\\pi_{\\mathrm{ref}}=\\pi_{\\theta_{r-1}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>&#960;</mi><mi>ref</mi></msub><mo>=</mo><msub><mi>&#960;</mi><msub><mi>&#952;</mi><mrow><mi>r</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub></msub></mrow><annotation encoding=\"application/x-tex\">\\pi_{\\mathrm{ref}}=\\pi_{\\theta_{r-1}}</annotation></semantics></math>.\nWe generate candidates with <math alttext=\"\\pi_{\\theta_{r-1}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><msub><mi>&#952;</mi><mrow><mi>r</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub></msub><annotation encoding=\"application/x-tex\">\\pi_{\\theta_{r-1}}</annotation></semantics></math>, collect 200 human preference pairs <math alttext=\"\\{(x,y^{+},y^{-})\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>,</mo><msup><mi>y</mi><mo>+</mo></msup><mo>,</mo><msup><mi>y</mi><mo>&#8722;</mo></msup><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{(x,y^{+},y^{-})\\}</annotation></semantics></math>, and update the policy by optimizing the DPO objective to obtain <math alttext=\"\\pi_{\\theta_{r}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m6\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><msub><mi>&#952;</mi><mi>r</mi></msub></msub><annotation encoding=\"application/x-tex\">\\pi_{\\theta_{r}}</annotation></semantics></math>.\nThis procedure yields <span class=\"ltx_text ltx_font_typewriter\">channel-base-dpo-v1</span>, <span class=\"ltx_text ltx_font_typewriter\">channel-base-dpo-v2</span>, and <span class=\"ltx_text ltx_font_typewriter\">channel-base-dpo-v3</span> for <math alttext=\"r=1,2,3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m7\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn></mrow></mrow><annotation encoding=\"application/x-tex\">r=1,2,3</annotation></semantics></math>, respectively.\nPreference data are not reused across rounds.</p>\n\n",
                "matched_terms": [
                    "channelbasedpov1",
                    "round",
                    "human",
                    "preference",
                    "dpo",
                    "channelbasedpov3",
                    "channelbasedpov2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Applying the reward function in Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18531v1#S3.E2\" title=\"In 3.3.1 Base reward &#8227; 3.3 Reinforcement Learning with GRPO &#8227; 3 Methodology &#8227; No Verifiable Reward for Prosody: Toward Preference-Guided Prosody Learning in TTS\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, GRPO consistently reduces CER to the lowest level among all variants. All GRPO models were trained on 1.6M text prompts.\nHowever, as illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18531v1#S4.F2\" title=\"Figure 2 &#8227; 4.5 Iterative DPO: Small Preference Sets Recover Prosody &amp; CER &#8227; 4 Experiments &#8227; No Verifiable Reward for Prosody: Toward Preference-Guided Prosody Learning in TTS\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the <math alttext=\"{\\log}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mi>log</mi><annotation encoding=\"application/x-tex\">{\\log}</annotation></semantics></math>F0 distribution of GRPO-trained models shows reduced pitch variability compared to the baseline, indicating a collapse toward monotonic prosody.\nAlthough such optimization improves transcription robustness, it results in speech that listeners perceive as unnatural, which explains the lower ELO scores relative to CER gains.</p>\n\n",
                "matched_terms": [
                    "elo",
                    "models",
                    "results",
                    "lower",
                    "grpo",
                    "cer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address monotonicity, we introduced an additional speaker-similarity term in the reward (Eq.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18531v1#S3.E4\" title=\"In 3.3.2 Speaker similarity extension &#8227; 3.3 Reinforcement Learning with GRPO &#8227; 3 Methodology &#8227; No Verifiable Reward for Prosody: Toward Preference-Guided Prosody Learning in TTS\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).\nWhile this modification increased similarity scores, it also destabilized training: CER degraded substantially, and we observed degenerate behaviors where the model generated excessively long outputs without producing an end-of-sequence token.\nAlthough the text was realized, utterances frequently failed to terminate, suggesting that the RL objective was partially &#8220;hacked.&#8221;\nThese results indicate that incorporating speaker-similarity rewards into GRPO introduces optimization challenges and reduces training stability, making it unsuitable as a direct solution for prosodic control.</p>\n\n",
                "matched_terms": [
                    "results",
                    "cer",
                    "model",
                    "grpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We next apply round-based Direct Preference Optimization (DPO) with 200 human-labeled pairs per round, using a moving reference\n(<math alttext=\"\\pi_{\\mathrm{ref}}=\\pi_{\\theta_{r-1}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>&#960;</mi><mi>ref</mi></msub><mo>=</mo><msub><mi>&#960;</mi><msub><mi>&#952;</mi><mrow><mi>r</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub></msub></mrow><annotation encoding=\"application/x-tex\">\\pi_{\\mathrm{ref}}=\\pi_{\\theta_{r-1}}</annotation></semantics></math>) and no replay from earlier rounds.\nEach round regenerates candidates, collects fresh A/B preferences, and optimizes Eq.&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18531v1#S3.E7\" title=\"In 3.4 Iterative Direct Preference Optimization (DPO) for Prosody &#8227; 3 Methodology &#8227; No Verifiable Reward for Prosody: Toward Preference-Guided Prosody Learning in TTS\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>).</p>\n\n",
                "matched_terms": [
                    "preference",
                    "round",
                    "dpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Round&#160;1:</span> ELO rises to 1096.5 while CER increases to 5.80% as the model explores more varied prosody.</p>\n\n",
                "matched_terms": [
                    "cer",
                    "elo",
                    "model",
                    "round"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Round&#160;2:</span> ELO peaks at <span class=\"ltx_text ltx_font_bold\">1190.1</span> and CER improves to 3.60%, outperforming external systems in preference and approaching baseline CER.</p>\n\n",
                "matched_terms": [
                    "cer",
                    "preference",
                    "elo",
                    "round"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Round&#160;3:</span> CER further improves to 3.30% with ELO = 1064.2, retaining a clear prosodic advantage over GRPO.</p>\n\n",
                "matched_terms": [
                    "cer",
                    "elo",
                    "round",
                    "grpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Takeaways</span>\nWith only 200 pairs per round, iterative DPO (i) restores prosodic variation favored by listeners, as reflected in higher ELO scores, and (ii) reduces CER after the initial exploration phase.\nAs shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18531v1#S4.F2\" title=\"Figure 2 &#8227; 4.5 Iterative DPO: Small Preference Sets Recover Prosody &amp; CER &#8227; 4 Experiments &#8227; No Verifiable Reward for Prosody: Toward Preference-Guided Prosody Learning in TTS\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> (increased <math alttext=\"F_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS5.p3.m1\" intent=\":literal\"><semantics><msub><mi>F</mi><mn>0</mn></msub><annotation encoding=\"application/x-tex\">F_{0}</annotation></semantics></math> variability compared with GRPO), these results indicate that preference learning complements GRPO by mitigating prosodic collapse while maintaining competitive transcription robustness.</p>\n\n",
                "matched_terms": [
                    "elo",
                    "round",
                    "results",
                    "preference",
                    "grpo",
                    "dpo",
                    "cer",
                    "higher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Without a verifiable, automatic reward for prosody, GRPO trained on transcription-centric signals (CER/Whisper-NLL) predictably optimizes what is measured&#8212;intelligibility&#8212;while collapsing what is not&#8212;prosodic variation&#8212;into near-monotone speech. Extending the reward with speaker-similarity injects noisy, non-prosodic supervision that destabilizes optimization (e.g., EOS failures) and inflates CER, indicating that the core limitation lies in the reward design, not the optimizer.</p>\n\n",
                "matched_terms": [
                    "cer",
                    "grpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We close this reward gap with small-batch iterative DPO, replacing unverifiable proxies with directly verifiable human preferences. With only 200 preference pairs per round, DPO consistently restores prosodic diversity favored by listeners (highest ELO) while keeping CER competitive, serving as a data-efficient complement to GRPO. We also release <span class=\"ltx_text ltx_font_bold\">KoCC-TTS</span> for robustness and conversational prosody evaluation. Our takeaway is simple: <em class=\"ltx_emph ltx_font_italic\">when prosody cannot be reliably rewarded automatically, human-in-the-loop preference optimization is the practical path to natural and robust TTS</em>.</p>\n\n",
                "matched_terms": [
                    "elo",
                    "round",
                    "cer",
                    "human",
                    "preference",
                    "grpo",
                    "dpo",
                    "kocctts"
                ]
            }
        ]
    }
}