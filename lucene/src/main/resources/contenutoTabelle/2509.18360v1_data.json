{
    "S4.T1": {
        "caption": "Table 1: Results for En-to-De and De-to-En on EPST test sets. Bold means better than speech mining baselines. Underline means the best overall.\n†Results from Duquenne et al. (2023a).\n‡Models trained by ourselves.\n*p-value <0.05<0.05.\nResults show that Speech Vecalign models perform better than baselines under almost all metrics in both directions.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Training Data</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">ASR-BLEU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">ASR-chrF2++</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">BLASER 2.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Alignment Method</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\"># Hours</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">w/ text ref</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">w/o text ref</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#BFBFBF;\">English-to-German</span></th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">State-of-the-art</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">SpeechMatrix<sup class=\"ltx_sup\">&#8224;</sup>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">1451</th>\n<td class=\"ltx_td ltx_align_center\">10.1</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">SpeechMatrix<sup class=\"ltx_sup\">&#8225;</sup>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">1451</th>\n<td class=\"ltx_td ltx_align_center\">11.27</td>\n<td class=\"ltx_td ltx_align_center\">39.98</td>\n<td class=\"ltx_td ltx_align_center\">3.52</td>\n<td class=\"ltx_td ltx_align_center\">3.86</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Baseline</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Local Mining</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">1500</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">12.91</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">44.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.02</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Global Mining</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">1000</th>\n<td class=\"ltx_td ltx_align_center\">12.21</td>\n<td class=\"ltx_td ltx_align_center\">42.65</td>\n<td class=\"ltx_td ltx_align_center\">3.65</td>\n<td class=\"ltx_td ltx_align_center\">3.97</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Our Method</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Speech Vecalign</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">750</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">12.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">44.34</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">3.73</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">4.05</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"2\">p-value w.r.t Local Mining</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.1069</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0999</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0050<sup class=\"ltx_sup\">*</sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0020<sup class=\"ltx_sup\">*</sup>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" colspan=\"2\">p-value w.r.t Global Mining</th>\n<td class=\"ltx_td ltx_align_center\">0.0769</td>\n<td class=\"ltx_td ltx_align_center\">0.0010<sup class=\"ltx_sup\">*</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.0010<sup class=\"ltx_sup\">*</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.0010<sup class=\"ltx_sup\">*</sup>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#BFBFBF;\">German-to-English</span></th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"/>\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_border_tt\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">State-of-the-art</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">SpeechMatrix<sup class=\"ltx_sup\">&#8224;</sup>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">1456</th>\n<td class=\"ltx_td ltx_align_center\">16.3</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">SpeechMatrix<sup class=\"ltx_sup\">&#8225;</sup>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">1456</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">16.62</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">43.77</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.81</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">4.11</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Baseline</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Local Mining</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">1250</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">15.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">42.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.02</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Global Mining</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">750</th>\n<td class=\"ltx_td ltx_align_center\">15.96</td>\n<td class=\"ltx_td ltx_align_center\">43.14</td>\n<td class=\"ltx_td ltx_align_center\">3.74</td>\n<td class=\"ltx_td ltx_align_center\">4.04</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Our Method</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Speech Vecalign</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">1000</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">16.14</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">43.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">3.76</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">4.07</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"2\">p-value w.r.t Local Mining</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0030<sup class=\"ltx_sup\">*</sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0010<sup class=\"ltx_sup\">*</sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0010<sup class=\"ltx_sup\">*</sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0010<sup class=\"ltx_sup\">*</sup>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_bb\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" colspan=\"2\">p-value w.r.t Global Mining</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.1449</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.0030<sup class=\"ltx_sup\">*</sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.0030<sup class=\"ltx_sup\">*</sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.0010<sup class=\"ltx_sup\">*</sup>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "pvalue",
            "models",
            "ref",
            "speechmatrix‡",
            "vecalign",
            "entode",
            "baselines",
            "overall",
            "‡models",
            "epst",
            "underline",
            "ourselves",
            "speechmatrix†",
            "our",
            "baseline",
            "hours",
            "†results",
            "show",
            "from",
            "training",
            "test",
            "trained",
            "text",
            "englishtogerman",
            "mining",
            "alignment",
            "than",
            "sets",
            "2023a",
            "metrics",
            "bold",
            "wrt",
            "perform",
            "under",
            "germantoenglish",
            "both",
            "results",
            "speech",
            "blaser",
            "stateoftheart",
            "asrbleu",
            "global",
            "almost",
            "means",
            "local",
            "better",
            "best",
            "all",
            "data",
            "directions",
            "method",
            "duquenne",
            "detoen",
            "asrchrf2"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As mentioned in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS1\" title=\"4.1 Training Data &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>, we train models on data of various sizes.\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.T1\" title=\"Table 1 &#8227; 4.3 Experiment Setup &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the best En-to-De and De-to-En results on the EPST test set, along with the corresponding data sizes.\nAdditional results on the FLEURS test set are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A5\" title=\"Appendix E Evaluation Results on FLEURS &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Speech Vecalign, a parallel speech document alignment method that monotonically aligns speech segment embeddings and does not depend on text transcriptions.\nCompared to the baseline method Global Mining&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>, a variant of speech mining, Speech Vecalign produces longer speech-to-speech alignments.\nIt also demonstrates greater robustness than Local Mining, another speech mining variant, as it produces less noise.\nWe applied Speech Vecalign to 3,000 hours of unlabeled parallel English-German&#160;(En-De) speech documents from VoxPopuli, yielding about 1,000 hours of high-quality alignments.\nWe then trained En-De speech-to-speech translation models on the aligned data.\nSpeech Vecalign improves the En-to-De and De-to-En performance over Global Mining by 0.37 and 0.18 ASR-BLEU, respectively.\nMoreover, our models match or outperform SpeechMatrix model performance, despite using 8 times fewer raw speech documents.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Data and code are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/mct10/Speech-Vecalign\" title=\"\">https://github.com/mct10/Speech-Vecalign</a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "models",
                    "detoen",
                    "vecalign",
                    "entode",
                    "our",
                    "hours",
                    "from",
                    "text",
                    "trained",
                    "mining",
                    "alignment",
                    "than",
                    "2023a",
                    "speech",
                    "asrbleu",
                    "global",
                    "local",
                    "data",
                    "method",
                    "duquenne",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "vecalign",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech-to-speech translation (S2ST) is the task of translating speech in one language into speech in another language.\nConventional S2ST systems concatenate automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) models <cite class=\"ltx_cite ltx_citemacro_cite\">Lavie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib27\" title=\"\">1997</a>); Nakamura et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib31\" title=\"\">2006</a>); Wahlster (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib46\" title=\"\">2013</a>)</cite>.\nThese components can be trained individually with datasets for the different components.\nDirect S2ST models, which translate source speech into target spectrograms or discrete units with a single architecture, have been recently proposed to alleviate error propagation and to reduce inference latency <cite class=\"ltx_cite ltx_citemacro_cite\">Jia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib21\" title=\"\">2019</a>); Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib28\" title=\"\">2022a</a>)</cite>.\nDespite the advantages, performance of direct models is limited by the amount of speech-to-speech aligned data, which is much more scarce than the data used for components of cascaded systems.</p>\n\n",
                "matched_terms": [
                    "models",
                    "than",
                    "data",
                    "speech",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There have been efforts to automatically curate alignments from multilingual <span class=\"ltx_text ltx_font_italic\">speech document</span>s.\nIn this paper, we define a <span class=\"ltx_text ltx_font_italic\">speech document</span> as a file containing more than one utterance and typically comprising several paragraphs, analogous to a <span class=\"ltx_text ltx_font_italic\">text document</span>.\nVoxPopuli&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib47\" title=\"\">2021a</a>)</cite> is one such corpus containing a large number of <span class=\"ltx_text ltx_font_italic\">parallel</span> speech documents, which are pairs of documents that have the same content but differ in language.</p>\n\n",
                "matched_terms": [
                    "than",
                    "text",
                    "speech",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech-to-speech alignment methods align short speech clips called <span class=\"ltx_text ltx_font_italic\">segment</span>s, and can be either transcription-based or transcription-free.\nWhen transcriptions are available, segments in parallel speech documents can be aligned through speech-to-text and text-to-text alignments.\nInspired by text mining&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Schwenk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib38\" title=\"\">2021</a>)</cite>, speech mining&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib9\" title=\"\">2021</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite> was proposed as a transcription-free method that aligns speech segments by finding segment pairs with the highest embedding similarity.\nIt scales well as it does not rely on the availability of text transcriptions.\nWhen speech mining is applied to a large amount of speech documents, as in all previous work, it is referred to as <span class=\"ltx_text ltx_font_bold\">Global Mining</span>.\nAnother variant, <span class=\"ltx_text ltx_font_bold\">Local Mining</span>, which applies speech mining to a single pair of parallel speech documents, has not been well explored.\nAs we formally define in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S2\" title=\"2 Speech Mining Overview &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, both Global Mining and Local Mining treat documents as bags of unordered segments.</p>\n\n",
                "matched_terms": [
                    "global",
                    "alignment",
                    "local",
                    "2023a",
                    "all",
                    "both",
                    "method",
                    "speech",
                    "duquenne",
                    "text",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since speech mining methods do not leverage the document pair structure, we wonder, <span class=\"ltx_text ltx_font_bold\">can we obtain better alignments by aligning speech segments within document pairs and preserving their time order?</span>\nThis allows us to utilize the extra knowledge that (1) segments within parallel document pairs are likely to be translations of each other, and (2) segment pairs right next to already aligned pairs are also likely to be aligned.\nWe draw inspiration from parallel <span class=\"ltx_text ltx_font_italic\">text</span> document alignment methods, which have been popular to create sentence-aligned bitext for training MT systems.\nUnlike mining, they align sentences for each document pair while maintaining the sentence order.\nOur work is based on the text alignment method Vecalign&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Thompson and Koehn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib43\" title=\"\">2019</a>)</cite>, which aligns parallel sentences by applying fast dynamic time warping&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Salvador and Chan (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib37\" title=\"\">2007</a>)</cite> to sentence embeddings.\nWith the advances of extending sentence embeddings to the speech modality&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib9\" title=\"\">2021</a>)</cite>, we can readily apply Vecalign to parallel speech documents.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "vecalign",
                    "better",
                    "mining",
                    "from",
                    "training",
                    "method",
                    "speech",
                    "duquenne",
                    "text",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce Speech Vecalign, a method that aligns parallel speech documents using speech segment embeddings.\nInstead of mining from bags of segments, our method aligns individual document pairs and maintains the chronological order of segments, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nAdditional preprocessing and postprocessing strategies are applied to improve alignment quality.\nWe compare Speech Vecalign with Local Mining and Global Mining and show that Speech Vecalign produces higher-quality alignments.\nWe further provide extensive analysis for all three methods, which could be useful for future research.</p>\n\n",
                "matched_terms": [
                    "our",
                    "global",
                    "alignment",
                    "local",
                    "vecalign",
                    "all",
                    "show",
                    "from",
                    "method",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We formally describe the speech mining methods in this section.\nOther related work is in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A1\" title=\"Appendix A Related Work &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech Mining, first proposed by <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib9\" title=\"\">2021</a>)</cite>, encodes speech segments into language- and modality-agnostic fixed-size embeddings, and then uses margin-based similarity search <cite class=\"ltx_cite ltx_citemacro_cite\">Artetxe and Schwenk (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib2\" title=\"\">2019a</a>)</cite> to find the closest embedding pairs.\nDepending on the search scope, it can be categorized as Global Mining or Local Mining.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "speech",
                    "duquenne",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Raw data</span>. The input data is a list of speech documents <math alttext=\"X=[X_{1},X_{2},\\ldots,X_{n}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>X</mi><mn>1</mn></msub><mo>,</mo><msub><mi>X</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>X</mi><mi>n</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">X=[X_{1},X_{2},\\ldots,X_{n}]</annotation></semantics></math> in the source language and a list <math alttext=\"Y=[Y_{1},Y_{2},\\ldots,Y_{m}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m2\" intent=\":literal\"><semantics><mrow><mi>Y</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>Y</mi><mn>1</mn></msub><mo>,</mo><msub><mi>Y</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>Y</mi><mi>m</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Y=[Y_{1},Y_{2},\\ldots,Y_{m}]</annotation></semantics></math> in the target language, where <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m3\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> and <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m4\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> are the numbers of documents.\nEach document can contain between a few seconds to a few hours of speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Bag of embeddings</span>.\nIn <span class=\"ltx_text ltx_font_italic\">Global Mining</span>, embeddings are grouped by <span class=\"ltx_text ltx_font_bold\">language</span>.\nWe define <math alttext=\"G_{X}=\\left\\{E_{\\tilde{X}_{1}},E_{\\tilde{X}_{2}},\\ldots,E_{\\tilde{X}_{n}}\\right\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m1\" intent=\":literal\"><semantics><mrow><msub><mi>G</mi><mi>X</mi></msub><mo>=</mo><mrow><mo>{</mo><msub><mi>E</mi><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mn>1</mn></msub></msub><mo>,</mo><msub><mi>E</mi><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mn>2</mn></msub></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>E</mi><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mi>n</mi></msub></msub><mo>}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">G_{X}=\\left\\{E_{\\tilde{X}_{1}},E_{\\tilde{X}_{2}},\\ldots,E_{\\tilde{X}_{n}}\\right\\}</annotation></semantics></math> and <math alttext=\"G_{Y}=\\left\\{E_{\\tilde{Y}_{1}},E_{\\tilde{Y}_{2}},\\ldots,E_{\\tilde{Y}_{m}}\\right\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m2\" intent=\":literal\"><semantics><mrow><msub><mi>G</mi><mi>Y</mi></msub><mo>=</mo><mrow><mo>{</mo><msub><mi>E</mi><msub><mover accent=\"true\"><mi>Y</mi><mo>~</mo></mover><mn>1</mn></msub></msub><mo>,</mo><msub><mi>E</mi><msub><mover accent=\"true\"><mi>Y</mi><mo>~</mo></mover><mn>2</mn></msub></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>E</mi><msub><mover accent=\"true\"><mi>Y</mi><mo>~</mo></mover><mi>m</mi></msub></msub><mo>}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">G_{Y}=\\left\\{E_{\\tilde{Y}_{1}},E_{\\tilde{Y}_{2}},\\ldots,E_{\\tilde{Y}_{m}}\\right\\}</annotation></semantics></math>, where <math alttext=\"G_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m3\" intent=\":literal\"><semantics><msub><mi>G</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">G_{X}</annotation></semantics></math> collects all segment embeddings in the source language and <math alttext=\"G_{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m4\" intent=\":literal\"><semantics><msub><mi>G</mi><mi>Y</mi></msub><annotation encoding=\"application/x-tex\">G_{Y}</annotation></semantics></math> collects those in the target language.\nIn <span class=\"ltx_text ltx_font_italic\">Local Mining</span>, embeddings are grouped by <span class=\"ltx_text ltx_font_bold\">document pairs</span>.\nSuppose there are <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m5\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> parallel documents, with <math alttext=\"X_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m6\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">X_{i}</annotation></semantics></math> paired with <math alttext=\"Y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m7\" intent=\":literal\"><semantics><msub><mi>Y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">Y_{i}</annotation></semantics></math> for <math alttext=\"1\\leq i\\leq s\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m8\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>&#8804;</mo><mi>i</mi><mo>&#8804;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">1\\leq i\\leq s</annotation></semantics></math>.\nDocuments without a parallel one are ignored.\nIn this case, <math alttext=\"E_{\\tilde{X}_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m9\" intent=\":literal\"><semantics><msub><mi>E</mi><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mi>i</mi></msub></msub><annotation encoding=\"application/x-tex\">E_{\\tilde{X}_{i}}</annotation></semantics></math> and <math alttext=\"E_{\\tilde{Y_{j}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m10\" intent=\":literal\"><semantics><msub><mi>E</mi><mover accent=\"true\"><msub><mi>Y</mi><mi>j</mi></msub><mo>~</mo></mover></msub><annotation encoding=\"application/x-tex\">E_{\\tilde{Y_{j}}}</annotation></semantics></math> are bags of embeddings themselves.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "all",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Embedding alignment</span>.\nSpeech mining is performed by finding the most similar embedding pairs between two bags of segment embeddings.\nThe margin-based similarity, or margin-score, between any two embeddings <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m1\" intent=\":literal\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math> and <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m2\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> is computed as</p>\n\n",
                "matched_terms": [
                    "speech",
                    "alignment",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m3\" intent=\":literal\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math> and <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m4\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> are in different languages and <math alttext=\"\\text{NN}_{k}(a)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m5\" intent=\":literal\"><semantics><mrow><msub><mtext>NN</mtext><mi>k</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{NN}_{k}(a)</annotation></semantics></math> denotes <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m6\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> nearest neighbors of <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m7\" intent=\":literal\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math> in the other language.\nThe denominator combats the hubness problem.\nA higher margin-score indicates better quality.\nThen, the mining function for embedding <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m8\" intent=\":literal\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math> from a bag of embeddings <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m9\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> is</p>\n\n",
                "matched_terms": [
                    "from",
                    "better",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More generally, given two bags of embeddings <math alttext=\"U=\\{u_{1},u_{2},\\ldots,u_{l_{u}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m10\" intent=\":literal\"><semantics><mrow><mi>U</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>u</mi><mn>1</mn></msub><mo>,</mo><msub><mi>u</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>u</mi><msub><mi>l</mi><mi>u</mi></msub></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">U=\\{u_{1},u_{2},\\ldots,u_{l_{u}}\\}</annotation></semantics></math> and <math alttext=\"V=\\{v_{1},v_{2},\\ldots,v_{l_{v}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m11\" intent=\":literal\"><semantics><mrow><mi>V</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>v</mi><mn>1</mn></msub><mo>,</mo><msub><mi>v</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>v</mi><msub><mi>l</mi><mi>v</mi></msub></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">V=\\{v_{1},v_{2},\\ldots,v_{l_{v}}\\}</annotation></semantics></math>, where <math alttext=\"l_{u}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m12\" intent=\":literal\"><semantics><msub><mi>l</mi><mi>u</mi></msub><annotation encoding=\"application/x-tex\">l_{u}</annotation></semantics></math> and <math alttext=\"l_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m13\" intent=\":literal\"><semantics><msub><mi>l</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">l_{v}</annotation></semantics></math> are number of embeddings, the collection of all speech mining alignments is</p>\n\n",
                "matched_terms": [
                    "speech",
                    "all",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we define Local Mining and Global Mining as</p>\n\n",
                "matched_terms": [
                    "global",
                    "mining",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Speech Vecalign pipeline consists of three steps:\nspeech preprocessing (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>), segment alignment with Vecalign (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS2\" title=\"3.2 Speech Segment Alignment &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), and alignment postprocessing (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS3\" title=\"3.3 Alignment Postprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>).\nAn illustration of our method is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "vecalign",
                    "method",
                    "speech",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Segmentation</span>. Same as speech mining, we first segment each speech document by VAD.\nWe apply Silero VAD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Silero Team (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib41\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We perform segment alignment based on the similarity between speech segment embeddings.\nUnlike speech mining, which solely relies on similarity scores, we use a dynamic programming&#160;(DP) algorithm to align segments in chronological order.</p>\n\n",
                "matched_terms": [
                    "perform",
                    "speech",
                    "alignment",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Segment concatenation</span>.\nSpeech segments do not necessarily correspond to complete sentences.\nSame as speech mining, we first progressively concatenate each segment with the subsequent ones.\nEach concatenated segment can contain up to 5 original segments and span a maximum of 20 seconds.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Obtaining segment embeddings</span>.\nAfter concatenations, we obtain speech segment embeddings using SpeechLASER models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib9\" title=\"\">2021</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nIdentical untranslated segments detected in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, along with all concatenated segments that include them, are skipped and replaced with <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><mn>0</mn></math>-valued vectors.</p>\n\n",
                "matched_terms": [
                    "models",
                    "2023a",
                    "all",
                    "speech",
                    "duquenne"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The embedding alignment algorithm is recursive DP.\nGiven a document pair and corresponding embeddings, the algorithm recursively averages every two consecutive embeddings, halving the sequence length until it reaches a small threshold.\nAt the bottom level, standard DP is applied to obtain an initial alignment.\nSubsequently, at each recursion level bottom-up, DP refines the alignment by searching within a small window around the alignment path from the previous level.\nBy constraining the search space and reducing the sequence length at each level, the algorithm achieves a linear time and space complexity.\nThe recursive DP algorithm runs on CPU and takes a few seconds on average per document pair.\nWe direct the readers to <cite class=\"ltx_cite ltx_citemacro_citet\">Thompson and Koehn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib43\" title=\"\">2019</a>)</cite> for a complete description.</p>\n\n",
                "matched_terms": [
                    "from",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because of DP, the resultant alignments strictly follow chronological order.\nWe use <math alttext=\"x_{a:b}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m1\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mrow><mi>a</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">x_{a:b}^{i}</annotation></semantics></math> to denote the concatenation of consecutive segments <math alttext=\"x_{a}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m2\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mi>a</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">x_{a}^{i}</annotation></semantics></math> through <math alttext=\"x_{b}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m3\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mi>b</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">x_{b}^{i}</annotation></semantics></math>.\nFor any two alignments <math alttext=\"(x_{a_{s}:a_{e}}^{i},y_{b_{s}:b_{e}}^{j})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mrow><msub><mi>a</mi><mi>s</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>a</mi><mi>e</mi></msub></mrow><mi>i</mi></msubsup><mo>,</mo><msubsup><mi>y</mi><mrow><msub><mi>b</mi><mi>s</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>b</mi><mi>e</mi></msub></mrow><mi>j</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{a_{s}:a_{e}}^{i},y_{b_{s}:b_{e}}^{j})</annotation></semantics></math> and <math alttext=\"(x_{c_{s}:c_{e}}^{i},y_{d_{s}:d_{e}}^{k})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mrow><msub><mi>c</mi><mi>s</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>c</mi><mi>e</mi></msub></mrow><mi>i</mi></msubsup><mo>,</mo><msubsup><mi>y</mi><mrow><msub><mi>d</mi><mi>s</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>d</mi><mi>e</mi></msub></mrow><mi>k</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{c_{s}:c_{e}}^{i},y_{d_{s}:d_{e}}^{k})</annotation></semantics></math>, Speech Vecalign guarantees that <math alttext=\"i=j=k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m6\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>=</mo><mi>j</mi><mo>=</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">i=j=k</annotation></semantics></math> and that either <math alttext=\"a_{e}&lt;c_{s},b_{e}&lt;d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m7\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>a</mi><mi>e</mi></msub><mo>&lt;</mo><msub><mi>c</mi><mi>s</mi></msub></mrow><mo>,</mo><mrow><msub><mi>b</mi><mi>e</mi></msub><mo>&lt;</mo><msub><mi>d</mi><mi>s</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">a_{e}&lt;c_{s},b_{e}&lt;d_{s}</annotation></semantics></math> or <math alttext=\"a_{s}&gt;c_{e},b_{s}&gt;d_{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m8\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>a</mi><mi>s</mi></msub><mo>&gt;</mo><msub><mi>c</mi><mi>e</mi></msub></mrow><mo>,</mo><mrow><msub><mi>b</mi><mi>s</mi></msub><mo>&gt;</mo><msub><mi>d</mi><mi>e</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">a_{s}&gt;c_{e},b_{s}&gt;d_{e}</annotation></semantics></math>.\nIn contrast, Local Mining ensures <math alttext=\"i=j=k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m9\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>=</mo><mi>j</mi><mo>=</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">i=j=k</annotation></semantics></math> but has no constraints on <math alttext=\"a,b,c,d\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m10\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi>c</mi><mo>,</mo><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">a,b,c,d</annotation></semantics></math>, while\nGlobal Mining makes no guarantees at all.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "vecalign",
                    "all",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Alignment concatenation</span>.\nAnother issue is that the raw alignments are too short: the average duration is 4.25 seconds, with 66% shorter than 5 seconds.\nTo cover more context, we progressively concatenate each alignment with the subsequent ones.\nThis can be easily done as alignments are in chronological order.\nEach concatenated alignment can contain up to 3 original alignments and span up to 20 seconds.</p>\n\n",
                "matched_terms": [
                    "than",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Global margin-scores computation</span>.\nThe raw alignments only have alignment costs as a quality indicator, which are computed <span class=\"ltx_text ltx_font_italic\">within</span> each document pair.\nTo assess alignment quality <span class=\"ltx_text ltx_font_italic\">across</span> document pairs, we train FAISS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Johnson et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib22\" title=\"\">2019</a>)</cite> indexes and compute margin-scores&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Artetxe and Schwenk (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib2\" title=\"\">2019a</a>)</cite> using Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S2.E1\" title=\"In 2 Speech Mining Overview &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> for <span class=\"ltx_text ltx_font_italic\">all</span> obtained alignments, following the common strategy in MT dataset curation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Sloto et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib42\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "all",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Removing highly-overlapped alignments</span>.\nFinally, we remove alignments that have too much overlap with others, following <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nFor any two consecutive alignments, we compute the ratio of the overlapped source duration to the maximum duration of the two source segments.\nIf the ratio exceeds a threshold, we discard the one with a lower margin-score.\nWe train S2ST models with multiple threshold values to determine the best one.\nOur experiments in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.SS1\" title=\"D.1 Optimizing &#119898;&#8290;&#119886;&#8290;&#119909;&#8290;_&#8290;&#119900;&#8290;&#119907;&#8290;&#119890;&#8290;&#119903;&#8290;&#119897;&#8290;&#119886;&#8290;&#119901; &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">D.1</span></a> suggest that 0.4 work best for Global Mining and 0.8 work best for Local Mining and Speech Vecalign.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "global",
                    "local",
                    "vecalign",
                    "2023a",
                    "best",
                    "speech",
                    "duquenne",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We apply Speech Vecalign, Global Mining, and Local Mining to the same raw data and train S2ST models on each type of alignments, providing a fair comparison.</p>\n\n",
                "matched_terms": [
                    "global",
                    "models",
                    "local",
                    "vecalign",
                    "data",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data source.</span>\nWe use the unlabeled, unsegmented English and German plenary session recordings from VoxPopuli v1&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib47\" title=\"\">2021a</a>)</cite> as raw data.\nVoxPopuli contains European Parliament plenary session recordings in each of the 23 European Union languages, paired with spoken interpretations into the other languages.\nThe document names are formatted as <code class=\"ltx_verbatim ltx_font_typewriter\">${session_id}_${language}.ogg</code>, and paired documents have the same <code class=\"ltx_verbatim ltx_font_typewriter\">${session_id}</code>.\nTo avoid overlapping with the test set (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS2\" title=\"4.2 Evaluation Data &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>), we only choose sessions from year 2013 to 2020.\nWe also exclude sessions in the development set&#160;(Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS2\" title=\"4.2 Evaluation Data &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>).\nFor En-to-De, the remaining data has 4,880 documents totaling about 3,000 hours for each language.\nFor De-to-En, there are 5,782 documents totaling 3,400 hours per language.\nThe difference is due to the different dev and test sets.\nAll documents are in pairs, allowing all methods to have exactly the same raw data.</p>\n\n",
                "matched_terms": [
                    "hours",
                    "sets",
                    "entode",
                    "all",
                    "data",
                    "from",
                    "test",
                    "detoen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Vecalign.</span>\nWe apply Speech Vecalign to each pair of speech documents and obtain alignments sorted by margin-scores.\nTraining data is chosen in descending order of margin-scores.\nWe train models on different data sizes and report the best results in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS5\" title=\"4.5 Main Results &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.5</span></a>.\nMore details on data size optimization can be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.SS2\" title=\"D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "vecalign",
                    "best",
                    "data",
                    "training",
                    "results",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech mining baselines.</span>\nWe apply Global Mining and Local Mining to the same raw data and embeddings as Speech Vecalign.\nThe implementation is based on <span class=\"ltx_text ltx_font_typewriter\">stopes<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">3</span></span><a class=\"ltx_ref ltx_url\" href=\"https://github.com/facebookresearch/stopes\" title=\"\">https://github.com/facebookresearch/stopes</a></span></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Andrews et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib1\" title=\"\">2022</a>)</cite>.\nAfter mining, we apply the same postprocessing strategies in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS3\" title=\"3.3 Alignment Postprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>, except for alignment concatenation which is not applicable.\nTraining data is chosen in descending order of margin-scores and details on data size optimization can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.SS2\" title=\"D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "alignment",
                    "local",
                    "vecalign",
                    "baselines",
                    "data",
                    "training",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Development set.</span>\nFollowing <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>, we choose 1000 samples from the highest scored sessions from the Voxpopuli S2ST dataset.\nAdditionally, we avoid choosing sessions that occur on the same dates as the test set.</p>\n\n",
                "matched_terms": [
                    "duquenne",
                    "2023a",
                    "from",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Test set.</span>\nWe use the Europarl-ST (EPST) test set&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Iranzo-S&#225;nchez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib18\" title=\"\">2020</a>)</cite> as an in-domain test set to evaluate the S2ST models.\nEPST is a multilingual S2TT dataset built on European Parliament debates from year 2008 to 2012.\nWe also adopt FLEURS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Conneau et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib6\" title=\"\">2023</a>)</cite> as an out-of-domain test set.</p>\n\n",
                "matched_terms": [
                    "models",
                    "epst",
                    "from",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train speech-to-unit translation (S2UT) models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib28\" title=\"\">2022a</a>)</cite> with <span class=\"ltx_text ltx_font_typewriter\">fairseq<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">4</span></span><a class=\"ltx_ref ltx_url\" href=\"https://github.com/facebookresearch/fairseq\" title=\"\">https://github.com/facebookresearch/fairseq</a></span></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">Ott et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib32\" title=\"\">2019</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib48\" title=\"\">2020</a>)</cite> on each type of alignments.\nThe S2UT model takes source speech as input and predicts a sequence of target discrete units.\nThe discrete units are obtained by applying\na k-means model to the <math alttext=\"11^{\\text{th}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><msup><mn>11</mn><mtext>th</mtext></msup><annotation encoding=\"application/x-tex\">11^{\\text{th}}</annotation></semantics></math> layer features of a HuBERT model <cite class=\"ltx_cite ltx_citemacro_cite\">Hsu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib16\" title=\"\">2021</a>)</cite>.\nFor English, we use the mHuBERT from <cite class=\"ltx_cite ltx_citemacro_citet\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib29\" title=\"\">2022b</a>)</cite>, and for German, we use the Germanic mHuBERT from <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nConsecutive duplicated units are removed.\nOur S2UT model architecture follows exactly <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nThe architecture details and training hyperparameters are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A2\" title=\"Appendix B Speech-to-Speech Translation &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "2023a",
                    "from",
                    "training",
                    "speech",
                    "duquenne",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the discrete units generated by S2UT models, we resynthesize speech using pretrained unit-based HiFi-GAN vocoders <cite class=\"ltx_cite ltx_citemacro_cite\">Polyak et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib33\" title=\"\">2021</a>)</cite> from <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nWe then evaluate the resynthesized speech using both transcription-based and transcription-free methods.</p>\n\n",
                "matched_terms": [
                    "models",
                    "2023a",
                    "from",
                    "both",
                    "speech",
                    "duquenne"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the transcription-based method, we transcribe the speech output using the same ASR models as <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nWe evaluate the transcriptions using SacreBLEU<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/mjpost/sacrebleu\" title=\"\">https://github.com/mjpost/sacrebleu</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">Post (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib34\" title=\"\">2018</a>)</cite> to compute BLEU<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Signature: nrefs:1 + case:mixed + eff:no\n+ tok:13a + smooth:exp + version:2.2.0</span></span></span> and chrF2++<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>Signature: nrefs:1 + case:mixed + eff:yes + nc:6 + nw:2 + space:no + version:2.2.0</span></span></span> scores.\nWe apply the significance test using paired bootstrap resampling&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Koehn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib26\" title=\"\">2004</a>)</cite> with 1000 bootstrap resamples.</p>\n\n",
                "matched_terms": [
                    "models",
                    "2023a",
                    "method",
                    "test",
                    "speech",
                    "duquenne"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also adopt BLASER 2.0 <cite class=\"ltx_cite ltx_citemacro_cite\">Dale and Costa-juss&#224; (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib7\" title=\"\">2024</a>)</cite> to directly evaluate speech output.\nWe compute the referenced score using <code class=\"ltx_verbatim ltx_font_typewriter\">blaser-2.0-ref</code><span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/blaser-2.0-ref\" title=\"\">https://huggingface.co/facebook/blaser-2.0-ref</a></span></span></span> for input and output speech, as well as the text reference.\nWe compute the reference-free score using <code class=\"ltx_verbatim ltx_font_typewriter\">blaser-2.0-qe</code><span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/blaser-2.0-qe\" title=\"\">https://huggingface.co/facebook/blaser-2.0-qe</a></span></span></span> for input and output speech only.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "blaser",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Intriguingly, for both directions, Speech Vecalign and speech mining models are competitive with or outperform SpeechMatrix&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite> models, despite the latter being mined from about 24k hours of speech per language, <span class=\"ltx_text ltx_font_italic\">8 times more</span> than our raw data.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>We do not aim for state-of-the-art performance. Our results are not directly comparable to SpeechMatrix. We report SpeechMatrix results only to show the performance gap.</span></span></span>\nFor En-to-De, our Global Mining and Speech Vecalign models achieve improvements of 0.94 and 1.31 BLEU, respectively.\nOur Local Mining model achieves even 1.64 BLEU improvement.\nWe suspect that SpeechMatrix has not removed identical untranslated segments prior to and after mining, which significantly hurts model performance.\nFurther discussion is in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.SS5\" title=\"5.5 Removing Identical Untranslated Segments is Critical &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5.5</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "global",
                    "hours",
                    "local",
                    "than",
                    "vecalign",
                    "2023a",
                    "entode",
                    "mining",
                    "show",
                    "from",
                    "both",
                    "results",
                    "directions",
                    "speech",
                    "stateoftheart",
                    "duquenne",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While Local Mining has not been previously explored, our results suggest that it is a potentially useful method.\nLocal Mining achieves the highest BLEU score in En-to-De, and only slightly underperforms Global Mining in De-to-En, indicating that constraining the mining scope to document pairs does not necessarily have a negative impact on alignment quality.\nYet we note that Local Mining requires more training data to achieve its optimal performance, as shown in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.SS2\" title=\"D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "alignment",
                    "local",
                    "entode",
                    "mining",
                    "data",
                    "training",
                    "results",
                    "method",
                    "detoen",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our Speech Vecalign models outperform both speech mining models in both directions.\nFor En-to-De, the Speech Vecalign model achieves 12.58 BLEU, comparable with our strong Global Mining and Local Mining baselines.\nIn terms of chrF2++, it surpasses Global Mining and Local Mining by 1.69 and 0.26, respectively.\nIt also significantly improves their referenced BLASER 2.0 by 0.08 and 0.03.\nFor De-to-En, Speech Vecalign and Global Mining models achieve comparable BLEU (16.14 vs. 15.96), but Speech Vecalign surpasses Global Mining by 0.57 in chrF2++.\nSpeech Vecalign significantly outperforms Local Mining under all metrics.\nThese results demonstrate that Speech Vecalign produces higher-quality alignments than both speech mining baselines.</p>\n\n",
                "matched_terms": [
                    "models",
                    "global",
                    "local",
                    "than",
                    "vecalign",
                    "metrics",
                    "entode",
                    "baselines",
                    "mining",
                    "all",
                    "under",
                    "both",
                    "results",
                    "directions",
                    "speech",
                    "blaser",
                    "detoen",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We analyze properties of speech mining methods and compare them with Speech Vecalign.\nAlthough we show that speech mining methods produce alignments similar to those of Speech Vecalign, the latter offers advantages of producing longer and less noisy alignments.</p>\n\n",
                "matched_terms": [
                    "show",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, we show that Global Mining mostly <span class=\"ltx_text ltx_font_bold\">locally</span> aligns speech documents.\nWhile Global Mining searches for the best matching segment pairs among roughly 10 million segments, one might expect its alignments to cover the spread of the entire dataset.\nOn the contrary, we find that Global Mining alignments are concentrated within document pairs, each typically containing hundreds to thousands of segments.</p>\n\n",
                "matched_terms": [
                    "global",
                    "best",
                    "show",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To quantify this, we examine the 1000-hour Global Mining data and count alignments whose source and target segments come from <span class=\"ltx_text ltx_font_italic\">different</span> document pairs.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F3\" title=\"Figure 3 &#8227; 5.1 Speech Mining Mostly Locally Aligns Segments in Time Order &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, fewer than 6% fall into this category, while the majority&#160;(<math alttext=\"&gt;93\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mrow><mn>93</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">&gt;93\\%</annotation></semantics></math>) are within paired documents.</p>\n\n",
                "matched_terms": [
                    "global",
                    "than",
                    "data",
                    "from",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, we analyze the time order of alignments produced by both speech mining methods.\nBorrowing the notation from Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS2\" title=\"3.2 Speech Segment Alignment &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, we define two pairs of alignments to be <span class=\"ltx_text ltx_font_italic\">in-order</span> if either <math alttext=\"a_{e}&lt;c_{s},b_{e}&lt;d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>a</mi><mi>e</mi></msub><mo>&lt;</mo><msub><mi>c</mi><mi>s</mi></msub></mrow><mo>,</mo><mrow><msub><mi>b</mi><mi>e</mi></msub><mo>&lt;</mo><msub><mi>d</mi><mi>s</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">a_{e}&lt;c_{s},b_{e}&lt;d_{s}</annotation></semantics></math> or <math alttext=\"a_{s}&gt;c_{e},b_{s}&gt;d_{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>a</mi><mi>s</mi></msub><mo>&gt;</mo><msub><mi>c</mi><mi>e</mi></msub></mrow><mo>,</mo><mrow><msub><mi>b</mi><mi>s</mi></msub><mo>&gt;</mo><msub><mi>d</mi><mi>e</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">a_{s}&gt;c_{e},b_{s}&gt;d_{e}</annotation></semantics></math>; otherwise, they are <span class=\"ltx_text ltx_font_italic\">out-of-order</span>.\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F3\" title=\"Figure 3 &#8227; 5.1 Speech Mining Mostly Locally Aligns Segments in Time Order &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows that only around 1% alignments are out-of-order for both speech mining methods.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "from",
                    "both",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Observations above indicate that speech mining alignments are mostly within paired documents and preserve time order.\nWe hypothesize that speech-to-speech alignments are sparse and high-quality ones mostly exist in paired documents.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a by-product, this property can be leveraged to identify parallel documents.\nIf Global Mining finds many alignments between two documents, they are likely to be parallel.\nIt is particularly useful when the pairing metadata is not readily available.</p>\n\n",
                "matched_terms": [
                    "global",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the observations in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.SS1\" title=\"5.1 Speech Mining Mostly Locally Aligns Segments in Time Order &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> that speech mining produces mostly local, in-order alignments, we analyze the similarity between them and Speech Vecalign alignments.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mining",
                    "vecalign",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ the alignment evaluation method<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/thompsonb/vecalign/blob/master/score.py\" title=\"\">https://github.com/thompsonb/vecalign/blob/master/score.py</a>.</span></span></span> from&#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Thompson and Koehn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib43\" title=\"\">2019</a>)</cite>, which computes precision and recall by comparing system alignments to a reference.\nThere are two modes: <span class=\"ltx_text ltx_font_italic\">Strict</span>, which counts only exact matches as true positives, and <span class=\"ltx_text ltx_font_italic\">Lax</span>, which considers an alignment as true positive if both its source and target segment overlap with the reference.\nIf not true positive, an alignment is false positive.\nRecall is computed by swapping the reference and the system alignments.</p>\n\n",
                "matched_terms": [
                    "both",
                    "from",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Without loss of generality, we use Speech Vecalign En-De alignments as the reference, and evaluate speech mining ones.\nWe choose 700k highest-scoring alignments from all three methods to ensure a fair comparison.\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.T2\" title=\"Table 2 &#8227; 5.2 Speech Mining Methods Produce Similar Alignments as Speech Vecalign &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows that about 30% of speech mining alignments are exactly the same as those of Speech Vecalign, and about 90% overlap with Speech Vecalign alignments.\nThis high similarity explains why Speech Vecalign and speech mining models have similar performance.</p>\n\n",
                "matched_terms": [
                    "models",
                    "vecalign",
                    "all",
                    "from",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As speech mining and Speech Vecalign produce similar alignments, we explore why Speech Vecalign models still perform better.\nA key advantage of Speech Vecalign is that it first produces fine-grained alignments and then constructs alignments with different amounts of context, thanks to the alignment concatenation strategy.\nSpeech mining methods, on the other hand, solely depend on margin-scores and tend to favor shorter alignments.</p>\n\n",
                "matched_terms": [
                    "models",
                    "alignment",
                    "vecalign",
                    "better",
                    "perform",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the best En-to-De models and corresponding data sizes from Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS5\" title=\"4.5 Main Results &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.5</span></a>, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F4\" title=\"Figure 4 &#8227; 5.3 Speech Vecalign Produces Longer Alignments &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the average sentence-level chrF2++ scores on the test set and the percentage of training alignments for different source speech duration ranges.\nNotably, Speech Vecalign has a large portion of long training samples: the blue bars are highest for durations longer than 12 seconds.\nSpecifically, the average source duration of Speech Vecalign is 8.51 seconds, while Global Mining and Local Mining have average durations of 7.50 and 8.53 seconds, respectively.\nAs a result, the Speech Vecalign model performs better on test samples longer than 10 seconds, while having comparable performance on shorter ones.\nThis highlights that Speech Vecalign is able to produce longer, context-rich alignments which help to improve S2ST model performance.\nInterestingly, Local Mining surpasses the Global Mining model on long inputs, which could be also attributed to its longer training samples.</p>\n\n",
                "matched_terms": [
                    "models",
                    "global",
                    "local",
                    "than",
                    "vecalign",
                    "better",
                    "entode",
                    "best",
                    "data",
                    "from",
                    "training",
                    "test",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We visualize alignments produced by different methods for the same document pair, which is about 10 minutes long and contains around 200 segments.\nFor reference, we manually created a gold segment-level alignment, with detailed procedure in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A6\" title=\"Appendix F Procedure of Manual Alignments &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>.\nWe illustrate the best 80 alignments for each of the speech mining methods.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "alignment",
                    "best",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F5\" title=\"Figure 5 &#8227; 5.3 Speech Vecalign Produces Longer Alignments &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows, Speech Vecalign produces the most fine-grained alignments and is most similar to the gold reference.\nGlobal Mining also performs well, aligning closely with the groundtruth path, whereas Local Mining produces more noise and misses more alignments along the correct path.\nWe hypothesize Local Mining has limited number of segments in a single document pair, making nearest neighbors less effective normalizers in the margin-score computation.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As presented in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS5\" title=\"4.5 Main Results &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.5</span></a>, our reproduced speech mining models achieve comparable or even better results than SpeechMatrix models.\nBy listening to samples of SpeechMatrix alignments, we observed many cases where the source and target segments contained identical untranslated content, which is an issue mentioned in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>.\nUsing the method described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, we identified approximately 100k out of 630k alignments with untranslated source and target segments, totaling 181 hours.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "hours",
                    "than",
                    "better",
                    "results",
                    "method",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the impact of untranslated segments, we trained models on the original SpeechMatrix En-De alignments and on a version with untranslated alignments <span class=\"ltx_text ltx_font_italic\">removed</span>.\nThe training data is chosen with a margin-score threshold of 1.09, following the original setup.\nAs shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.T3\" title=\"Table 3 &#8227; 5.5 Removing Identical Untranslated Segments is Critical &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the cleaned data produces better models, improving BLEU score by <span class=\"ltx_text ltx_font_bold\">1.00</span> for En-to-De and <span class=\"ltx_text ltx_font_bold\">0.11</span> for De-to-En, despite having 13% less training data.\nThe smaller gain on De-to-En may be due to most untranslated segments being in English, which have smaller impact on into-English translation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "better",
                    "entode",
                    "data",
                    "training",
                    "detoen",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also re-produced our alignment pipelines <span class=\"ltx_text ltx_font_italic\">without</span> removing identical untranslated segments, referred to as &#8220;noisy\" in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.T4\" title=\"Table 4 &#8227; 5.5 Removing Identical Untranslated Segments is Critical &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nWe trained models on 500 hours of this data.\nAlthough these untranslated segments account for less than 1% of the training data, performance degrades noticeably.</p>\n\n",
                "matched_terms": [
                    "models",
                    "hours",
                    "alignment",
                    "than",
                    "data",
                    "training",
                    "trained",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, the experiments highlight that removing untranslated alignments is essential for S2ST training, corroborating <cite class=\"ltx_cite ltx_citemacro_citet\">Khayrallah and Koehn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib23\" title=\"\">2018</a>)</cite>, who found that the untranslated sentences are most catastrophic in neural machine translation.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present Speech Vecalign, a parallel speech document alignment method that aligns speech segment embeddings within document pairs and in chronological order.\nWe apply Speech Vecalign to parallel English-German VoxPopuli speech documents and conduct S2ST experiments to demonstrate its superiority over two strong speech mining baselines.\nOur analysis reveals that although speech mining methods primarily align documents locally and in-order, Global Mining falls short of producing long alignments, and Local Mining in particular produces more noise.\nFor long-term future work, we plan to extend Speech Vecalign to other language pairs or other data sources.\nWe can also explore aligning speech and text embeddings to construct S2TT datasets.</p>\n\n",
                "matched_terms": [
                    "our",
                    "global",
                    "alignment",
                    "local",
                    "vecalign",
                    "baselines",
                    "data",
                    "method",
                    "speech",
                    "text",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech features for identical untranslated segment detection could be improved.</span>\nOur current approach uses filterbank features, which are based on power spectrum, to detect identical untranslated segments.\nHowever, filterbank features are likely to fail for segments that have identical content but differ in signal power.\nAs one of the anonymous reviewers pointed out, cepstral features may be a more robust alternative.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limited language pair.</span>\nWe have only conducted experiments for English and German speech from the VoxPopuli dataset.\nAs Speech Vecalign heavily relies on the quality of speech embeddings, the performance is unclear for other language pairs and other domains of speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "vecalign",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dependency on parallel speech documents.</span>\nSpeech Vecalign requires parallel speech documents, which is often not available.\nWe may rely on Global Mining to discover parallel documents, as Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.SS1\" title=\"5.1 Speech Mining Mostly Locally Aligns Segments in Time Order &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> suggests, but doing so will introduce extra computation costs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "global",
                    "vecalign",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-to-speech translation (S2ST).</span>\nThe early S2ST systems consist of cascaded ASR, MT, and TTS models <cite class=\"ltx_cite ltx_citemacro_cite\">Lavie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib27\" title=\"\">1997</a>); Nakamura et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib31\" title=\"\">2006</a>); Wahlster (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib46\" title=\"\">2013</a>)</cite>.\nDirect S2ST models have recently been proposed to alleviate error propagation, support unwritten languages, and improve inference speed.\nTranslatotron models <cite class=\"ltx_cite ltx_citemacro_cite\">Jia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib21\" title=\"\">2019</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib19\" title=\"\">2022a</a>)</cite> are trained with spectrograms as targets, while the S2UT model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib28\" title=\"\">2022a</a>)</cite> outputs discrete units.\nUnitY&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Inaguma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib17\" title=\"\">2023</a>)</cite> and UnitY2 <cite class=\"ltx_cite ltx_citemacro_cite\">Seamless Communication et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib39\" title=\"\">2023</a>)</cite> are two-pass direct S2ST models that predict both subwords and discrete units with a single model.\nDespite advances in architectures, the amount of supervised training data is still insufficient and thus limits model performance.</p>\n\n",
                "matched_terms": [
                    "models",
                    "data",
                    "both",
                    "training",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Bilingual text sentence alignment.</span>\nText alignment is very related to speech alignment.\nMethods apply dynamic programming <cite class=\"ltx_cite ltx_citemacro_cite\">Bellman (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib4\" title=\"\">1954</a>)</cite> and mainly differ in the design of scoring functions.\nEarly works <cite class=\"ltx_cite ltx_citemacro_cite\">Brown et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib5\" title=\"\">1991</a>); Gale and Church (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib12\" title=\"\">1993</a>)</cite> are based on sentence lengths.\nLater methods incorporate translations in various ways&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Moore (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib30\" title=\"\">2002</a>); Varga et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib44\" title=\"\">2007</a>); Sennrich and Volk (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib40\" title=\"\">2010</a>); Gomes and Lopes (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib13\" title=\"\">2016</a>)</cite>.\nOur work is inspired by Vecalign <cite class=\"ltx_cite ltx_citemacro_cite\">Thompson and Koehn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib43\" title=\"\">2019</a>)</cite>, which utilizes margin-based cosine similarities between multilingual sentence embeddings like LASER&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Artetxe and Schwenk (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib3\" title=\"\">2019b</a>); Heffernan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib14\" title=\"\">2022</a>)</cite> and LaBSE <cite class=\"ltx_cite ltx_citemacro_cite\">Feng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib11\" title=\"\">2022</a>)</cite>.\nVecalign is also more efficient than previous methods.\nBy applying fast dynamic time warping <cite class=\"ltx_cite ltx_citemacro_cite\">Salvador and Chan (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib37\" title=\"\">2007</a>)</cite>, it has a linear time and space complexity with respect to the number of input sentences.\nThe recent progress of extending multilingual sentence embeddings to the speech modality <cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib9\" title=\"\">2021</a>); Khurana et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib24\" title=\"\">2022</a>); Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib10\" title=\"\">b</a>)</cite> enables us to align speech segments by their speech embeddings using the same algorithm.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "than",
                    "vecalign",
                    "2023a",
                    "speech",
                    "duquenne",
                    "text",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">S2ST datasets.</span>\nThere are two common ways to automatically build an S2ST dataset: (1)&#160;building alignments from multilingual speech data; (2)&#160;synthesizing speech for text translations from existing speech-to-text translation (S2TT) corpora.\nThe first line of work has human spoken speech on both source and target sides.\nVoxPopuli <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib47\" title=\"\">2021a</a>)</cite> aligns multilingual speech documents based on text transcriptions, yielding 17.3k-hour alignments between 15 source and target languages.\nSpeechMatrix <cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite> applies Global Mining with SpeechLASER embeddings on VoxPopuli. It obtains alignments for 136 language pairs with an average of 1537 hours per direction.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Seamless Communication et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib39\" title=\"\">2023</a>)</cite> apply Global Mining to web-crawled speech data with SONAR embeddings.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Seamless Communication et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib39\" title=\"\">2023</a>)</cite> also mine a SeamlessAlignExpressive dataset with expressively- and semantically-aligned segment pairs, based on a blend of both semantic and prosodic similarity score <cite class=\"ltx_cite ltx_citemacro_cite\">Heffernan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib15\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "hours",
                    "2023a",
                    "data",
                    "from",
                    "both",
                    "speech",
                    "duquenne",
                    "text",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second line of work has synthesized speech on the target side.\nFisher <cite class=\"ltx_cite ltx_citemacro_cite\">Post et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib35\" title=\"\">2013</a>)</cite> is a Spanish-English S2TT dataset containing about 170 hours of Spanish telephone conversations and English translations which are used to synthesize English speech.\nCVSS <cite class=\"ltx_cite ltx_citemacro_cite\">Jia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib20\" title=\"\">2022b</a>)</cite> is an S2ST dataset covering utterances from 21 languages to English, obtained by synthesizing the text translations in CoVoST 2 <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib49\" title=\"\">2021b</a>)</cite>.\nBesides automatic methods, FLEURS <cite class=\"ltx_cite ltx_citemacro_cite\">Conneau et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib6\" title=\"\">2023</a>)</cite> has collected human read speech covering 102 languages. But it contains only about 12 hours per language and is intended for evaluation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "text",
                    "from",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to limited computing resources, we adopt different training strategies for different purposes.\nThe 500-hour datasets are used for hyperparameter optimization, and larger datasets are used for reporting main results.\nAll models are trained for up to 400k steps, with the first 10,000 steps as a warmup stage.\nFor experiments on a 500-hour dataset, we use a batch size of 320k tokens and apply early-stopping if there is no improvement on the development set for 30 epochs.\nThese models are trained on 4 NVIDIA GeForce GTX 1080 Ti GPUs for approximately 15 days.\nFor larger datasets, we increase the batch size to 640k tokens and early-stopping is not applied.\nThese models are trained on 2 NVIDIA A100-SXM4-80GB GPUs for approximately 15 days.\nThe best checkpoint is selected based on the development set loss.\nAll experiments are conducted in fp32, as we found training with fp16 and amp very unstable.</p>\n\n",
                "matched_terms": [
                    "models",
                    "best",
                    "all",
                    "training",
                    "results",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Segment embedding.</span>\nThis is the most time-consuming step.\nWe use a mixture of NVIDIA GeForce GTX 1080 and 2080 Ti GPUs.\nEmbedding about 6,000 hours of speech (3,000 hours for each language) took approximately 1,100 GPU hours.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Alignment.</span>\nLocal Mining and Global Mining run on a single GPU.\nThey take about 2 hours.\nSpeech Vecalign runs on a single CPU and takes about 2 hours.</p>\n\n",
                "matched_terms": [
                    "global",
                    "hours",
                    "alignment",
                    "local",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There are two hyperparameters that affect training data: (1) the maximum source duration overlap ratio between alignments, <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m1\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math>, which is mentioned in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS3\" title=\"3.3 Alignment Postprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>, and\n(2) the data size.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m1\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> controls the trade-off between overlapped durations and data quality.\nFor instance, a lower <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m2\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> reduces the overlap but also discards alignments more aggressively.\nOverlapped alignments usually have similar margin-scores, so more high-quality alignments are lost.\nThe data size controls the trade-off between data size and data quality cutoff.\nFor instance, a larger dataset will have a lower quality cutoff, as alignments are selected in descending order of margin-scores.\nIn this section, we optimize the combination of <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m3\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> and data size by training S2UT models on different datasets.\nNote that the raw data stays the same.</p>\n\n",
                "matched_terms": [
                    "models",
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first experiment with different values of <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math>.\nWe apply different <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> thresholds during the postprocessing stage, and always choose the best 500 hours as the training data.\nThe optimal value is determined based on development set ASR-BLEU.\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.F6\" title=\"Figure 6 &#8227; D.1 Optimizing &#119898;&#8290;&#119886;&#8290;&#119909;&#8290;_&#8290;&#119900;&#8290;&#119907;&#8290;&#119890;&#8290;&#119903;&#8290;&#119897;&#8290;&#119886;&#8290;&#119901; &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows that 0.8 works best for Speech Vecalign and Local Mining and 0.4 works best for Global Mining.\nThe test set performance is also drawn in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.F6\" title=\"Figure 6 &#8227; D.1 Optimizing &#119898;&#8290;&#119886;&#8290;&#119909;&#8290;_&#8290;&#119900;&#8290;&#119907;&#8290;&#119890;&#8290;&#119903;&#8290;&#119897;&#8290;&#119886;&#8290;&#119901; &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, exhibiting a similar trend.</p>\n\n",
                "matched_terms": [
                    "global",
                    "hours",
                    "local",
                    "vecalign",
                    "best",
                    "data",
                    "training",
                    "test",
                    "speech",
                    "asrbleu",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next we optimize the training data size.\nWe fix <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> at 0.4 for Global Mining and 0.8 for Speech Vecalign and Local Mining during postprocessing, only lowering the quality cutoff to include more training data.\nThe models are trained on different amounts of data until we find the peak performance.\nResults are shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.F7\" title=\"Figure 7 &#8227; D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "global",
                    "local",
                    "vecalign",
                    "data",
                    "training",
                    "results",
                    "speech",
                    "trained",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For En-to-De, the best Speech Vecalign model is trained on the 750-hour dataset, achieving 12.58 BLEU.\nIt outperforms the best Global Mining model which achieves 12.21 BLEU.\nThe best Local Mining model achieves 12.91 BLEU.\nHowever, we note that it requires a lot more data than the other two methods to achieve the peak performance.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "than",
                    "vecalign",
                    "entode",
                    "best",
                    "data",
                    "speech",
                    "trained",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For De-to-En, the 1000-hour dataset works best for Speech Vecalign while the 750-hour dataset works best for Global Mining.\nLocal Mining achieves the peak performance when the data size is 1250 hours, still requiring more data than the other methods.\nThe Speech Vecalign performs better than both the Global Mining and the Local Mining models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "global",
                    "hours",
                    "local",
                    "than",
                    "vecalign",
                    "better",
                    "best",
                    "data",
                    "both",
                    "speech",
                    "detoen",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide evaluation results on the FLEURS test set in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.T5\" title=\"Table 5 &#8227; D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.\nSimilar to Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS5\" title=\"4.5 Main Results &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.5</span></a>, our results match or outperform SpeechMatrix results.\nFor both En-to-De and De-to-En, Speech Vecalign and Global Mining achieve comparable performance when using the transcription-based metrics ASR-BLEU and ASR-chrF2++.\nTheir performance is especially close on De-to-En.\nHowever, Speech Vecalign is significantly better than Global Mining when using the BLASER 2.0 metrics, achieving an improvement of 0.06 and 0.04 referenced BLASER 2.0 scores on En-to-De and De-to-En, respectively.</p>\n\n",
                "matched_terms": [
                    "global",
                    "detoen",
                    "than",
                    "vecalign",
                    "metrics",
                    "better",
                    "entode",
                    "mining",
                    "both",
                    "results",
                    "test",
                    "speech",
                    "blaser",
                    "asrbleu",
                    "asrchrf2",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For En-to-De, Speech Vecalign achieves comparable performance with Local Mining on all metrics.\nFor De-to-En, Speech Vecalign is significantly better than Local Mining when using BLASER 2.0 metrics.</p>\n\n",
                "matched_terms": [
                    "local",
                    "than",
                    "vecalign",
                    "metrics",
                    "better",
                    "entode",
                    "all",
                    "speech",
                    "blaser",
                    "detoen",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, we can show that Speech Vecalign performs better than both Local Mining and Global Mining.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "than",
                    "vecalign",
                    "better",
                    "overall",
                    "show",
                    "both",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We manually select the corresponding words for each speech segment from the obtained transcriptions;</p>\n\n",
                "matched_terms": [
                    "speech",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although this process depends on models such as Whisper and Google Translate, we argue that they should perform extremely well on German and English and should produce almost perfect transcriptions and translations.</p>\n\n",
                "matched_terms": [
                    "perform",
                    "models",
                    "almost"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the same alignment evaluation method as Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.SS2\" title=\"5.2 Speech Mining Methods Produce Similar Alignments as Speech Vecalign &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>, but we use the manual alignments as the reference.\nThere are 144 raw Speech Vecalign alignments, and we choose the same number of alignments from Global Mining and Local Mining in descending order of margin-scores.\nThe Recall and Precision of raw Speech Vecalign, Local Mining, and Global Mining alignments are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A7.T6\" title=\"Table 6 &#8227; Appendix G Evaluation of System Alignments using the Manual Alignments &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "alignment",
                    "local",
                    "vecalign",
                    "from",
                    "method",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The three methods have similar Lax Precisions, while that of Local Mining and Global Mining are slightly higher.\nSpeech Vecalign has the highest recall values than both the speech mining baselines.\nAmong the three methods, Local Mining has the worst performance in general.\nThis follows Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F5\" title=\"Figure 5 &#8227; 5.3 Speech Vecalign Produces Longer Alignments &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> that both Speech Vecalign and Global Mining have good performance but Local Mining does not perform well.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "than",
                    "vecalign",
                    "baselines",
                    "perform",
                    "both",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As our proposed alignment pipeline consists of several intermediate steps, we report numbers of segments or alignments in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A7.T7\" title=\"Table 7 &#8227; Appendix G Evaluation of System Alignments using the Manual Alignments &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.\nWe use English-to-German alignment as an example.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "englishtogerman",
                    "our"
                ]
            }
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Precision and Recall for speech mining alignments when Speech Vecalign is used as the reference.\nThe high precision and recall in the Lax mode indicate the methods produce similar alignments.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Mode</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Global Mining</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Local Mining</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Precision</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Recall</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Precision</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Recall</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">Strict</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.325</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.326</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.305</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.305</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text ltx_font_italic\">Lax</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.865</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.965</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.963</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.814</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "vecalign",
            "mode",
            "high",
            "when",
            "methods",
            "used",
            "strict",
            "mining",
            "indicate",
            "alignments",
            "recall",
            "speech",
            "global",
            "produce",
            "lax",
            "local",
            "precision",
            "similar",
            "reference"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Without loss of generality, we use Speech Vecalign En-De alignments as the reference, and evaluate speech mining ones.\nWe choose 700k highest-scoring alignments from all three methods to ensure a fair comparison.\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.T2\" title=\"Table 2 &#8227; 5.2 Speech Mining Methods Produce Similar Alignments as Speech Vecalign &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows that about 30% of speech mining alignments are exactly the same as those of Speech Vecalign, and about 90% overlap with Speech Vecalign alignments.\nThis high similarity explains why Speech Vecalign and speech mining models have similar performance.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Speech Vecalign, a parallel speech document alignment method that monotonically aligns speech segment embeddings and does not depend on text transcriptions.\nCompared to the baseline method Global Mining&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>, a variant of speech mining, Speech Vecalign produces longer speech-to-speech alignments.\nIt also demonstrates greater robustness than Local Mining, another speech mining variant, as it produces less noise.\nWe applied Speech Vecalign to 3,000 hours of unlabeled parallel English-German&#160;(En-De) speech documents from VoxPopuli, yielding about 1,000 hours of high-quality alignments.\nWe then trained En-De speech-to-speech translation models on the aligned data.\nSpeech Vecalign improves the En-to-De and De-to-En performance over Global Mining by 0.37 and 0.18 ASR-BLEU, respectively.\nMoreover, our models match or outperform SpeechMatrix model performance, despite using 8 times fewer raw speech documents.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Data and code are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/mct10/Speech-Vecalign\" title=\"\">https://github.com/mct10/Speech-Vecalign</a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "alignments",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "vecalign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech-to-speech translation (S2ST) is the task of translating speech in one language into speech in another language.\nConventional S2ST systems concatenate automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) models <cite class=\"ltx_cite ltx_citemacro_cite\">Lavie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib27\" title=\"\">1997</a>); Nakamura et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib31\" title=\"\">2006</a>); Wahlster (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib46\" title=\"\">2013</a>)</cite>.\nThese components can be trained individually with datasets for the different components.\nDirect S2ST models, which translate source speech into target spectrograms or discrete units with a single architecture, have been recently proposed to alleviate error propagation and to reduce inference latency <cite class=\"ltx_cite ltx_citemacro_cite\">Jia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib21\" title=\"\">2019</a>); Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib28\" title=\"\">2022a</a>)</cite>.\nDespite the advantages, performance of direct models is limited by the amount of speech-to-speech aligned data, which is much more scarce than the data used for components of cascaded systems.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There have been efforts to automatically curate alignments from multilingual <span class=\"ltx_text ltx_font_italic\">speech document</span>s.\nIn this paper, we define a <span class=\"ltx_text ltx_font_italic\">speech document</span> as a file containing more than one utterance and typically comprising several paragraphs, analogous to a <span class=\"ltx_text ltx_font_italic\">text document</span>.\nVoxPopuli&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib47\" title=\"\">2021a</a>)</cite> is one such corpus containing a large number of <span class=\"ltx_text ltx_font_italic\">parallel</span> speech documents, which are pairs of documents that have the same content but differ in language.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "alignments"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech-to-speech alignment methods align short speech clips called <span class=\"ltx_text ltx_font_italic\">segment</span>s, and can be either transcription-based or transcription-free.\nWhen transcriptions are available, segments in parallel speech documents can be aligned through speech-to-text and text-to-text alignments.\nInspired by text mining&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Schwenk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib38\" title=\"\">2021</a>)</cite>, speech mining&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib9\" title=\"\">2021</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite> was proposed as a transcription-free method that aligns speech segments by finding segment pairs with the highest embedding similarity.\nIt scales well as it does not rely on the availability of text transcriptions.\nWhen speech mining is applied to a large amount of speech documents, as in all previous work, it is referred to as <span class=\"ltx_text ltx_font_bold\">Global Mining</span>.\nAnother variant, <span class=\"ltx_text ltx_font_bold\">Local Mining</span>, which applies speech mining to a single pair of parallel speech documents, has not been well explored.\nAs we formally define in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S2\" title=\"2 Speech Mining Overview &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, both Global Mining and Local Mining treat documents as bags of unordered segments.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "alignments",
                    "when",
                    "methods",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since speech mining methods do not leverage the document pair structure, we wonder, <span class=\"ltx_text ltx_font_bold\">can we obtain better alignments by aligning speech segments within document pairs and preserving their time order?</span>\nThis allows us to utilize the extra knowledge that (1) segments within parallel document pairs are likely to be translations of each other, and (2) segment pairs right next to already aligned pairs are also likely to be aligned.\nWe draw inspiration from parallel <span class=\"ltx_text ltx_font_italic\">text</span> document alignment methods, which have been popular to create sentence-aligned bitext for training MT systems.\nUnlike mining, they align sentences for each document pair while maintaining the sentence order.\nOur work is based on the text alignment method Vecalign&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Thompson and Koehn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib43\" title=\"\">2019</a>)</cite>, which aligns parallel sentences by applying fast dynamic time warping&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Salvador and Chan (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib37\" title=\"\">2007</a>)</cite> to sentence embeddings.\nWith the advances of extending sentence embeddings to the speech modality&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib9\" title=\"\">2021</a>)</cite>, we can readily apply Vecalign to parallel speech documents.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "vecalign",
                    "methods",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce Speech Vecalign, a method that aligns parallel speech documents using speech segment embeddings.\nInstead of mining from bags of segments, our method aligns individual document pairs and maintains the chronological order of segments, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nAdditional preprocessing and postprocessing strategies are applied to improve alignment quality.\nWe compare Speech Vecalign with Local Mining and Global Mining and show that Speech Vecalign produces higher-quality alignments.\nWe further provide extensive analysis for all three methods, which could be useful for future research.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "alignments",
                    "vecalign",
                    "methods",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We formally describe the speech mining methods in this section.\nOther related work is in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A1\" title=\"Appendix A Related Work &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mining",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech Mining, first proposed by <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib9\" title=\"\">2021</a>)</cite>, encodes speech segments into language- and modality-agnostic fixed-size embeddings, and then uses margin-based similarity search <cite class=\"ltx_cite ltx_citemacro_cite\">Artetxe and Schwenk (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib2\" title=\"\">2019a</a>)</cite> to find the closest embedding pairs.\nDepending on the search scope, it can be categorized as Global Mining or Local Mining.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "global",
                    "local",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Bag of embeddings</span>.\nIn <span class=\"ltx_text ltx_font_italic\">Global Mining</span>, embeddings are grouped by <span class=\"ltx_text ltx_font_bold\">language</span>.\nWe define <math alttext=\"G_{X}=\\left\\{E_{\\tilde{X}_{1}},E_{\\tilde{X}_{2}},\\ldots,E_{\\tilde{X}_{n}}\\right\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m1\" intent=\":literal\"><semantics><mrow><msub><mi>G</mi><mi>X</mi></msub><mo>=</mo><mrow><mo>{</mo><msub><mi>E</mi><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mn>1</mn></msub></msub><mo>,</mo><msub><mi>E</mi><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mn>2</mn></msub></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>E</mi><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mi>n</mi></msub></msub><mo>}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">G_{X}=\\left\\{E_{\\tilde{X}_{1}},E_{\\tilde{X}_{2}},\\ldots,E_{\\tilde{X}_{n}}\\right\\}</annotation></semantics></math> and <math alttext=\"G_{Y}=\\left\\{E_{\\tilde{Y}_{1}},E_{\\tilde{Y}_{2}},\\ldots,E_{\\tilde{Y}_{m}}\\right\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m2\" intent=\":literal\"><semantics><mrow><msub><mi>G</mi><mi>Y</mi></msub><mo>=</mo><mrow><mo>{</mo><msub><mi>E</mi><msub><mover accent=\"true\"><mi>Y</mi><mo>~</mo></mover><mn>1</mn></msub></msub><mo>,</mo><msub><mi>E</mi><msub><mover accent=\"true\"><mi>Y</mi><mo>~</mo></mover><mn>2</mn></msub></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>E</mi><msub><mover accent=\"true\"><mi>Y</mi><mo>~</mo></mover><mi>m</mi></msub></msub><mo>}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">G_{Y}=\\left\\{E_{\\tilde{Y}_{1}},E_{\\tilde{Y}_{2}},\\ldots,E_{\\tilde{Y}_{m}}\\right\\}</annotation></semantics></math>, where <math alttext=\"G_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m3\" intent=\":literal\"><semantics><msub><mi>G</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">G_{X}</annotation></semantics></math> collects all segment embeddings in the source language and <math alttext=\"G_{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m4\" intent=\":literal\"><semantics><msub><mi>G</mi><mi>Y</mi></msub><annotation encoding=\"application/x-tex\">G_{Y}</annotation></semantics></math> collects those in the target language.\nIn <span class=\"ltx_text ltx_font_italic\">Local Mining</span>, embeddings are grouped by <span class=\"ltx_text ltx_font_bold\">document pairs</span>.\nSuppose there are <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m5\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> parallel documents, with <math alttext=\"X_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m6\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">X_{i}</annotation></semantics></math> paired with <math alttext=\"Y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m7\" intent=\":literal\"><semantics><msub><mi>Y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">Y_{i}</annotation></semantics></math> for <math alttext=\"1\\leq i\\leq s\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m8\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>&#8804;</mo><mi>i</mi><mo>&#8804;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">1\\leq i\\leq s</annotation></semantics></math>.\nDocuments without a parallel one are ignored.\nIn this case, <math alttext=\"E_{\\tilde{X}_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m9\" intent=\":literal\"><semantics><msub><mi>E</mi><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mi>i</mi></msub></msub><annotation encoding=\"application/x-tex\">E_{\\tilde{X}_{i}}</annotation></semantics></math> and <math alttext=\"E_{\\tilde{Y_{j}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m10\" intent=\":literal\"><semantics><msub><mi>E</mi><mover accent=\"true\"><msub><mi>Y</mi><mi>j</mi></msub><mo>~</mo></mover></msub><annotation encoding=\"application/x-tex\">E_{\\tilde{Y_{j}}}</annotation></semantics></math> are bags of embeddings themselves.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Embedding alignment</span>.\nSpeech mining is performed by finding the most similar embedding pairs between two bags of segment embeddings.\nThe margin-based similarity, or margin-score, between any two embeddings <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m1\" intent=\":literal\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math> and <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m2\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> is computed as</p>\n\n",
                "matched_terms": [
                    "speech",
                    "similar",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More generally, given two bags of embeddings <math alttext=\"U=\\{u_{1},u_{2},\\ldots,u_{l_{u}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m10\" intent=\":literal\"><semantics><mrow><mi>U</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>u</mi><mn>1</mn></msub><mo>,</mo><msub><mi>u</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>u</mi><msub><mi>l</mi><mi>u</mi></msub></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">U=\\{u_{1},u_{2},\\ldots,u_{l_{u}}\\}</annotation></semantics></math> and <math alttext=\"V=\\{v_{1},v_{2},\\ldots,v_{l_{v}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m11\" intent=\":literal\"><semantics><mrow><mi>V</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>v</mi><mn>1</mn></msub><mo>,</mo><msub><mi>v</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>v</mi><msub><mi>l</mi><mi>v</mi></msub></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">V=\\{v_{1},v_{2},\\ldots,v_{l_{v}}\\}</annotation></semantics></math>, where <math alttext=\"l_{u}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m12\" intent=\":literal\"><semantics><msub><mi>l</mi><mi>u</mi></msub><annotation encoding=\"application/x-tex\">l_{u}</annotation></semantics></math> and <math alttext=\"l_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m13\" intent=\":literal\"><semantics><msub><mi>l</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">l_{v}</annotation></semantics></math> are number of embeddings, the collection of all speech mining alignments is</p>\n\n",
                "matched_terms": [
                    "speech",
                    "alignments",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we define Local Mining and Global Mining as</p>\n\n",
                "matched_terms": [
                    "global",
                    "mining",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Speech Vecalign pipeline consists of three steps:\nspeech preprocessing (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>), segment alignment with Vecalign (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS2\" title=\"3.2 Speech Segment Alignment &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), and alignment postprocessing (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS3\" title=\"3.3 Alignment Postprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>).\nAn illustration of our method is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "vecalign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Segmentation</span>. Same as speech mining, we first segment each speech document by VAD.\nWe apply Silero VAD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Silero Team (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib41\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We perform segment alignment based on the similarity between speech segment embeddings.\nUnlike speech mining, which solely relies on similarity scores, we use a dynamic programming&#160;(DP) algorithm to align segments in chronological order.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Segment concatenation</span>.\nSpeech segments do not necessarily correspond to complete sentences.\nSame as speech mining, we first progressively concatenate each segment with the subsequent ones.\nEach concatenated segment can contain up to 5 original segments and span a maximum of 20 seconds.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because of DP, the resultant alignments strictly follow chronological order.\nWe use <math alttext=\"x_{a:b}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m1\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mrow><mi>a</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">x_{a:b}^{i}</annotation></semantics></math> to denote the concatenation of consecutive segments <math alttext=\"x_{a}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m2\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mi>a</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">x_{a}^{i}</annotation></semantics></math> through <math alttext=\"x_{b}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m3\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mi>b</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">x_{b}^{i}</annotation></semantics></math>.\nFor any two alignments <math alttext=\"(x_{a_{s}:a_{e}}^{i},y_{b_{s}:b_{e}}^{j})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mrow><msub><mi>a</mi><mi>s</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>a</mi><mi>e</mi></msub></mrow><mi>i</mi></msubsup><mo>,</mo><msubsup><mi>y</mi><mrow><msub><mi>b</mi><mi>s</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>b</mi><mi>e</mi></msub></mrow><mi>j</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{a_{s}:a_{e}}^{i},y_{b_{s}:b_{e}}^{j})</annotation></semantics></math> and <math alttext=\"(x_{c_{s}:c_{e}}^{i},y_{d_{s}:d_{e}}^{k})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mrow><msub><mi>c</mi><mi>s</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>c</mi><mi>e</mi></msub></mrow><mi>i</mi></msubsup><mo>,</mo><msubsup><mi>y</mi><mrow><msub><mi>d</mi><mi>s</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>d</mi><mi>e</mi></msub></mrow><mi>k</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{c_{s}:c_{e}}^{i},y_{d_{s}:d_{e}}^{k})</annotation></semantics></math>, Speech Vecalign guarantees that <math alttext=\"i=j=k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m6\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>=</mo><mi>j</mi><mo>=</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">i=j=k</annotation></semantics></math> and that either <math alttext=\"a_{e}&lt;c_{s},b_{e}&lt;d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m7\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>a</mi><mi>e</mi></msub><mo>&lt;</mo><msub><mi>c</mi><mi>s</mi></msub></mrow><mo>,</mo><mrow><msub><mi>b</mi><mi>e</mi></msub><mo>&lt;</mo><msub><mi>d</mi><mi>s</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">a_{e}&lt;c_{s},b_{e}&lt;d_{s}</annotation></semantics></math> or <math alttext=\"a_{s}&gt;c_{e},b_{s}&gt;d_{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m8\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>a</mi><mi>s</mi></msub><mo>&gt;</mo><msub><mi>c</mi><mi>e</mi></msub></mrow><mo>,</mo><mrow><msub><mi>b</mi><mi>s</mi></msub><mo>&gt;</mo><msub><mi>d</mi><mi>e</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">a_{s}&gt;c_{e},b_{s}&gt;d_{e}</annotation></semantics></math>.\nIn contrast, Local Mining ensures <math alttext=\"i=j=k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m9\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>=</mo><mi>j</mi><mo>=</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">i=j=k</annotation></semantics></math> but has no constraints on <math alttext=\"a,b,c,d\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m10\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi>c</mi><mo>,</mo><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">a,b,c,d</annotation></semantics></math>, while\nGlobal Mining makes no guarantees at all.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "alignments",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Global margin-scores computation</span>.\nThe raw alignments only have alignment costs as a quality indicator, which are computed <span class=\"ltx_text ltx_font_italic\">within</span> each document pair.\nTo assess alignment quality <span class=\"ltx_text ltx_font_italic\">across</span> document pairs, we train FAISS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Johnson et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib22\" title=\"\">2019</a>)</cite> indexes and compute margin-scores&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Artetxe and Schwenk (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib2\" title=\"\">2019a</a>)</cite> using Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S2.E1\" title=\"In 2 Speech Mining Overview &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> for <span class=\"ltx_text ltx_font_italic\">all</span> obtained alignments, following the common strategy in MT dataset curation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Sloto et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib42\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "global"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Removing highly-overlapped alignments</span>.\nFinally, we remove alignments that have too much overlap with others, following <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nFor any two consecutive alignments, we compute the ratio of the overlapped source duration to the maximum duration of the two source segments.\nIf the ratio exceeds a threshold, we discard the one with a lower margin-score.\nWe train S2ST models with multiple threshold values to determine the best one.\nOur experiments in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.SS1\" title=\"D.1 Optimizing &#119898;&#8290;&#119886;&#8290;&#119909;&#8290;_&#8290;&#119900;&#8290;&#119907;&#8290;&#119890;&#8290;&#119903;&#8290;&#119897;&#8290;&#119886;&#8290;&#119901; &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">D.1</span></a> suggest that 0.4 work best for Global Mining and 0.8 work best for Local Mining and Speech Vecalign.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "alignments",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We apply Speech Vecalign, Global Mining, and Local Mining to the same raw data and train S2ST models on each type of alignments, providing a fair comparison.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "alignments",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Vecalign.</span>\nWe apply Speech Vecalign to each pair of speech documents and obtain alignments sorted by margin-scores.\nTraining data is chosen in descending order of margin-scores.\nWe train models on different data sizes and report the best results in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS5\" title=\"4.5 Main Results &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.5</span></a>.\nMore details on data size optimization can be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.SS2\" title=\"D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "alignments",
                    "vecalign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech mining baselines.</span>\nWe apply Global Mining and Local Mining to the same raw data and embeddings as Speech Vecalign.\nThe implementation is based on <span class=\"ltx_text ltx_font_typewriter\">stopes<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">3</span></span><a class=\"ltx_ref ltx_url\" href=\"https://github.com/facebookresearch/stopes\" title=\"\">https://github.com/facebookresearch/stopes</a></span></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Andrews et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib1\" title=\"\">2022</a>)</cite>.\nAfter mining, we apply the same postprocessing strategies in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS3\" title=\"3.3 Alignment Postprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>, except for alignment concatenation which is not applicable.\nTraining data is chosen in descending order of margin-scores and details on data size optimization can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.SS2\" title=\"D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train speech-to-unit translation (S2UT) models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib28\" title=\"\">2022a</a>)</cite> with <span class=\"ltx_text ltx_font_typewriter\">fairseq<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">4</span></span><a class=\"ltx_ref ltx_url\" href=\"https://github.com/facebookresearch/fairseq\" title=\"\">https://github.com/facebookresearch/fairseq</a></span></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">Ott et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib32\" title=\"\">2019</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib48\" title=\"\">2020</a>)</cite> on each type of alignments.\nThe S2UT model takes source speech as input and predicts a sequence of target discrete units.\nThe discrete units are obtained by applying\na k-means model to the <math alttext=\"11^{\\text{th}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><msup><mn>11</mn><mtext>th</mtext></msup><annotation encoding=\"application/x-tex\">11^{\\text{th}}</annotation></semantics></math> layer features of a HuBERT model <cite class=\"ltx_cite ltx_citemacro_cite\">Hsu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib16\" title=\"\">2021</a>)</cite>.\nFor English, we use the mHuBERT from <cite class=\"ltx_cite ltx_citemacro_citet\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib29\" title=\"\">2022b</a>)</cite>, and for German, we use the Germanic mHuBERT from <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nConsecutive duplicated units are removed.\nOur S2UT model architecture follows exactly <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nThe architecture details and training hyperparameters are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A2\" title=\"Appendix B Speech-to-Speech Translation &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "alignments"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the discrete units generated by S2UT models, we resynthesize speech using pretrained unit-based HiFi-GAN vocoders <cite class=\"ltx_cite ltx_citemacro_cite\">Polyak et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib33\" title=\"\">2021</a>)</cite> from <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nWe then evaluate the resynthesized speech using both transcription-based and transcription-free methods.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also adopt BLASER 2.0 <cite class=\"ltx_cite ltx_citemacro_cite\">Dale and Costa-juss&#224; (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib7\" title=\"\">2024</a>)</cite> to directly evaluate speech output.\nWe compute the referenced score using <code class=\"ltx_verbatim ltx_font_typewriter\">blaser-2.0-ref</code><span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/blaser-2.0-ref\" title=\"\">https://huggingface.co/facebook/blaser-2.0-ref</a></span></span></span> for input and output speech, as well as the text reference.\nWe compute the reference-free score using <code class=\"ltx_verbatim ltx_font_typewriter\">blaser-2.0-qe</code><span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/blaser-2.0-qe\" title=\"\">https://huggingface.co/facebook/blaser-2.0-qe</a></span></span></span> for input and output speech only.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Intriguingly, for both directions, Speech Vecalign and speech mining models are competitive with or outperform SpeechMatrix&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite> models, despite the latter being mined from about 24k hours of speech per language, <span class=\"ltx_text ltx_font_italic\">8 times more</span> than our raw data.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>We do not aim for state-of-the-art performance. Our results are not directly comparable to SpeechMatrix. We report SpeechMatrix results only to show the performance gap.</span></span></span>\nFor En-to-De, our Global Mining and Speech Vecalign models achieve improvements of 0.94 and 1.31 BLEU, respectively.\nOur Local Mining model achieves even 1.64 BLEU improvement.\nWe suspect that SpeechMatrix has not removed identical untranslated segments prior to and after mining, which significantly hurts model performance.\nFurther discussion is in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.SS5\" title=\"5.5 Removing Identical Untranslated Segments is Critical &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5.5</span></a>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While Local Mining has not been previously explored, our results suggest that it is a potentially useful method.\nLocal Mining achieves the highest BLEU score in En-to-De, and only slightly underperforms Global Mining in De-to-En, indicating that constraining the mining scope to document pairs does not necessarily have a negative impact on alignment quality.\nYet we note that Local Mining requires more training data to achieve its optimal performance, as shown in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.SS2\" title=\"D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our Speech Vecalign models outperform both speech mining models in both directions.\nFor En-to-De, the Speech Vecalign model achieves 12.58 BLEU, comparable with our strong Global Mining and Local Mining baselines.\nIn terms of chrF2++, it surpasses Global Mining and Local Mining by 1.69 and 0.26, respectively.\nIt also significantly improves their referenced BLASER 2.0 by 0.08 and 0.03.\nFor De-to-En, Speech Vecalign and Global Mining models achieve comparable BLEU (16.14 vs. 15.96), but Speech Vecalign surpasses Global Mining by 0.57 in chrF2++.\nSpeech Vecalign significantly outperforms Local Mining under all metrics.\nThese results demonstrate that Speech Vecalign produces higher-quality alignments than both speech mining baselines.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "alignments",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We analyze properties of speech mining methods and compare them with Speech Vecalign.\nAlthough we show that speech mining methods produce alignments similar to those of Speech Vecalign, the latter offers advantages of producing longer and less noisy alignments.</p>\n\n",
                "matched_terms": [
                    "produce",
                    "alignments",
                    "vecalign",
                    "similar",
                    "methods",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, we show that Global Mining mostly <span class=\"ltx_text ltx_font_bold\">locally</span> aligns speech documents.\nWhile Global Mining searches for the best matching segment pairs among roughly 10 million segments, one might expect its alignments to cover the spread of the entire dataset.\nOn the contrary, we find that Global Mining alignments are concentrated within document pairs, each typically containing hundreds to thousands of segments.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "alignments",
                    "global",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To quantify this, we examine the 1000-hour Global Mining data and count alignments whose source and target segments come from <span class=\"ltx_text ltx_font_italic\">different</span> document pairs.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F3\" title=\"Figure 3 &#8227; 5.1 Speech Mining Mostly Locally Aligns Segments in Time Order &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, fewer than 6% fall into this category, while the majority&#160;(<math alttext=\"&gt;93\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mrow><mn>93</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">&gt;93\\%</annotation></semantics></math>) are within paired documents.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "global",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, we analyze the time order of alignments produced by both speech mining methods.\nBorrowing the notation from Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS2\" title=\"3.2 Speech Segment Alignment &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, we define two pairs of alignments to be <span class=\"ltx_text ltx_font_italic\">in-order</span> if either <math alttext=\"a_{e}&lt;c_{s},b_{e}&lt;d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>a</mi><mi>e</mi></msub><mo>&lt;</mo><msub><mi>c</mi><mi>s</mi></msub></mrow><mo>,</mo><mrow><msub><mi>b</mi><mi>e</mi></msub><mo>&lt;</mo><msub><mi>d</mi><mi>s</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">a_{e}&lt;c_{s},b_{e}&lt;d_{s}</annotation></semantics></math> or <math alttext=\"a_{s}&gt;c_{e},b_{s}&gt;d_{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>a</mi><mi>s</mi></msub><mo>&gt;</mo><msub><mi>c</mi><mi>e</mi></msub></mrow><mo>,</mo><mrow><msub><mi>b</mi><mi>s</mi></msub><mo>&gt;</mo><msub><mi>d</mi><mi>e</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">a_{s}&gt;c_{e},b_{s}&gt;d_{e}</annotation></semantics></math>; otherwise, they are <span class=\"ltx_text ltx_font_italic\">out-of-order</span>.\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F3\" title=\"Figure 3 &#8227; 5.1 Speech Mining Mostly Locally Aligns Segments in Time Order &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows that only around 1% alignments are out-of-order for both speech mining methods.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "mining",
                    "speech",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Observations above indicate that speech mining alignments are mostly within paired documents and preserve time order.\nWe hypothesize that speech-to-speech alignments are sparse and high-quality ones mostly exist in paired documents.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "alignments",
                    "indicate",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a by-product, this property can be leveraged to identify parallel documents.\nIf Global Mining finds many alignments between two documents, they are likely to be parallel.\nIt is particularly useful when the pairing metadata is not readily available.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "global",
                    "when",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the observations in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.SS1\" title=\"5.1 Speech Mining Mostly Locally Aligns Segments in Time Order &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> that speech mining produces mostly local, in-order alignments, we analyze the similarity between them and Speech Vecalign alignments.</p>\n\n",
                "matched_terms": [
                    "local",
                    "alignments",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ the alignment evaluation method<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/thompsonb/vecalign/blob/master/score.py\" title=\"\">https://github.com/thompsonb/vecalign/blob/master/score.py</a>.</span></span></span> from&#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Thompson and Koehn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib43\" title=\"\">2019</a>)</cite>, which computes precision and recall by comparing system alignments to a reference.\nThere are two modes: <span class=\"ltx_text ltx_font_italic\">Strict</span>, which counts only exact matches as true positives, and <span class=\"ltx_text ltx_font_italic\">Lax</span>, which considers an alignment as true positive if both its source and target segment overlap with the reference.\nIf not true positive, an alignment is false positive.\nRecall is computed by swapping the reference and the system alignments.</p>\n\n",
                "matched_terms": [
                    "lax",
                    "alignments",
                    "recall",
                    "precision",
                    "reference",
                    "strict"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As speech mining and Speech Vecalign produce similar alignments, we explore why Speech Vecalign models still perform better.\nA key advantage of Speech Vecalign is that it first produces fine-grained alignments and then constructs alignments with different amounts of context, thanks to the alignment concatenation strategy.\nSpeech mining methods, on the other hand, solely depend on margin-scores and tend to favor shorter alignments.</p>\n\n",
                "matched_terms": [
                    "produce",
                    "alignments",
                    "vecalign",
                    "similar",
                    "methods",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the best En-to-De models and corresponding data sizes from Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS5\" title=\"4.5 Main Results &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.5</span></a>, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F4\" title=\"Figure 4 &#8227; 5.3 Speech Vecalign Produces Longer Alignments &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the average sentence-level chrF2++ scores on the test set and the percentage of training alignments for different source speech duration ranges.\nNotably, Speech Vecalign has a large portion of long training samples: the blue bars are highest for durations longer than 12 seconds.\nSpecifically, the average source duration of Speech Vecalign is 8.51 seconds, while Global Mining and Local Mining have average durations of 7.50 and 8.53 seconds, respectively.\nAs a result, the Speech Vecalign model performs better on test samples longer than 10 seconds, while having comparable performance on shorter ones.\nThis highlights that Speech Vecalign is able to produce longer, context-rich alignments which help to improve S2ST model performance.\nInterestingly, Local Mining surpasses the Global Mining model on long inputs, which could be also attributed to its longer training samples.</p>\n\n",
                "matched_terms": [
                    "global",
                    "produce",
                    "local",
                    "alignments",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We visualize alignments produced by different methods for the same document pair, which is about 10 minutes long and contains around 200 segments.\nFor reference, we manually created a gold segment-level alignment, with detailed procedure in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A6\" title=\"Appendix F Procedure of Manual Alignments &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>.\nWe illustrate the best 80 alignments for each of the speech mining methods.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "methods",
                    "speech",
                    "reference",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F5\" title=\"Figure 5 &#8227; 5.3 Speech Vecalign Produces Longer Alignments &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows, Speech Vecalign produces the most fine-grained alignments and is most similar to the gold reference.\nGlobal Mining also performs well, aligning closely with the groundtruth path, whereas Local Mining produces more noise and misses more alignments along the correct path.\nWe hypothesize Local Mining has limited number of segments in a single document pair, making nearest neighbors less effective normalizers in the margin-score computation.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "alignments",
                    "vecalign",
                    "similar",
                    "speech",
                    "reference",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As presented in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS5\" title=\"4.5 Main Results &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.5</span></a>, our reproduced speech mining models achieve comparable or even better results than SpeechMatrix models.\nBy listening to samples of SpeechMatrix alignments, we observed many cases where the source and target segments contained identical untranslated content, which is an issue mentioned in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>.\nUsing the method described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, we identified approximately 100k out of 630k alignments with untranslated source and target segments, totaling 181 hours.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "alignments",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present Speech Vecalign, a parallel speech document alignment method that aligns speech segment embeddings within document pairs and in chronological order.\nWe apply Speech Vecalign to parallel English-German VoxPopuli speech documents and conduct S2ST experiments to demonstrate its superiority over two strong speech mining baselines.\nOur analysis reveals that although speech mining methods primarily align documents locally and in-order, Global Mining falls short of producing long alignments, and Local Mining in particular produces more noise.\nFor long-term future work, we plan to extend Speech Vecalign to other language pairs or other data sources.\nWe can also explore aligning speech and text embeddings to construct S2TT datasets.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "alignments",
                    "vecalign",
                    "methods",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limited language pair.</span>\nWe have only conducted experiments for English and German speech from the VoxPopuli dataset.\nAs Speech Vecalign heavily relies on the quality of speech embeddings, the performance is unclear for other language pairs and other domains of speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "vecalign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dependency on parallel speech documents.</span>\nSpeech Vecalign requires parallel speech documents, which is often not available.\nWe may rely on Global Mining to discover parallel documents, as Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.SS1\" title=\"5.1 Speech Mining Mostly Locally Aligns Segments in Time Order &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> suggests, but doing so will introduce extra computation costs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "global",
                    "vecalign",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Bilingual text sentence alignment.</span>\nText alignment is very related to speech alignment.\nMethods apply dynamic programming <cite class=\"ltx_cite ltx_citemacro_cite\">Bellman (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib4\" title=\"\">1954</a>)</cite> and mainly differ in the design of scoring functions.\nEarly works <cite class=\"ltx_cite ltx_citemacro_cite\">Brown et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib5\" title=\"\">1991</a>); Gale and Church (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib12\" title=\"\">1993</a>)</cite> are based on sentence lengths.\nLater methods incorporate translations in various ways&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Moore (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib30\" title=\"\">2002</a>); Varga et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib44\" title=\"\">2007</a>); Sennrich and Volk (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib40\" title=\"\">2010</a>); Gomes and Lopes (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib13\" title=\"\">2016</a>)</cite>.\nOur work is inspired by Vecalign <cite class=\"ltx_cite ltx_citemacro_cite\">Thompson and Koehn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib43\" title=\"\">2019</a>)</cite>, which utilizes margin-based cosine similarities between multilingual sentence embeddings like LASER&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Artetxe and Schwenk (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib3\" title=\"\">2019b</a>); Heffernan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib14\" title=\"\">2022</a>)</cite> and LaBSE <cite class=\"ltx_cite ltx_citemacro_cite\">Feng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib11\" title=\"\">2022</a>)</cite>.\nVecalign is also more efficient than previous methods.\nBy applying fast dynamic time warping <cite class=\"ltx_cite ltx_citemacro_cite\">Salvador and Chan (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib37\" title=\"\">2007</a>)</cite>, it has a linear time and space complexity with respect to the number of input sentences.\nThe recent progress of extending multilingual sentence embeddings to the speech modality <cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib9\" title=\"\">2021</a>); Khurana et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib24\" title=\"\">2022</a>); Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib10\" title=\"\">b</a>)</cite> enables us to align speech segments by their speech embeddings using the same algorithm.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "vecalign",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">S2ST datasets.</span>\nThere are two common ways to automatically build an S2ST dataset: (1)&#160;building alignments from multilingual speech data; (2)&#160;synthesizing speech for text translations from existing speech-to-text translation (S2TT) corpora.\nThe first line of work has human spoken speech on both source and target sides.\nVoxPopuli <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib47\" title=\"\">2021a</a>)</cite> aligns multilingual speech documents based on text transcriptions, yielding 17.3k-hour alignments between 15 source and target languages.\nSpeechMatrix <cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite> applies Global Mining with SpeechLASER embeddings on VoxPopuli. It obtains alignments for 136 language pairs with an average of 1537 hours per direction.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Seamless Communication et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib39\" title=\"\">2023</a>)</cite> apply Global Mining to web-crawled speech data with SONAR embeddings.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Seamless Communication et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib39\" title=\"\">2023</a>)</cite> also mine a SeamlessAlignExpressive dataset with expressively- and semantically-aligned segment pairs, based on a blend of both semantic and prosodic similarity score <cite class=\"ltx_cite ltx_citemacro_cite\">Heffernan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib15\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "alignments",
                    "global",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second line of work has synthesized speech on the target side.\nFisher <cite class=\"ltx_cite ltx_citemacro_cite\">Post et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib35\" title=\"\">2013</a>)</cite> is a Spanish-English S2TT dataset containing about 170 hours of Spanish telephone conversations and English translations which are used to synthesize English speech.\nCVSS <cite class=\"ltx_cite ltx_citemacro_cite\">Jia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib20\" title=\"\">2022b</a>)</cite> is an S2ST dataset covering utterances from 21 languages to English, obtained by synthesizing the text translations in CoVoST 2 <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib49\" title=\"\">2021b</a>)</cite>.\nBesides automatic methods, FLEURS <cite class=\"ltx_cite ltx_citemacro_cite\">Conneau et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib6\" title=\"\">2023</a>)</cite> has collected human read speech covering 102 languages. But it contains only about 12 hours per language and is intended for evaluation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "used",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Alignment.</span>\nLocal Mining and Global Mining run on a single GPU.\nThey take about 2 hours.\nSpeech Vecalign runs on a single CPU and takes about 2 hours.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m1\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> controls the trade-off between overlapped durations and data quality.\nFor instance, a lower <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m2\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> reduces the overlap but also discards alignments more aggressively.\nOverlapped alignments usually have similar margin-scores, so more high-quality alignments are lost.\nThe data size controls the trade-off between data size and data quality cutoff.\nFor instance, a larger dataset will have a lower quality cutoff, as alignments are selected in descending order of margin-scores.\nIn this section, we optimize the combination of <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m3\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> and data size by training S2UT models on different datasets.\nNote that the raw data stays the same.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "similar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first experiment with different values of <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math>.\nWe apply different <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> thresholds during the postprocessing stage, and always choose the best 500 hours as the training data.\nThe optimal value is determined based on development set ASR-BLEU.\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.F6\" title=\"Figure 6 &#8227; D.1 Optimizing &#119898;&#8290;&#119886;&#8290;&#119909;&#8290;_&#8290;&#119900;&#8290;&#119907;&#8290;&#119890;&#8290;&#119903;&#8290;&#119897;&#8290;&#119886;&#8290;&#119901; &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows that 0.8 works best for Speech Vecalign and Local Mining and 0.4 works best for Global Mining.\nThe test set performance is also drawn in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.F6\" title=\"Figure 6 &#8227; D.1 Optimizing &#119898;&#8290;&#119886;&#8290;&#119909;&#8290;_&#8290;&#119900;&#8290;&#119907;&#8290;&#119890;&#8290;&#119903;&#8290;&#119897;&#8290;&#119886;&#8290;&#119901; &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, exhibiting a similar trend.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "vecalign",
                    "similar",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next we optimize the training data size.\nWe fix <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> at 0.4 for Global Mining and 0.8 for Speech Vecalign and Local Mining during postprocessing, only lowering the quality cutoff to include more training data.\nThe models are trained on different amounts of data until we find the peak performance.\nResults are shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.F7\" title=\"Figure 7 &#8227; D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For En-to-De, the best Speech Vecalign model is trained on the 750-hour dataset, achieving 12.58 BLEU.\nIt outperforms the best Global Mining model which achieves 12.21 BLEU.\nThe best Local Mining model achieves 12.91 BLEU.\nHowever, we note that it requires a lot more data than the other two methods to achieve the peak performance.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "vecalign",
                    "methods",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For De-to-En, the 1000-hour dataset works best for Speech Vecalign while the 750-hour dataset works best for Global Mining.\nLocal Mining achieves the peak performance when the data size is 1250 hours, still requiring more data than the other methods.\nThe Speech Vecalign performs better than both the Global Mining and the Local Mining models.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "vecalign",
                    "when",
                    "methods",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide evaluation results on the FLEURS test set in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.T5\" title=\"Table 5 &#8227; D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.\nSimilar to Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS5\" title=\"4.5 Main Results &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.5</span></a>, our results match or outperform SpeechMatrix results.\nFor both En-to-De and De-to-En, Speech Vecalign and Global Mining achieve comparable performance when using the transcription-based metrics ASR-BLEU and ASR-chrF2++.\nTheir performance is especially close on De-to-En.\nHowever, Speech Vecalign is significantly better than Global Mining when using the BLASER 2.0 metrics, achieving an improvement of 0.06 and 0.04 referenced BLASER 2.0 scores on En-to-De and De-to-En, respectively.</p>\n\n",
                "matched_terms": [
                    "global",
                    "vecalign",
                    "when",
                    "similar",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For En-to-De, Speech Vecalign achieves comparable performance with Local Mining on all metrics.\nFor De-to-En, Speech Vecalign is significantly better than Local Mining when using BLASER 2.0 metrics.</p>\n\n",
                "matched_terms": [
                    "local",
                    "vecalign",
                    "when",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, we can show that Speech Vecalign performs better than both Local Mining and Global Mining.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the same alignment evaluation method as Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.SS2\" title=\"5.2 Speech Mining Methods Produce Similar Alignments as Speech Vecalign &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>, but we use the manual alignments as the reference.\nThere are 144 raw Speech Vecalign alignments, and we choose the same number of alignments from Global Mining and Local Mining in descending order of margin-scores.\nThe Recall and Precision of raw Speech Vecalign, Local Mining, and Global Mining alignments are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A7.T6\" title=\"Table 6 &#8227; Appendix G Evaluation of System Alignments using the Manual Alignments &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "alignments",
                    "vecalign",
                    "recall",
                    "precision",
                    "speech",
                    "reference",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The three methods have similar Lax Precisions, while that of Local Mining and Global Mining are slightly higher.\nSpeech Vecalign has the highest recall values than both the speech mining baselines.\nAmong the three methods, Local Mining has the worst performance in general.\nThis follows Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F5\" title=\"Figure 5 &#8227; 5.3 Speech Vecalign Produces Longer Alignments &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> that both Speech Vecalign and Global Mining have good performance but Local Mining does not perform well.</p>\n\n",
                "matched_terms": [
                    "global",
                    "lax",
                    "local",
                    "vecalign",
                    "recall",
                    "similar",
                    "methods",
                    "speech",
                    "mining"
                ]
            }
        ]
    },
    "S5.T3": {
        "caption": "Table 3: \nPerformance of models trained on SpeechMatrix, before and after removing identical untranslated alignments.\nResults are measured on En-to-De and De-to-En EPST test sets.\nThe removal of untranslated segments boosts model performance.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Hours</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">ASR-BLEU</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#BFBFBF;\">English-to-German</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">SpeechMatrix</td>\n<td class=\"ltx_td ltx_align_center\">1451</td>\n<td class=\"ltx_td ltx_align_center\">11.27</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">SpeechMatrix cleaned</td>\n<td class=\"ltx_td ltx_align_center\">1265</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">12.27</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#BFBFBF;\">German-to-English</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">SpeechMatrix</td>\n<td class=\"ltx_td ltx_align_center\">1456</td>\n<td class=\"ltx_td ltx_align_center\">16.62</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">SpeechMatrix cleaned</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1276</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">16.73</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "removal",
            "entode",
            "epst",
            "hours",
            "before",
            "after",
            "test",
            "trained",
            "cleaned",
            "englishtogerman",
            "performance",
            "removing",
            "speechmatrix",
            "alignments",
            "sets",
            "germantoenglish",
            "results",
            "measured",
            "untranslated",
            "asrbleu",
            "model",
            "segments",
            "identical",
            "boosts",
            "dataset",
            "detoen"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To evaluate the impact of untranslated segments, we trained models on the original SpeechMatrix En-De alignments and on a version with untranslated alignments <span class=\"ltx_text ltx_font_italic\">removed</span>.\nThe training data is chosen with a margin-score threshold of 1.09, following the original setup.\nAs shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.T3\" title=\"Table 3 &#8227; 5.5 Removing Identical Untranslated Segments is Critical &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the cleaned data produces better models, improving BLEU score by <span class=\"ltx_text ltx_font_bold\">1.00</span> for En-to-De and <span class=\"ltx_text ltx_font_bold\">0.11</span> for De-to-En, despite having 13% less training data.\nThe smaller gain on De-to-En may be due to most untranslated segments being in English, which have smaller impact on into-English translation.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Speech Vecalign, a parallel speech document alignment method that monotonically aligns speech segment embeddings and does not depend on text transcriptions.\nCompared to the baseline method Global Mining&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>, a variant of speech mining, Speech Vecalign produces longer speech-to-speech alignments.\nIt also demonstrates greater robustness than Local Mining, another speech mining variant, as it produces less noise.\nWe applied Speech Vecalign to 3,000 hours of unlabeled parallel English-German&#160;(En-De) speech documents from VoxPopuli, yielding about 1,000 hours of high-quality alignments.\nWe then trained En-De speech-to-speech translation models on the aligned data.\nSpeech Vecalign improves the En-to-De and De-to-En performance over Global Mining by 0.37 and 0.18 ASR-BLEU, respectively.\nMoreover, our models match or outperform SpeechMatrix model performance, despite using 8 times fewer raw speech documents.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Data and code are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/mct10/Speech-Vecalign\" title=\"\">https://github.com/mct10/Speech-Vecalign</a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "models",
                    "hours",
                    "speechmatrix",
                    "detoen",
                    "alignments",
                    "model",
                    "entode",
                    "asrbleu",
                    "trained",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech-to-speech translation (S2ST) is the task of translating speech in one language into speech in another language.\nConventional S2ST systems concatenate automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) models <cite class=\"ltx_cite ltx_citemacro_cite\">Lavie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib27\" title=\"\">1997</a>); Nakamura et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib31\" title=\"\">2006</a>); Wahlster (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib46\" title=\"\">2013</a>)</cite>.\nThese components can be trained individually with datasets for the different components.\nDirect S2ST models, which translate source speech into target spectrograms or discrete units with a single architecture, have been recently proposed to alleviate error propagation and to reduce inference latency <cite class=\"ltx_cite ltx_citemacro_cite\">Jia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib21\" title=\"\">2019</a>); Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib28\" title=\"\">2022a</a>)</cite>.\nDespite the advantages, performance of direct models is limited by the amount of speech-to-speech aligned data, which is much more scarce than the data used for components of cascaded systems.</p>\n\n",
                "matched_terms": [
                    "models",
                    "trained",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech-to-speech alignment methods align short speech clips called <span class=\"ltx_text ltx_font_italic\">segment</span>s, and can be either transcription-based or transcription-free.\nWhen transcriptions are available, segments in parallel speech documents can be aligned through speech-to-text and text-to-text alignments.\nInspired by text mining&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Schwenk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib38\" title=\"\">2021</a>)</cite>, speech mining&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib9\" title=\"\">2021</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite> was proposed as a transcription-free method that aligns speech segments by finding segment pairs with the highest embedding similarity.\nIt scales well as it does not rely on the availability of text transcriptions.\nWhen speech mining is applied to a large amount of speech documents, as in all previous work, it is referred to as <span class=\"ltx_text ltx_font_bold\">Global Mining</span>.\nAnother variant, <span class=\"ltx_text ltx_font_bold\">Local Mining</span>, which applies speech mining to a single pair of parallel speech documents, has not been well explored.\nAs we formally define in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S2\" title=\"2 Speech Mining Overview &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, both Global Mining and Local Mining treat documents as bags of unordered segments.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "segments"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since speech mining methods do not leverage the document pair structure, we wonder, <span class=\"ltx_text ltx_font_bold\">can we obtain better alignments by aligning speech segments within document pairs and preserving their time order?</span>\nThis allows us to utilize the extra knowledge that (1) segments within parallel document pairs are likely to be translations of each other, and (2) segment pairs right next to already aligned pairs are also likely to be aligned.\nWe draw inspiration from parallel <span class=\"ltx_text ltx_font_italic\">text</span> document alignment methods, which have been popular to create sentence-aligned bitext for training MT systems.\nUnlike mining, they align sentences for each document pair while maintaining the sentence order.\nOur work is based on the text alignment method Vecalign&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Thompson and Koehn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib43\" title=\"\">2019</a>)</cite>, which aligns parallel sentences by applying fast dynamic time warping&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Salvador and Chan (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib37\" title=\"\">2007</a>)</cite> to sentence embeddings.\nWith the advances of extending sentence embeddings to the speech modality&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib9\" title=\"\">2021</a>)</cite>, we can readily apply Vecalign to parallel speech documents.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "segments"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce Speech Vecalign, a method that aligns parallel speech documents using speech segment embeddings.\nInstead of mining from bags of segments, our method aligns individual document pairs and maintains the chronological order of segments, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nAdditional preprocessing and postprocessing strategies are applied to improve alignment quality.\nWe compare Speech Vecalign with Local Mining and Global Mining and show that Speech Vecalign produces higher-quality alignments.\nWe further provide extensive analysis for all three methods, which could be useful for future research.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "segments"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech segment embedding</span>.\nEach segment is encoded into a fixed-size embedding using an embedding model.\nThe segment embeddings for <math alttext=\"\\tilde{X}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p5.m1\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\tilde{X}_{i}</annotation></semantics></math> are represented as <math alttext=\"E_{\\tilde{X}_{i}}=\\left[e^{\\tilde{X}_{i}}_{1},e^{\\tilde{X}_{i}}_{2},\\ldots,e^{\\tilde{X}_{i}}_{\\tilde{n}_{i}}\\right]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p5.m2\" intent=\":literal\"><semantics><mrow><msub><mi>E</mi><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mi>i</mi></msub></msub><mo>=</mo><mrow><mo>[</mo><msubsup><mi>e</mi><mn>1</mn><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mi>i</mi></msub></msubsup><mo>,</mo><msubsup><mi>e</mi><mn>2</mn><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mi>i</mi></msub></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>e</mi><msub><mover accent=\"true\"><mi>n</mi><mo>~</mo></mover><mi>i</mi></msub><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mi>i</mi></msub></msubsup><mo>]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">E_{\\tilde{X}_{i}}=\\left[e^{\\tilde{X}_{i}}_{1},e^{\\tilde{X}_{i}}_{2},\\ldots,e^{\\tilde{X}_{i}}_{\\tilde{n}_{i}}\\right]</annotation></semantics></math>.\nSimilarly, the segments in <math alttext=\"\\tilde{Y_{j}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p5.m3\" intent=\":literal\"><semantics><mover accent=\"true\"><msub><mi>Y</mi><mi>j</mi></msub><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\tilde{Y_{j}}</annotation></semantics></math> are encoded as <math alttext=\"E_{\\tilde{Y_{j}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p5.m4\" intent=\":literal\"><semantics><msub><mi>E</mi><mover accent=\"true\"><msub><mi>Y</mi><mi>j</mi></msub><mo>~</mo></mover></msub><annotation encoding=\"application/x-tex\">E_{\\tilde{Y_{j}}}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "segments"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech preprocessing consists of document segmentation and detection of identical untranslated segments.</p>\n\n",
                "matched_terms": [
                    "untranslated",
                    "segments",
                    "identical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Detection of identical untranslated segments</span>.\nAs mentioned by <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib47\" title=\"\">2021a</a>)</cite>, some source and target segments contain identical untranslated content due to recording issues.\nWe introduce this additional step to detect such pairs of segments <span class=\"ltx_text ltx_font_italic\">prior to</span> applying the alignment algorithms, in order to make sure they are not aligned.</p>\n\n",
                "matched_terms": [
                    "untranslated",
                    "segments",
                    "identical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To find potentially identical untranslated segment pairs, we use a <span class=\"ltx_text ltx_font_italic\">location heuristic</span> that they tend to locate in roughly the same position within the source and target documents.\nFor instance, within each pair of parallel documents, for a source segment <math alttext=\"x_{a}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m1\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mi>a</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">x_{a}^{i}</annotation></semantics></math> spanning timestamp <math alttext=\"s_{x_{a}}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m2\" intent=\":literal\"><semantics><msubsup><mi>s</mi><msub><mi>x</mi><mi>a</mi></msub><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">s_{x_{a}}^{i}</annotation></semantics></math> to <math alttext=\"e_{x_{a}}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m3\" intent=\":literal\"><semantics><msubsup><mi>e</mi><msub><mi>x</mi><mi>a</mi></msub><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">e_{x_{a}}^{i}</annotation></semantics></math>, we search for a target segment <math alttext=\"y_{b}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m4\" intent=\":literal\"><semantics><msubsup><mi>y</mi><mi>b</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">y_{b}^{i}</annotation></semantics></math> whose midpoint <math alttext=\"\\frac{s_{y_{b}}^{i}+e_{y_{b}}^{i}}{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m5\" intent=\":literal\"><semantics><mfrac><mrow><msubsup><mi>s</mi><msub><mi>y</mi><mi>b</mi></msub><mi>i</mi></msubsup><mo>+</mo><msubsup><mi>e</mi><msub><mi>y</mi><mi>b</mi></msub><mi>i</mi></msubsup></mrow><mn>2</mn></mfrac><annotation encoding=\"application/x-tex\">\\frac{s_{y_{b}}^{i}+e_{y_{b}}^{i}}{2}</annotation></semantics></math> is closest to <math alttext=\"\\frac{s_{x_{a}}^{i}+e_{x_{a}}^{i}}{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m6\" intent=\":literal\"><semantics><mfrac><mrow><msubsup><mi>s</mi><msub><mi>x</mi><mi>a</mi></msub><mi>i</mi></msubsup><mo>+</mo><msubsup><mi>e</mi><msub><mi>x</mi><mi>a</mi></msub><mi>i</mi></msubsup></mrow><mn>2</mn></mfrac><annotation encoding=\"application/x-tex\">\\frac{s_{x_{a}}^{i}+e_{x_{a}}^{i}}{2}</annotation></semantics></math>, midpoint of <math alttext=\"x_{a}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m7\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mi>a</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">x_{a}^{i}</annotation></semantics></math>, since the untranslated target segment is very likely to have a similar time span (<math alttext=\"s_{y_{b}}^{i}\\approx s_{x_{a}}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m8\" intent=\":literal\"><semantics><mrow><msubsup><mi>s</mi><msub><mi>y</mi><mi>b</mi></msub><mi>i</mi></msubsup><mo>&#8776;</mo><msubsup><mi>s</mi><msub><mi>x</mi><mi>a</mi></msub><mi>i</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">s_{y_{b}}^{i}\\approx s_{x_{a}}^{i}</annotation></semantics></math>, <math alttext=\"e_{y_{b}}^{i}\\approx e_{x_{a}}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m9\" intent=\":literal\"><semantics><mrow><msubsup><mi>e</mi><msub><mi>y</mi><mi>b</mi></msub><mi>i</mi></msubsup><mo>&#8776;</mo><msubsup><mi>e</mi><msub><mi>x</mi><mi>a</mi></msub><mi>i</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">e_{y_{b}}^{i}\\approx e_{x_{a}}^{i}</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "untranslated",
                    "identical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the two segments have both similar durations and filterbank features, we classify them as identical.\nFor durations, we compute the time difference.\nFor filterbank feature, we compute Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.E6\" title=\"In 3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>:</p>\n\n",
                "matched_terms": [
                    "segments",
                    "identical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Obtaining segment embeddings</span>.\nAfter concatenations, we obtain speech segment embeddings using SpeechLASER models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib9\" title=\"\">2021</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nIdentical untranslated segments detected in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, along with all concatenated segments that include them, are skipped and replaced with <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><mn>0</mn></math>-valued vectors.</p>\n\n",
                "matched_terms": [
                    "models",
                    "segments",
                    "after",
                    "identical",
                    "untranslated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because of DP, the resultant alignments strictly follow chronological order.\nWe use <math alttext=\"x_{a:b}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m1\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mrow><mi>a</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">x_{a:b}^{i}</annotation></semantics></math> to denote the concatenation of consecutive segments <math alttext=\"x_{a}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m2\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mi>a</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">x_{a}^{i}</annotation></semantics></math> through <math alttext=\"x_{b}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m3\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mi>b</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">x_{b}^{i}</annotation></semantics></math>.\nFor any two alignments <math alttext=\"(x_{a_{s}:a_{e}}^{i},y_{b_{s}:b_{e}}^{j})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mrow><msub><mi>a</mi><mi>s</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>a</mi><mi>e</mi></msub></mrow><mi>i</mi></msubsup><mo>,</mo><msubsup><mi>y</mi><mrow><msub><mi>b</mi><mi>s</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>b</mi><mi>e</mi></msub></mrow><mi>j</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{a_{s}:a_{e}}^{i},y_{b_{s}:b_{e}}^{j})</annotation></semantics></math> and <math alttext=\"(x_{c_{s}:c_{e}}^{i},y_{d_{s}:d_{e}}^{k})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mrow><msub><mi>c</mi><mi>s</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>c</mi><mi>e</mi></msub></mrow><mi>i</mi></msubsup><mo>,</mo><msubsup><mi>y</mi><mrow><msub><mi>d</mi><mi>s</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>d</mi><mi>e</mi></msub></mrow><mi>k</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{c_{s}:c_{e}}^{i},y_{d_{s}:d_{e}}^{k})</annotation></semantics></math>, Speech Vecalign guarantees that <math alttext=\"i=j=k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m6\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>=</mo><mi>j</mi><mo>=</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">i=j=k</annotation></semantics></math> and that either <math alttext=\"a_{e}&lt;c_{s},b_{e}&lt;d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m7\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>a</mi><mi>e</mi></msub><mo>&lt;</mo><msub><mi>c</mi><mi>s</mi></msub></mrow><mo>,</mo><mrow><msub><mi>b</mi><mi>e</mi></msub><mo>&lt;</mo><msub><mi>d</mi><mi>s</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">a_{e}&lt;c_{s},b_{e}&lt;d_{s}</annotation></semantics></math> or <math alttext=\"a_{s}&gt;c_{e},b_{s}&gt;d_{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m8\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>a</mi><mi>s</mi></msub><mo>&gt;</mo><msub><mi>c</mi><mi>e</mi></msub></mrow><mo>,</mo><mrow><msub><mi>b</mi><mi>s</mi></msub><mo>&gt;</mo><msub><mi>d</mi><mi>e</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">a_{s}&gt;c_{e},b_{s}&gt;d_{e}</annotation></semantics></math>.\nIn contrast, Local Mining ensures <math alttext=\"i=j=k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m9\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>=</mo><mi>j</mi><mo>=</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">i=j=k</annotation></semantics></math> but has no constraints on <math alttext=\"a,b,c,d\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m10\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi>c</mi><mo>,</mo><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">a,b,c,d</annotation></semantics></math>, while\nGlobal Mining makes no guarantees at all.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "segments"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The goal of postprocessing is to clean the raw alignments and construct alignments with longer durations to improve S2ST models.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Removing low-quality alignments</span>.\nFirst, we remove unaligned segments and high-cost alignments.\nThe unaligned segments are due to deletions in the DP algorithm.\nIdentical untranslated segments detected in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a> may fall into either category due to their 0-valued vectors.</p>\n\n",
                "matched_terms": [
                    "removing",
                    "alignments",
                    "segments",
                    "identical",
                    "untranslated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Detection of identical untranslated segments, again</span>.\nOccasionally, the location heuristic in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a> may fail, resulting in a small number of low-cost alignments with identical untranslated source and target segments.\nSearching is not needed at this step, as we already have the alignments.\nWe apply Equation <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.E6\" title=\"In 3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> to remaining alignments, where <math alttext=\"\\mathbf{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><mi>&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{A}</annotation></semantics></math> and <math alttext=\"\\mathbf{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m2\" intent=\":literal\"><semantics><mi>&#119809;</mi><annotation encoding=\"application/x-tex\">\\mathbf{B}</annotation></semantics></math> are source and target segments in each alignment.\nWe use the same thresholds in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a> to remove alignments.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "untranslated",
                    "segments",
                    "identical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Global margin-scores computation</span>.\nThe raw alignments only have alignment costs as a quality indicator, which are computed <span class=\"ltx_text ltx_font_italic\">within</span> each document pair.\nTo assess alignment quality <span class=\"ltx_text ltx_font_italic\">across</span> document pairs, we train FAISS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Johnson et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib22\" title=\"\">2019</a>)</cite> indexes and compute margin-scores&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Artetxe and Schwenk (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib2\" title=\"\">2019a</a>)</cite> using Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S2.E1\" title=\"In 2 Speech Mining Overview &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> for <span class=\"ltx_text ltx_font_italic\">all</span> obtained alignments, following the common strategy in MT dataset curation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Sloto et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib42\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Removing highly-overlapped alignments</span>.\nFinally, we remove alignments that have too much overlap with others, following <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nFor any two consecutive alignments, we compute the ratio of the overlapped source duration to the maximum duration of the two source segments.\nIf the ratio exceeds a threshold, we discard the one with a lower margin-score.\nWe train S2ST models with multiple threshold values to determine the best one.\nOur experiments in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.SS1\" title=\"D.1 Optimizing &#119898;&#8290;&#119886;&#8290;&#119909;&#8290;_&#8290;&#119900;&#8290;&#119907;&#8290;&#119890;&#8290;&#119903;&#8290;&#119897;&#8290;&#119886;&#8290;&#119901; &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">D.1</span></a> suggest that 0.4 work best for Global Mining and 0.8 work best for Local Mining and Speech Vecalign.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "models",
                    "removing",
                    "segments"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We apply Speech Vecalign, Global Mining, and Local Mining to the same raw data and train S2ST models on each type of alignments, providing a fair comparison.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data source.</span>\nWe use the unlabeled, unsegmented English and German plenary session recordings from VoxPopuli v1&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib47\" title=\"\">2021a</a>)</cite> as raw data.\nVoxPopuli contains European Parliament plenary session recordings in each of the 23 European Union languages, paired with spoken interpretations into the other languages.\nThe document names are formatted as <code class=\"ltx_verbatim ltx_font_typewriter\">${session_id}_${language}.ogg</code>, and paired documents have the same <code class=\"ltx_verbatim ltx_font_typewriter\">${session_id}</code>.\nTo avoid overlapping with the test set (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS2\" title=\"4.2 Evaluation Data &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>), we only choose sessions from year 2013 to 2020.\nWe also exclude sessions in the development set&#160;(Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS2\" title=\"4.2 Evaluation Data &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>).\nFor En-to-De, the remaining data has 4,880 documents totaling about 3,000 hours for each language.\nFor De-to-En, there are 5,782 documents totaling 3,400 hours per language.\nThe difference is due to the different dev and test sets.\nAll documents are in pairs, allowing all methods to have exactly the same raw data.</p>\n\n",
                "matched_terms": [
                    "hours",
                    "sets",
                    "entode",
                    "test",
                    "detoen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Vecalign.</span>\nWe apply Speech Vecalign to each pair of speech documents and obtain alignments sorted by margin-scores.\nTraining data is chosen in descending order of margin-scores.\nWe train models on different data sizes and report the best results in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS5\" title=\"4.5 Main Results &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.5</span></a>.\nMore details on data size optimization can be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.SS2\" title=\"D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "models",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Development set.</span>\nFollowing <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>, we choose 1000 samples from the highest scored sessions from the Voxpopuli S2ST dataset.\nAdditionally, we avoid choosing sessions that occur on the same dates as the test set.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Test set.</span>\nWe use the Europarl-ST (EPST) test set&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Iranzo-S&#225;nchez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib18\" title=\"\">2020</a>)</cite> as an in-domain test set to evaluate the S2ST models.\nEPST is a multilingual S2TT dataset built on European Parliament debates from year 2008 to 2012.\nWe also adopt FLEURS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Conneau et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib6\" title=\"\">2023</a>)</cite> as an out-of-domain test set.</p>\n\n",
                "matched_terms": [
                    "models",
                    "epst",
                    "dataset",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train speech-to-unit translation (S2UT) models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib28\" title=\"\">2022a</a>)</cite> with <span class=\"ltx_text ltx_font_typewriter\">fairseq<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">4</span></span><a class=\"ltx_ref ltx_url\" href=\"https://github.com/facebookresearch/fairseq\" title=\"\">https://github.com/facebookresearch/fairseq</a></span></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">Ott et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib32\" title=\"\">2019</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib48\" title=\"\">2020</a>)</cite> on each type of alignments.\nThe S2UT model takes source speech as input and predicts a sequence of target discrete units.\nThe discrete units are obtained by applying\na k-means model to the <math alttext=\"11^{\\text{th}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><msup><mn>11</mn><mtext>th</mtext></msup><annotation encoding=\"application/x-tex\">11^{\\text{th}}</annotation></semantics></math> layer features of a HuBERT model <cite class=\"ltx_cite ltx_citemacro_cite\">Hsu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib16\" title=\"\">2021</a>)</cite>.\nFor English, we use the mHuBERT from <cite class=\"ltx_cite ltx_citemacro_citet\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib29\" title=\"\">2022b</a>)</cite>, and for German, we use the Germanic mHuBERT from <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nConsecutive duplicated units are removed.\nOur S2UT model architecture follows exactly <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nThe architecture details and training hyperparameters are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A2\" title=\"Appendix B Speech-to-Speech Translation &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the transcription-based method, we transcribe the speech output using the same ASR models as <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nWe evaluate the transcriptions using SacreBLEU<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/mjpost/sacrebleu\" title=\"\">https://github.com/mjpost/sacrebleu</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">Post (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib34\" title=\"\">2018</a>)</cite> to compute BLEU<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Signature: nrefs:1 + case:mixed + eff:no\n+ tok:13a + smooth:exp + version:2.2.0</span></span></span> and chrF2++<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>Signature: nrefs:1 + case:mixed + eff:yes + nc:6 + nw:2 + space:no + version:2.2.0</span></span></span> scores.\nWe apply the significance test using paired bootstrap resampling&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Koehn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib26\" title=\"\">2004</a>)</cite> with 1000 bootstrap resamples.</p>\n\n",
                "matched_terms": [
                    "models",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As mentioned in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS1\" title=\"4.1 Training Data &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>, we train models on data of various sizes.\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.T1\" title=\"Table 1 &#8227; 4.3 Experiment Setup &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the best En-to-De and De-to-En results on the EPST test set, along with the corresponding data sizes.\nAdditional results on the FLEURS test set are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A5\" title=\"Appendix E Evaluation Results on FLEURS &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "entode",
                    "results",
                    "test",
                    "epst",
                    "detoen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Intriguingly, for both directions, Speech Vecalign and speech mining models are competitive with or outperform SpeechMatrix&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite> models, despite the latter being mined from about 24k hours of speech per language, <span class=\"ltx_text ltx_font_italic\">8 times more</span> than our raw data.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>We do not aim for state-of-the-art performance. Our results are not directly comparable to SpeechMatrix. We report SpeechMatrix results only to show the performance gap.</span></span></span>\nFor En-to-De, our Global Mining and Speech Vecalign models achieve improvements of 0.94 and 1.31 BLEU, respectively.\nOur Local Mining model achieves even 1.64 BLEU improvement.\nWe suspect that SpeechMatrix has not removed identical untranslated segments prior to and after mining, which significantly hurts model performance.\nFurther discussion is in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.SS5\" title=\"5.5 Removing Identical Untranslated Segments is Critical &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5.5</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "hours",
                    "speechmatrix",
                    "model",
                    "segments",
                    "after",
                    "entode",
                    "identical",
                    "results",
                    "untranslated",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While Local Mining has not been previously explored, our results suggest that it is a potentially useful method.\nLocal Mining achieves the highest BLEU score in En-to-De, and only slightly underperforms Global Mining in De-to-En, indicating that constraining the mining scope to document pairs does not necessarily have a negative impact on alignment quality.\nYet we note that Local Mining requires more training data to achieve its optimal performance, as shown in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.SS2\" title=\"D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "detoen",
                    "performance",
                    "results",
                    "entode"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our Speech Vecalign models outperform both speech mining models in both directions.\nFor En-to-De, the Speech Vecalign model achieves 12.58 BLEU, comparable with our strong Global Mining and Local Mining baselines.\nIn terms of chrF2++, it surpasses Global Mining and Local Mining by 1.69 and 0.26, respectively.\nIt also significantly improves their referenced BLASER 2.0 by 0.08 and 0.03.\nFor De-to-En, Speech Vecalign and Global Mining models achieve comparable BLEU (16.14 vs. 15.96), but Speech Vecalign surpasses Global Mining by 0.57 in chrF2++.\nSpeech Vecalign significantly outperforms Local Mining under all metrics.\nThese results demonstrate that Speech Vecalign produces higher-quality alignments than both speech mining baselines.</p>\n\n",
                "matched_terms": [
                    "models",
                    "alignments",
                    "model",
                    "entode",
                    "results",
                    "detoen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, we show that Global Mining mostly <span class=\"ltx_text ltx_font_bold\">locally</span> aligns speech documents.\nWhile Global Mining searches for the best matching segment pairs among roughly 10 million segments, one might expect its alignments to cover the spread of the entire dataset.\nOn the contrary, we find that Global Mining alignments are concentrated within document pairs, each typically containing hundreds to thousands of segments.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "segments",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To quantify this, we examine the 1000-hour Global Mining data and count alignments whose source and target segments come from <span class=\"ltx_text ltx_font_italic\">different</span> document pairs.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F3\" title=\"Figure 3 &#8227; 5.1 Speech Mining Mostly Locally Aligns Segments in Time Order &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, fewer than 6% fall into this category, while the majority&#160;(<math alttext=\"&gt;93\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mrow><mn>93</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">&gt;93\\%</annotation></semantics></math>) are within paired documents.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "segments"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Without loss of generality, we use Speech Vecalign En-De alignments as the reference, and evaluate speech mining ones.\nWe choose 700k highest-scoring alignments from all three methods to ensure a fair comparison.\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.T2\" title=\"Table 2 &#8227; 5.2 Speech Mining Methods Produce Similar Alignments as Speech Vecalign &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows that about 30% of speech mining alignments are exactly the same as those of Speech Vecalign, and about 90% overlap with Speech Vecalign alignments.\nThis high similarity explains why Speech Vecalign and speech mining models have similar performance.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As speech mining and Speech Vecalign produce similar alignments, we explore why Speech Vecalign models still perform better.\nA key advantage of Speech Vecalign is that it first produces fine-grained alignments and then constructs alignments with different amounts of context, thanks to the alignment concatenation strategy.\nSpeech mining methods, on the other hand, solely depend on margin-scores and tend to favor shorter alignments.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the best En-to-De models and corresponding data sizes from Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS5\" title=\"4.5 Main Results &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.5</span></a>, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F4\" title=\"Figure 4 &#8227; 5.3 Speech Vecalign Produces Longer Alignments &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the average sentence-level chrF2++ scores on the test set and the percentage of training alignments for different source speech duration ranges.\nNotably, Speech Vecalign has a large portion of long training samples: the blue bars are highest for durations longer than 12 seconds.\nSpecifically, the average source duration of Speech Vecalign is 8.51 seconds, while Global Mining and Local Mining have average durations of 7.50 and 8.53 seconds, respectively.\nAs a result, the Speech Vecalign model performs better on test samples longer than 10 seconds, while having comparable performance on shorter ones.\nThis highlights that Speech Vecalign is able to produce longer, context-rich alignments which help to improve S2ST model performance.\nInterestingly, Local Mining surpasses the Global Mining model on long inputs, which could be also attributed to its longer training samples.</p>\n\n",
                "matched_terms": [
                    "models",
                    "alignments",
                    "model",
                    "entode",
                    "test",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We visualize alignments produced by different methods for the same document pair, which is about 10 minutes long and contains around 200 segments.\nFor reference, we manually created a gold segment-level alignment, with detailed procedure in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A6\" title=\"Appendix F Procedure of Manual Alignments &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>.\nWe illustrate the best 80 alignments for each of the speech mining methods.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "segments"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F5\" title=\"Figure 5 &#8227; 5.3 Speech Vecalign Produces Longer Alignments &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows, Speech Vecalign produces the most fine-grained alignments and is most similar to the gold reference.\nGlobal Mining also performs well, aligning closely with the groundtruth path, whereas Local Mining produces more noise and misses more alignments along the correct path.\nWe hypothesize Local Mining has limited number of segments in a single document pair, making nearest neighbors less effective normalizers in the margin-score computation.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "segments"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As presented in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS5\" title=\"4.5 Main Results &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.5</span></a>, our reproduced speech mining models achieve comparable or even better results than SpeechMatrix models.\nBy listening to samples of SpeechMatrix alignments, we observed many cases where the source and target segments contained identical untranslated content, which is an issue mentioned in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>.\nUsing the method described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, we identified approximately 100k out of 630k alignments with untranslated source and target segments, totaling 181 hours.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speechmatrix",
                    "hours",
                    "alignments",
                    "segments",
                    "identical",
                    "results",
                    "untranslated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also re-produced our alignment pipelines <span class=\"ltx_text ltx_font_italic\">without</span> removing identical untranslated segments, referred to as &#8220;noisy\" in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.T4\" title=\"Table 4 &#8227; 5.5 Removing Identical Untranslated Segments is Critical &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nWe trained models on 500 hours of this data.\nAlthough these untranslated segments account for less than 1% of the training data, performance degrades noticeably.</p>\n\n",
                "matched_terms": [
                    "models",
                    "removing",
                    "hours",
                    "segments",
                    "identical",
                    "untranslated",
                    "trained",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, the experiments highlight that removing untranslated alignments is essential for S2ST training, corroborating <cite class=\"ltx_cite ltx_citemacro_citet\">Khayrallah and Koehn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib23\" title=\"\">2018</a>)</cite>, who found that the untranslated sentences are most catastrophic in neural machine translation.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "untranslated",
                    "removing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech features for identical untranslated segment detection could be improved.</span>\nOur current approach uses filterbank features, which are based on power spectrum, to detect identical untranslated segments.\nHowever, filterbank features are likely to fail for segments that have identical content but differ in signal power.\nAs one of the anonymous reviewers pointed out, cepstral features may be a more robust alternative.</p>\n\n",
                "matched_terms": [
                    "untranslated",
                    "segments",
                    "identical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limited language pair.</span>\nWe have only conducted experiments for English and German speech from the VoxPopuli dataset.\nAs Speech Vecalign heavily relies on the quality of speech embeddings, the performance is unclear for other language pairs and other domains of speech.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-to-speech translation (S2ST).</span>\nThe early S2ST systems consist of cascaded ASR, MT, and TTS models <cite class=\"ltx_cite ltx_citemacro_cite\">Lavie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib27\" title=\"\">1997</a>); Nakamura et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib31\" title=\"\">2006</a>); Wahlster (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib46\" title=\"\">2013</a>)</cite>.\nDirect S2ST models have recently been proposed to alleviate error propagation, support unwritten languages, and improve inference speed.\nTranslatotron models <cite class=\"ltx_cite ltx_citemacro_cite\">Jia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib21\" title=\"\">2019</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib19\" title=\"\">2022a</a>)</cite> are trained with spectrograms as targets, while the S2UT model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib28\" title=\"\">2022a</a>)</cite> outputs discrete units.\nUnitY&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Inaguma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib17\" title=\"\">2023</a>)</cite> and UnitY2 <cite class=\"ltx_cite ltx_citemacro_cite\">Seamless Communication et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib39\" title=\"\">2023</a>)</cite> are two-pass direct S2ST models that predict both subwords and discrete units with a single model.\nDespite advances in architectures, the amount of supervised training data is still insufficient and thus limits model performance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "trained",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">S2ST datasets.</span>\nThere are two common ways to automatically build an S2ST dataset: (1)&#160;building alignments from multilingual speech data; (2)&#160;synthesizing speech for text translations from existing speech-to-text translation (S2TT) corpora.\nThe first line of work has human spoken speech on both source and target sides.\nVoxPopuli <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib47\" title=\"\">2021a</a>)</cite> aligns multilingual speech documents based on text transcriptions, yielding 17.3k-hour alignments between 15 source and target languages.\nSpeechMatrix <cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite> applies Global Mining with SpeechLASER embeddings on VoxPopuli. It obtains alignments for 136 language pairs with an average of 1537 hours per direction.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Seamless Communication et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib39\" title=\"\">2023</a>)</cite> apply Global Mining to web-crawled speech data with SONAR embeddings.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Seamless Communication et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib39\" title=\"\">2023</a>)</cite> also mine a SeamlessAlignExpressive dataset with expressively- and semantically-aligned segment pairs, based on a blend of both semantic and prosodic similarity score <cite class=\"ltx_cite ltx_citemacro_cite\">Heffernan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib15\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speechmatrix",
                    "alignments",
                    "hours",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second line of work has synthesized speech on the target side.\nFisher <cite class=\"ltx_cite ltx_citemacro_cite\">Post et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib35\" title=\"\">2013</a>)</cite> is a Spanish-English S2TT dataset containing about 170 hours of Spanish telephone conversations and English translations which are used to synthesize English speech.\nCVSS <cite class=\"ltx_cite ltx_citemacro_cite\">Jia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib20\" title=\"\">2022b</a>)</cite> is an S2ST dataset covering utterances from 21 languages to English, obtained by synthesizing the text translations in CoVoST 2 <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib49\" title=\"\">2021b</a>)</cite>.\nBesides automatic methods, FLEURS <cite class=\"ltx_cite ltx_citemacro_cite\">Conneau et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib6\" title=\"\">2023</a>)</cite> has collected human read speech covering 102 languages. But it contains only about 12 hours per language and is intended for evaluation.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to limited computing resources, we adopt different training strategies for different purposes.\nThe 500-hour datasets are used for hyperparameter optimization, and larger datasets are used for reporting main results.\nAll models are trained for up to 400k steps, with the first 10,000 steps as a warmup stage.\nFor experiments on a 500-hour dataset, we use a batch size of 320k tokens and apply early-stopping if there is no improvement on the development set for 30 epochs.\nThese models are trained on 4 NVIDIA GeForce GTX 1080 Ti GPUs for approximately 15 days.\nFor larger datasets, we increase the batch size to 640k tokens and early-stopping is not applied.\nThese models are trained on 2 NVIDIA A100-SXM4-80GB GPUs for approximately 15 days.\nThe best checkpoint is selected based on the development set loss.\nAll experiments are conducted in fp32, as we found training with fp16 and amp very unstable.</p>\n\n",
                "matched_terms": [
                    "models",
                    "trained",
                    "dataset",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m1\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> controls the trade-off between overlapped durations and data quality.\nFor instance, a lower <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m2\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> reduces the overlap but also discards alignments more aggressively.\nOverlapped alignments usually have similar margin-scores, so more high-quality alignments are lost.\nThe data size controls the trade-off between data size and data quality cutoff.\nFor instance, a larger dataset will have a lower quality cutoff, as alignments are selected in descending order of margin-scores.\nIn this section, we optimize the combination of <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m3\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> and data size by training S2UT models on different datasets.\nNote that the raw data stays the same.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "models",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first experiment with different values of <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math>.\nWe apply different <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> thresholds during the postprocessing stage, and always choose the best 500 hours as the training data.\nThe optimal value is determined based on development set ASR-BLEU.\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.F6\" title=\"Figure 6 &#8227; D.1 Optimizing &#119898;&#8290;&#119886;&#8290;&#119909;&#8290;_&#8290;&#119900;&#8290;&#119907;&#8290;&#119890;&#8290;&#119903;&#8290;&#119897;&#8290;&#119886;&#8290;&#119901; &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows that 0.8 works best for Speech Vecalign and Local Mining and 0.4 works best for Global Mining.\nThe test set performance is also drawn in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.F6\" title=\"Figure 6 &#8227; D.1 Optimizing &#119898;&#8290;&#119886;&#8290;&#119909;&#8290;_&#8290;&#119900;&#8290;&#119907;&#8290;&#119890;&#8290;&#119903;&#8290;&#119897;&#8290;&#119886;&#8290;&#119901; &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, exhibiting a similar trend.</p>\n\n",
                "matched_terms": [
                    "asrbleu",
                    "hours",
                    "performance",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next we optimize the training data size.\nWe fix <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> at 0.4 for Global Mining and 0.8 for Speech Vecalign and Local Mining during postprocessing, only lowering the quality cutoff to include more training data.\nThe models are trained on different amounts of data until we find the peak performance.\nResults are shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.F7\" title=\"Figure 7 &#8227; D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "trained",
                    "results",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For En-to-De, the best Speech Vecalign model is trained on the 750-hour dataset, achieving 12.58 BLEU.\nIt outperforms the best Global Mining model which achieves 12.21 BLEU.\nThe best Local Mining model achieves 12.91 BLEU.\nHowever, we note that it requires a lot more data than the other two methods to achieve the peak performance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "entode",
                    "dataset",
                    "trained",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For De-to-En, the 1000-hour dataset works best for Speech Vecalign while the 750-hour dataset works best for Global Mining.\nLocal Mining achieves the peak performance when the data size is 1250 hours, still requiring more data than the other methods.\nThe Speech Vecalign performs better than both the Global Mining and the Local Mining models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "hours",
                    "dataset",
                    "detoen",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide evaluation results on the FLEURS test set in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.T5\" title=\"Table 5 &#8227; D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.\nSimilar to Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS5\" title=\"4.5 Main Results &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.5</span></a>, our results match or outperform SpeechMatrix results.\nFor both En-to-De and De-to-En, Speech Vecalign and Global Mining achieve comparable performance when using the transcription-based metrics ASR-BLEU and ASR-chrF2++.\nTheir performance is especially close on De-to-En.\nHowever, Speech Vecalign is significantly better than Global Mining when using the BLASER 2.0 metrics, achieving an improvement of 0.06 and 0.04 referenced BLASER 2.0 scores on En-to-De and De-to-En, respectively.</p>\n\n",
                "matched_terms": [
                    "speechmatrix",
                    "detoen",
                    "entode",
                    "results",
                    "test",
                    "asrbleu",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For En-to-De, Speech Vecalign achieves comparable performance with Local Mining on all metrics.\nFor De-to-En, Speech Vecalign is significantly better than Local Mining when using BLASER 2.0 metrics.</p>\n\n",
                "matched_terms": [
                    "detoen",
                    "performance",
                    "entode"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As our proposed alignment pipeline consists of several intermediate steps, we report numbers of segments or alignments in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A7.T7\" title=\"Table 7 &#8227; Appendix G Evaluation of System Alignments using the Manual Alignments &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.\nWe use English-to-German alignment as an example.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "segments",
                    "englishtogerman"
                ]
            }
        ]
    },
    "S5.T4": {
        "caption": "Table 4: Results for En-to-De on EPST test sets.\n“noisy\" means the steps of removing identical untranslated segments are not applied.\nAll datasets have 500 hours of training data.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">#Noisy/All Aligns.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">ASR-BLEU</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Local Mining</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">11.18</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Local Mining noisy</th>\n<td class=\"ltx_td ltx_align_center\">2.66k/236k</td>\n<td class=\"ltx_td ltx_align_center\">10.76</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Global Mining</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">11.54</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Global Mining noisy</th>\n<td class=\"ltx_td ltx_align_center\">1.46k/254k</td>\n<td class=\"ltx_td ltx_align_center\">11.27</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Speech Vecalign</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">11.78</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Speech Vecalign noisy</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1.48k/222k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">11.69</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "vecalign",
            "entode",
            "have",
            "266k236k",
            "epst",
            "not",
            "applied",
            "hours",
            "training",
            "test",
            "aligns",
            "mining",
            "removing",
            "noisyall",
            "sets",
            "results",
            "speech",
            "untranslated",
            "asrbleu",
            "datasets",
            "global",
            "means",
            "local",
            "segments",
            "identical",
            "steps",
            "all",
            "146k254k",
            "data",
            "148k222k",
            "dataset",
            "noisy",
            "“noisy"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We also re-produced our alignment pipelines <span class=\"ltx_text ltx_font_italic\">without</span> removing identical untranslated segments, referred to as &#8220;noisy\" in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.T4\" title=\"Table 4 &#8227; 5.5 Removing Identical Untranslated Segments is Critical &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nWe trained models on 500 hours of this data.\nAlthough these untranslated segments account for less than 1% of the training data, performance degrades noticeably.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Speech Vecalign, a parallel speech document alignment method that monotonically aligns speech segment embeddings and does not depend on text transcriptions.\nCompared to the baseline method Global Mining&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>, a variant of speech mining, Speech Vecalign produces longer speech-to-speech alignments.\nIt also demonstrates greater robustness than Local Mining, another speech mining variant, as it produces less noise.\nWe applied Speech Vecalign to 3,000 hours of unlabeled parallel English-German&#160;(En-De) speech documents from VoxPopuli, yielding about 1,000 hours of high-quality alignments.\nWe then trained En-De speech-to-speech translation models on the aligned data.\nSpeech Vecalign improves the En-to-De and De-to-En performance over Global Mining by 0.37 and 0.18 ASR-BLEU, respectively.\nMoreover, our models match or outperform SpeechMatrix model performance, despite using 8 times fewer raw speech documents.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Data and code are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/mct10/Speech-Vecalign\" title=\"\">https://github.com/mct10/Speech-Vecalign</a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "global",
                    "hours",
                    "local",
                    "vecalign",
                    "entode",
                    "mining",
                    "data",
                    "speech",
                    "asrbleu",
                    "not",
                    "applied",
                    "aligns"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "vecalign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech-to-speech translation (S2ST) is the task of translating speech in one language into speech in another language.\nConventional S2ST systems concatenate automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) models <cite class=\"ltx_cite ltx_citemacro_cite\">Lavie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib27\" title=\"\">1997</a>); Nakamura et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib31\" title=\"\">2006</a>); Wahlster (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib46\" title=\"\">2013</a>)</cite>.\nThese components can be trained individually with datasets for the different components.\nDirect S2ST models, which translate source speech into target spectrograms or discrete units with a single architecture, have been recently proposed to alleviate error propagation and to reduce inference latency <cite class=\"ltx_cite ltx_citemacro_cite\">Jia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib21\" title=\"\">2019</a>); Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib28\" title=\"\">2022a</a>)</cite>.\nDespite the advantages, performance of direct models is limited by the amount of speech-to-speech aligned data, which is much more scarce than the data used for components of cascaded systems.</p>\n\n",
                "matched_terms": [
                    "have",
                    "data",
                    "speech",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There have been efforts to automatically curate alignments from multilingual <span class=\"ltx_text ltx_font_italic\">speech document</span>s.\nIn this paper, we define a <span class=\"ltx_text ltx_font_italic\">speech document</span> as a file containing more than one utterance and typically comprising several paragraphs, analogous to a <span class=\"ltx_text ltx_font_italic\">text document</span>.\nVoxPopuli&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib47\" title=\"\">2021a</a>)</cite> is one such corpus containing a large number of <span class=\"ltx_text ltx_font_italic\">parallel</span> speech documents, which are pairs of documents that have the same content but differ in language.</p>\n\n",
                "matched_terms": [
                    "have",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech-to-speech alignment methods align short speech clips called <span class=\"ltx_text ltx_font_italic\">segment</span>s, and can be either transcription-based or transcription-free.\nWhen transcriptions are available, segments in parallel speech documents can be aligned through speech-to-text and text-to-text alignments.\nInspired by text mining&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Schwenk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib38\" title=\"\">2021</a>)</cite>, speech mining&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib9\" title=\"\">2021</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite> was proposed as a transcription-free method that aligns speech segments by finding segment pairs with the highest embedding similarity.\nIt scales well as it does not rely on the availability of text transcriptions.\nWhen speech mining is applied to a large amount of speech documents, as in all previous work, it is referred to as <span class=\"ltx_text ltx_font_bold\">Global Mining</span>.\nAnother variant, <span class=\"ltx_text ltx_font_bold\">Local Mining</span>, which applies speech mining to a single pair of parallel speech documents, has not been well explored.\nAs we formally define in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S2\" title=\"2 Speech Mining Overview &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, both Global Mining and Local Mining treat documents as bags of unordered segments.</p>\n\n",
                "matched_terms": [
                    "aligns",
                    "global",
                    "local",
                    "segments",
                    "all",
                    "speech",
                    "not",
                    "applied",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since speech mining methods do not leverage the document pair structure, we wonder, <span class=\"ltx_text ltx_font_bold\">can we obtain better alignments by aligning speech segments within document pairs and preserving their time order?</span>\nThis allows us to utilize the extra knowledge that (1) segments within parallel document pairs are likely to be translations of each other, and (2) segment pairs right next to already aligned pairs are also likely to be aligned.\nWe draw inspiration from parallel <span class=\"ltx_text ltx_font_italic\">text</span> document alignment methods, which have been popular to create sentence-aligned bitext for training MT systems.\nUnlike mining, they align sentences for each document pair while maintaining the sentence order.\nOur work is based on the text alignment method Vecalign&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Thompson and Koehn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib43\" title=\"\">2019</a>)</cite>, which aligns parallel sentences by applying fast dynamic time warping&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Salvador and Chan (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib37\" title=\"\">2007</a>)</cite> to sentence embeddings.\nWith the advances of extending sentence embeddings to the speech modality&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib9\" title=\"\">2021</a>)</cite>, we can readily apply Vecalign to parallel speech documents.</p>\n\n",
                "matched_terms": [
                    "vecalign",
                    "segments",
                    "have",
                    "mining",
                    "training",
                    "speech",
                    "not",
                    "aligns"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce Speech Vecalign, a method that aligns parallel speech documents using speech segment embeddings.\nInstead of mining from bags of segments, our method aligns individual document pairs and maintains the chronological order of segments, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nAdditional preprocessing and postprocessing strategies are applied to improve alignment quality.\nWe compare Speech Vecalign with Local Mining and Global Mining and show that Speech Vecalign produces higher-quality alignments.\nWe further provide extensive analysis for all three methods, which could be useful for future research.</p>\n\n",
                "matched_terms": [
                    "aligns",
                    "global",
                    "local",
                    "vecalign",
                    "segments",
                    "all",
                    "speech",
                    "applied",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We formally describe the speech mining methods in this section.\nOther related work is in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A1\" title=\"Appendix A Related Work &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech Mining, first proposed by <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib9\" title=\"\">2021</a>)</cite>, encodes speech segments into language- and modality-agnostic fixed-size embeddings, and then uses margin-based similarity search <cite class=\"ltx_cite ltx_citemacro_cite\">Artetxe and Schwenk (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib2\" title=\"\">2019a</a>)</cite> to find the closest embedding pairs.\nDepending on the search scope, it can be categorized as Global Mining or Local Mining.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "segments",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Raw data</span>. The input data is a list of speech documents <math alttext=\"X=[X_{1},X_{2},\\ldots,X_{n}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>X</mi><mn>1</mn></msub><mo>,</mo><msub><mi>X</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>X</mi><mi>n</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">X=[X_{1},X_{2},\\ldots,X_{n}]</annotation></semantics></math> in the source language and a list <math alttext=\"Y=[Y_{1},Y_{2},\\ldots,Y_{m}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m2\" intent=\":literal\"><semantics><mrow><mi>Y</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>Y</mi><mn>1</mn></msub><mo>,</mo><msub><mi>Y</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>Y</mi><mi>m</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Y=[Y_{1},Y_{2},\\ldots,Y_{m}]</annotation></semantics></math> in the target language, where <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m3\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> and <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m4\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> are the numbers of documents.\nEach document can contain between a few seconds to a few hours of speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech segmentation</span>. Voice activity detection&#160;(VAD) is applied to each document to obtain short segments, typically lasting a few seconds.\nFor instance, <math alttext=\"X_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m1\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">X_{i}</annotation></semantics></math> is segmented into <math alttext=\"X_{i}=[x_{1}^{i},x_{2}^{i},\\ldots,x_{n_{i}}^{i}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m2\" intent=\":literal\"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>x</mi><mn>1</mn><mi>i</mi></msubsup><mo>,</mo><msubsup><mi>x</mi><mn>2</mn><mi>i</mi></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>x</mi><msub><mi>n</mi><mi>i</mi></msub><mi>i</mi></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">X_{i}=[x_{1}^{i},x_{2}^{i},\\ldots,x_{n_{i}}^{i}]</annotation></semantics></math>, where <math alttext=\"{n_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m3\" intent=\":literal\"><semantics><msub><mi>n</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">{n_{i}}</annotation></semantics></math> is the number of segments.\nTo have segments at different granularities, consecutive segments are progressively concatenated.\n<math alttext=\"X_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m4\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">X_{i}</annotation></semantics></math> becomes <math alttext=\"\\tilde{X_{i}}=\\left[\\tilde{x}_{1}^{i},\\tilde{x}_{2}^{i},\\ldots,\\tilde{x}_{\\tilde{n}_{i}}^{i}\\right]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m5\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><msub><mi>X</mi><mi>i</mi></msub><mo>~</mo></mover><mo>=</mo><mrow><mo>[</mo><msubsup><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mn>1</mn><mi>i</mi></msubsup><mo>,</mo><msubsup><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mn>2</mn><mi>i</mi></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><msub><mover accent=\"true\"><mi>n</mi><mo>~</mo></mover><mi>i</mi></msub><mi>i</mi></msubsup><mo>]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tilde{X_{i}}=\\left[\\tilde{x}_{1}^{i},\\tilde{x}_{2}^{i},\\ldots,\\tilde{x}_{\\tilde{n}_{i}}^{i}\\right]</annotation></semantics></math>, where <math alttext=\"\\tilde{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m6\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\tilde{x}</annotation></semantics></math> denotes a concatenated segment and <math alttext=\"\\tilde{n_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m7\" intent=\":literal\"><semantics><mover accent=\"true\"><msub><mi>n</mi><mi>i</mi></msub><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\tilde{n_{i}}</annotation></semantics></math> denotes the number of resulting segments.\nThe same process applies to <math alttext=\"Y_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m8\" intent=\":literal\"><semantics><msub><mi>Y</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">Y_{j}</annotation></semantics></math>, producing <math alttext=\"\\tilde{Y_{j}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m9\" intent=\":literal\"><semantics><mover accent=\"true\"><msub><mi>Y</mi><mi>j</mi></msub><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\tilde{Y_{j}}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "have",
                    "segments",
                    "speech",
                    "applied"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech segment embedding</span>.\nEach segment is encoded into a fixed-size embedding using an embedding model.\nThe segment embeddings for <math alttext=\"\\tilde{X}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p5.m1\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\tilde{X}_{i}</annotation></semantics></math> are represented as <math alttext=\"E_{\\tilde{X}_{i}}=\\left[e^{\\tilde{X}_{i}}_{1},e^{\\tilde{X}_{i}}_{2},\\ldots,e^{\\tilde{X}_{i}}_{\\tilde{n}_{i}}\\right]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p5.m2\" intent=\":literal\"><semantics><mrow><msub><mi>E</mi><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mi>i</mi></msub></msub><mo>=</mo><mrow><mo>[</mo><msubsup><mi>e</mi><mn>1</mn><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mi>i</mi></msub></msubsup><mo>,</mo><msubsup><mi>e</mi><mn>2</mn><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mi>i</mi></msub></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>e</mi><msub><mover accent=\"true\"><mi>n</mi><mo>~</mo></mover><mi>i</mi></msub><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mi>i</mi></msub></msubsup><mo>]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">E_{\\tilde{X}_{i}}=\\left[e^{\\tilde{X}_{i}}_{1},e^{\\tilde{X}_{i}}_{2},\\ldots,e^{\\tilde{X}_{i}}_{\\tilde{n}_{i}}\\right]</annotation></semantics></math>.\nSimilarly, the segments in <math alttext=\"\\tilde{Y_{j}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p5.m3\" intent=\":literal\"><semantics><mover accent=\"true\"><msub><mi>Y</mi><mi>j</mi></msub><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\tilde{Y_{j}}</annotation></semantics></math> are encoded as <math alttext=\"E_{\\tilde{Y_{j}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p5.m4\" intent=\":literal\"><semantics><msub><mi>E</mi><mover accent=\"true\"><msub><mi>Y</mi><mi>j</mi></msub><mo>~</mo></mover></msub><annotation encoding=\"application/x-tex\">E_{\\tilde{Y_{j}}}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "segments"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Bag of embeddings</span>.\nIn <span class=\"ltx_text ltx_font_italic\">Global Mining</span>, embeddings are grouped by <span class=\"ltx_text ltx_font_bold\">language</span>.\nWe define <math alttext=\"G_{X}=\\left\\{E_{\\tilde{X}_{1}},E_{\\tilde{X}_{2}},\\ldots,E_{\\tilde{X}_{n}}\\right\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m1\" intent=\":literal\"><semantics><mrow><msub><mi>G</mi><mi>X</mi></msub><mo>=</mo><mrow><mo>{</mo><msub><mi>E</mi><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mn>1</mn></msub></msub><mo>,</mo><msub><mi>E</mi><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mn>2</mn></msub></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>E</mi><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mi>n</mi></msub></msub><mo>}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">G_{X}=\\left\\{E_{\\tilde{X}_{1}},E_{\\tilde{X}_{2}},\\ldots,E_{\\tilde{X}_{n}}\\right\\}</annotation></semantics></math> and <math alttext=\"G_{Y}=\\left\\{E_{\\tilde{Y}_{1}},E_{\\tilde{Y}_{2}},\\ldots,E_{\\tilde{Y}_{m}}\\right\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m2\" intent=\":literal\"><semantics><mrow><msub><mi>G</mi><mi>Y</mi></msub><mo>=</mo><mrow><mo>{</mo><msub><mi>E</mi><msub><mover accent=\"true\"><mi>Y</mi><mo>~</mo></mover><mn>1</mn></msub></msub><mo>,</mo><msub><mi>E</mi><msub><mover accent=\"true\"><mi>Y</mi><mo>~</mo></mover><mn>2</mn></msub></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>E</mi><msub><mover accent=\"true\"><mi>Y</mi><mo>~</mo></mover><mi>m</mi></msub></msub><mo>}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">G_{Y}=\\left\\{E_{\\tilde{Y}_{1}},E_{\\tilde{Y}_{2}},\\ldots,E_{\\tilde{Y}_{m}}\\right\\}</annotation></semantics></math>, where <math alttext=\"G_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m3\" intent=\":literal\"><semantics><msub><mi>G</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">G_{X}</annotation></semantics></math> collects all segment embeddings in the source language and <math alttext=\"G_{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m4\" intent=\":literal\"><semantics><msub><mi>G</mi><mi>Y</mi></msub><annotation encoding=\"application/x-tex\">G_{Y}</annotation></semantics></math> collects those in the target language.\nIn <span class=\"ltx_text ltx_font_italic\">Local Mining</span>, embeddings are grouped by <span class=\"ltx_text ltx_font_bold\">document pairs</span>.\nSuppose there are <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m5\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> parallel documents, with <math alttext=\"X_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m6\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">X_{i}</annotation></semantics></math> paired with <math alttext=\"Y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m7\" intent=\":literal\"><semantics><msub><mi>Y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">Y_{i}</annotation></semantics></math> for <math alttext=\"1\\leq i\\leq s\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m8\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>&#8804;</mo><mi>i</mi><mo>&#8804;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">1\\leq i\\leq s</annotation></semantics></math>.\nDocuments without a parallel one are ignored.\nIn this case, <math alttext=\"E_{\\tilde{X}_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m9\" intent=\":literal\"><semantics><msub><mi>E</mi><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mi>i</mi></msub></msub><annotation encoding=\"application/x-tex\">E_{\\tilde{X}_{i}}</annotation></semantics></math> and <math alttext=\"E_{\\tilde{Y_{j}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m10\" intent=\":literal\"><semantics><msub><mi>E</mi><mover accent=\"true\"><msub><mi>Y</mi><mi>j</mi></msub><mo>~</mo></mover></msub><annotation encoding=\"application/x-tex\">E_{\\tilde{Y_{j}}}</annotation></semantics></math> are bags of embeddings themselves.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "all",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Embedding alignment</span>.\nSpeech mining is performed by finding the most similar embedding pairs between two bags of segment embeddings.\nThe margin-based similarity, or margin-score, between any two embeddings <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m1\" intent=\":literal\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math> and <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m2\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> is computed as</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More generally, given two bags of embeddings <math alttext=\"U=\\{u_{1},u_{2},\\ldots,u_{l_{u}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m10\" intent=\":literal\"><semantics><mrow><mi>U</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>u</mi><mn>1</mn></msub><mo>,</mo><msub><mi>u</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>u</mi><msub><mi>l</mi><mi>u</mi></msub></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">U=\\{u_{1},u_{2},\\ldots,u_{l_{u}}\\}</annotation></semantics></math> and <math alttext=\"V=\\{v_{1},v_{2},\\ldots,v_{l_{v}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m11\" intent=\":literal\"><semantics><mrow><mi>V</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>v</mi><mn>1</mn></msub><mo>,</mo><msub><mi>v</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>v</mi><msub><mi>l</mi><mi>v</mi></msub></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">V=\\{v_{1},v_{2},\\ldots,v_{l_{v}}\\}</annotation></semantics></math>, where <math alttext=\"l_{u}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m12\" intent=\":literal\"><semantics><msub><mi>l</mi><mi>u</mi></msub><annotation encoding=\"application/x-tex\">l_{u}</annotation></semantics></math> and <math alttext=\"l_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m13\" intent=\":literal\"><semantics><msub><mi>l</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">l_{v}</annotation></semantics></math> are number of embeddings, the collection of all speech mining alignments is</p>\n\n",
                "matched_terms": [
                    "speech",
                    "all",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we define Local Mining and Global Mining as</p>\n\n",
                "matched_terms": [
                    "global",
                    "mining",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Speech Vecalign pipeline consists of three steps:\nspeech preprocessing (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>), segment alignment with Vecalign (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS2\" title=\"3.2 Speech Segment Alignment &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), and alignment postprocessing (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS3\" title=\"3.3 Alignment Postprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>).\nAn illustration of our method is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "steps",
                    "vecalign",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech preprocessing consists of document segmentation and detection of identical untranslated segments.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "untranslated",
                    "segments",
                    "identical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Segmentation</span>. Same as speech mining, we first segment each speech document by VAD.\nWe apply Silero VAD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Silero Team (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib41\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Detection of identical untranslated segments</span>.\nAs mentioned by <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib47\" title=\"\">2021a</a>)</cite>, some source and target segments contain identical untranslated content due to recording issues.\nWe introduce this additional step to detect such pairs of segments <span class=\"ltx_text ltx_font_italic\">prior to</span> applying the alignment algorithms, in order to make sure they are not aligned.</p>\n\n",
                "matched_terms": [
                    "untranslated",
                    "not",
                    "segments",
                    "identical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To find potentially identical untranslated segment pairs, we use a <span class=\"ltx_text ltx_font_italic\">location heuristic</span> that they tend to locate in roughly the same position within the source and target documents.\nFor instance, within each pair of parallel documents, for a source segment <math alttext=\"x_{a}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m1\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mi>a</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">x_{a}^{i}</annotation></semantics></math> spanning timestamp <math alttext=\"s_{x_{a}}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m2\" intent=\":literal\"><semantics><msubsup><mi>s</mi><msub><mi>x</mi><mi>a</mi></msub><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">s_{x_{a}}^{i}</annotation></semantics></math> to <math alttext=\"e_{x_{a}}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m3\" intent=\":literal\"><semantics><msubsup><mi>e</mi><msub><mi>x</mi><mi>a</mi></msub><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">e_{x_{a}}^{i}</annotation></semantics></math>, we search for a target segment <math alttext=\"y_{b}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m4\" intent=\":literal\"><semantics><msubsup><mi>y</mi><mi>b</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">y_{b}^{i}</annotation></semantics></math> whose midpoint <math alttext=\"\\frac{s_{y_{b}}^{i}+e_{y_{b}}^{i}}{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m5\" intent=\":literal\"><semantics><mfrac><mrow><msubsup><mi>s</mi><msub><mi>y</mi><mi>b</mi></msub><mi>i</mi></msubsup><mo>+</mo><msubsup><mi>e</mi><msub><mi>y</mi><mi>b</mi></msub><mi>i</mi></msubsup></mrow><mn>2</mn></mfrac><annotation encoding=\"application/x-tex\">\\frac{s_{y_{b}}^{i}+e_{y_{b}}^{i}}{2}</annotation></semantics></math> is closest to <math alttext=\"\\frac{s_{x_{a}}^{i}+e_{x_{a}}^{i}}{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m6\" intent=\":literal\"><semantics><mfrac><mrow><msubsup><mi>s</mi><msub><mi>x</mi><mi>a</mi></msub><mi>i</mi></msubsup><mo>+</mo><msubsup><mi>e</mi><msub><mi>x</mi><mi>a</mi></msub><mi>i</mi></msubsup></mrow><mn>2</mn></mfrac><annotation encoding=\"application/x-tex\">\\frac{s_{x_{a}}^{i}+e_{x_{a}}^{i}}{2}</annotation></semantics></math>, midpoint of <math alttext=\"x_{a}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m7\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mi>a</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">x_{a}^{i}</annotation></semantics></math>, since the untranslated target segment is very likely to have a similar time span (<math alttext=\"s_{y_{b}}^{i}\\approx s_{x_{a}}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m8\" intent=\":literal\"><semantics><mrow><msubsup><mi>s</mi><msub><mi>y</mi><mi>b</mi></msub><mi>i</mi></msubsup><mo>&#8776;</mo><msubsup><mi>s</mi><msub><mi>x</mi><mi>a</mi></msub><mi>i</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">s_{y_{b}}^{i}\\approx s_{x_{a}}^{i}</annotation></semantics></math>, <math alttext=\"e_{y_{b}}^{i}\\approx e_{x_{a}}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m9\" intent=\":literal\"><semantics><mrow><msubsup><mi>e</mi><msub><mi>y</mi><mi>b</mi></msub><mi>i</mi></msubsup><mo>&#8776;</mo><msubsup><mi>e</mi><msub><mi>x</mi><mi>a</mi></msub><mi>i</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">e_{y_{b}}^{i}\\approx e_{x_{a}}^{i}</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "have",
                    "untranslated",
                    "identical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the two segments have both similar durations and filterbank features, we classify them as identical.\nFor durations, we compute the time difference.\nFor filterbank feature, we compute Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.E6\" title=\"In 3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>:</p>\n\n",
                "matched_terms": [
                    "have",
                    "segments",
                    "identical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We perform segment alignment based on the similarity between speech segment embeddings.\nUnlike speech mining, which solely relies on similarity scores, we use a dynamic programming&#160;(DP) algorithm to align segments in chronological order.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "segments",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Segment concatenation</span>.\nSpeech segments do not necessarily correspond to complete sentences.\nSame as speech mining, we first progressively concatenate each segment with the subsequent ones.\nEach concatenated segment can contain up to 5 original segments and span a maximum of 20 seconds.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "not",
                    "segments",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Obtaining segment embeddings</span>.\nAfter concatenations, we obtain speech segment embeddings using SpeechLASER models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib9\" title=\"\">2021</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nIdentical untranslated segments detected in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, along with all concatenated segments that include them, are skipped and replaced with <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><mn>0</mn></math>-valued vectors.</p>\n\n",
                "matched_terms": [
                    "segments",
                    "identical",
                    "all",
                    "speech",
                    "untranslated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because of DP, the resultant alignments strictly follow chronological order.\nWe use <math alttext=\"x_{a:b}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m1\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mrow><mi>a</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">x_{a:b}^{i}</annotation></semantics></math> to denote the concatenation of consecutive segments <math alttext=\"x_{a}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m2\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mi>a</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">x_{a}^{i}</annotation></semantics></math> through <math alttext=\"x_{b}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m3\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mi>b</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">x_{b}^{i}</annotation></semantics></math>.\nFor any two alignments <math alttext=\"(x_{a_{s}:a_{e}}^{i},y_{b_{s}:b_{e}}^{j})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mrow><msub><mi>a</mi><mi>s</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>a</mi><mi>e</mi></msub></mrow><mi>i</mi></msubsup><mo>,</mo><msubsup><mi>y</mi><mrow><msub><mi>b</mi><mi>s</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>b</mi><mi>e</mi></msub></mrow><mi>j</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{a_{s}:a_{e}}^{i},y_{b_{s}:b_{e}}^{j})</annotation></semantics></math> and <math alttext=\"(x_{c_{s}:c_{e}}^{i},y_{d_{s}:d_{e}}^{k})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mrow><msub><mi>c</mi><mi>s</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>c</mi><mi>e</mi></msub></mrow><mi>i</mi></msubsup><mo>,</mo><msubsup><mi>y</mi><mrow><msub><mi>d</mi><mi>s</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>d</mi><mi>e</mi></msub></mrow><mi>k</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{c_{s}:c_{e}}^{i},y_{d_{s}:d_{e}}^{k})</annotation></semantics></math>, Speech Vecalign guarantees that <math alttext=\"i=j=k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m6\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>=</mo><mi>j</mi><mo>=</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">i=j=k</annotation></semantics></math> and that either <math alttext=\"a_{e}&lt;c_{s},b_{e}&lt;d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m7\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>a</mi><mi>e</mi></msub><mo>&lt;</mo><msub><mi>c</mi><mi>s</mi></msub></mrow><mo>,</mo><mrow><msub><mi>b</mi><mi>e</mi></msub><mo>&lt;</mo><msub><mi>d</mi><mi>s</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">a_{e}&lt;c_{s},b_{e}&lt;d_{s}</annotation></semantics></math> or <math alttext=\"a_{s}&gt;c_{e},b_{s}&gt;d_{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m8\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>a</mi><mi>s</mi></msub><mo>&gt;</mo><msub><mi>c</mi><mi>e</mi></msub></mrow><mo>,</mo><mrow><msub><mi>b</mi><mi>s</mi></msub><mo>&gt;</mo><msub><mi>d</mi><mi>e</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">a_{s}&gt;c_{e},b_{s}&gt;d_{e}</annotation></semantics></math>.\nIn contrast, Local Mining ensures <math alttext=\"i=j=k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m9\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>=</mo><mi>j</mi><mo>=</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">i=j=k</annotation></semantics></math> but has no constraints on <math alttext=\"a,b,c,d\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m10\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi>c</mi><mo>,</mo><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">a,b,c,d</annotation></semantics></math>, while\nGlobal Mining makes no guarantees at all.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "vecalign",
                    "segments",
                    "all",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Removing low-quality alignments</span>.\nFirst, we remove unaligned segments and high-cost alignments.\nThe unaligned segments are due to deletions in the DP algorithm.\nIdentical untranslated segments detected in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a> may fall into either category due to their 0-valued vectors.</p>\n\n",
                "matched_terms": [
                    "untranslated",
                    "removing",
                    "segments",
                    "identical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Detection of identical untranslated segments, again</span>.\nOccasionally, the location heuristic in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a> may fail, resulting in a small number of low-cost alignments with identical untranslated source and target segments.\nSearching is not needed at this step, as we already have the alignments.\nWe apply Equation <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.E6\" title=\"In 3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> to remaining alignments, where <math alttext=\"\\mathbf{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><mi>&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{A}</annotation></semantics></math> and <math alttext=\"\\mathbf{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m2\" intent=\":literal\"><semantics><mi>&#119809;</mi><annotation encoding=\"application/x-tex\">\\mathbf{B}</annotation></semantics></math> are source and target segments in each alignment.\nWe use the same thresholds in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a> to remove alignments.</p>\n\n",
                "matched_terms": [
                    "segments",
                    "identical",
                    "have",
                    "untranslated",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Global margin-scores computation</span>.\nThe raw alignments only have alignment costs as a quality indicator, which are computed <span class=\"ltx_text ltx_font_italic\">within</span> each document pair.\nTo assess alignment quality <span class=\"ltx_text ltx_font_italic\">across</span> document pairs, we train FAISS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Johnson et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib22\" title=\"\">2019</a>)</cite> indexes and compute margin-scores&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Artetxe and Schwenk (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib2\" title=\"\">2019a</a>)</cite> using Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S2.E1\" title=\"In 2 Speech Mining Overview &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> for <span class=\"ltx_text ltx_font_italic\">all</span> obtained alignments, following the common strategy in MT dataset curation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Sloto et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib42\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "have",
                    "global",
                    "all",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Removing highly-overlapped alignments</span>.\nFinally, we remove alignments that have too much overlap with others, following <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nFor any two consecutive alignments, we compute the ratio of the overlapped source duration to the maximum duration of the two source segments.\nIf the ratio exceeds a threshold, we discard the one with a lower margin-score.\nWe train S2ST models with multiple threshold values to determine the best one.\nOur experiments in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.SS1\" title=\"D.1 Optimizing &#119898;&#8290;&#119886;&#8290;&#119909;&#8290;_&#8290;&#119900;&#8290;&#119907;&#8290;&#119890;&#8290;&#119903;&#8290;&#119897;&#8290;&#119886;&#8290;&#119901; &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">D.1</span></a> suggest that 0.4 work best for Global Mining and 0.8 work best for Local Mining and Speech Vecalign.</p>\n\n",
                "matched_terms": [
                    "global",
                    "removing",
                    "local",
                    "vecalign",
                    "segments",
                    "have",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We apply Speech Vecalign, Global Mining, and Local Mining to the same raw data and train S2ST models on each type of alignments, providing a fair comparison.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "vecalign",
                    "data",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data source.</span>\nWe use the unlabeled, unsegmented English and German plenary session recordings from VoxPopuli v1&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib47\" title=\"\">2021a</a>)</cite> as raw data.\nVoxPopuli contains European Parliament plenary session recordings in each of the 23 European Union languages, paired with spoken interpretations into the other languages.\nThe document names are formatted as <code class=\"ltx_verbatim ltx_font_typewriter\">${session_id}_${language}.ogg</code>, and paired documents have the same <code class=\"ltx_verbatim ltx_font_typewriter\">${session_id}</code>.\nTo avoid overlapping with the test set (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS2\" title=\"4.2 Evaluation Data &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>), we only choose sessions from year 2013 to 2020.\nWe also exclude sessions in the development set&#160;(Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS2\" title=\"4.2 Evaluation Data &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>).\nFor En-to-De, the remaining data has 4,880 documents totaling about 3,000 hours for each language.\nFor De-to-En, there are 5,782 documents totaling 3,400 hours per language.\nThe difference is due to the different dev and test sets.\nAll documents are in pairs, allowing all methods to have exactly the same raw data.</p>\n\n",
                "matched_terms": [
                    "hours",
                    "sets",
                    "entode",
                    "have",
                    "all",
                    "data",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Vecalign.</span>\nWe apply Speech Vecalign to each pair of speech documents and obtain alignments sorted by margin-scores.\nTraining data is chosen in descending order of margin-scores.\nWe train models on different data sizes and report the best results in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS5\" title=\"4.5 Main Results &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.5</span></a>.\nMore details on data size optimization can be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.SS2\" title=\"D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "vecalign",
                    "data",
                    "training",
                    "results",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech mining baselines.</span>\nWe apply Global Mining and Local Mining to the same raw data and embeddings as Speech Vecalign.\nThe implementation is based on <span class=\"ltx_text ltx_font_typewriter\">stopes<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">3</span></span><a class=\"ltx_ref ltx_url\" href=\"https://github.com/facebookresearch/stopes\" title=\"\">https://github.com/facebookresearch/stopes</a></span></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Andrews et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib1\" title=\"\">2022</a>)</cite>.\nAfter mining, we apply the same postprocessing strategies in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS3\" title=\"3.3 Alignment Postprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>, except for alignment concatenation which is not applicable.\nTraining data is chosen in descending order of margin-scores and details on data size optimization can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.SS2\" title=\"D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "vecalign",
                    "data",
                    "training",
                    "speech",
                    "not",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Development set.</span>\nFollowing <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>, we choose 1000 samples from the highest scored sessions from the Voxpopuli S2ST dataset.\nAdditionally, we avoid choosing sessions that occur on the same dates as the test set.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Test set.</span>\nWe use the Europarl-ST (EPST) test set&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Iranzo-S&#225;nchez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib18\" title=\"\">2020</a>)</cite> as an in-domain test set to evaluate the S2ST models.\nEPST is a multilingual S2TT dataset built on European Parliament debates from year 2008 to 2012.\nWe also adopt FLEURS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Conneau et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib6\" title=\"\">2023</a>)</cite> as an out-of-domain test set.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "epst",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train speech-to-unit translation (S2UT) models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib28\" title=\"\">2022a</a>)</cite> with <span class=\"ltx_text ltx_font_typewriter\">fairseq<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">4</span></span><a class=\"ltx_ref ltx_url\" href=\"https://github.com/facebookresearch/fairseq\" title=\"\">https://github.com/facebookresearch/fairseq</a></span></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">Ott et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib32\" title=\"\">2019</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib48\" title=\"\">2020</a>)</cite> on each type of alignments.\nThe S2UT model takes source speech as input and predicts a sequence of target discrete units.\nThe discrete units are obtained by applying\na k-means model to the <math alttext=\"11^{\\text{th}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><msup><mn>11</mn><mtext>th</mtext></msup><annotation encoding=\"application/x-tex\">11^{\\text{th}}</annotation></semantics></math> layer features of a HuBERT model <cite class=\"ltx_cite ltx_citemacro_cite\">Hsu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib16\" title=\"\">2021</a>)</cite>.\nFor English, we use the mHuBERT from <cite class=\"ltx_cite ltx_citemacro_citet\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib29\" title=\"\">2022b</a>)</cite>, and for German, we use the Germanic mHuBERT from <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nConsecutive duplicated units are removed.\nOur S2UT model architecture follows exactly <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nThe architecture details and training hyperparameters are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A2\" title=\"Appendix B Speech-to-Speech Translation &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the transcription-based method, we transcribe the speech output using the same ASR models as <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nWe evaluate the transcriptions using SacreBLEU<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/mjpost/sacrebleu\" title=\"\">https://github.com/mjpost/sacrebleu</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">Post (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib34\" title=\"\">2018</a>)</cite> to compute BLEU<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Signature: nrefs:1 + case:mixed + eff:no\n+ tok:13a + smooth:exp + version:2.2.0</span></span></span> and chrF2++<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>Signature: nrefs:1 + case:mixed + eff:yes + nc:6 + nw:2 + space:no + version:2.2.0</span></span></span> scores.\nWe apply the significance test using paired bootstrap resampling&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Koehn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib26\" title=\"\">2004</a>)</cite> with 1000 bootstrap resamples.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As mentioned in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS1\" title=\"4.1 Training Data &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>, we train models on data of various sizes.\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.T1\" title=\"Table 1 &#8227; 4.3 Experiment Setup &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the best En-to-De and De-to-En results on the EPST test set, along with the corresponding data sizes.\nAdditional results on the FLEURS test set are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A5\" title=\"Appendix E Evaluation Results on FLEURS &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</p>\n\n",
                "matched_terms": [
                    "entode",
                    "data",
                    "results",
                    "test",
                    "epst"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Intriguingly, for both directions, Speech Vecalign and speech mining models are competitive with or outperform SpeechMatrix&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite> models, despite the latter being mined from about 24k hours of speech per language, <span class=\"ltx_text ltx_font_italic\">8 times more</span> than our raw data.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>We do not aim for state-of-the-art performance. Our results are not directly comparable to SpeechMatrix. We report SpeechMatrix results only to show the performance gap.</span></span></span>\nFor En-to-De, our Global Mining and Speech Vecalign models achieve improvements of 0.94 and 1.31 BLEU, respectively.\nOur Local Mining model achieves even 1.64 BLEU improvement.\nWe suspect that SpeechMatrix has not removed identical untranslated segments prior to and after mining, which significantly hurts model performance.\nFurther discussion is in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.SS5\" title=\"5.5 Removing Identical Untranslated Segments is Critical &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5.5</span></a>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "hours",
                    "local",
                    "vecalign",
                    "segments",
                    "entode",
                    "identical",
                    "results",
                    "speech",
                    "untranslated",
                    "not",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While Local Mining has not been previously explored, our results suggest that it is a potentially useful method.\nLocal Mining achieves the highest BLEU score in En-to-De, and only slightly underperforms Global Mining in De-to-En, indicating that constraining the mining scope to document pairs does not necessarily have a negative impact on alignment quality.\nYet we note that Local Mining requires more training data to achieve its optimal performance, as shown in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.SS2\" title=\"D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "entode",
                    "have",
                    "data",
                    "training",
                    "results",
                    "not",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our Speech Vecalign models outperform both speech mining models in both directions.\nFor En-to-De, the Speech Vecalign model achieves 12.58 BLEU, comparable with our strong Global Mining and Local Mining baselines.\nIn terms of chrF2++, it surpasses Global Mining and Local Mining by 1.69 and 0.26, respectively.\nIt also significantly improves their referenced BLASER 2.0 by 0.08 and 0.03.\nFor De-to-En, Speech Vecalign and Global Mining models achieve comparable BLEU (16.14 vs. 15.96), but Speech Vecalign surpasses Global Mining by 0.57 in chrF2++.\nSpeech Vecalign significantly outperforms Local Mining under all metrics.\nThese results demonstrate that Speech Vecalign produces higher-quality alignments than both speech mining baselines.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "vecalign",
                    "entode",
                    "all",
                    "results",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We analyze properties of speech mining methods and compare them with Speech Vecalign.\nAlthough we show that speech mining methods produce alignments similar to those of Speech Vecalign, the latter offers advantages of producing longer and less noisy alignments.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "vecalign",
                    "noisy",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, we show that Global Mining mostly <span class=\"ltx_text ltx_font_bold\">locally</span> aligns speech documents.\nWhile Global Mining searches for the best matching segment pairs among roughly 10 million segments, one might expect its alignments to cover the spread of the entire dataset.\nOn the contrary, we find that Global Mining alignments are concentrated within document pairs, each typically containing hundreds to thousands of segments.</p>\n\n",
                "matched_terms": [
                    "global",
                    "segments",
                    "speech",
                    "dataset",
                    "aligns",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To quantify this, we examine the 1000-hour Global Mining data and count alignments whose source and target segments come from <span class=\"ltx_text ltx_font_italic\">different</span> document pairs.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F3\" title=\"Figure 3 &#8227; 5.1 Speech Mining Mostly Locally Aligns Segments in Time Order &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, fewer than 6% fall into this category, while the majority&#160;(<math alttext=\"&gt;93\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mrow><mn>93</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">&gt;93\\%</annotation></semantics></math>) are within paired documents.</p>\n\n",
                "matched_terms": [
                    "global",
                    "data",
                    "segments",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, we analyze the time order of alignments produced by both speech mining methods.\nBorrowing the notation from Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS2\" title=\"3.2 Speech Segment Alignment &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, we define two pairs of alignments to be <span class=\"ltx_text ltx_font_italic\">in-order</span> if either <math alttext=\"a_{e}&lt;c_{s},b_{e}&lt;d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>a</mi><mi>e</mi></msub><mo>&lt;</mo><msub><mi>c</mi><mi>s</mi></msub></mrow><mo>,</mo><mrow><msub><mi>b</mi><mi>e</mi></msub><mo>&lt;</mo><msub><mi>d</mi><mi>s</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">a_{e}&lt;c_{s},b_{e}&lt;d_{s}</annotation></semantics></math> or <math alttext=\"a_{s}&gt;c_{e},b_{s}&gt;d_{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>a</mi><mi>s</mi></msub><mo>&gt;</mo><msub><mi>c</mi><mi>e</mi></msub></mrow><mo>,</mo><mrow><msub><mi>b</mi><mi>s</mi></msub><mo>&gt;</mo><msub><mi>d</mi><mi>e</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">a_{s}&gt;c_{e},b_{s}&gt;d_{e}</annotation></semantics></math>; otherwise, they are <span class=\"ltx_text ltx_font_italic\">out-of-order</span>.\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F3\" title=\"Figure 3 &#8227; 5.1 Speech Mining Mostly Locally Aligns Segments in Time Order &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows that only around 1% alignments are out-of-order for both speech mining methods.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Observations above indicate that speech mining alignments are mostly within paired documents and preserve time order.\nWe hypothesize that speech-to-speech alignments are sparse and high-quality ones mostly exist in paired documents.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a by-product, this property can be leveraged to identify parallel documents.\nIf Global Mining finds many alignments between two documents, they are likely to be parallel.\nIt is particularly useful when the pairing metadata is not readily available.</p>\n\n",
                "matched_terms": [
                    "global",
                    "not",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the observations in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.SS1\" title=\"5.1 Speech Mining Mostly Locally Aligns Segments in Time Order &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> that speech mining produces mostly local, in-order alignments, we analyze the similarity between them and Speech Vecalign alignments.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mining",
                    "vecalign",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Without loss of generality, we use Speech Vecalign En-De alignments as the reference, and evaluate speech mining ones.\nWe choose 700k highest-scoring alignments from all three methods to ensure a fair comparison.\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.T2\" title=\"Table 2 &#8227; 5.2 Speech Mining Methods Produce Similar Alignments as Speech Vecalign &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows that about 30% of speech mining alignments are exactly the same as those of Speech Vecalign, and about 90% overlap with Speech Vecalign alignments.\nThis high similarity explains why Speech Vecalign and speech mining models have similar performance.</p>\n\n",
                "matched_terms": [
                    "vecalign",
                    "have",
                    "all",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As speech mining and Speech Vecalign produce similar alignments, we explore why Speech Vecalign models still perform better.\nA key advantage of Speech Vecalign is that it first produces fine-grained alignments and then constructs alignments with different amounts of context, thanks to the alignment concatenation strategy.\nSpeech mining methods, on the other hand, solely depend on margin-scores and tend to favor shorter alignments.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "vecalign",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the best En-to-De models and corresponding data sizes from Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS5\" title=\"4.5 Main Results &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.5</span></a>, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F4\" title=\"Figure 4 &#8227; 5.3 Speech Vecalign Produces Longer Alignments &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the average sentence-level chrF2++ scores on the test set and the percentage of training alignments for different source speech duration ranges.\nNotably, Speech Vecalign has a large portion of long training samples: the blue bars are highest for durations longer than 12 seconds.\nSpecifically, the average source duration of Speech Vecalign is 8.51 seconds, while Global Mining and Local Mining have average durations of 7.50 and 8.53 seconds, respectively.\nAs a result, the Speech Vecalign model performs better on test samples longer than 10 seconds, while having comparable performance on shorter ones.\nThis highlights that Speech Vecalign is able to produce longer, context-rich alignments which help to improve S2ST model performance.\nInterestingly, Local Mining surpasses the Global Mining model on long inputs, which could be also attributed to its longer training samples.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "vecalign",
                    "entode",
                    "have",
                    "data",
                    "training",
                    "test",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We visualize alignments produced by different methods for the same document pair, which is about 10 minutes long and contains around 200 segments.\nFor reference, we manually created a gold segment-level alignment, with detailed procedure in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A6\" title=\"Appendix F Procedure of Manual Alignments &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>.\nWe illustrate the best 80 alignments for each of the speech mining methods.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "segments",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F5\" title=\"Figure 5 &#8227; 5.3 Speech Vecalign Produces Longer Alignments &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows, Speech Vecalign produces the most fine-grained alignments and is most similar to the gold reference.\nGlobal Mining also performs well, aligning closely with the groundtruth path, whereas Local Mining produces more noise and misses more alignments along the correct path.\nWe hypothesize Local Mining has limited number of segments in a single document pair, making nearest neighbors less effective normalizers in the margin-score computation.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "vecalign",
                    "segments",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As presented in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS5\" title=\"4.5 Main Results &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.5</span></a>, our reproduced speech mining models achieve comparable or even better results than SpeechMatrix models.\nBy listening to samples of SpeechMatrix alignments, we observed many cases where the source and target segments contained identical untranslated content, which is an issue mentioned in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>.\nUsing the method described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, we identified approximately 100k out of 630k alignments with untranslated source and target segments, totaling 181 hours.</p>\n\n",
                "matched_terms": [
                    "hours",
                    "segments",
                    "identical",
                    "results",
                    "speech",
                    "untranslated",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the impact of untranslated segments, we trained models on the original SpeechMatrix En-De alignments and on a version with untranslated alignments <span class=\"ltx_text ltx_font_italic\">removed</span>.\nThe training data is chosen with a margin-score threshold of 1.09, following the original setup.\nAs shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.T3\" title=\"Table 3 &#8227; 5.5 Removing Identical Untranslated Segments is Critical &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the cleaned data produces better models, improving BLEU score by <span class=\"ltx_text ltx_font_bold\">1.00</span> for En-to-De and <span class=\"ltx_text ltx_font_bold\">0.11</span> for De-to-En, despite having 13% less training data.\nThe smaller gain on De-to-En may be due to most untranslated segments being in English, which have smaller impact on into-English translation.</p>\n\n",
                "matched_terms": [
                    "segments",
                    "entode",
                    "have",
                    "data",
                    "training",
                    "untranslated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, the experiments highlight that removing untranslated alignments is essential for S2ST training, corroborating <cite class=\"ltx_cite ltx_citemacro_citet\">Khayrallah and Koehn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib23\" title=\"\">2018</a>)</cite>, who found that the untranslated sentences are most catastrophic in neural machine translation.</p>\n\n",
                "matched_terms": [
                    "untranslated",
                    "removing",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present Speech Vecalign, a parallel speech document alignment method that aligns speech segment embeddings within document pairs and in chronological order.\nWe apply Speech Vecalign to parallel English-German VoxPopuli speech documents and conduct S2ST experiments to demonstrate its superiority over two strong speech mining baselines.\nOur analysis reveals that although speech mining methods primarily align documents locally and in-order, Global Mining falls short of producing long alignments, and Local Mining in particular produces more noise.\nFor long-term future work, we plan to extend Speech Vecalign to other language pairs or other data sources.\nWe can also explore aligning speech and text embeddings to construct S2TT datasets.</p>\n\n",
                "matched_terms": [
                    "aligns",
                    "global",
                    "local",
                    "vecalign",
                    "data",
                    "speech",
                    "datasets",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech features for identical untranslated segment detection could be improved.</span>\nOur current approach uses filterbank features, which are based on power spectrum, to detect identical untranslated segments.\nHowever, filterbank features are likely to fail for segments that have identical content but differ in signal power.\nAs one of the anonymous reviewers pointed out, cepstral features may be a more robust alternative.</p>\n\n",
                "matched_terms": [
                    "segments",
                    "identical",
                    "have",
                    "speech",
                    "untranslated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limited language pair.</span>\nWe have only conducted experiments for English and German speech from the VoxPopuli dataset.\nAs Speech Vecalign heavily relies on the quality of speech embeddings, the performance is unclear for other language pairs and other domains of speech.</p>\n\n",
                "matched_terms": [
                    "have",
                    "dataset",
                    "vecalign",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dependency on parallel speech documents.</span>\nSpeech Vecalign requires parallel speech documents, which is often not available.\nWe may rely on Global Mining to discover parallel documents, as Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.SS1\" title=\"5.1 Speech Mining Mostly Locally Aligns Segments in Time Order &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> suggests, but doing so will introduce extra computation costs.</p>\n\n",
                "matched_terms": [
                    "global",
                    "vecalign",
                    "speech",
                    "not",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-to-speech translation (S2ST).</span>\nThe early S2ST systems consist of cascaded ASR, MT, and TTS models <cite class=\"ltx_cite ltx_citemacro_cite\">Lavie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib27\" title=\"\">1997</a>); Nakamura et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib31\" title=\"\">2006</a>); Wahlster (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib46\" title=\"\">2013</a>)</cite>.\nDirect S2ST models have recently been proposed to alleviate error propagation, support unwritten languages, and improve inference speed.\nTranslatotron models <cite class=\"ltx_cite ltx_citemacro_cite\">Jia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib21\" title=\"\">2019</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib19\" title=\"\">2022a</a>)</cite> are trained with spectrograms as targets, while the S2UT model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib28\" title=\"\">2022a</a>)</cite> outputs discrete units.\nUnitY&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Inaguma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib17\" title=\"\">2023</a>)</cite> and UnitY2 <cite class=\"ltx_cite ltx_citemacro_cite\">Seamless Communication et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib39\" title=\"\">2023</a>)</cite> are two-pass direct S2ST models that predict both subwords and discrete units with a single model.\nDespite advances in architectures, the amount of supervised training data is still insufficient and thus limits model performance.</p>\n\n",
                "matched_terms": [
                    "have",
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Bilingual text sentence alignment.</span>\nText alignment is very related to speech alignment.\nMethods apply dynamic programming <cite class=\"ltx_cite ltx_citemacro_cite\">Bellman (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib4\" title=\"\">1954</a>)</cite> and mainly differ in the design of scoring functions.\nEarly works <cite class=\"ltx_cite ltx_citemacro_cite\">Brown et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib5\" title=\"\">1991</a>); Gale and Church (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib12\" title=\"\">1993</a>)</cite> are based on sentence lengths.\nLater methods incorporate translations in various ways&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Moore (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib30\" title=\"\">2002</a>); Varga et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib44\" title=\"\">2007</a>); Sennrich and Volk (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib40\" title=\"\">2010</a>); Gomes and Lopes (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib13\" title=\"\">2016</a>)</cite>.\nOur work is inspired by Vecalign <cite class=\"ltx_cite ltx_citemacro_cite\">Thompson and Koehn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib43\" title=\"\">2019</a>)</cite>, which utilizes margin-based cosine similarities between multilingual sentence embeddings like LASER&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Artetxe and Schwenk (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib3\" title=\"\">2019b</a>); Heffernan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib14\" title=\"\">2022</a>)</cite> and LaBSE <cite class=\"ltx_cite ltx_citemacro_cite\">Feng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib11\" title=\"\">2022</a>)</cite>.\nVecalign is also more efficient than previous methods.\nBy applying fast dynamic time warping <cite class=\"ltx_cite ltx_citemacro_cite\">Salvador and Chan (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib37\" title=\"\">2007</a>)</cite>, it has a linear time and space complexity with respect to the number of input sentences.\nThe recent progress of extending multilingual sentence embeddings to the speech modality <cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib9\" title=\"\">2021</a>); Khurana et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib24\" title=\"\">2022</a>); Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib10\" title=\"\">b</a>)</cite> enables us to align speech segments by their speech embeddings using the same algorithm.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "vecalign",
                    "segments"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">S2ST datasets.</span>\nThere are two common ways to automatically build an S2ST dataset: (1)&#160;building alignments from multilingual speech data; (2)&#160;synthesizing speech for text translations from existing speech-to-text translation (S2TT) corpora.\nThe first line of work has human spoken speech on both source and target sides.\nVoxPopuli <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib47\" title=\"\">2021a</a>)</cite> aligns multilingual speech documents based on text transcriptions, yielding 17.3k-hour alignments between 15 source and target languages.\nSpeechMatrix <cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite> applies Global Mining with SpeechLASER embeddings on VoxPopuli. It obtains alignments for 136 language pairs with an average of 1537 hours per direction.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Seamless Communication et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib39\" title=\"\">2023</a>)</cite> apply Global Mining to web-crawled speech data with SONAR embeddings.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Seamless Communication et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib39\" title=\"\">2023</a>)</cite> also mine a SeamlessAlignExpressive dataset with expressively- and semantically-aligned segment pairs, based on a blend of both semantic and prosodic similarity score <cite class=\"ltx_cite ltx_citemacro_cite\">Heffernan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib15\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "aligns",
                    "global",
                    "hours",
                    "data",
                    "speech",
                    "dataset",
                    "datasets",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second line of work has synthesized speech on the target side.\nFisher <cite class=\"ltx_cite ltx_citemacro_cite\">Post et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib35\" title=\"\">2013</a>)</cite> is a Spanish-English S2TT dataset containing about 170 hours of Spanish telephone conversations and English translations which are used to synthesize English speech.\nCVSS <cite class=\"ltx_cite ltx_citemacro_cite\">Jia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib20\" title=\"\">2022b</a>)</cite> is an S2ST dataset covering utterances from 21 languages to English, obtained by synthesizing the text translations in CoVoST 2 <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib49\" title=\"\">2021b</a>)</cite>.\nBesides automatic methods, FLEURS <cite class=\"ltx_cite ltx_citemacro_cite\">Conneau et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib6\" title=\"\">2023</a>)</cite> has collected human read speech covering 102 languages. But it contains only about 12 hours per language and is intended for evaluation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dataset",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to limited computing resources, we adopt different training strategies for different purposes.\nThe 500-hour datasets are used for hyperparameter optimization, and larger datasets are used for reporting main results.\nAll models are trained for up to 400k steps, with the first 10,000 steps as a warmup stage.\nFor experiments on a 500-hour dataset, we use a batch size of 320k tokens and apply early-stopping if there is no improvement on the development set for 30 epochs.\nThese models are trained on 4 NVIDIA GeForce GTX 1080 Ti GPUs for approximately 15 days.\nFor larger datasets, we increase the batch size to 640k tokens and early-stopping is not applied.\nThese models are trained on 2 NVIDIA A100-SXM4-80GB GPUs for approximately 15 days.\nThe best checkpoint is selected based on the development set loss.\nAll experiments are conducted in fp32, as we found training with fp16 and amp very unstable.</p>\n\n",
                "matched_terms": [
                    "steps",
                    "all",
                    "training",
                    "applied",
                    "results",
                    "dataset",
                    "not",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Segment embedding.</span>\nThis is the most time-consuming step.\nWe use a mixture of NVIDIA GeForce GTX 1080 and 2080 Ti GPUs.\nEmbedding about 6,000 hours of speech (3,000 hours for each language) took approximately 1,100 GPU hours.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Alignment.</span>\nLocal Mining and Global Mining run on a single GPU.\nThey take about 2 hours.\nSpeech Vecalign runs on a single CPU and takes about 2 hours.</p>\n\n",
                "matched_terms": [
                    "global",
                    "hours",
                    "local",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There are two hyperparameters that affect training data: (1) the maximum source duration overlap ratio between alignments, <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m1\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math>, which is mentioned in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS3\" title=\"3.3 Alignment Postprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>, and\n(2) the data size.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m1\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> controls the trade-off between overlapped durations and data quality.\nFor instance, a lower <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m2\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> reduces the overlap but also discards alignments more aggressively.\nOverlapped alignments usually have similar margin-scores, so more high-quality alignments are lost.\nThe data size controls the trade-off between data size and data quality cutoff.\nFor instance, a larger dataset will have a lower quality cutoff, as alignments are selected in descending order of margin-scores.\nIn this section, we optimize the combination of <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m3\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> and data size by training S2UT models on different datasets.\nNote that the raw data stays the same.</p>\n\n",
                "matched_terms": [
                    "have",
                    "data",
                    "training",
                    "dataset",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first experiment with different values of <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math>.\nWe apply different <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> thresholds during the postprocessing stage, and always choose the best 500 hours as the training data.\nThe optimal value is determined based on development set ASR-BLEU.\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.F6\" title=\"Figure 6 &#8227; D.1 Optimizing &#119898;&#8290;&#119886;&#8290;&#119909;&#8290;_&#8290;&#119900;&#8290;&#119907;&#8290;&#119890;&#8290;&#119903;&#8290;&#119897;&#8290;&#119886;&#8290;&#119901; &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows that 0.8 works best for Speech Vecalign and Local Mining and 0.4 works best for Global Mining.\nThe test set performance is also drawn in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.F6\" title=\"Figure 6 &#8227; D.1 Optimizing &#119898;&#8290;&#119886;&#8290;&#119909;&#8290;_&#8290;&#119900;&#8290;&#119907;&#8290;&#119890;&#8290;&#119903;&#8290;&#119897;&#8290;&#119886;&#8290;&#119901; &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, exhibiting a similar trend.</p>\n\n",
                "matched_terms": [
                    "global",
                    "hours",
                    "local",
                    "vecalign",
                    "data",
                    "training",
                    "test",
                    "speech",
                    "asrbleu",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next we optimize the training data size.\nWe fix <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> at 0.4 for Global Mining and 0.8 for Speech Vecalign and Local Mining during postprocessing, only lowering the quality cutoff to include more training data.\nThe models are trained on different amounts of data until we find the peak performance.\nResults are shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.F7\" title=\"Figure 7 &#8227; D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "vecalign",
                    "data",
                    "training",
                    "results",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For En-to-De, the best Speech Vecalign model is trained on the 750-hour dataset, achieving 12.58 BLEU.\nIt outperforms the best Global Mining model which achieves 12.21 BLEU.\nThe best Local Mining model achieves 12.91 BLEU.\nHowever, we note that it requires a lot more data than the other two methods to achieve the peak performance.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "vecalign",
                    "entode",
                    "data",
                    "speech",
                    "dataset",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For De-to-En, the 1000-hour dataset works best for Speech Vecalign while the 750-hour dataset works best for Global Mining.\nLocal Mining achieves the peak performance when the data size is 1250 hours, still requiring more data than the other methods.\nThe Speech Vecalign performs better than both the Global Mining and the Local Mining models.</p>\n\n",
                "matched_terms": [
                    "global",
                    "hours",
                    "local",
                    "vecalign",
                    "data",
                    "speech",
                    "dataset",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide evaluation results on the FLEURS test set in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.T5\" title=\"Table 5 &#8227; D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.\nSimilar to Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS5\" title=\"4.5 Main Results &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.5</span></a>, our results match or outperform SpeechMatrix results.\nFor both En-to-De and De-to-En, Speech Vecalign and Global Mining achieve comparable performance when using the transcription-based metrics ASR-BLEU and ASR-chrF2++.\nTheir performance is especially close on De-to-En.\nHowever, Speech Vecalign is significantly better than Global Mining when using the BLASER 2.0 metrics, achieving an improvement of 0.06 and 0.04 referenced BLASER 2.0 scores on En-to-De and De-to-En, respectively.</p>\n\n",
                "matched_terms": [
                    "global",
                    "vecalign",
                    "entode",
                    "results",
                    "test",
                    "speech",
                    "asrbleu",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For En-to-De, Speech Vecalign achieves comparable performance with Local Mining on all metrics.\nFor De-to-En, Speech Vecalign is significantly better than Local Mining when using BLASER 2.0 metrics.</p>\n\n",
                "matched_terms": [
                    "local",
                    "vecalign",
                    "entode",
                    "all",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, we can show that Speech Vecalign performs better than both Local Mining and Global Mining.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the same alignment evaluation method as Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.SS2\" title=\"5.2 Speech Mining Methods Produce Similar Alignments as Speech Vecalign &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>, but we use the manual alignments as the reference.\nThere are 144 raw Speech Vecalign alignments, and we choose the same number of alignments from Global Mining and Local Mining in descending order of margin-scores.\nThe Recall and Precision of raw Speech Vecalign, Local Mining, and Global Mining alignments are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A7.T6\" title=\"Table 6 &#8227; Appendix G Evaluation of System Alignments using the Manual Alignments &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The three methods have similar Lax Precisions, while that of Local Mining and Global Mining are slightly higher.\nSpeech Vecalign has the highest recall values than both the speech mining baselines.\nAmong the three methods, Local Mining has the worst performance in general.\nThis follows Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F5\" title=\"Figure 5 &#8227; 5.3 Speech Vecalign Produces Longer Alignments &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> that both Speech Vecalign and Global Mining have good performance but Local Mining does not perform well.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "vecalign",
                    "have",
                    "speech",
                    "not",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As our proposed alignment pipeline consists of several intermediate steps, we report numbers of segments or alignments in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A7.T7\" title=\"Table 7 &#8227; Appendix G Evaluation of System Alignments using the Manual Alignments &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.\nWe use English-to-German alignment as an example.</p>\n\n",
                "matched_terms": [
                    "steps",
                    "segments"
                ]
            }
        ]
    },
    "A4.T5": {
        "caption": "Table 5: Results for En-to-De and De-to-En on FLEURS test sets. Bold means better than speech mining baselines. Underline means the best overall.\n†Results from Duquenne et al. (2023a).\n‡Models trained by ourselves.\n*p-value <0.05<0.05.\nResults show that Speech Vecalign models perform better than baselines under most metrics in both directions.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Training Data</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">ASR-BLEU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">ASR-chrF2++</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">BLASER 2.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Alignment Method</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\"># Hours</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">w/ text ref</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">w/o text ref</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#BFBFBF;\">English-to-German</span></th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">State-of-the-art</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">SpeechMatrix<sup class=\"ltx_sup\">&#8224;</sup>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">1451</th>\n<td class=\"ltx_td ltx_align_center\">2.7</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">SpeechMatrix<sup class=\"ltx_sup\">&#8225;</sup>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">1451</th>\n<td class=\"ltx_td ltx_align_center\">3.36</td>\n<td class=\"ltx_td ltx_align_center\">25.65</td>\n<td class=\"ltx_td ltx_align_center\">2.50</td>\n<td class=\"ltx_td ltx_align_center\">2.87</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Baseline</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Local Mining</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">1500</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">3.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">28.86</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.53</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.91</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Global Mining</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">1000</th>\n<td class=\"ltx_td ltx_align_center\">3.42</td>\n<td class=\"ltx_td ltx_align_center\">27.84</td>\n<td class=\"ltx_td ltx_align_center\">2.49</td>\n<td class=\"ltx_td ltx_align_center\">2.86</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Our Method</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Speech Vecalign</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">750</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">28.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">2.55</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">2.94</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"2\">p-value w.r.t Local Mining</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.1638</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.1938</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0659</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0610</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" colspan=\"2\">p-value w.r.t Global Mining</th>\n<td class=\"ltx_td ltx_align_center\">0.0939</td>\n<td class=\"ltx_td ltx_align_center\">0.0050<sup class=\"ltx_sup\">*</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.0020<sup class=\"ltx_sup\">*</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.0010<sup class=\"ltx_sup\">*</sup>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#BFBFBF;\">German-to-English</span></th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"/>\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_border_tt\"/>\n<td class=\"ltx_td ltx_border_tt\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">State-of-the-art</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">SpeechMatrix<sup class=\"ltx_sup\">&#8224;</sup>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">1456</th>\n<td class=\"ltx_td ltx_align_center\">8.3</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n<td class=\"ltx_td ltx_align_center\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">SpeechMatrix<sup class=\"ltx_sup\">&#8225;</sup>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">1456</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">12.18</span></td>\n<td class=\"ltx_td ltx_align_center\">35.08</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.16</span></td>\n<td class=\"ltx_td ltx_align_center\">3.51</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Baseline</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Local Mining</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">1250</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">11.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">37.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.46</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Global Mining</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">750</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">11.55</span></td>\n<td class=\"ltx_td ltx_align_center\">38.24</td>\n<td class=\"ltx_td ltx_align_center\">3.12</td>\n<td class=\"ltx_td ltx_align_center\">3.49</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Our Method</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Speech Vecalign</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">1000</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">11.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">38.34</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">3.16</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">3.53</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"2\">p-value w.r.t Local Mining</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.2468</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0829</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0010<sup class=\"ltx_sup\">*</sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0010<sup class=\"ltx_sup\">*</sup>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_bb\"/>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" colspan=\"2\">p-value w.r.t Global Mining</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.2907</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.2118</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.0160<sup class=\"ltx_sup\">*</sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.0120<sup class=\"ltx_sup\">*</sup>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "pvalue",
            "models",
            "ref",
            "speechmatrix‡",
            "vecalign",
            "entode",
            "baselines",
            "overall",
            "‡models",
            "underline",
            "ourselves",
            "speechmatrix†",
            "our",
            "baseline",
            "hours",
            "most",
            "fleurs",
            "†results",
            "show",
            "from",
            "training",
            "test",
            "trained",
            "text",
            "englishtogerman",
            "mining",
            "alignment",
            "than",
            "sets",
            "2023a",
            "metrics",
            "bold",
            "wrt",
            "perform",
            "under",
            "germantoenglish",
            "both",
            "results",
            "speech",
            "blaser",
            "stateoftheart",
            "asrbleu",
            "global",
            "means",
            "local",
            "better",
            "best",
            "data",
            "directions",
            "method",
            "duquenne",
            "detoen",
            "asrchrf2"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We provide evaluation results on the FLEURS test set in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.T5\" title=\"Table 5 &#8227; D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.\nSimilar to Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS5\" title=\"4.5 Main Results &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.5</span></a>, our results match or outperform SpeechMatrix results.\nFor both En-to-De and De-to-En, Speech Vecalign and Global Mining achieve comparable performance when using the transcription-based metrics ASR-BLEU and ASR-chrF2++.\nTheir performance is especially close on De-to-En.\nHowever, Speech Vecalign is significantly better than Global Mining when using the BLASER 2.0 metrics, achieving an improvement of 0.06 and 0.04 referenced BLASER 2.0 scores on En-to-De and De-to-En, respectively.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Speech Vecalign, a parallel speech document alignment method that monotonically aligns speech segment embeddings and does not depend on text transcriptions.\nCompared to the baseline method Global Mining&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>, a variant of speech mining, Speech Vecalign produces longer speech-to-speech alignments.\nIt also demonstrates greater robustness than Local Mining, another speech mining variant, as it produces less noise.\nWe applied Speech Vecalign to 3,000 hours of unlabeled parallel English-German&#160;(En-De) speech documents from VoxPopuli, yielding about 1,000 hours of high-quality alignments.\nWe then trained En-De speech-to-speech translation models on the aligned data.\nSpeech Vecalign improves the En-to-De and De-to-En performance over Global Mining by 0.37 and 0.18 ASR-BLEU, respectively.\nMoreover, our models match or outperform SpeechMatrix model performance, despite using 8 times fewer raw speech documents.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Data and code are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/mct10/Speech-Vecalign\" title=\"\">https://github.com/mct10/Speech-Vecalign</a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "models",
                    "detoen",
                    "vecalign",
                    "entode",
                    "our",
                    "hours",
                    "from",
                    "text",
                    "trained",
                    "mining",
                    "alignment",
                    "than",
                    "2023a",
                    "speech",
                    "asrbleu",
                    "global",
                    "local",
                    "data",
                    "method",
                    "duquenne",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "vecalign",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech-to-speech translation (S2ST) is the task of translating speech in one language into speech in another language.\nConventional S2ST systems concatenate automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) models <cite class=\"ltx_cite ltx_citemacro_cite\">Lavie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib27\" title=\"\">1997</a>); Nakamura et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib31\" title=\"\">2006</a>); Wahlster (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib46\" title=\"\">2013</a>)</cite>.\nThese components can be trained individually with datasets for the different components.\nDirect S2ST models, which translate source speech into target spectrograms or discrete units with a single architecture, have been recently proposed to alleviate error propagation and to reduce inference latency <cite class=\"ltx_cite ltx_citemacro_cite\">Jia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib21\" title=\"\">2019</a>); Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib28\" title=\"\">2022a</a>)</cite>.\nDespite the advantages, performance of direct models is limited by the amount of speech-to-speech aligned data, which is much more scarce than the data used for components of cascaded systems.</p>\n\n",
                "matched_terms": [
                    "models",
                    "than",
                    "data",
                    "speech",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There have been efforts to automatically curate alignments from multilingual <span class=\"ltx_text ltx_font_italic\">speech document</span>s.\nIn this paper, we define a <span class=\"ltx_text ltx_font_italic\">speech document</span> as a file containing more than one utterance and typically comprising several paragraphs, analogous to a <span class=\"ltx_text ltx_font_italic\">text document</span>.\nVoxPopuli&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib47\" title=\"\">2021a</a>)</cite> is one such corpus containing a large number of <span class=\"ltx_text ltx_font_italic\">parallel</span> speech documents, which are pairs of documents that have the same content but differ in language.</p>\n\n",
                "matched_terms": [
                    "than",
                    "text",
                    "speech",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech-to-speech alignment methods align short speech clips called <span class=\"ltx_text ltx_font_italic\">segment</span>s, and can be either transcription-based or transcription-free.\nWhen transcriptions are available, segments in parallel speech documents can be aligned through speech-to-text and text-to-text alignments.\nInspired by text mining&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Schwenk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib38\" title=\"\">2021</a>)</cite>, speech mining&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib9\" title=\"\">2021</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite> was proposed as a transcription-free method that aligns speech segments by finding segment pairs with the highest embedding similarity.\nIt scales well as it does not rely on the availability of text transcriptions.\nWhen speech mining is applied to a large amount of speech documents, as in all previous work, it is referred to as <span class=\"ltx_text ltx_font_bold\">Global Mining</span>.\nAnother variant, <span class=\"ltx_text ltx_font_bold\">Local Mining</span>, which applies speech mining to a single pair of parallel speech documents, has not been well explored.\nAs we formally define in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S2\" title=\"2 Speech Mining Overview &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, both Global Mining and Local Mining treat documents as bags of unordered segments.</p>\n\n",
                "matched_terms": [
                    "global",
                    "alignment",
                    "local",
                    "2023a",
                    "both",
                    "method",
                    "speech",
                    "duquenne",
                    "text",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since speech mining methods do not leverage the document pair structure, we wonder, <span class=\"ltx_text ltx_font_bold\">can we obtain better alignments by aligning speech segments within document pairs and preserving their time order?</span>\nThis allows us to utilize the extra knowledge that (1) segments within parallel document pairs are likely to be translations of each other, and (2) segment pairs right next to already aligned pairs are also likely to be aligned.\nWe draw inspiration from parallel <span class=\"ltx_text ltx_font_italic\">text</span> document alignment methods, which have been popular to create sentence-aligned bitext for training MT systems.\nUnlike mining, they align sentences for each document pair while maintaining the sentence order.\nOur work is based on the text alignment method Vecalign&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Thompson and Koehn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib43\" title=\"\">2019</a>)</cite>, which aligns parallel sentences by applying fast dynamic time warping&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Salvador and Chan (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib37\" title=\"\">2007</a>)</cite> to sentence embeddings.\nWith the advances of extending sentence embeddings to the speech modality&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib9\" title=\"\">2021</a>)</cite>, we can readily apply Vecalign to parallel speech documents.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "vecalign",
                    "better",
                    "mining",
                    "from",
                    "training",
                    "method",
                    "speech",
                    "duquenne",
                    "text",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce Speech Vecalign, a method that aligns parallel speech documents using speech segment embeddings.\nInstead of mining from bags of segments, our method aligns individual document pairs and maintains the chronological order of segments, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nAdditional preprocessing and postprocessing strategies are applied to improve alignment quality.\nWe compare Speech Vecalign with Local Mining and Global Mining and show that Speech Vecalign produces higher-quality alignments.\nWe further provide extensive analysis for all three methods, which could be useful for future research.</p>\n\n",
                "matched_terms": [
                    "our",
                    "global",
                    "alignment",
                    "local",
                    "vecalign",
                    "show",
                    "from",
                    "method",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We formally describe the speech mining methods in this section.\nOther related work is in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A1\" title=\"Appendix A Related Work &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech Mining, first proposed by <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib9\" title=\"\">2021</a>)</cite>, encodes speech segments into language- and modality-agnostic fixed-size embeddings, and then uses margin-based similarity search <cite class=\"ltx_cite ltx_citemacro_cite\">Artetxe and Schwenk (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib2\" title=\"\">2019a</a>)</cite> to find the closest embedding pairs.\nDepending on the search scope, it can be categorized as Global Mining or Local Mining.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "speech",
                    "duquenne",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Raw data</span>. The input data is a list of speech documents <math alttext=\"X=[X_{1},X_{2},\\ldots,X_{n}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>X</mi><mn>1</mn></msub><mo>,</mo><msub><mi>X</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>X</mi><mi>n</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">X=[X_{1},X_{2},\\ldots,X_{n}]</annotation></semantics></math> in the source language and a list <math alttext=\"Y=[Y_{1},Y_{2},\\ldots,Y_{m}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m2\" intent=\":literal\"><semantics><mrow><mi>Y</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>Y</mi><mn>1</mn></msub><mo>,</mo><msub><mi>Y</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>Y</mi><mi>m</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Y=[Y_{1},Y_{2},\\ldots,Y_{m}]</annotation></semantics></math> in the target language, where <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m3\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> and <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m4\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> are the numbers of documents.\nEach document can contain between a few seconds to a few hours of speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Bag of embeddings</span>.\nIn <span class=\"ltx_text ltx_font_italic\">Global Mining</span>, embeddings are grouped by <span class=\"ltx_text ltx_font_bold\">language</span>.\nWe define <math alttext=\"G_{X}=\\left\\{E_{\\tilde{X}_{1}},E_{\\tilde{X}_{2}},\\ldots,E_{\\tilde{X}_{n}}\\right\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m1\" intent=\":literal\"><semantics><mrow><msub><mi>G</mi><mi>X</mi></msub><mo>=</mo><mrow><mo>{</mo><msub><mi>E</mi><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mn>1</mn></msub></msub><mo>,</mo><msub><mi>E</mi><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mn>2</mn></msub></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>E</mi><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mi>n</mi></msub></msub><mo>}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">G_{X}=\\left\\{E_{\\tilde{X}_{1}},E_{\\tilde{X}_{2}},\\ldots,E_{\\tilde{X}_{n}}\\right\\}</annotation></semantics></math> and <math alttext=\"G_{Y}=\\left\\{E_{\\tilde{Y}_{1}},E_{\\tilde{Y}_{2}},\\ldots,E_{\\tilde{Y}_{m}}\\right\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m2\" intent=\":literal\"><semantics><mrow><msub><mi>G</mi><mi>Y</mi></msub><mo>=</mo><mrow><mo>{</mo><msub><mi>E</mi><msub><mover accent=\"true\"><mi>Y</mi><mo>~</mo></mover><mn>1</mn></msub></msub><mo>,</mo><msub><mi>E</mi><msub><mover accent=\"true\"><mi>Y</mi><mo>~</mo></mover><mn>2</mn></msub></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>E</mi><msub><mover accent=\"true\"><mi>Y</mi><mo>~</mo></mover><mi>m</mi></msub></msub><mo>}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">G_{Y}=\\left\\{E_{\\tilde{Y}_{1}},E_{\\tilde{Y}_{2}},\\ldots,E_{\\tilde{Y}_{m}}\\right\\}</annotation></semantics></math>, where <math alttext=\"G_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m3\" intent=\":literal\"><semantics><msub><mi>G</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">G_{X}</annotation></semantics></math> collects all segment embeddings in the source language and <math alttext=\"G_{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m4\" intent=\":literal\"><semantics><msub><mi>G</mi><mi>Y</mi></msub><annotation encoding=\"application/x-tex\">G_{Y}</annotation></semantics></math> collects those in the target language.\nIn <span class=\"ltx_text ltx_font_italic\">Local Mining</span>, embeddings are grouped by <span class=\"ltx_text ltx_font_bold\">document pairs</span>.\nSuppose there are <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m5\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> parallel documents, with <math alttext=\"X_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m6\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">X_{i}</annotation></semantics></math> paired with <math alttext=\"Y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m7\" intent=\":literal\"><semantics><msub><mi>Y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">Y_{i}</annotation></semantics></math> for <math alttext=\"1\\leq i\\leq s\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m8\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>&#8804;</mo><mi>i</mi><mo>&#8804;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">1\\leq i\\leq s</annotation></semantics></math>.\nDocuments without a parallel one are ignored.\nIn this case, <math alttext=\"E_{\\tilde{X}_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m9\" intent=\":literal\"><semantics><msub><mi>E</mi><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mi>i</mi></msub></msub><annotation encoding=\"application/x-tex\">E_{\\tilde{X}_{i}}</annotation></semantics></math> and <math alttext=\"E_{\\tilde{Y_{j}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m10\" intent=\":literal\"><semantics><msub><mi>E</mi><mover accent=\"true\"><msub><mi>Y</mi><mi>j</mi></msub><mo>~</mo></mover></msub><annotation encoding=\"application/x-tex\">E_{\\tilde{Y_{j}}}</annotation></semantics></math> are bags of embeddings themselves.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Embedding alignment</span>.\nSpeech mining is performed by finding the most similar embedding pairs between two bags of segment embeddings.\nThe margin-based similarity, or margin-score, between any two embeddings <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m1\" intent=\":literal\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math> and <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m2\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> is computed as</p>\n\n",
                "matched_terms": [
                    "speech",
                    "most",
                    "alignment",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m3\" intent=\":literal\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math> and <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m4\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> are in different languages and <math alttext=\"\\text{NN}_{k}(a)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m5\" intent=\":literal\"><semantics><mrow><msub><mtext>NN</mtext><mi>k</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{NN}_{k}(a)</annotation></semantics></math> denotes <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m6\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> nearest neighbors of <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m7\" intent=\":literal\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math> in the other language.\nThe denominator combats the hubness problem.\nA higher margin-score indicates better quality.\nThen, the mining function for embedding <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m8\" intent=\":literal\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math> from a bag of embeddings <math alttext=\"B\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m9\" intent=\":literal\"><semantics><mi>B</mi><annotation encoding=\"application/x-tex\">B</annotation></semantics></math> is</p>\n\n",
                "matched_terms": [
                    "from",
                    "better",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More generally, given two bags of embeddings <math alttext=\"U=\\{u_{1},u_{2},\\ldots,u_{l_{u}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m10\" intent=\":literal\"><semantics><mrow><mi>U</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>u</mi><mn>1</mn></msub><mo>,</mo><msub><mi>u</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>u</mi><msub><mi>l</mi><mi>u</mi></msub></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">U=\\{u_{1},u_{2},\\ldots,u_{l_{u}}\\}</annotation></semantics></math> and <math alttext=\"V=\\{v_{1},v_{2},\\ldots,v_{l_{v}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m11\" intent=\":literal\"><semantics><mrow><mi>V</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>v</mi><mn>1</mn></msub><mo>,</mo><msub><mi>v</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>v</mi><msub><mi>l</mi><mi>v</mi></msub></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">V=\\{v_{1},v_{2},\\ldots,v_{l_{v}}\\}</annotation></semantics></math>, where <math alttext=\"l_{u}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m12\" intent=\":literal\"><semantics><msub><mi>l</mi><mi>u</mi></msub><annotation encoding=\"application/x-tex\">l_{u}</annotation></semantics></math> and <math alttext=\"l_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m13\" intent=\":literal\"><semantics><msub><mi>l</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">l_{v}</annotation></semantics></math> are number of embeddings, the collection of all speech mining alignments is</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we define Local Mining and Global Mining as</p>\n\n",
                "matched_terms": [
                    "global",
                    "mining",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Speech Vecalign pipeline consists of three steps:\nspeech preprocessing (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>), segment alignment with Vecalign (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS2\" title=\"3.2 Speech Segment Alignment &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), and alignment postprocessing (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS3\" title=\"3.3 Alignment Postprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>).\nAn illustration of our method is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "vecalign",
                    "method",
                    "speech",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Segmentation</span>. Same as speech mining, we first segment each speech document by VAD.\nWe apply Silero VAD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Silero Team (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib41\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We perform segment alignment based on the similarity between speech segment embeddings.\nUnlike speech mining, which solely relies on similarity scores, we use a dynamic programming&#160;(DP) algorithm to align segments in chronological order.</p>\n\n",
                "matched_terms": [
                    "perform",
                    "speech",
                    "alignment",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Segment concatenation</span>.\nSpeech segments do not necessarily correspond to complete sentences.\nSame as speech mining, we first progressively concatenate each segment with the subsequent ones.\nEach concatenated segment can contain up to 5 original segments and span a maximum of 20 seconds.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Obtaining segment embeddings</span>.\nAfter concatenations, we obtain speech segment embeddings using SpeechLASER models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib9\" title=\"\">2021</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nIdentical untranslated segments detected in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, along with all concatenated segments that include them, are skipped and replaced with <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><mn>0</mn></math>-valued vectors.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models",
                    "duquenne",
                    "2023a"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The embedding alignment algorithm is recursive DP.\nGiven a document pair and corresponding embeddings, the algorithm recursively averages every two consecutive embeddings, halving the sequence length until it reaches a small threshold.\nAt the bottom level, standard DP is applied to obtain an initial alignment.\nSubsequently, at each recursion level bottom-up, DP refines the alignment by searching within a small window around the alignment path from the previous level.\nBy constraining the search space and reducing the sequence length at each level, the algorithm achieves a linear time and space complexity.\nThe recursive DP algorithm runs on CPU and takes a few seconds on average per document pair.\nWe direct the readers to <cite class=\"ltx_cite ltx_citemacro_citet\">Thompson and Koehn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib43\" title=\"\">2019</a>)</cite> for a complete description.</p>\n\n",
                "matched_terms": [
                    "from",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because of DP, the resultant alignments strictly follow chronological order.\nWe use <math alttext=\"x_{a:b}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m1\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mrow><mi>a</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">x_{a:b}^{i}</annotation></semantics></math> to denote the concatenation of consecutive segments <math alttext=\"x_{a}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m2\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mi>a</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">x_{a}^{i}</annotation></semantics></math> through <math alttext=\"x_{b}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m3\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mi>b</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">x_{b}^{i}</annotation></semantics></math>.\nFor any two alignments <math alttext=\"(x_{a_{s}:a_{e}}^{i},y_{b_{s}:b_{e}}^{j})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mrow><msub><mi>a</mi><mi>s</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>a</mi><mi>e</mi></msub></mrow><mi>i</mi></msubsup><mo>,</mo><msubsup><mi>y</mi><mrow><msub><mi>b</mi><mi>s</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>b</mi><mi>e</mi></msub></mrow><mi>j</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{a_{s}:a_{e}}^{i},y_{b_{s}:b_{e}}^{j})</annotation></semantics></math> and <math alttext=\"(x_{c_{s}:c_{e}}^{i},y_{d_{s}:d_{e}}^{k})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mrow><msub><mi>c</mi><mi>s</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>c</mi><mi>e</mi></msub></mrow><mi>i</mi></msubsup><mo>,</mo><msubsup><mi>y</mi><mrow><msub><mi>d</mi><mi>s</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>d</mi><mi>e</mi></msub></mrow><mi>k</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{c_{s}:c_{e}}^{i},y_{d_{s}:d_{e}}^{k})</annotation></semantics></math>, Speech Vecalign guarantees that <math alttext=\"i=j=k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m6\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>=</mo><mi>j</mi><mo>=</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">i=j=k</annotation></semantics></math> and that either <math alttext=\"a_{e}&lt;c_{s},b_{e}&lt;d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m7\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>a</mi><mi>e</mi></msub><mo>&lt;</mo><msub><mi>c</mi><mi>s</mi></msub></mrow><mo>,</mo><mrow><msub><mi>b</mi><mi>e</mi></msub><mo>&lt;</mo><msub><mi>d</mi><mi>s</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">a_{e}&lt;c_{s},b_{e}&lt;d_{s}</annotation></semantics></math> or <math alttext=\"a_{s}&gt;c_{e},b_{s}&gt;d_{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m8\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>a</mi><mi>s</mi></msub><mo>&gt;</mo><msub><mi>c</mi><mi>e</mi></msub></mrow><mo>,</mo><mrow><msub><mi>b</mi><mi>s</mi></msub><mo>&gt;</mo><msub><mi>d</mi><mi>e</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">a_{s}&gt;c_{e},b_{s}&gt;d_{e}</annotation></semantics></math>.\nIn contrast, Local Mining ensures <math alttext=\"i=j=k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m9\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>=</mo><mi>j</mi><mo>=</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">i=j=k</annotation></semantics></math> but has no constraints on <math alttext=\"a,b,c,d\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m10\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi>c</mi><mo>,</mo><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">a,b,c,d</annotation></semantics></math>, while\nGlobal Mining makes no guarantees at all.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Alignment concatenation</span>.\nAnother issue is that the raw alignments are too short: the average duration is 4.25 seconds, with 66% shorter than 5 seconds.\nTo cover more context, we progressively concatenate each alignment with the subsequent ones.\nThis can be easily done as alignments are in chronological order.\nEach concatenated alignment can contain up to 3 original alignments and span up to 20 seconds.</p>\n\n",
                "matched_terms": [
                    "than",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Global margin-scores computation</span>.\nThe raw alignments only have alignment costs as a quality indicator, which are computed <span class=\"ltx_text ltx_font_italic\">within</span> each document pair.\nTo assess alignment quality <span class=\"ltx_text ltx_font_italic\">across</span> document pairs, we train FAISS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Johnson et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib22\" title=\"\">2019</a>)</cite> indexes and compute margin-scores&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Artetxe and Schwenk (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib2\" title=\"\">2019a</a>)</cite> using Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S2.E1\" title=\"In 2 Speech Mining Overview &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> for <span class=\"ltx_text ltx_font_italic\">all</span> obtained alignments, following the common strategy in MT dataset curation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Sloto et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib42\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Removing highly-overlapped alignments</span>.\nFinally, we remove alignments that have too much overlap with others, following <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nFor any two consecutive alignments, we compute the ratio of the overlapped source duration to the maximum duration of the two source segments.\nIf the ratio exceeds a threshold, we discard the one with a lower margin-score.\nWe train S2ST models with multiple threshold values to determine the best one.\nOur experiments in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.SS1\" title=\"D.1 Optimizing &#119898;&#8290;&#119886;&#8290;&#119909;&#8290;_&#8290;&#119900;&#8290;&#119907;&#8290;&#119890;&#8290;&#119903;&#8290;&#119897;&#8290;&#119886;&#8290;&#119901; &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">D.1</span></a> suggest that 0.4 work best for Global Mining and 0.8 work best for Local Mining and Speech Vecalign.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "global",
                    "local",
                    "vecalign",
                    "2023a",
                    "best",
                    "speech",
                    "duquenne",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We apply Speech Vecalign, Global Mining, and Local Mining to the same raw data and train S2ST models on each type of alignments, providing a fair comparison.</p>\n\n",
                "matched_terms": [
                    "global",
                    "models",
                    "local",
                    "vecalign",
                    "data",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data source.</span>\nWe use the unlabeled, unsegmented English and German plenary session recordings from VoxPopuli v1&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib47\" title=\"\">2021a</a>)</cite> as raw data.\nVoxPopuli contains European Parliament plenary session recordings in each of the 23 European Union languages, paired with spoken interpretations into the other languages.\nThe document names are formatted as <code class=\"ltx_verbatim ltx_font_typewriter\">${session_id}_${language}.ogg</code>, and paired documents have the same <code class=\"ltx_verbatim ltx_font_typewriter\">${session_id}</code>.\nTo avoid overlapping with the test set (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS2\" title=\"4.2 Evaluation Data &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>), we only choose sessions from year 2013 to 2020.\nWe also exclude sessions in the development set&#160;(Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS2\" title=\"4.2 Evaluation Data &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>).\nFor En-to-De, the remaining data has 4,880 documents totaling about 3,000 hours for each language.\nFor De-to-En, there are 5,782 documents totaling 3,400 hours per language.\nThe difference is due to the different dev and test sets.\nAll documents are in pairs, allowing all methods to have exactly the same raw data.</p>\n\n",
                "matched_terms": [
                    "hours",
                    "sets",
                    "entode",
                    "data",
                    "from",
                    "test",
                    "detoen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Vecalign.</span>\nWe apply Speech Vecalign to each pair of speech documents and obtain alignments sorted by margin-scores.\nTraining data is chosen in descending order of margin-scores.\nWe train models on different data sizes and report the best results in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS5\" title=\"4.5 Main Results &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.5</span></a>.\nMore details on data size optimization can be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.SS2\" title=\"D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "vecalign",
                    "best",
                    "data",
                    "training",
                    "results",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech mining baselines.</span>\nWe apply Global Mining and Local Mining to the same raw data and embeddings as Speech Vecalign.\nThe implementation is based on <span class=\"ltx_text ltx_font_typewriter\">stopes<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">3</span></span><a class=\"ltx_ref ltx_url\" href=\"https://github.com/facebookresearch/stopes\" title=\"\">https://github.com/facebookresearch/stopes</a></span></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Andrews et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib1\" title=\"\">2022</a>)</cite>.\nAfter mining, we apply the same postprocessing strategies in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS3\" title=\"3.3 Alignment Postprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>, except for alignment concatenation which is not applicable.\nTraining data is chosen in descending order of margin-scores and details on data size optimization can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.SS2\" title=\"D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "alignment",
                    "local",
                    "vecalign",
                    "baselines",
                    "data",
                    "training",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Development set.</span>\nFollowing <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>, we choose 1000 samples from the highest scored sessions from the Voxpopuli S2ST dataset.\nAdditionally, we avoid choosing sessions that occur on the same dates as the test set.</p>\n\n",
                "matched_terms": [
                    "duquenne",
                    "2023a",
                    "from",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Test set.</span>\nWe use the Europarl-ST (EPST) test set&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Iranzo-S&#225;nchez et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib18\" title=\"\">2020</a>)</cite> as an in-domain test set to evaluate the S2ST models.\nEPST is a multilingual S2TT dataset built on European Parliament debates from year 2008 to 2012.\nWe also adopt FLEURS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Conneau et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib6\" title=\"\">2023</a>)</cite> as an out-of-domain test set.</p>\n\n",
                "matched_terms": [
                    "models",
                    "fleurs",
                    "from",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train speech-to-unit translation (S2UT) models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib28\" title=\"\">2022a</a>)</cite> with <span class=\"ltx_text ltx_font_typewriter\">fairseq<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">4</span></span><a class=\"ltx_ref ltx_url\" href=\"https://github.com/facebookresearch/fairseq\" title=\"\">https://github.com/facebookresearch/fairseq</a></span></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">Ott et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib32\" title=\"\">2019</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib48\" title=\"\">2020</a>)</cite> on each type of alignments.\nThe S2UT model takes source speech as input and predicts a sequence of target discrete units.\nThe discrete units are obtained by applying\na k-means model to the <math alttext=\"11^{\\text{th}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><msup><mn>11</mn><mtext>th</mtext></msup><annotation encoding=\"application/x-tex\">11^{\\text{th}}</annotation></semantics></math> layer features of a HuBERT model <cite class=\"ltx_cite ltx_citemacro_cite\">Hsu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib16\" title=\"\">2021</a>)</cite>.\nFor English, we use the mHuBERT from <cite class=\"ltx_cite ltx_citemacro_citet\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib29\" title=\"\">2022b</a>)</cite>, and for German, we use the Germanic mHuBERT from <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nConsecutive duplicated units are removed.\nOur S2UT model architecture follows exactly <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nThe architecture details and training hyperparameters are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A2\" title=\"Appendix B Speech-to-Speech Translation &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "2023a",
                    "from",
                    "training",
                    "speech",
                    "duquenne",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the discrete units generated by S2UT models, we resynthesize speech using pretrained unit-based HiFi-GAN vocoders <cite class=\"ltx_cite ltx_citemacro_cite\">Polyak et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib33\" title=\"\">2021</a>)</cite> from <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nWe then evaluate the resynthesized speech using both transcription-based and transcription-free methods.</p>\n\n",
                "matched_terms": [
                    "models",
                    "2023a",
                    "from",
                    "both",
                    "speech",
                    "duquenne"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the transcription-based method, we transcribe the speech output using the same ASR models as <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nWe evaluate the transcriptions using SacreBLEU<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/mjpost/sacrebleu\" title=\"\">https://github.com/mjpost/sacrebleu</a></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">Post (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib34\" title=\"\">2018</a>)</cite> to compute BLEU<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Signature: nrefs:1 + case:mixed + eff:no\n+ tok:13a + smooth:exp + version:2.2.0</span></span></span> and chrF2++<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>Signature: nrefs:1 + case:mixed + eff:yes + nc:6 + nw:2 + space:no + version:2.2.0</span></span></span> scores.\nWe apply the significance test using paired bootstrap resampling&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Koehn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib26\" title=\"\">2004</a>)</cite> with 1000 bootstrap resamples.</p>\n\n",
                "matched_terms": [
                    "models",
                    "2023a",
                    "method",
                    "test",
                    "speech",
                    "duquenne"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also adopt BLASER 2.0 <cite class=\"ltx_cite ltx_citemacro_cite\">Dale and Costa-juss&#224; (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib7\" title=\"\">2024</a>)</cite> to directly evaluate speech output.\nWe compute the referenced score using <code class=\"ltx_verbatim ltx_font_typewriter\">blaser-2.0-ref</code><span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/blaser-2.0-ref\" title=\"\">https://huggingface.co/facebook/blaser-2.0-ref</a></span></span></span> for input and output speech, as well as the text reference.\nWe compute the reference-free score using <code class=\"ltx_verbatim ltx_font_typewriter\">blaser-2.0-qe</code><span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/blaser-2.0-qe\" title=\"\">https://huggingface.co/facebook/blaser-2.0-qe</a></span></span></span> for input and output speech only.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "blaser",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As mentioned in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS1\" title=\"4.1 Training Data &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.1</span></a>, we train models on data of various sizes.\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.T1\" title=\"Table 1 &#8227; 4.3 Experiment Setup &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the best En-to-De and De-to-En results on the EPST test set, along with the corresponding data sizes.\nAdditional results on the FLEURS test set are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A5\" title=\"Appendix E Evaluation Results on FLEURS &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "fleurs",
                    "entode",
                    "best",
                    "data",
                    "results",
                    "test",
                    "detoen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Intriguingly, for both directions, Speech Vecalign and speech mining models are competitive with or outperform SpeechMatrix&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite> models, despite the latter being mined from about 24k hours of speech per language, <span class=\"ltx_text ltx_font_italic\">8 times more</span> than our raw data.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>We do not aim for state-of-the-art performance. Our results are not directly comparable to SpeechMatrix. We report SpeechMatrix results only to show the performance gap.</span></span></span>\nFor En-to-De, our Global Mining and Speech Vecalign models achieve improvements of 0.94 and 1.31 BLEU, respectively.\nOur Local Mining model achieves even 1.64 BLEU improvement.\nWe suspect that SpeechMatrix has not removed identical untranslated segments prior to and after mining, which significantly hurts model performance.\nFurther discussion is in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.SS5\" title=\"5.5 Removing Identical Untranslated Segments is Critical &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5.5</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "global",
                    "hours",
                    "local",
                    "than",
                    "vecalign",
                    "2023a",
                    "entode",
                    "mining",
                    "show",
                    "from",
                    "both",
                    "results",
                    "directions",
                    "speech",
                    "stateoftheart",
                    "duquenne",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While Local Mining has not been previously explored, our results suggest that it is a potentially useful method.\nLocal Mining achieves the highest BLEU score in En-to-De, and only slightly underperforms Global Mining in De-to-En, indicating that constraining the mining scope to document pairs does not necessarily have a negative impact on alignment quality.\nYet we note that Local Mining requires more training data to achieve its optimal performance, as shown in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.SS2\" title=\"D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "alignment",
                    "local",
                    "entode",
                    "mining",
                    "data",
                    "training",
                    "results",
                    "method",
                    "detoen",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our Speech Vecalign models outperform both speech mining models in both directions.\nFor En-to-De, the Speech Vecalign model achieves 12.58 BLEU, comparable with our strong Global Mining and Local Mining baselines.\nIn terms of chrF2++, it surpasses Global Mining and Local Mining by 1.69 and 0.26, respectively.\nIt also significantly improves their referenced BLASER 2.0 by 0.08 and 0.03.\nFor De-to-En, Speech Vecalign and Global Mining models achieve comparable BLEU (16.14 vs. 15.96), but Speech Vecalign surpasses Global Mining by 0.57 in chrF2++.\nSpeech Vecalign significantly outperforms Local Mining under all metrics.\nThese results demonstrate that Speech Vecalign produces higher-quality alignments than both speech mining baselines.</p>\n\n",
                "matched_terms": [
                    "models",
                    "global",
                    "local",
                    "than",
                    "vecalign",
                    "metrics",
                    "entode",
                    "baselines",
                    "mining",
                    "under",
                    "both",
                    "results",
                    "directions",
                    "speech",
                    "blaser",
                    "detoen",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We analyze properties of speech mining methods and compare them with Speech Vecalign.\nAlthough we show that speech mining methods produce alignments similar to those of Speech Vecalign, the latter offers advantages of producing longer and less noisy alignments.</p>\n\n",
                "matched_terms": [
                    "show",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, we show that Global Mining mostly <span class=\"ltx_text ltx_font_bold\">locally</span> aligns speech documents.\nWhile Global Mining searches for the best matching segment pairs among roughly 10 million segments, one might expect its alignments to cover the spread of the entire dataset.\nOn the contrary, we find that Global Mining alignments are concentrated within document pairs, each typically containing hundreds to thousands of segments.</p>\n\n",
                "matched_terms": [
                    "global",
                    "best",
                    "show",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To quantify this, we examine the 1000-hour Global Mining data and count alignments whose source and target segments come from <span class=\"ltx_text ltx_font_italic\">different</span> document pairs.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F3\" title=\"Figure 3 &#8227; 5.1 Speech Mining Mostly Locally Aligns Segments in Time Order &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, fewer than 6% fall into this category, while the majority&#160;(<math alttext=\"&gt;93\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mrow><mn>93</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">&gt;93\\%</annotation></semantics></math>) are within paired documents.</p>\n\n",
                "matched_terms": [
                    "global",
                    "than",
                    "data",
                    "from",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, we analyze the time order of alignments produced by both speech mining methods.\nBorrowing the notation from Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS2\" title=\"3.2 Speech Segment Alignment &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, we define two pairs of alignments to be <span class=\"ltx_text ltx_font_italic\">in-order</span> if either <math alttext=\"a_{e}&lt;c_{s},b_{e}&lt;d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>a</mi><mi>e</mi></msub><mo>&lt;</mo><msub><mi>c</mi><mi>s</mi></msub></mrow><mo>,</mo><mrow><msub><mi>b</mi><mi>e</mi></msub><mo>&lt;</mo><msub><mi>d</mi><mi>s</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">a_{e}&lt;c_{s},b_{e}&lt;d_{s}</annotation></semantics></math> or <math alttext=\"a_{s}&gt;c_{e},b_{s}&gt;d_{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>a</mi><mi>s</mi></msub><mo>&gt;</mo><msub><mi>c</mi><mi>e</mi></msub></mrow><mo>,</mo><mrow><msub><mi>b</mi><mi>s</mi></msub><mo>&gt;</mo><msub><mi>d</mi><mi>e</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">a_{s}&gt;c_{e},b_{s}&gt;d_{e}</annotation></semantics></math>; otherwise, they are <span class=\"ltx_text ltx_font_italic\">out-of-order</span>.\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F3\" title=\"Figure 3 &#8227; 5.1 Speech Mining Mostly Locally Aligns Segments in Time Order &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows that only around 1% alignments are out-of-order for both speech mining methods.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "from",
                    "both",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Observations above indicate that speech mining alignments are mostly within paired documents and preserve time order.\nWe hypothesize that speech-to-speech alignments are sparse and high-quality ones mostly exist in paired documents.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a by-product, this property can be leveraged to identify parallel documents.\nIf Global Mining finds many alignments between two documents, they are likely to be parallel.\nIt is particularly useful when the pairing metadata is not readily available.</p>\n\n",
                "matched_terms": [
                    "global",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the observations in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.SS1\" title=\"5.1 Speech Mining Mostly Locally Aligns Segments in Time Order &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> that speech mining produces mostly local, in-order alignments, we analyze the similarity between them and Speech Vecalign alignments.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mining",
                    "vecalign",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ the alignment evaluation method<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/thompsonb/vecalign/blob/master/score.py\" title=\"\">https://github.com/thompsonb/vecalign/blob/master/score.py</a>.</span></span></span> from&#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Thompson and Koehn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib43\" title=\"\">2019</a>)</cite>, which computes precision and recall by comparing system alignments to a reference.\nThere are two modes: <span class=\"ltx_text ltx_font_italic\">Strict</span>, which counts only exact matches as true positives, and <span class=\"ltx_text ltx_font_italic\">Lax</span>, which considers an alignment as true positive if both its source and target segment overlap with the reference.\nIf not true positive, an alignment is false positive.\nRecall is computed by swapping the reference and the system alignments.</p>\n\n",
                "matched_terms": [
                    "both",
                    "from",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Without loss of generality, we use Speech Vecalign En-De alignments as the reference, and evaluate speech mining ones.\nWe choose 700k highest-scoring alignments from all three methods to ensure a fair comparison.\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.T2\" title=\"Table 2 &#8227; 5.2 Speech Mining Methods Produce Similar Alignments as Speech Vecalign &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows that about 30% of speech mining alignments are exactly the same as those of Speech Vecalign, and about 90% overlap with Speech Vecalign alignments.\nThis high similarity explains why Speech Vecalign and speech mining models have similar performance.</p>\n\n",
                "matched_terms": [
                    "models",
                    "vecalign",
                    "from",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As speech mining and Speech Vecalign produce similar alignments, we explore why Speech Vecalign models still perform better.\nA key advantage of Speech Vecalign is that it first produces fine-grained alignments and then constructs alignments with different amounts of context, thanks to the alignment concatenation strategy.\nSpeech mining methods, on the other hand, solely depend on margin-scores and tend to favor shorter alignments.</p>\n\n",
                "matched_terms": [
                    "models",
                    "alignment",
                    "vecalign",
                    "better",
                    "perform",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the best En-to-De models and corresponding data sizes from Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS5\" title=\"4.5 Main Results &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.5</span></a>, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F4\" title=\"Figure 4 &#8227; 5.3 Speech Vecalign Produces Longer Alignments &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the average sentence-level chrF2++ scores on the test set and the percentage of training alignments for different source speech duration ranges.\nNotably, Speech Vecalign has a large portion of long training samples: the blue bars are highest for durations longer than 12 seconds.\nSpecifically, the average source duration of Speech Vecalign is 8.51 seconds, while Global Mining and Local Mining have average durations of 7.50 and 8.53 seconds, respectively.\nAs a result, the Speech Vecalign model performs better on test samples longer than 10 seconds, while having comparable performance on shorter ones.\nThis highlights that Speech Vecalign is able to produce longer, context-rich alignments which help to improve S2ST model performance.\nInterestingly, Local Mining surpasses the Global Mining model on long inputs, which could be also attributed to its longer training samples.</p>\n\n",
                "matched_terms": [
                    "models",
                    "global",
                    "local",
                    "than",
                    "vecalign",
                    "better",
                    "entode",
                    "best",
                    "data",
                    "from",
                    "training",
                    "test",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We visualize alignments produced by different methods for the same document pair, which is about 10 minutes long and contains around 200 segments.\nFor reference, we manually created a gold segment-level alignment, with detailed procedure in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A6\" title=\"Appendix F Procedure of Manual Alignments &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>.\nWe illustrate the best 80 alignments for each of the speech mining methods.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "alignment",
                    "best",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F5\" title=\"Figure 5 &#8227; 5.3 Speech Vecalign Produces Longer Alignments &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows, Speech Vecalign produces the most fine-grained alignments and is most similar to the gold reference.\nGlobal Mining also performs well, aligning closely with the groundtruth path, whereas Local Mining produces more noise and misses more alignments along the correct path.\nWe hypothesize Local Mining has limited number of segments in a single document pair, making nearest neighbors less effective normalizers in the margin-score computation.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "vecalign",
                    "most",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As presented in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS5\" title=\"4.5 Main Results &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.5</span></a>, our reproduced speech mining models achieve comparable or even better results than SpeechMatrix models.\nBy listening to samples of SpeechMatrix alignments, we observed many cases where the source and target segments contained identical untranslated content, which is an issue mentioned in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>.\nUsing the method described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, we identified approximately 100k out of 630k alignments with untranslated source and target segments, totaling 181 hours.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "hours",
                    "than",
                    "better",
                    "results",
                    "method",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the impact of untranslated segments, we trained models on the original SpeechMatrix En-De alignments and on a version with untranslated alignments <span class=\"ltx_text ltx_font_italic\">removed</span>.\nThe training data is chosen with a margin-score threshold of 1.09, following the original setup.\nAs shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.T3\" title=\"Table 3 &#8227; 5.5 Removing Identical Untranslated Segments is Critical &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the cleaned data produces better models, improving BLEU score by <span class=\"ltx_text ltx_font_bold\">1.00</span> for En-to-De and <span class=\"ltx_text ltx_font_bold\">0.11</span> for De-to-En, despite having 13% less training data.\nThe smaller gain on De-to-En may be due to most untranslated segments being in English, which have smaller impact on into-English translation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "most",
                    "better",
                    "entode",
                    "data",
                    "training",
                    "detoen",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also re-produced our alignment pipelines <span class=\"ltx_text ltx_font_italic\">without</span> removing identical untranslated segments, referred to as &#8220;noisy\" in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.T4\" title=\"Table 4 &#8227; 5.5 Removing Identical Untranslated Segments is Critical &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nWe trained models on 500 hours of this data.\nAlthough these untranslated segments account for less than 1% of the training data, performance degrades noticeably.</p>\n\n",
                "matched_terms": [
                    "models",
                    "hours",
                    "alignment",
                    "than",
                    "data",
                    "training",
                    "trained",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, the experiments highlight that removing untranslated alignments is essential for S2ST training, corroborating <cite class=\"ltx_cite ltx_citemacro_citet\">Khayrallah and Koehn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib23\" title=\"\">2018</a>)</cite>, who found that the untranslated sentences are most catastrophic in neural machine translation.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "most",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present Speech Vecalign, a parallel speech document alignment method that aligns speech segment embeddings within document pairs and in chronological order.\nWe apply Speech Vecalign to parallel English-German VoxPopuli speech documents and conduct S2ST experiments to demonstrate its superiority over two strong speech mining baselines.\nOur analysis reveals that although speech mining methods primarily align documents locally and in-order, Global Mining falls short of producing long alignments, and Local Mining in particular produces more noise.\nFor long-term future work, we plan to extend Speech Vecalign to other language pairs or other data sources.\nWe can also explore aligning speech and text embeddings to construct S2TT datasets.</p>\n\n",
                "matched_terms": [
                    "our",
                    "global",
                    "alignment",
                    "local",
                    "vecalign",
                    "baselines",
                    "data",
                    "method",
                    "speech",
                    "text",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech features for identical untranslated segment detection could be improved.</span>\nOur current approach uses filterbank features, which are based on power spectrum, to detect identical untranslated segments.\nHowever, filterbank features are likely to fail for segments that have identical content but differ in signal power.\nAs one of the anonymous reviewers pointed out, cepstral features may be a more robust alternative.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limited language pair.</span>\nWe have only conducted experiments for English and German speech from the VoxPopuli dataset.\nAs Speech Vecalign heavily relies on the quality of speech embeddings, the performance is unclear for other language pairs and other domains of speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "vecalign",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dependency on parallel speech documents.</span>\nSpeech Vecalign requires parallel speech documents, which is often not available.\nWe may rely on Global Mining to discover parallel documents, as Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.SS1\" title=\"5.1 Speech Mining Mostly Locally Aligns Segments in Time Order &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> suggests, but doing so will introduce extra computation costs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "global",
                    "vecalign",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-to-speech translation (S2ST).</span>\nThe early S2ST systems consist of cascaded ASR, MT, and TTS models <cite class=\"ltx_cite ltx_citemacro_cite\">Lavie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib27\" title=\"\">1997</a>); Nakamura et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib31\" title=\"\">2006</a>); Wahlster (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib46\" title=\"\">2013</a>)</cite>.\nDirect S2ST models have recently been proposed to alleviate error propagation, support unwritten languages, and improve inference speed.\nTranslatotron models <cite class=\"ltx_cite ltx_citemacro_cite\">Jia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib21\" title=\"\">2019</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib19\" title=\"\">2022a</a>)</cite> are trained with spectrograms as targets, while the S2UT model&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib28\" title=\"\">2022a</a>)</cite> outputs discrete units.\nUnitY&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Inaguma et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib17\" title=\"\">2023</a>)</cite> and UnitY2 <cite class=\"ltx_cite ltx_citemacro_cite\">Seamless Communication et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib39\" title=\"\">2023</a>)</cite> are two-pass direct S2ST models that predict both subwords and discrete units with a single model.\nDespite advances in architectures, the amount of supervised training data is still insufficient and thus limits model performance.</p>\n\n",
                "matched_terms": [
                    "models",
                    "data",
                    "both",
                    "training",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Bilingual text sentence alignment.</span>\nText alignment is very related to speech alignment.\nMethods apply dynamic programming <cite class=\"ltx_cite ltx_citemacro_cite\">Bellman (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib4\" title=\"\">1954</a>)</cite> and mainly differ in the design of scoring functions.\nEarly works <cite class=\"ltx_cite ltx_citemacro_cite\">Brown et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib5\" title=\"\">1991</a>); Gale and Church (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib12\" title=\"\">1993</a>)</cite> are based on sentence lengths.\nLater methods incorporate translations in various ways&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Moore (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib30\" title=\"\">2002</a>); Varga et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib44\" title=\"\">2007</a>); Sennrich and Volk (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib40\" title=\"\">2010</a>); Gomes and Lopes (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib13\" title=\"\">2016</a>)</cite>.\nOur work is inspired by Vecalign <cite class=\"ltx_cite ltx_citemacro_cite\">Thompson and Koehn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib43\" title=\"\">2019</a>)</cite>, which utilizes margin-based cosine similarities between multilingual sentence embeddings like LASER&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Artetxe and Schwenk (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib3\" title=\"\">2019b</a>); Heffernan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib14\" title=\"\">2022</a>)</cite> and LaBSE <cite class=\"ltx_cite ltx_citemacro_cite\">Feng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib11\" title=\"\">2022</a>)</cite>.\nVecalign is also more efficient than previous methods.\nBy applying fast dynamic time warping <cite class=\"ltx_cite ltx_citemacro_cite\">Salvador and Chan (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib37\" title=\"\">2007</a>)</cite>, it has a linear time and space complexity with respect to the number of input sentences.\nThe recent progress of extending multilingual sentence embeddings to the speech modality <cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib9\" title=\"\">2021</a>); Khurana et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib24\" title=\"\">2022</a>); Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib10\" title=\"\">b</a>)</cite> enables us to align speech segments by their speech embeddings using the same algorithm.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "than",
                    "vecalign",
                    "2023a",
                    "speech",
                    "duquenne",
                    "text",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">S2ST datasets.</span>\nThere are two common ways to automatically build an S2ST dataset: (1)&#160;building alignments from multilingual speech data; (2)&#160;synthesizing speech for text translations from existing speech-to-text translation (S2TT) corpora.\nThe first line of work has human spoken speech on both source and target sides.\nVoxPopuli <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib47\" title=\"\">2021a</a>)</cite> aligns multilingual speech documents based on text transcriptions, yielding 17.3k-hour alignments between 15 source and target languages.\nSpeechMatrix <cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite> applies Global Mining with SpeechLASER embeddings on VoxPopuli. It obtains alignments for 136 language pairs with an average of 1537 hours per direction.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Seamless Communication et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib39\" title=\"\">2023</a>)</cite> apply Global Mining to web-crawled speech data with SONAR embeddings.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Seamless Communication et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib39\" title=\"\">2023</a>)</cite> also mine a SeamlessAlignExpressive dataset with expressively- and semantically-aligned segment pairs, based on a blend of both semantic and prosodic similarity score <cite class=\"ltx_cite ltx_citemacro_cite\">Heffernan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib15\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "hours",
                    "2023a",
                    "data",
                    "from",
                    "both",
                    "speech",
                    "duquenne",
                    "text",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second line of work has synthesized speech on the target side.\nFisher <cite class=\"ltx_cite ltx_citemacro_cite\">Post et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib35\" title=\"\">2013</a>)</cite> is a Spanish-English S2TT dataset containing about 170 hours of Spanish telephone conversations and English translations which are used to synthesize English speech.\nCVSS <cite class=\"ltx_cite ltx_citemacro_cite\">Jia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib20\" title=\"\">2022b</a>)</cite> is an S2ST dataset covering utterances from 21 languages to English, obtained by synthesizing the text translations in CoVoST 2 <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib49\" title=\"\">2021b</a>)</cite>.\nBesides automatic methods, FLEURS <cite class=\"ltx_cite ltx_citemacro_cite\">Conneau et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib6\" title=\"\">2023</a>)</cite> has collected human read speech covering 102 languages. But it contains only about 12 hours per language and is intended for evaluation.</p>\n\n",
                "matched_terms": [
                    "hours",
                    "fleurs",
                    "from",
                    "speech",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to limited computing resources, we adopt different training strategies for different purposes.\nThe 500-hour datasets are used for hyperparameter optimization, and larger datasets are used for reporting main results.\nAll models are trained for up to 400k steps, with the first 10,000 steps as a warmup stage.\nFor experiments on a 500-hour dataset, we use a batch size of 320k tokens and apply early-stopping if there is no improvement on the development set for 30 epochs.\nThese models are trained on 4 NVIDIA GeForce GTX 1080 Ti GPUs for approximately 15 days.\nFor larger datasets, we increase the batch size to 640k tokens and early-stopping is not applied.\nThese models are trained on 2 NVIDIA A100-SXM4-80GB GPUs for approximately 15 days.\nThe best checkpoint is selected based on the development set loss.\nAll experiments are conducted in fp32, as we found training with fp16 and amp very unstable.</p>\n\n",
                "matched_terms": [
                    "models",
                    "best",
                    "training",
                    "results",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Segment embedding.</span>\nThis is the most time-consuming step.\nWe use a mixture of NVIDIA GeForce GTX 1080 and 2080 Ti GPUs.\nEmbedding about 6,000 hours of speech (3,000 hours for each language) took approximately 1,100 GPU hours.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "most",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Alignment.</span>\nLocal Mining and Global Mining run on a single GPU.\nThey take about 2 hours.\nSpeech Vecalign runs on a single CPU and takes about 2 hours.</p>\n\n",
                "matched_terms": [
                    "global",
                    "hours",
                    "alignment",
                    "local",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There are two hyperparameters that affect training data: (1) the maximum source duration overlap ratio between alignments, <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m1\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math>, which is mentioned in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS3\" title=\"3.3 Alignment Postprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>, and\n(2) the data size.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m1\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> controls the trade-off between overlapped durations and data quality.\nFor instance, a lower <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m2\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> reduces the overlap but also discards alignments more aggressively.\nOverlapped alignments usually have similar margin-scores, so more high-quality alignments are lost.\nThe data size controls the trade-off between data size and data quality cutoff.\nFor instance, a larger dataset will have a lower quality cutoff, as alignments are selected in descending order of margin-scores.\nIn this section, we optimize the combination of <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m3\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> and data size by training S2UT models on different datasets.\nNote that the raw data stays the same.</p>\n\n",
                "matched_terms": [
                    "models",
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first experiment with different values of <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math>.\nWe apply different <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> thresholds during the postprocessing stage, and always choose the best 500 hours as the training data.\nThe optimal value is determined based on development set ASR-BLEU.\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.F6\" title=\"Figure 6 &#8227; D.1 Optimizing &#119898;&#8290;&#119886;&#8290;&#119909;&#8290;_&#8290;&#119900;&#8290;&#119907;&#8290;&#119890;&#8290;&#119903;&#8290;&#119897;&#8290;&#119886;&#8290;&#119901; &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows that 0.8 works best for Speech Vecalign and Local Mining and 0.4 works best for Global Mining.\nThe test set performance is also drawn in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.F6\" title=\"Figure 6 &#8227; D.1 Optimizing &#119898;&#8290;&#119886;&#8290;&#119909;&#8290;_&#8290;&#119900;&#8290;&#119907;&#8290;&#119890;&#8290;&#119903;&#8290;&#119897;&#8290;&#119886;&#8290;&#119901; &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, exhibiting a similar trend.</p>\n\n",
                "matched_terms": [
                    "global",
                    "hours",
                    "local",
                    "vecalign",
                    "best",
                    "data",
                    "training",
                    "test",
                    "speech",
                    "asrbleu",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next we optimize the training data size.\nWe fix <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> at 0.4 for Global Mining and 0.8 for Speech Vecalign and Local Mining during postprocessing, only lowering the quality cutoff to include more training data.\nThe models are trained on different amounts of data until we find the peak performance.\nResults are shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.F7\" title=\"Figure 7 &#8227; D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "global",
                    "local",
                    "vecalign",
                    "data",
                    "training",
                    "results",
                    "speech",
                    "trained",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For En-to-De, the best Speech Vecalign model is trained on the 750-hour dataset, achieving 12.58 BLEU.\nIt outperforms the best Global Mining model which achieves 12.21 BLEU.\nThe best Local Mining model achieves 12.91 BLEU.\nHowever, we note that it requires a lot more data than the other two methods to achieve the peak performance.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "than",
                    "vecalign",
                    "entode",
                    "best",
                    "data",
                    "speech",
                    "trained",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For De-to-En, the 1000-hour dataset works best for Speech Vecalign while the 750-hour dataset works best for Global Mining.\nLocal Mining achieves the peak performance when the data size is 1250 hours, still requiring more data than the other methods.\nThe Speech Vecalign performs better than both the Global Mining and the Local Mining models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "global",
                    "hours",
                    "local",
                    "than",
                    "vecalign",
                    "better",
                    "best",
                    "data",
                    "both",
                    "speech",
                    "detoen",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For En-to-De, Speech Vecalign achieves comparable performance with Local Mining on all metrics.\nFor De-to-En, Speech Vecalign is significantly better than Local Mining when using BLASER 2.0 metrics.</p>\n\n",
                "matched_terms": [
                    "local",
                    "than",
                    "vecalign",
                    "metrics",
                    "better",
                    "entode",
                    "speech",
                    "blaser",
                    "detoen",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, we can show that Speech Vecalign performs better than both Local Mining and Global Mining.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "than",
                    "vecalign",
                    "better",
                    "overall",
                    "show",
                    "both",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We manually select the corresponding words for each speech segment from the obtained transcriptions;</p>\n\n",
                "matched_terms": [
                    "speech",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although this process depends on models such as Whisper and Google Translate, we argue that they should perform extremely well on German and English and should produce almost perfect transcriptions and translations.</p>\n\n",
                "matched_terms": [
                    "perform",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the same alignment evaluation method as Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.SS2\" title=\"5.2 Speech Mining Methods Produce Similar Alignments as Speech Vecalign &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>, but we use the manual alignments as the reference.\nThere are 144 raw Speech Vecalign alignments, and we choose the same number of alignments from Global Mining and Local Mining in descending order of margin-scores.\nThe Recall and Precision of raw Speech Vecalign, Local Mining, and Global Mining alignments are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A7.T6\" title=\"Table 6 &#8227; Appendix G Evaluation of System Alignments using the Manual Alignments &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "alignment",
                    "local",
                    "vecalign",
                    "from",
                    "method",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The three methods have similar Lax Precisions, while that of Local Mining and Global Mining are slightly higher.\nSpeech Vecalign has the highest recall values than both the speech mining baselines.\nAmong the three methods, Local Mining has the worst performance in general.\nThis follows Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F5\" title=\"Figure 5 &#8227; 5.3 Speech Vecalign Produces Longer Alignments &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> that both Speech Vecalign and Global Mining have good performance but Local Mining does not perform well.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "than",
                    "vecalign",
                    "baselines",
                    "perform",
                    "both",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As our proposed alignment pipeline consists of several intermediate steps, we report numbers of segments or alignments in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A7.T7\" title=\"Table 7 &#8227; Appendix G Evaluation of System Alignments using the Manual Alignments &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.\nWe use English-to-German alignment as an example.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "englishtogerman",
                    "our"
                ]
            }
        ]
    },
    "A7.T6": {
        "caption": "Table 6: Precision and Recall for raw Speech Vecalign, Global Mining and raw Local Mining alignments when manual alignments are used as the reference.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Precision</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Recall</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">Strict</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">Lax</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">Strict</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_italic\">Lax</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">raw Local Mining</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.139</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.993</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.147</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.676</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Global Mining</th>\n<td class=\"ltx_td ltx_align_center\">0.188</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.993</span></td>\n<td class=\"ltx_td ltx_align_center\">0.199</td>\n<td class=\"ltx_td ltx_align_center\">0.868</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">raw Speech Vecalign</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.597</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.979</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.632</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.978</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "global",
            "lax",
            "local",
            "alignments",
            "vecalign",
            "manual",
            "strict",
            "recall",
            "precision",
            "when",
            "speech",
            "used",
            "raw",
            "reference",
            "mining"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We use the same alignment evaluation method as Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.SS2\" title=\"5.2 Speech Mining Methods Produce Similar Alignments as Speech Vecalign &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>, but we use the manual alignments as the reference.\nThere are 144 raw Speech Vecalign alignments, and we choose the same number of alignments from Global Mining and Local Mining in descending order of margin-scores.\nThe Recall and Precision of raw Speech Vecalign, Local Mining, and Global Mining alignments are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A7.T6\" title=\"Table 6 &#8227; Appendix G Evaluation of System Alignments using the Manual Alignments &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Speech Vecalign, a parallel speech document alignment method that monotonically aligns speech segment embeddings and does not depend on text transcriptions.\nCompared to the baseline method Global Mining&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>, a variant of speech mining, Speech Vecalign produces longer speech-to-speech alignments.\nIt also demonstrates greater robustness than Local Mining, another speech mining variant, as it produces less noise.\nWe applied Speech Vecalign to 3,000 hours of unlabeled parallel English-German&#160;(En-De) speech documents from VoxPopuli, yielding about 1,000 hours of high-quality alignments.\nWe then trained En-De speech-to-speech translation models on the aligned data.\nSpeech Vecalign improves the En-to-De and De-to-En performance over Global Mining by 0.37 and 0.18 ASR-BLEU, respectively.\nMoreover, our models match or outperform SpeechMatrix model performance, despite using 8 times fewer raw speech documents.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Data and code are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/mct10/Speech-Vecalign\" title=\"\">https://github.com/mct10/Speech-Vecalign</a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "alignments",
                    "vecalign",
                    "speech",
                    "raw",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "vecalign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech-to-speech translation (S2ST) is the task of translating speech in one language into speech in another language.\nConventional S2ST systems concatenate automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) models <cite class=\"ltx_cite ltx_citemacro_cite\">Lavie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib27\" title=\"\">1997</a>); Nakamura et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib31\" title=\"\">2006</a>); Wahlster (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib46\" title=\"\">2013</a>)</cite>.\nThese components can be trained individually with datasets for the different components.\nDirect S2ST models, which translate source speech into target spectrograms or discrete units with a single architecture, have been recently proposed to alleviate error propagation and to reduce inference latency <cite class=\"ltx_cite ltx_citemacro_cite\">Jia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib21\" title=\"\">2019</a>); Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib28\" title=\"\">2022a</a>)</cite>.\nDespite the advantages, performance of direct models is limited by the amount of speech-to-speech aligned data, which is much more scarce than the data used for components of cascaded systems.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There have been efforts to automatically curate alignments from multilingual <span class=\"ltx_text ltx_font_italic\">speech document</span>s.\nIn this paper, we define a <span class=\"ltx_text ltx_font_italic\">speech document</span> as a file containing more than one utterance and typically comprising several paragraphs, analogous to a <span class=\"ltx_text ltx_font_italic\">text document</span>.\nVoxPopuli&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib47\" title=\"\">2021a</a>)</cite> is one such corpus containing a large number of <span class=\"ltx_text ltx_font_italic\">parallel</span> speech documents, which are pairs of documents that have the same content but differ in language.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "alignments"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech-to-speech alignment methods align short speech clips called <span class=\"ltx_text ltx_font_italic\">segment</span>s, and can be either transcription-based or transcription-free.\nWhen transcriptions are available, segments in parallel speech documents can be aligned through speech-to-text and text-to-text alignments.\nInspired by text mining&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Schwenk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib38\" title=\"\">2021</a>)</cite>, speech mining&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib9\" title=\"\">2021</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite> was proposed as a transcription-free method that aligns speech segments by finding segment pairs with the highest embedding similarity.\nIt scales well as it does not rely on the availability of text transcriptions.\nWhen speech mining is applied to a large amount of speech documents, as in all previous work, it is referred to as <span class=\"ltx_text ltx_font_bold\">Global Mining</span>.\nAnother variant, <span class=\"ltx_text ltx_font_bold\">Local Mining</span>, which applies speech mining to a single pair of parallel speech documents, has not been well explored.\nAs we formally define in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S2\" title=\"2 Speech Mining Overview &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, both Global Mining and Local Mining treat documents as bags of unordered segments.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "alignments",
                    "when",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since speech mining methods do not leverage the document pair structure, we wonder, <span class=\"ltx_text ltx_font_bold\">can we obtain better alignments by aligning speech segments within document pairs and preserving their time order?</span>\nThis allows us to utilize the extra knowledge that (1) segments within parallel document pairs are likely to be translations of each other, and (2) segment pairs right next to already aligned pairs are also likely to be aligned.\nWe draw inspiration from parallel <span class=\"ltx_text ltx_font_italic\">text</span> document alignment methods, which have been popular to create sentence-aligned bitext for training MT systems.\nUnlike mining, they align sentences for each document pair while maintaining the sentence order.\nOur work is based on the text alignment method Vecalign&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Thompson and Koehn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib43\" title=\"\">2019</a>)</cite>, which aligns parallel sentences by applying fast dynamic time warping&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Salvador and Chan (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib37\" title=\"\">2007</a>)</cite> to sentence embeddings.\nWith the advances of extending sentence embeddings to the speech modality&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib9\" title=\"\">2021</a>)</cite>, we can readily apply Vecalign to parallel speech documents.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "alignments",
                    "vecalign",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce Speech Vecalign, a method that aligns parallel speech documents using speech segment embeddings.\nInstead of mining from bags of segments, our method aligns individual document pairs and maintains the chronological order of segments, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nAdditional preprocessing and postprocessing strategies are applied to improve alignment quality.\nWe compare Speech Vecalign with Local Mining and Global Mining and show that Speech Vecalign produces higher-quality alignments.\nWe further provide extensive analysis for all three methods, which could be useful for future research.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "alignments",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We formally describe the speech mining methods in this section.\nOther related work is in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A1\" title=\"Appendix A Related Work &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech Mining, first proposed by <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib9\" title=\"\">2021</a>)</cite>, encodes speech segments into language- and modality-agnostic fixed-size embeddings, and then uses margin-based similarity search <cite class=\"ltx_cite ltx_citemacro_cite\">Artetxe and Schwenk (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib2\" title=\"\">2019a</a>)</cite> to find the closest embedding pairs.\nDepending on the search scope, it can be categorized as Global Mining or Local Mining.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "global",
                    "mining",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Raw data</span>. The input data is a list of speech documents <math alttext=\"X=[X_{1},X_{2},\\ldots,X_{n}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>X</mi><mn>1</mn></msub><mo>,</mo><msub><mi>X</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>X</mi><mi>n</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">X=[X_{1},X_{2},\\ldots,X_{n}]</annotation></semantics></math> in the source language and a list <math alttext=\"Y=[Y_{1},Y_{2},\\ldots,Y_{m}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m2\" intent=\":literal\"><semantics><mrow><mi>Y</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>Y</mi><mn>1</mn></msub><mo>,</mo><msub><mi>Y</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>Y</mi><mi>m</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Y=[Y_{1},Y_{2},\\ldots,Y_{m}]</annotation></semantics></math> in the target language, where <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m3\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> and <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m4\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> are the numbers of documents.\nEach document can contain between a few seconds to a few hours of speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "raw"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Bag of embeddings</span>.\nIn <span class=\"ltx_text ltx_font_italic\">Global Mining</span>, embeddings are grouped by <span class=\"ltx_text ltx_font_bold\">language</span>.\nWe define <math alttext=\"G_{X}=\\left\\{E_{\\tilde{X}_{1}},E_{\\tilde{X}_{2}},\\ldots,E_{\\tilde{X}_{n}}\\right\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m1\" intent=\":literal\"><semantics><mrow><msub><mi>G</mi><mi>X</mi></msub><mo>=</mo><mrow><mo>{</mo><msub><mi>E</mi><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mn>1</mn></msub></msub><mo>,</mo><msub><mi>E</mi><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mn>2</mn></msub></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>E</mi><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mi>n</mi></msub></msub><mo>}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">G_{X}=\\left\\{E_{\\tilde{X}_{1}},E_{\\tilde{X}_{2}},\\ldots,E_{\\tilde{X}_{n}}\\right\\}</annotation></semantics></math> and <math alttext=\"G_{Y}=\\left\\{E_{\\tilde{Y}_{1}},E_{\\tilde{Y}_{2}},\\ldots,E_{\\tilde{Y}_{m}}\\right\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m2\" intent=\":literal\"><semantics><mrow><msub><mi>G</mi><mi>Y</mi></msub><mo>=</mo><mrow><mo>{</mo><msub><mi>E</mi><msub><mover accent=\"true\"><mi>Y</mi><mo>~</mo></mover><mn>1</mn></msub></msub><mo>,</mo><msub><mi>E</mi><msub><mover accent=\"true\"><mi>Y</mi><mo>~</mo></mover><mn>2</mn></msub></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>E</mi><msub><mover accent=\"true\"><mi>Y</mi><mo>~</mo></mover><mi>m</mi></msub></msub><mo>}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">G_{Y}=\\left\\{E_{\\tilde{Y}_{1}},E_{\\tilde{Y}_{2}},\\ldots,E_{\\tilde{Y}_{m}}\\right\\}</annotation></semantics></math>, where <math alttext=\"G_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m3\" intent=\":literal\"><semantics><msub><mi>G</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">G_{X}</annotation></semantics></math> collects all segment embeddings in the source language and <math alttext=\"G_{Y}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m4\" intent=\":literal\"><semantics><msub><mi>G</mi><mi>Y</mi></msub><annotation encoding=\"application/x-tex\">G_{Y}</annotation></semantics></math> collects those in the target language.\nIn <span class=\"ltx_text ltx_font_italic\">Local Mining</span>, embeddings are grouped by <span class=\"ltx_text ltx_font_bold\">document pairs</span>.\nSuppose there are <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m5\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> parallel documents, with <math alttext=\"X_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m6\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">X_{i}</annotation></semantics></math> paired with <math alttext=\"Y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m7\" intent=\":literal\"><semantics><msub><mi>Y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">Y_{i}</annotation></semantics></math> for <math alttext=\"1\\leq i\\leq s\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m8\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo>&#8804;</mo><mi>i</mi><mo>&#8804;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">1\\leq i\\leq s</annotation></semantics></math>.\nDocuments without a parallel one are ignored.\nIn this case, <math alttext=\"E_{\\tilde{X}_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m9\" intent=\":literal\"><semantics><msub><mi>E</mi><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mi>i</mi></msub></msub><annotation encoding=\"application/x-tex\">E_{\\tilde{X}_{i}}</annotation></semantics></math> and <math alttext=\"E_{\\tilde{Y_{j}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p6.m10\" intent=\":literal\"><semantics><msub><mi>E</mi><mover accent=\"true\"><msub><mi>Y</mi><mi>j</mi></msub><mo>~</mo></mover></msub><annotation encoding=\"application/x-tex\">E_{\\tilde{Y_{j}}}</annotation></semantics></math> are bags of embeddings themselves.</p>\n\n",
                "matched_terms": [
                    "global",
                    "mining",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Embedding alignment</span>.\nSpeech mining is performed by finding the most similar embedding pairs between two bags of segment embeddings.\nThe margin-based similarity, or margin-score, between any two embeddings <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m1\" intent=\":literal\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math> and <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m2\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> is computed as</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More generally, given two bags of embeddings <math alttext=\"U=\\{u_{1},u_{2},\\ldots,u_{l_{u}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m10\" intent=\":literal\"><semantics><mrow><mi>U</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>u</mi><mn>1</mn></msub><mo>,</mo><msub><mi>u</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>u</mi><msub><mi>l</mi><mi>u</mi></msub></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">U=\\{u_{1},u_{2},\\ldots,u_{l_{u}}\\}</annotation></semantics></math> and <math alttext=\"V=\\{v_{1},v_{2},\\ldots,v_{l_{v}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m11\" intent=\":literal\"><semantics><mrow><mi>V</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>v</mi><mn>1</mn></msub><mo>,</mo><msub><mi>v</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>v</mi><msub><mi>l</mi><mi>v</mi></msub></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">V=\\{v_{1},v_{2},\\ldots,v_{l_{v}}\\}</annotation></semantics></math>, where <math alttext=\"l_{u}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m12\" intent=\":literal\"><semantics><msub><mi>l</mi><mi>u</mi></msub><annotation encoding=\"application/x-tex\">l_{u}</annotation></semantics></math> and <math alttext=\"l_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m13\" intent=\":literal\"><semantics><msub><mi>l</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">l_{v}</annotation></semantics></math> are number of embeddings, the collection of all speech mining alignments is</p>\n\n",
                "matched_terms": [
                    "speech",
                    "alignments",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we define Local Mining and Global Mining as</p>\n\n",
                "matched_terms": [
                    "global",
                    "mining",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Speech Vecalign pipeline consists of three steps:\nspeech preprocessing (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>), segment alignment with Vecalign (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS2\" title=\"3.2 Speech Segment Alignment &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), and alignment postprocessing (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS3\" title=\"3.3 Alignment Postprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>).\nAn illustration of our method is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "vecalign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Segmentation</span>. Same as speech mining, we first segment each speech document by VAD.\nWe apply Silero VAD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Silero Team (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib41\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We perform segment alignment based on the similarity between speech segment embeddings.\nUnlike speech mining, which solely relies on similarity scores, we use a dynamic programming&#160;(DP) algorithm to align segments in chronological order.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Segment concatenation</span>.\nSpeech segments do not necessarily correspond to complete sentences.\nSame as speech mining, we first progressively concatenate each segment with the subsequent ones.\nEach concatenated segment can contain up to 5 original segments and span a maximum of 20 seconds.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because of DP, the resultant alignments strictly follow chronological order.\nWe use <math alttext=\"x_{a:b}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m1\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mrow><mi>a</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">x_{a:b}^{i}</annotation></semantics></math> to denote the concatenation of consecutive segments <math alttext=\"x_{a}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m2\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mi>a</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">x_{a}^{i}</annotation></semantics></math> through <math alttext=\"x_{b}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m3\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mi>b</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">x_{b}^{i}</annotation></semantics></math>.\nFor any two alignments <math alttext=\"(x_{a_{s}:a_{e}}^{i},y_{b_{s}:b_{e}}^{j})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mrow><msub><mi>a</mi><mi>s</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>a</mi><mi>e</mi></msub></mrow><mi>i</mi></msubsup><mo>,</mo><msubsup><mi>y</mi><mrow><msub><mi>b</mi><mi>s</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>b</mi><mi>e</mi></msub></mrow><mi>j</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{a_{s}:a_{e}}^{i},y_{b_{s}:b_{e}}^{j})</annotation></semantics></math> and <math alttext=\"(x_{c_{s}:c_{e}}^{i},y_{d_{s}:d_{e}}^{k})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mrow><msub><mi>c</mi><mi>s</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>c</mi><mi>e</mi></msub></mrow><mi>i</mi></msubsup><mo>,</mo><msubsup><mi>y</mi><mrow><msub><mi>d</mi><mi>s</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>d</mi><mi>e</mi></msub></mrow><mi>k</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{c_{s}:c_{e}}^{i},y_{d_{s}:d_{e}}^{k})</annotation></semantics></math>, Speech Vecalign guarantees that <math alttext=\"i=j=k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m6\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>=</mo><mi>j</mi><mo>=</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">i=j=k</annotation></semantics></math> and that either <math alttext=\"a_{e}&lt;c_{s},b_{e}&lt;d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m7\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>a</mi><mi>e</mi></msub><mo>&lt;</mo><msub><mi>c</mi><mi>s</mi></msub></mrow><mo>,</mo><mrow><msub><mi>b</mi><mi>e</mi></msub><mo>&lt;</mo><msub><mi>d</mi><mi>s</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">a_{e}&lt;c_{s},b_{e}&lt;d_{s}</annotation></semantics></math> or <math alttext=\"a_{s}&gt;c_{e},b_{s}&gt;d_{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m8\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>a</mi><mi>s</mi></msub><mo>&gt;</mo><msub><mi>c</mi><mi>e</mi></msub></mrow><mo>,</mo><mrow><msub><mi>b</mi><mi>s</mi></msub><mo>&gt;</mo><msub><mi>d</mi><mi>e</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">a_{s}&gt;c_{e},b_{s}&gt;d_{e}</annotation></semantics></math>.\nIn contrast, Local Mining ensures <math alttext=\"i=j=k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m9\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>=</mo><mi>j</mi><mo>=</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">i=j=k</annotation></semantics></math> but has no constraints on <math alttext=\"a,b,c,d\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m10\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi>c</mi><mo>,</mo><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">a,b,c,d</annotation></semantics></math>, while\nGlobal Mining makes no guarantees at all.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "alignments",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The goal of postprocessing is to clean the raw alignments and construct alignments with longer durations to improve S2ST models.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "raw"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Alignment concatenation</span>.\nAnother issue is that the raw alignments are too short: the average duration is 4.25 seconds, with 66% shorter than 5 seconds.\nTo cover more context, we progressively concatenate each alignment with the subsequent ones.\nThis can be easily done as alignments are in chronological order.\nEach concatenated alignment can contain up to 3 original alignments and span up to 20 seconds.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "raw"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Global margin-scores computation</span>.\nThe raw alignments only have alignment costs as a quality indicator, which are computed <span class=\"ltx_text ltx_font_italic\">within</span> each document pair.\nTo assess alignment quality <span class=\"ltx_text ltx_font_italic\">across</span> document pairs, we train FAISS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Johnson et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib22\" title=\"\">2019</a>)</cite> indexes and compute margin-scores&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Artetxe and Schwenk (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib2\" title=\"\">2019a</a>)</cite> using Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S2.E1\" title=\"In 2 Speech Mining Overview &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> for <span class=\"ltx_text ltx_font_italic\">all</span> obtained alignments, following the common strategy in MT dataset curation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Sloto et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib42\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "global",
                    "raw"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Removing highly-overlapped alignments</span>.\nFinally, we remove alignments that have too much overlap with others, following <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nFor any two consecutive alignments, we compute the ratio of the overlapped source duration to the maximum duration of the two source segments.\nIf the ratio exceeds a threshold, we discard the one with a lower margin-score.\nWe train S2ST models with multiple threshold values to determine the best one.\nOur experiments in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.SS1\" title=\"D.1 Optimizing &#119898;&#8290;&#119886;&#8290;&#119909;&#8290;_&#8290;&#119900;&#8290;&#119907;&#8290;&#119890;&#8290;&#119903;&#8290;&#119897;&#8290;&#119886;&#8290;&#119901; &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">D.1</span></a> suggest that 0.4 work best for Global Mining and 0.8 work best for Local Mining and Speech Vecalign.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "alignments",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We apply Speech Vecalign, Global Mining, and Local Mining to the same raw data and train S2ST models on each type of alignments, providing a fair comparison.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "alignments",
                    "vecalign",
                    "speech",
                    "raw",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Vecalign.</span>\nWe apply Speech Vecalign to each pair of speech documents and obtain alignments sorted by margin-scores.\nTraining data is chosen in descending order of margin-scores.\nWe train models on different data sizes and report the best results in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS5\" title=\"4.5 Main Results &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.5</span></a>.\nMore details on data size optimization can be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.SS2\" title=\"D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "alignments",
                    "vecalign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech mining baselines.</span>\nWe apply Global Mining and Local Mining to the same raw data and embeddings as Speech Vecalign.\nThe implementation is based on <span class=\"ltx_text ltx_font_typewriter\">stopes<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">3</span></span><a class=\"ltx_ref ltx_url\" href=\"https://github.com/facebookresearch/stopes\" title=\"\">https://github.com/facebookresearch/stopes</a></span></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Andrews et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib1\" title=\"\">2022</a>)</cite>.\nAfter mining, we apply the same postprocessing strategies in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS3\" title=\"3.3 Alignment Postprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>, except for alignment concatenation which is not applicable.\nTraining data is chosen in descending order of margin-scores and details on data size optimization can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.SS2\" title=\"D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "vecalign",
                    "speech",
                    "raw",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train speech-to-unit translation (S2UT) models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib28\" title=\"\">2022a</a>)</cite> with <span class=\"ltx_text ltx_font_typewriter\">fairseq<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">4</span></span><a class=\"ltx_ref ltx_url\" href=\"https://github.com/facebookresearch/fairseq\" title=\"\">https://github.com/facebookresearch/fairseq</a></span></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">Ott et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib32\" title=\"\">2019</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib48\" title=\"\">2020</a>)</cite> on each type of alignments.\nThe S2UT model takes source speech as input and predicts a sequence of target discrete units.\nThe discrete units are obtained by applying\na k-means model to the <math alttext=\"11^{\\text{th}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><msup><mn>11</mn><mtext>th</mtext></msup><annotation encoding=\"application/x-tex\">11^{\\text{th}}</annotation></semantics></math> layer features of a HuBERT model <cite class=\"ltx_cite ltx_citemacro_cite\">Hsu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib16\" title=\"\">2021</a>)</cite>.\nFor English, we use the mHuBERT from <cite class=\"ltx_cite ltx_citemacro_citet\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib29\" title=\"\">2022b</a>)</cite>, and for German, we use the Germanic mHuBERT from <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nConsecutive duplicated units are removed.\nOur S2UT model architecture follows exactly <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nThe architecture details and training hyperparameters are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A2\" title=\"Appendix B Speech-to-Speech Translation &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "alignments"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also adopt BLASER 2.0 <cite class=\"ltx_cite ltx_citemacro_cite\">Dale and Costa-juss&#224; (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib7\" title=\"\">2024</a>)</cite> to directly evaluate speech output.\nWe compute the referenced score using <code class=\"ltx_verbatim ltx_font_typewriter\">blaser-2.0-ref</code><span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/blaser-2.0-ref\" title=\"\">https://huggingface.co/facebook/blaser-2.0-ref</a></span></span></span> for input and output speech, as well as the text reference.\nWe compute the reference-free score using <code class=\"ltx_verbatim ltx_font_typewriter\">blaser-2.0-qe</code><span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/facebook/blaser-2.0-qe\" title=\"\">https://huggingface.co/facebook/blaser-2.0-qe</a></span></span></span> for input and output speech only.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Intriguingly, for both directions, Speech Vecalign and speech mining models are competitive with or outperform SpeechMatrix&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite> models, despite the latter being mined from about 24k hours of speech per language, <span class=\"ltx_text ltx_font_italic\">8 times more</span> than our raw data.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>We do not aim for state-of-the-art performance. Our results are not directly comparable to SpeechMatrix. We report SpeechMatrix results only to show the performance gap.</span></span></span>\nFor En-to-De, our Global Mining and Speech Vecalign models achieve improvements of 0.94 and 1.31 BLEU, respectively.\nOur Local Mining model achieves even 1.64 BLEU improvement.\nWe suspect that SpeechMatrix has not removed identical untranslated segments prior to and after mining, which significantly hurts model performance.\nFurther discussion is in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.SS5\" title=\"5.5 Removing Identical Untranslated Segments is Critical &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5.5</span></a>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "vecalign",
                    "speech",
                    "raw",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While Local Mining has not been previously explored, our results suggest that it is a potentially useful method.\nLocal Mining achieves the highest BLEU score in En-to-De, and only slightly underperforms Global Mining in De-to-En, indicating that constraining the mining scope to document pairs does not necessarily have a negative impact on alignment quality.\nYet we note that Local Mining requires more training data to achieve its optimal performance, as shown in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.SS2\" title=\"D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "mining",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our Speech Vecalign models outperform both speech mining models in both directions.\nFor En-to-De, the Speech Vecalign model achieves 12.58 BLEU, comparable with our strong Global Mining and Local Mining baselines.\nIn terms of chrF2++, it surpasses Global Mining and Local Mining by 1.69 and 0.26, respectively.\nIt also significantly improves their referenced BLASER 2.0 by 0.08 and 0.03.\nFor De-to-En, Speech Vecalign and Global Mining models achieve comparable BLEU (16.14 vs. 15.96), but Speech Vecalign surpasses Global Mining by 0.57 in chrF2++.\nSpeech Vecalign significantly outperforms Local Mining under all metrics.\nThese results demonstrate that Speech Vecalign produces higher-quality alignments than both speech mining baselines.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "alignments",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We analyze properties of speech mining methods and compare them with Speech Vecalign.\nAlthough we show that speech mining methods produce alignments similar to those of Speech Vecalign, the latter offers advantages of producing longer and less noisy alignments.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "alignments",
                    "vecalign",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, we show that Global Mining mostly <span class=\"ltx_text ltx_font_bold\">locally</span> aligns speech documents.\nWhile Global Mining searches for the best matching segment pairs among roughly 10 million segments, one might expect its alignments to cover the spread of the entire dataset.\nOn the contrary, we find that Global Mining alignments are concentrated within document pairs, each typically containing hundreds to thousands of segments.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "global",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To quantify this, we examine the 1000-hour Global Mining data and count alignments whose source and target segments come from <span class=\"ltx_text ltx_font_italic\">different</span> document pairs.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F3\" title=\"Figure 3 &#8227; 5.1 Speech Mining Mostly Locally Aligns Segments in Time Order &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, fewer than 6% fall into this category, while the majority&#160;(<math alttext=\"&gt;93\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mrow><mn>93</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">&gt;93\\%</annotation></semantics></math>) are within paired documents.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "global",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, we analyze the time order of alignments produced by both speech mining methods.\nBorrowing the notation from Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS2\" title=\"3.2 Speech Segment Alignment &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, we define two pairs of alignments to be <span class=\"ltx_text ltx_font_italic\">in-order</span> if either <math alttext=\"a_{e}&lt;c_{s},b_{e}&lt;d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>a</mi><mi>e</mi></msub><mo>&lt;</mo><msub><mi>c</mi><mi>s</mi></msub></mrow><mo>,</mo><mrow><msub><mi>b</mi><mi>e</mi></msub><mo>&lt;</mo><msub><mi>d</mi><mi>s</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">a_{e}&lt;c_{s},b_{e}&lt;d_{s}</annotation></semantics></math> or <math alttext=\"a_{s}&gt;c_{e},b_{s}&gt;d_{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>a</mi><mi>s</mi></msub><mo>&gt;</mo><msub><mi>c</mi><mi>e</mi></msub></mrow><mo>,</mo><mrow><msub><mi>b</mi><mi>s</mi></msub><mo>&gt;</mo><msub><mi>d</mi><mi>e</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">a_{s}&gt;c_{e},b_{s}&gt;d_{e}</annotation></semantics></math>; otherwise, they are <span class=\"ltx_text ltx_font_italic\">out-of-order</span>.\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F3\" title=\"Figure 3 &#8227; 5.1 Speech Mining Mostly Locally Aligns Segments in Time Order &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows that only around 1% alignments are out-of-order for both speech mining methods.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "alignments",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Observations above indicate that speech mining alignments are mostly within paired documents and preserve time order.\nWe hypothesize that speech-to-speech alignments are sparse and high-quality ones mostly exist in paired documents.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "alignments",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As a by-product, this property can be leveraged to identify parallel documents.\nIf Global Mining finds many alignments between two documents, they are likely to be parallel.\nIt is particularly useful when the pairing metadata is not readily available.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "global",
                    "when",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the observations in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.SS1\" title=\"5.1 Speech Mining Mostly Locally Aligns Segments in Time Order &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> that speech mining produces mostly local, in-order alignments, we analyze the similarity between them and Speech Vecalign alignments.</p>\n\n",
                "matched_terms": [
                    "local",
                    "alignments",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ the alignment evaluation method<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/thompsonb/vecalign/blob/master/score.py\" title=\"\">https://github.com/thompsonb/vecalign/blob/master/score.py</a>.</span></span></span> from&#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Thompson and Koehn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib43\" title=\"\">2019</a>)</cite>, which computes precision and recall by comparing system alignments to a reference.\nThere are two modes: <span class=\"ltx_text ltx_font_italic\">Strict</span>, which counts only exact matches as true positives, and <span class=\"ltx_text ltx_font_italic\">Lax</span>, which considers an alignment as true positive if both its source and target segment overlap with the reference.\nIf not true positive, an alignment is false positive.\nRecall is computed by swapping the reference and the system alignments.</p>\n\n",
                "matched_terms": [
                    "lax",
                    "alignments",
                    "recall",
                    "precision",
                    "reference",
                    "strict"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Without loss of generality, we use Speech Vecalign En-De alignments as the reference, and evaluate speech mining ones.\nWe choose 700k highest-scoring alignments from all three methods to ensure a fair comparison.\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.T2\" title=\"Table 2 &#8227; 5.2 Speech Mining Methods Produce Similar Alignments as Speech Vecalign &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows that about 30% of speech mining alignments are exactly the same as those of Speech Vecalign, and about 90% overlap with Speech Vecalign alignments.\nThis high similarity explains why Speech Vecalign and speech mining models have similar performance.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "vecalign",
                    "speech",
                    "reference",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As speech mining and Speech Vecalign produce similar alignments, we explore why Speech Vecalign models still perform better.\nA key advantage of Speech Vecalign is that it first produces fine-grained alignments and then constructs alignments with different amounts of context, thanks to the alignment concatenation strategy.\nSpeech mining methods, on the other hand, solely depend on margin-scores and tend to favor shorter alignments.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "alignments",
                    "vecalign",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the best En-to-De models and corresponding data sizes from Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS5\" title=\"4.5 Main Results &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.5</span></a>, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F4\" title=\"Figure 4 &#8227; 5.3 Speech Vecalign Produces Longer Alignments &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the average sentence-level chrF2++ scores on the test set and the percentage of training alignments for different source speech duration ranges.\nNotably, Speech Vecalign has a large portion of long training samples: the blue bars are highest for durations longer than 12 seconds.\nSpecifically, the average source duration of Speech Vecalign is 8.51 seconds, while Global Mining and Local Mining have average durations of 7.50 and 8.53 seconds, respectively.\nAs a result, the Speech Vecalign model performs better on test samples longer than 10 seconds, while having comparable performance on shorter ones.\nThis highlights that Speech Vecalign is able to produce longer, context-rich alignments which help to improve S2ST model performance.\nInterestingly, Local Mining surpasses the Global Mining model on long inputs, which could be also attributed to its longer training samples.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "alignments",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We visualize alignments produced by different methods for the same document pair, which is about 10 minutes long and contains around 200 segments.\nFor reference, we manually created a gold segment-level alignment, with detailed procedure in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A6\" title=\"Appendix F Procedure of Manual Alignments &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>.\nWe illustrate the best 80 alignments for each of the speech mining methods.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "alignments",
                    "reference",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F5\" title=\"Figure 5 &#8227; 5.3 Speech Vecalign Produces Longer Alignments &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows, Speech Vecalign produces the most fine-grained alignments and is most similar to the gold reference.\nGlobal Mining also performs well, aligning closely with the groundtruth path, whereas Local Mining produces more noise and misses more alignments along the correct path.\nWe hypothesize Local Mining has limited number of segments in a single document pair, making nearest neighbors less effective normalizers in the margin-score computation.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "alignments",
                    "vecalign",
                    "speech",
                    "reference",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As presented in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS5\" title=\"4.5 Main Results &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.5</span></a>, our reproduced speech mining models achieve comparable or even better results than SpeechMatrix models.\nBy listening to samples of SpeechMatrix alignments, we observed many cases where the source and target segments contained identical untranslated content, which is an issue mentioned in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>.\nUsing the method described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, we identified approximately 100k out of 630k alignments with untranslated source and target segments, totaling 181 hours.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "alignments",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present Speech Vecalign, a parallel speech document alignment method that aligns speech segment embeddings within document pairs and in chronological order.\nWe apply Speech Vecalign to parallel English-German VoxPopuli speech documents and conduct S2ST experiments to demonstrate its superiority over two strong speech mining baselines.\nOur analysis reveals that although speech mining methods primarily align documents locally and in-order, Global Mining falls short of producing long alignments, and Local Mining in particular produces more noise.\nFor long-term future work, we plan to extend Speech Vecalign to other language pairs or other data sources.\nWe can also explore aligning speech and text embeddings to construct S2TT datasets.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "alignments",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Limited language pair.</span>\nWe have only conducted experiments for English and German speech from the VoxPopuli dataset.\nAs Speech Vecalign heavily relies on the quality of speech embeddings, the performance is unclear for other language pairs and other domains of speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "vecalign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dependency on parallel speech documents.</span>\nSpeech Vecalign requires parallel speech documents, which is often not available.\nWe may rely on Global Mining to discover parallel documents, as Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.SS1\" title=\"5.1 Speech Mining Mostly Locally Aligns Segments in Time Order &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> suggests, but doing so will introduce extra computation costs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "global",
                    "vecalign",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Bilingual text sentence alignment.</span>\nText alignment is very related to speech alignment.\nMethods apply dynamic programming <cite class=\"ltx_cite ltx_citemacro_cite\">Bellman (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib4\" title=\"\">1954</a>)</cite> and mainly differ in the design of scoring functions.\nEarly works <cite class=\"ltx_cite ltx_citemacro_cite\">Brown et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib5\" title=\"\">1991</a>); Gale and Church (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib12\" title=\"\">1993</a>)</cite> are based on sentence lengths.\nLater methods incorporate translations in various ways&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Moore (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib30\" title=\"\">2002</a>); Varga et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib44\" title=\"\">2007</a>); Sennrich and Volk (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib40\" title=\"\">2010</a>); Gomes and Lopes (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib13\" title=\"\">2016</a>)</cite>.\nOur work is inspired by Vecalign <cite class=\"ltx_cite ltx_citemacro_cite\">Thompson and Koehn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib43\" title=\"\">2019</a>)</cite>, which utilizes margin-based cosine similarities between multilingual sentence embeddings like LASER&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Artetxe and Schwenk (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib3\" title=\"\">2019b</a>); Heffernan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib14\" title=\"\">2022</a>)</cite> and LaBSE <cite class=\"ltx_cite ltx_citemacro_cite\">Feng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib11\" title=\"\">2022</a>)</cite>.\nVecalign is also more efficient than previous methods.\nBy applying fast dynamic time warping <cite class=\"ltx_cite ltx_citemacro_cite\">Salvador and Chan (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib37\" title=\"\">2007</a>)</cite>, it has a linear time and space complexity with respect to the number of input sentences.\nThe recent progress of extending multilingual sentence embeddings to the speech modality <cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib9\" title=\"\">2021</a>); Khurana et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib24\" title=\"\">2022</a>); Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib10\" title=\"\">b</a>)</cite> enables us to align speech segments by their speech embeddings using the same algorithm.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "vecalign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">S2ST datasets.</span>\nThere are two common ways to automatically build an S2ST dataset: (1)&#160;building alignments from multilingual speech data; (2)&#160;synthesizing speech for text translations from existing speech-to-text translation (S2TT) corpora.\nThe first line of work has human spoken speech on both source and target sides.\nVoxPopuli <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib47\" title=\"\">2021a</a>)</cite> aligns multilingual speech documents based on text transcriptions, yielding 17.3k-hour alignments between 15 source and target languages.\nSpeechMatrix <cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite> applies Global Mining with SpeechLASER embeddings on VoxPopuli. It obtains alignments for 136 language pairs with an average of 1537 hours per direction.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Seamless Communication et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib39\" title=\"\">2023</a>)</cite> apply Global Mining to web-crawled speech data with SONAR embeddings.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Seamless Communication et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib39\" title=\"\">2023</a>)</cite> also mine a SeamlessAlignExpressive dataset with expressively- and semantically-aligned segment pairs, based on a blend of both semantic and prosodic similarity score <cite class=\"ltx_cite ltx_citemacro_cite\">Heffernan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib15\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "global",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second line of work has synthesized speech on the target side.\nFisher <cite class=\"ltx_cite ltx_citemacro_cite\">Post et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib35\" title=\"\">2013</a>)</cite> is a Spanish-English S2TT dataset containing about 170 hours of Spanish telephone conversations and English translations which are used to synthesize English speech.\nCVSS <cite class=\"ltx_cite ltx_citemacro_cite\">Jia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib20\" title=\"\">2022b</a>)</cite> is an S2ST dataset covering utterances from 21 languages to English, obtained by synthesizing the text translations in CoVoST 2 <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib49\" title=\"\">2021b</a>)</cite>.\nBesides automatic methods, FLEURS <cite class=\"ltx_cite ltx_citemacro_cite\">Conneau et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib6\" title=\"\">2023</a>)</cite> has collected human read speech covering 102 languages. But it contains only about 12 hours per language and is intended for evaluation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Alignment.</span>\nLocal Mining and Global Mining run on a single GPU.\nThey take about 2 hours.\nSpeech Vecalign runs on a single CPU and takes about 2 hours.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m1\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> controls the trade-off between overlapped durations and data quality.\nFor instance, a lower <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m2\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> reduces the overlap but also discards alignments more aggressively.\nOverlapped alignments usually have similar margin-scores, so more high-quality alignments are lost.\nThe data size controls the trade-off between data size and data quality cutoff.\nFor instance, a larger dataset will have a lower quality cutoff, as alignments are selected in descending order of margin-scores.\nIn this section, we optimize the combination of <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m3\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> and data size by training S2UT models on different datasets.\nNote that the raw data stays the same.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "raw"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first experiment with different values of <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math>.\nWe apply different <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> thresholds during the postprocessing stage, and always choose the best 500 hours as the training data.\nThe optimal value is determined based on development set ASR-BLEU.\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.F6\" title=\"Figure 6 &#8227; D.1 Optimizing &#119898;&#8290;&#119886;&#8290;&#119909;&#8290;_&#8290;&#119900;&#8290;&#119907;&#8290;&#119890;&#8290;&#119903;&#8290;&#119897;&#8290;&#119886;&#8290;&#119901; &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows that 0.8 works best for Speech Vecalign and Local Mining and 0.4 works best for Global Mining.\nThe test set performance is also drawn in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.F6\" title=\"Figure 6 &#8227; D.1 Optimizing &#119898;&#8290;&#119886;&#8290;&#119909;&#8290;_&#8290;&#119900;&#8290;&#119907;&#8290;&#119890;&#8290;&#119903;&#8290;&#119897;&#8290;&#119886;&#8290;&#119901; &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, exhibiting a similar trend.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next we optimize the training data size.\nWe fix <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> at 0.4 for Global Mining and 0.8 for Speech Vecalign and Local Mining during postprocessing, only lowering the quality cutoff to include more training data.\nThe models are trained on different amounts of data until we find the peak performance.\nResults are shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.F7\" title=\"Figure 7 &#8227; D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For En-to-De, the best Speech Vecalign model is trained on the 750-hour dataset, achieving 12.58 BLEU.\nIt outperforms the best Global Mining model which achieves 12.21 BLEU.\nThe best Local Mining model achieves 12.91 BLEU.\nHowever, we note that it requires a lot more data than the other two methods to achieve the peak performance.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For De-to-En, the 1000-hour dataset works best for Speech Vecalign while the 750-hour dataset works best for Global Mining.\nLocal Mining achieves the peak performance when the data size is 1250 hours, still requiring more data than the other methods.\nThe Speech Vecalign performs better than both the Global Mining and the Local Mining models.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "vecalign",
                    "when",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide evaluation results on the FLEURS test set in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.T5\" title=\"Table 5 &#8227; D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>.\nSimilar to Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS5\" title=\"4.5 Main Results &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.5</span></a>, our results match or outperform SpeechMatrix results.\nFor both En-to-De and De-to-En, Speech Vecalign and Global Mining achieve comparable performance when using the transcription-based metrics ASR-BLEU and ASR-chrF2++.\nTheir performance is especially close on De-to-En.\nHowever, Speech Vecalign is significantly better than Global Mining when using the BLASER 2.0 metrics, achieving an improvement of 0.06 and 0.04 referenced BLASER 2.0 scores on En-to-De and De-to-En, respectively.</p>\n\n",
                "matched_terms": [
                    "global",
                    "vecalign",
                    "when",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For En-to-De, Speech Vecalign achieves comparable performance with Local Mining on all metrics.\nFor De-to-En, Speech Vecalign is significantly better than Local Mining when using BLASER 2.0 metrics.</p>\n\n",
                "matched_terms": [
                    "local",
                    "vecalign",
                    "when",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, we can show that Speech Vecalign performs better than both Local Mining and Global Mining.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "vecalign",
                    "speech",
                    "mining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The three methods have similar Lax Precisions, while that of Local Mining and Global Mining are slightly higher.\nSpeech Vecalign has the highest recall values than both the speech mining baselines.\nAmong the three methods, Local Mining has the worst performance in general.\nThis follows Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F5\" title=\"Figure 5 &#8227; 5.3 Speech Vecalign Produces Longer Alignments &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> that both Speech Vecalign and Global Mining have good performance but Local Mining does not perform well.</p>\n\n",
                "matched_terms": [
                    "global",
                    "lax",
                    "local",
                    "vecalign",
                    "recall",
                    "speech",
                    "mining"
                ]
            }
        ]
    },
    "A7.T7": {
        "caption": "Table 7: Number of segments or alignments at each stage.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Stage</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\"># En Segments</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\"># De Segments</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\"># Alignments</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Segmentation</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4,113,319</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3,056,797</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Detection of identical untranslated segments</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">47,008</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">47,008</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Segment concatenation</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">19,331,307</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">13,012,502</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Vecalign</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2,597,796</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">After removing low-quality alignments</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1,968,141</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">After removing untranslated alignments</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1,962,032</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Alignment concatenation</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3,661,860</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">After removing short alignments (<math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A7.T7.m1\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math> 1 second)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2,810,697</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">After removing highly-overlapped alignments</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\">851,446</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">(<math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A7.T7.m2\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A7.T7.m3\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math> 0.8 and duration <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A7.T7.m4\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math> 2 seconds)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">Choose the best 750 hours</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">317,293</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "second",
            "seconds",
            "highlyoverlapped",
            "vecalign",
            "detection",
            "each",
            "m​a​x​​o​v​e​r​l​a​pmaxoverlap",
            "hours",
            "stage",
            "after",
            "short",
            "segment",
            "removing",
            "alignment",
            "alignments",
            "lowquality",
            "untranslated",
            "number",
            "concatenation",
            "segments",
            "identical",
            "best",
            "duration",
            "segmentation",
            "choose"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As our proposed alignment pipeline consists of several intermediate steps, we report numbers of segments or alignments in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A7.T7\" title=\"Table 7 &#8227; Appendix G Evaluation of System Alignments using the Manual Alignments &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.\nWe use English-to-German alignment as an example.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We present Speech Vecalign, a parallel speech document alignment method that monotonically aligns speech segment embeddings and does not depend on text transcriptions.\nCompared to the baseline method Global Mining&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>, a variant of speech mining, Speech Vecalign produces longer speech-to-speech alignments.\nIt also demonstrates greater robustness than Local Mining, another speech mining variant, as it produces less noise.\nWe applied Speech Vecalign to 3,000 hours of unlabeled parallel English-German&#160;(En-De) speech documents from VoxPopuli, yielding about 1,000 hours of high-quality alignments.\nWe then trained En-De speech-to-speech translation models on the aligned data.\nSpeech Vecalign improves the En-to-De and De-to-En performance over Global Mining by 0.37 and 0.18 ASR-BLEU, respectively.\nMoreover, our models match or outperform SpeechMatrix model performance, despite using 8 times fewer raw speech documents.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Data and code are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/mct10/Speech-Vecalign\" title=\"\">https://github.com/mct10/Speech-Vecalign</a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "alignment",
                    "hours",
                    "alignments",
                    "vecalign",
                    "segment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There have been efforts to automatically curate alignments from multilingual <span class=\"ltx_text ltx_font_italic\">speech document</span>s.\nIn this paper, we define a <span class=\"ltx_text ltx_font_italic\">speech document</span> as a file containing more than one utterance and typically comprising several paragraphs, analogous to a <span class=\"ltx_text ltx_font_italic\">text document</span>.\nVoxPopuli&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib47\" title=\"\">2021a</a>)</cite> is one such corpus containing a large number of <span class=\"ltx_text ltx_font_italic\">parallel</span> speech documents, which are pairs of documents that have the same content but differ in language.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech-to-speech alignment methods align short speech clips called <span class=\"ltx_text ltx_font_italic\">segment</span>s, and can be either transcription-based or transcription-free.\nWhen transcriptions are available, segments in parallel speech documents can be aligned through speech-to-text and text-to-text alignments.\nInspired by text mining&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Schwenk et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib38\" title=\"\">2021</a>)</cite>, speech mining&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib9\" title=\"\">2021</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite> was proposed as a transcription-free method that aligns speech segments by finding segment pairs with the highest embedding similarity.\nIt scales well as it does not rely on the availability of text transcriptions.\nWhen speech mining is applied to a large amount of speech documents, as in all previous work, it is referred to as <span class=\"ltx_text ltx_font_bold\">Global Mining</span>.\nAnother variant, <span class=\"ltx_text ltx_font_bold\">Local Mining</span>, which applies speech mining to a single pair of parallel speech documents, has not been well explored.\nAs we formally define in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S2\" title=\"2 Speech Mining Overview &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, both Global Mining and Local Mining treat documents as bags of unordered segments.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "alignments",
                    "segments",
                    "short",
                    "segment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since speech mining methods do not leverage the document pair structure, we wonder, <span class=\"ltx_text ltx_font_bold\">can we obtain better alignments by aligning speech segments within document pairs and preserving their time order?</span>\nThis allows us to utilize the extra knowledge that (1) segments within parallel document pairs are likely to be translations of each other, and (2) segment pairs right next to already aligned pairs are also likely to be aligned.\nWe draw inspiration from parallel <span class=\"ltx_text ltx_font_italic\">text</span> document alignment methods, which have been popular to create sentence-aligned bitext for training MT systems.\nUnlike mining, they align sentences for each document pair while maintaining the sentence order.\nOur work is based on the text alignment method Vecalign&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Thompson and Koehn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib43\" title=\"\">2019</a>)</cite>, which aligns parallel sentences by applying fast dynamic time warping&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Salvador and Chan (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib37\" title=\"\">2007</a>)</cite> to sentence embeddings.\nWith the advances of extending sentence embeddings to the speech modality&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib9\" title=\"\">2021</a>)</cite>, we can readily apply Vecalign to parallel speech documents.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "alignments",
                    "vecalign",
                    "segments",
                    "each",
                    "segment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we introduce Speech Vecalign, a method that aligns parallel speech documents using speech segment embeddings.\nInstead of mining from bags of segments, our method aligns individual document pairs and maintains the chronological order of segments, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nAdditional preprocessing and postprocessing strategies are applied to improve alignment quality.\nWe compare Speech Vecalign with Local Mining and Global Mining and show that Speech Vecalign produces higher-quality alignments.\nWe further provide extensive analysis for all three methods, which could be useful for future research.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "alignments",
                    "vecalign",
                    "segments",
                    "segment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Raw data</span>. The input data is a list of speech documents <math alttext=\"X=[X_{1},X_{2},\\ldots,X_{n}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m1\" intent=\":literal\"><semantics><mrow><mi>X</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>X</mi><mn>1</mn></msub><mo>,</mo><msub><mi>X</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>X</mi><mi>n</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">X=[X_{1},X_{2},\\ldots,X_{n}]</annotation></semantics></math> in the source language and a list <math alttext=\"Y=[Y_{1},Y_{2},\\ldots,Y_{m}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m2\" intent=\":literal\"><semantics><mrow><mi>Y</mi><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>Y</mi><mn>1</mn></msub><mo>,</mo><msub><mi>Y</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>Y</mi><mi>m</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">Y=[Y_{1},Y_{2},\\ldots,Y_{m}]</annotation></semantics></math> in the target language, where <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m3\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math> and <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p3.m4\" intent=\":literal\"><semantics><mi>m</mi><annotation encoding=\"application/x-tex\">m</annotation></semantics></math> are the numbers of documents.\nEach document can contain between a few seconds to a few hours of speech.</p>\n\n",
                "matched_terms": [
                    "seconds",
                    "each",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech segmentation</span>. Voice activity detection&#160;(VAD) is applied to each document to obtain short segments, typically lasting a few seconds.\nFor instance, <math alttext=\"X_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m1\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">X_{i}</annotation></semantics></math> is segmented into <math alttext=\"X_{i}=[x_{1}^{i},x_{2}^{i},\\ldots,x_{n_{i}}^{i}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m2\" intent=\":literal\"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msubsup><mi>x</mi><mn>1</mn><mi>i</mi></msubsup><mo>,</mo><msubsup><mi>x</mi><mn>2</mn><mi>i</mi></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>x</mi><msub><mi>n</mi><mi>i</mi></msub><mi>i</mi></msubsup><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">X_{i}=[x_{1}^{i},x_{2}^{i},\\ldots,x_{n_{i}}^{i}]</annotation></semantics></math>, where <math alttext=\"{n_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m3\" intent=\":literal\"><semantics><msub><mi>n</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">{n_{i}}</annotation></semantics></math> is the number of segments.\nTo have segments at different granularities, consecutive segments are progressively concatenated.\n<math alttext=\"X_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m4\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">X_{i}</annotation></semantics></math> becomes <math alttext=\"\\tilde{X_{i}}=\\left[\\tilde{x}_{1}^{i},\\tilde{x}_{2}^{i},\\ldots,\\tilde{x}_{\\tilde{n}_{i}}^{i}\\right]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m5\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><msub><mi>X</mi><mi>i</mi></msub><mo>~</mo></mover><mo>=</mo><mrow><mo>[</mo><msubsup><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mn>1</mn><mi>i</mi></msubsup><mo>,</mo><msubsup><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mn>2</mn><mi>i</mi></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><msub><mover accent=\"true\"><mi>n</mi><mo>~</mo></mover><mi>i</mi></msub><mi>i</mi></msubsup><mo>]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tilde{X_{i}}=\\left[\\tilde{x}_{1}^{i},\\tilde{x}_{2}^{i},\\ldots,\\tilde{x}_{\\tilde{n}_{i}}^{i}\\right]</annotation></semantics></math>, where <math alttext=\"\\tilde{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m6\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\tilde{x}</annotation></semantics></math> denotes a concatenated segment and <math alttext=\"\\tilde{n_{i}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m7\" intent=\":literal\"><semantics><mover accent=\"true\"><msub><mi>n</mi><mi>i</mi></msub><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\tilde{n_{i}}</annotation></semantics></math> denotes the number of resulting segments.\nThe same process applies to <math alttext=\"Y_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m8\" intent=\":literal\"><semantics><msub><mi>Y</mi><mi>j</mi></msub><annotation encoding=\"application/x-tex\">Y_{j}</annotation></semantics></math>, producing <math alttext=\"\\tilde{Y_{j}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p4.m9\" intent=\":literal\"><semantics><mover accent=\"true\"><msub><mi>Y</mi><mi>j</mi></msub><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\tilde{Y_{j}}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "seconds",
                    "segments",
                    "detection",
                    "short",
                    "each",
                    "segmentation",
                    "segment",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech segment embedding</span>.\nEach segment is encoded into a fixed-size embedding using an embedding model.\nThe segment embeddings for <math alttext=\"\\tilde{X}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p5.m1\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\tilde{X}_{i}</annotation></semantics></math> are represented as <math alttext=\"E_{\\tilde{X}_{i}}=\\left[e^{\\tilde{X}_{i}}_{1},e^{\\tilde{X}_{i}}_{2},\\ldots,e^{\\tilde{X}_{i}}_{\\tilde{n}_{i}}\\right]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p5.m2\" intent=\":literal\"><semantics><mrow><msub><mi>E</mi><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mi>i</mi></msub></msub><mo>=</mo><mrow><mo>[</mo><msubsup><mi>e</mi><mn>1</mn><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mi>i</mi></msub></msubsup><mo>,</mo><msubsup><mi>e</mi><mn>2</mn><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mi>i</mi></msub></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>e</mi><msub><mover accent=\"true\"><mi>n</mi><mo>~</mo></mover><mi>i</mi></msub><msub><mover accent=\"true\"><mi>X</mi><mo>~</mo></mover><mi>i</mi></msub></msubsup><mo>]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">E_{\\tilde{X}_{i}}=\\left[e^{\\tilde{X}_{i}}_{1},e^{\\tilde{X}_{i}}_{2},\\ldots,e^{\\tilde{X}_{i}}_{\\tilde{n}_{i}}\\right]</annotation></semantics></math>.\nSimilarly, the segments in <math alttext=\"\\tilde{Y_{j}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p5.m3\" intent=\":literal\"><semantics><mover accent=\"true\"><msub><mi>Y</mi><mi>j</mi></msub><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\tilde{Y_{j}}</annotation></semantics></math> are encoded as <math alttext=\"E_{\\tilde{Y_{j}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p5.m4\" intent=\":literal\"><semantics><msub><mi>E</mi><mover accent=\"true\"><msub><mi>Y</mi><mi>j</mi></msub><mo>~</mo></mover></msub><annotation encoding=\"application/x-tex\">E_{\\tilde{Y_{j}}}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "segment",
                    "each",
                    "segments"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Embedding alignment</span>.\nSpeech mining is performed by finding the most similar embedding pairs between two bags of segment embeddings.\nThe margin-based similarity, or margin-score, between any two embeddings <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m1\" intent=\":literal\"><semantics><mi>a</mi><annotation encoding=\"application/x-tex\">a</annotation></semantics></math> and <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m2\" intent=\":literal\"><semantics><mi>b</mi><annotation encoding=\"application/x-tex\">b</annotation></semantics></math> is computed as</p>\n\n",
                "matched_terms": [
                    "segment",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">More generally, given two bags of embeddings <math alttext=\"U=\\{u_{1},u_{2},\\ldots,u_{l_{u}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m10\" intent=\":literal\"><semantics><mrow><mi>U</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>u</mi><mn>1</mn></msub><mo>,</mo><msub><mi>u</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>u</mi><msub><mi>l</mi><mi>u</mi></msub></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">U=\\{u_{1},u_{2},\\ldots,u_{l_{u}}\\}</annotation></semantics></math> and <math alttext=\"V=\\{v_{1},v_{2},\\ldots,v_{l_{v}}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m11\" intent=\":literal\"><semantics><mrow><mi>V</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>v</mi><mn>1</mn></msub><mo>,</mo><msub><mi>v</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>v</mi><msub><mi>l</mi><mi>v</mi></msub></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">V=\\{v_{1},v_{2},\\ldots,v_{l_{v}}\\}</annotation></semantics></math>, where <math alttext=\"l_{u}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m12\" intent=\":literal\"><semantics><msub><mi>l</mi><mi>u</mi></msub><annotation encoding=\"application/x-tex\">l_{u}</annotation></semantics></math> and <math alttext=\"l_{v}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p7.m13\" intent=\":literal\"><semantics><msub><mi>l</mi><mi>v</mi></msub><annotation encoding=\"application/x-tex\">l_{v}</annotation></semantics></math> are number of embeddings, the collection of all speech mining alignments is</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Speech Vecalign pipeline consists of three steps:\nspeech preprocessing (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>), segment alignment with Vecalign (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS2\" title=\"3.2 Speech Segment Alignment &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), and alignment postprocessing (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS3\" title=\"3.3 Alignment Postprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>).\nAn illustration of our method is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.</p>\n\n",
                "matched_terms": [
                    "vecalign",
                    "segment",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech preprocessing consists of document segmentation and detection of identical untranslated segments.</p>\n\n",
                "matched_terms": [
                    "segments",
                    "detection",
                    "identical",
                    "segmentation",
                    "untranslated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Segmentation</span>. Same as speech mining, we first segment each speech document by VAD.\nWe apply Silero VAD&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Silero Team (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib41\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "segment",
                    "segmentation",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Detection of identical untranslated segments</span>.\nAs mentioned by <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib47\" title=\"\">2021a</a>)</cite>, some source and target segments contain identical untranslated content due to recording issues.\nWe introduce this additional step to detect such pairs of segments <span class=\"ltx_text ltx_font_italic\">prior to</span> applying the alignment algorithms, in order to make sure they are not aligned.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "segments",
                    "detection",
                    "identical",
                    "untranslated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To find potentially identical untranslated segment pairs, we use a <span class=\"ltx_text ltx_font_italic\">location heuristic</span> that they tend to locate in roughly the same position within the source and target documents.\nFor instance, within each pair of parallel documents, for a source segment <math alttext=\"x_{a}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m1\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mi>a</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">x_{a}^{i}</annotation></semantics></math> spanning timestamp <math alttext=\"s_{x_{a}}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m2\" intent=\":literal\"><semantics><msubsup><mi>s</mi><msub><mi>x</mi><mi>a</mi></msub><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">s_{x_{a}}^{i}</annotation></semantics></math> to <math alttext=\"e_{x_{a}}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m3\" intent=\":literal\"><semantics><msubsup><mi>e</mi><msub><mi>x</mi><mi>a</mi></msub><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">e_{x_{a}}^{i}</annotation></semantics></math>, we search for a target segment <math alttext=\"y_{b}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m4\" intent=\":literal\"><semantics><msubsup><mi>y</mi><mi>b</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">y_{b}^{i}</annotation></semantics></math> whose midpoint <math alttext=\"\\frac{s_{y_{b}}^{i}+e_{y_{b}}^{i}}{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m5\" intent=\":literal\"><semantics><mfrac><mrow><msubsup><mi>s</mi><msub><mi>y</mi><mi>b</mi></msub><mi>i</mi></msubsup><mo>+</mo><msubsup><mi>e</mi><msub><mi>y</mi><mi>b</mi></msub><mi>i</mi></msubsup></mrow><mn>2</mn></mfrac><annotation encoding=\"application/x-tex\">\\frac{s_{y_{b}}^{i}+e_{y_{b}}^{i}}{2}</annotation></semantics></math> is closest to <math alttext=\"\\frac{s_{x_{a}}^{i}+e_{x_{a}}^{i}}{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m6\" intent=\":literal\"><semantics><mfrac><mrow><msubsup><mi>s</mi><msub><mi>x</mi><mi>a</mi></msub><mi>i</mi></msubsup><mo>+</mo><msubsup><mi>e</mi><msub><mi>x</mi><mi>a</mi></msub><mi>i</mi></msubsup></mrow><mn>2</mn></mfrac><annotation encoding=\"application/x-tex\">\\frac{s_{x_{a}}^{i}+e_{x_{a}}^{i}}{2}</annotation></semantics></math>, midpoint of <math alttext=\"x_{a}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m7\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mi>a</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">x_{a}^{i}</annotation></semantics></math>, since the untranslated target segment is very likely to have a similar time span (<math alttext=\"s_{y_{b}}^{i}\\approx s_{x_{a}}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m8\" intent=\":literal\"><semantics><mrow><msubsup><mi>s</mi><msub><mi>y</mi><mi>b</mi></msub><mi>i</mi></msubsup><mo>&#8776;</mo><msubsup><mi>s</mi><msub><mi>x</mi><mi>a</mi></msub><mi>i</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">s_{y_{b}}^{i}\\approx s_{x_{a}}^{i}</annotation></semantics></math>, <math alttext=\"e_{y_{b}}^{i}\\approx e_{x_{a}}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m9\" intent=\":literal\"><semantics><mrow><msubsup><mi>e</mi><msub><mi>y</mi><mi>b</mi></msub><mi>i</mi></msubsup><mo>&#8776;</mo><msubsup><mi>e</mi><msub><mi>x</mi><mi>a</mi></msub><mi>i</mi></msubsup></mrow><annotation encoding=\"application/x-tex\">e_{y_{b}}^{i}\\approx e_{x_{a}}^{i}</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "segment",
                    "each",
                    "untranslated",
                    "identical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the two segments have both similar durations and filterbank features, we classify them as identical.\nFor durations, we compute the time difference.\nFor filterbank feature, we compute Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.E6\" title=\"In 3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>:</p>\n\n",
                "matched_terms": [
                    "segments",
                    "identical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We empirically determine 0.1 second and <math alttext=\"\\text{sim}(\\mathbf{A},\\mathbf{B})=5.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p6.m1\" intent=\":literal\"><semantics><mrow><mrow><mtext>sim</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119808;</mi><mo>,</mo><mi>&#119809;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mn>5.0</mn></mrow><annotation encoding=\"application/x-tex\">\\text{sim}(\\mathbf{A},\\mathbf{B})=5.0</annotation></semantics></math> as the thresholds for duration and filterbank similarities.\nNote that we cannot depend on speech embeddings for detection, as speech encoders are multilingual and their embeddings are language-agnostic.</p>\n\n",
                "matched_terms": [
                    "duration",
                    "second",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We perform segment alignment based on the similarity between speech segment embeddings.\nUnlike speech mining, which solely relies on similarity scores, we use a dynamic programming&#160;(DP) algorithm to align segments in chronological order.</p>\n\n",
                "matched_terms": [
                    "segments",
                    "segment",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Segment concatenation</span>.\nSpeech segments do not necessarily correspond to complete sentences.\nSame as speech mining, we first progressively concatenate each segment with the subsequent ones.\nEach concatenated segment can contain up to 5 original segments and span a maximum of 20 seconds.</p>\n\n",
                "matched_terms": [
                    "seconds",
                    "concatenation",
                    "segments",
                    "each",
                    "segment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Obtaining segment embeddings</span>.\nAfter concatenations, we obtain speech segment embeddings using SpeechLASER models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib9\" title=\"\">2021</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nIdentical untranslated segments detected in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, along with all concatenated segments that include them, are skipped and replaced with <math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><mn>0</mn></math>-valued vectors.</p>\n\n",
                "matched_terms": [
                    "segments",
                    "after",
                    "identical",
                    "untranslated",
                    "segment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Applying Vecalign to embeddings</span>.\nWe follow&#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Thompson and Koehn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib43\" title=\"\">2019</a>)</cite> to define the cost of aligning two segment embeddings, which serves as the cost function for DP:</p>\n\n",
                "matched_terms": [
                    "vecalign",
                    "segment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math> and <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><mi>y</mi><annotation encoding=\"application/x-tex\">y</annotation></semantics></math> are segment embeddings.\n<math alttext=\"\\text{cos}(\\cdot,\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><mrow><mtext>cos</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo rspace=\"0em\">,</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{cos}(\\cdot,\\cdot)</annotation></semantics></math> computes the cosine similarity.\n<math alttext=\"x_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m4\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">x_{s}</annotation></semantics></math> and <math alttext=\"y_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m5\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>s</mi></msub><annotation encoding=\"application/x-tex\">y_{s}</annotation></semantics></math> are uniformly sampled source and target embeddings, and <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m6\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> is the sample size.\n<math alttext=\"\\text{nSegs}(x)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m7\" intent=\":literal\"><semantics><mrow><mtext>nSegs</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{nSegs}(x)</annotation></semantics></math> is used to denote the number of original segments in <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m8\" intent=\":literal\"><semantics><mi>x</mi><annotation encoding=\"application/x-tex\">x</annotation></semantics></math>, which penalizes aligning long concatenations.</p>\n\n",
                "matched_terms": [
                    "segments",
                    "segment",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The embedding alignment algorithm is recursive DP.\nGiven a document pair and corresponding embeddings, the algorithm recursively averages every two consecutive embeddings, halving the sequence length until it reaches a small threshold.\nAt the bottom level, standard DP is applied to obtain an initial alignment.\nSubsequently, at each recursion level bottom-up, DP refines the alignment by searching within a small window around the alignment path from the previous level.\nBy constraining the search space and reducing the sequence length at each level, the algorithm achieves a linear time and space complexity.\nThe recursive DP algorithm runs on CPU and takes a few seconds on average per document pair.\nWe direct the readers to <cite class=\"ltx_cite ltx_citemacro_citet\">Thompson and Koehn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib43\" title=\"\">2019</a>)</cite> for a complete description.</p>\n\n",
                "matched_terms": [
                    "seconds",
                    "each",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because of DP, the resultant alignments strictly follow chronological order.\nWe use <math alttext=\"x_{a:b}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m1\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mrow><mi>a</mi><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><mi>b</mi></mrow><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">x_{a:b}^{i}</annotation></semantics></math> to denote the concatenation of consecutive segments <math alttext=\"x_{a}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m2\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mi>a</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">x_{a}^{i}</annotation></semantics></math> through <math alttext=\"x_{b}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m3\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mi>b</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">x_{b}^{i}</annotation></semantics></math>.\nFor any two alignments <math alttext=\"(x_{a_{s}:a_{e}}^{i},y_{b_{s}:b_{e}}^{j})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mrow><msub><mi>a</mi><mi>s</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>a</mi><mi>e</mi></msub></mrow><mi>i</mi></msubsup><mo>,</mo><msubsup><mi>y</mi><mrow><msub><mi>b</mi><mi>s</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>b</mi><mi>e</mi></msub></mrow><mi>j</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{a_{s}:a_{e}}^{i},y_{b_{s}:b_{e}}^{j})</annotation></semantics></math> and <math alttext=\"(x_{c_{s}:c_{e}}^{i},y_{d_{s}:d_{e}}^{k})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mrow><msub><mi>c</mi><mi>s</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>c</mi><mi>e</mi></msub></mrow><mi>i</mi></msubsup><mo>,</mo><msubsup><mi>y</mi><mrow><msub><mi>d</mi><mi>s</mi></msub><mo lspace=\"0.278em\" rspace=\"0.278em\">:</mo><msub><mi>d</mi><mi>e</mi></msub></mrow><mi>k</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{c_{s}:c_{e}}^{i},y_{d_{s}:d_{e}}^{k})</annotation></semantics></math>, Speech Vecalign guarantees that <math alttext=\"i=j=k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m6\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>=</mo><mi>j</mi><mo>=</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">i=j=k</annotation></semantics></math> and that either <math alttext=\"a_{e}&lt;c_{s},b_{e}&lt;d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m7\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>a</mi><mi>e</mi></msub><mo>&lt;</mo><msub><mi>c</mi><mi>s</mi></msub></mrow><mo>,</mo><mrow><msub><mi>b</mi><mi>e</mi></msub><mo>&lt;</mo><msub><mi>d</mi><mi>s</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">a_{e}&lt;c_{s},b_{e}&lt;d_{s}</annotation></semantics></math> or <math alttext=\"a_{s}&gt;c_{e},b_{s}&gt;d_{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m8\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>a</mi><mi>s</mi></msub><mo>&gt;</mo><msub><mi>c</mi><mi>e</mi></msub></mrow><mo>,</mo><mrow><msub><mi>b</mi><mi>s</mi></msub><mo>&gt;</mo><msub><mi>d</mi><mi>e</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">a_{s}&gt;c_{e},b_{s}&gt;d_{e}</annotation></semantics></math>.\nIn contrast, Local Mining ensures <math alttext=\"i=j=k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m9\" intent=\":literal\"><semantics><mrow><mi>i</mi><mo>=</mo><mi>j</mi><mo>=</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">i=j=k</annotation></semantics></math> but has no constraints on <math alttext=\"a,b,c,d\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p6.m10\" intent=\":literal\"><semantics><mrow><mi>a</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi>c</mi><mo>,</mo><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">a,b,c,d</annotation></semantics></math>, while\nGlobal Mining makes no guarantees at all.</p>\n\n",
                "matched_terms": [
                    "segments",
                    "alignments",
                    "vecalign",
                    "concatenation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Removing low-quality alignments</span>.\nFirst, we remove unaligned segments and high-cost alignments.\nThe unaligned segments are due to deletions in the DP algorithm.\nIdentical untranslated segments detected in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a> may fall into either category due to their 0-valued vectors.</p>\n\n",
                "matched_terms": [
                    "removing",
                    "alignments",
                    "segments",
                    "identical",
                    "lowquality",
                    "untranslated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Detection of identical untranslated segments, again</span>.\nOccasionally, the location heuristic in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a> may fail, resulting in a small number of low-cost alignments with identical untranslated source and target segments.\nSearching is not needed at this step, as we already have the alignments.\nWe apply Equation <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.E6\" title=\"In 3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> to remaining alignments, where <math alttext=\"\\mathbf{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m1\" intent=\":literal\"><semantics><mi>&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{A}</annotation></semantics></math> and <math alttext=\"\\mathbf{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p3.m2\" intent=\":literal\"><semantics><mi>&#119809;</mi><annotation encoding=\"application/x-tex\">\\mathbf{B}</annotation></semantics></math> are source and target segments in each alignment.\nWe use the same thresholds in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a> to remove alignments.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "alignments",
                    "segments",
                    "detection",
                    "identical",
                    "each",
                    "untranslated",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Alignment concatenation</span>.\nAnother issue is that the raw alignments are too short: the average duration is 4.25 seconds, with 66% shorter than 5 seconds.\nTo cover more context, we progressively concatenate each alignment with the subsequent ones.\nThis can be easily done as alignments are in chronological order.\nEach concatenated alignment can contain up to 3 original alignments and span up to 20 seconds.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "seconds",
                    "alignments",
                    "concatenation",
                    "duration",
                    "short",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Global margin-scores computation</span>.\nThe raw alignments only have alignment costs as a quality indicator, which are computed <span class=\"ltx_text ltx_font_italic\">within</span> each document pair.\nTo assess alignment quality <span class=\"ltx_text ltx_font_italic\">across</span> document pairs, we train FAISS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Johnson et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib22\" title=\"\">2019</a>)</cite> indexes and compute margin-scores&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Artetxe and Schwenk (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib2\" title=\"\">2019a</a>)</cite> using Equation&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S2.E1\" title=\"In 2 Speech Mining Overview &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> for <span class=\"ltx_text ltx_font_italic\">all</span> obtained alignments, following the common strategy in MT dataset curation&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Sloto et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib42\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "each",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Removing highly-overlapped alignments</span>.\nFinally, we remove alignments that have too much overlap with others, following <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nFor any two consecutive alignments, we compute the ratio of the overlapped source duration to the maximum duration of the two source segments.\nIf the ratio exceeds a threshold, we discard the one with a lower margin-score.\nWe train S2ST models with multiple threshold values to determine the best one.\nOur experiments in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.SS1\" title=\"D.1 Optimizing &#119898;&#8290;&#119886;&#8290;&#119909;&#8290;_&#8290;&#119900;&#8290;&#119907;&#8290;&#119890;&#8290;&#119903;&#8290;&#119897;&#8290;&#119886;&#8290;&#119901; &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">D.1</span></a> suggest that 0.4 work best for Global Mining and 0.8 work best for Local Mining and Speech Vecalign.</p>\n\n",
                "matched_terms": [
                    "removing",
                    "alignments",
                    "highlyoverlapped",
                    "vecalign",
                    "segments",
                    "best",
                    "duration"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We apply Speech Vecalign, Global Mining, and Local Mining to the same raw data and train S2ST models on each type of alignments, providing a fair comparison.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "vecalign",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data source.</span>\nWe use the unlabeled, unsegmented English and German plenary session recordings from VoxPopuli v1&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib47\" title=\"\">2021a</a>)</cite> as raw data.\nVoxPopuli contains European Parliament plenary session recordings in each of the 23 European Union languages, paired with spoken interpretations into the other languages.\nThe document names are formatted as <code class=\"ltx_verbatim ltx_font_typewriter\">${session_id}_${language}.ogg</code>, and paired documents have the same <code class=\"ltx_verbatim ltx_font_typewriter\">${session_id}</code>.\nTo avoid overlapping with the test set (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS2\" title=\"4.2 Evaluation Data &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>), we only choose sessions from year 2013 to 2020.\nWe also exclude sessions in the development set&#160;(Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS2\" title=\"4.2 Evaluation Data &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>).\nFor En-to-De, the remaining data has 4,880 documents totaling about 3,000 hours for each language.\nFor De-to-En, there are 5,782 documents totaling 3,400 hours per language.\nThe difference is due to the different dev and test sets.\nAll documents are in pairs, allowing all methods to have exactly the same raw data.</p>\n\n",
                "matched_terms": [
                    "choose",
                    "each",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Vecalign.</span>\nWe apply Speech Vecalign to each pair of speech documents and obtain alignments sorted by margin-scores.\nTraining data is chosen in descending order of margin-scores.\nWe train models on different data sizes and report the best results in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS5\" title=\"4.5 Main Results &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.5</span></a>.\nMore details on data size optimization can be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.SS2\" title=\"D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "best",
                    "alignments",
                    "vecalign",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech mining baselines.</span>\nWe apply Global Mining and Local Mining to the same raw data and embeddings as Speech Vecalign.\nThe implementation is based on <span class=\"ltx_text ltx_font_typewriter\">stopes<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">3</span></span><a class=\"ltx_ref ltx_url\" href=\"https://github.com/facebookresearch/stopes\" title=\"\">https://github.com/facebookresearch/stopes</a></span></span></span></span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Andrews et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib1\" title=\"\">2022</a>)</cite>.\nAfter mining, we apply the same postprocessing strategies in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS3\" title=\"3.3 Alignment Postprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>, except for alignment concatenation which is not applicable.\nTraining data is chosen in descending order of margin-scores and details on data size optimization can be found in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.SS2\" title=\"D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "concatenation",
                    "vecalign",
                    "alignment",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train speech-to-unit translation (S2UT) models&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib28\" title=\"\">2022a</a>)</cite> with <span class=\"ltx_text ltx_font_typewriter\">fairseq<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">4</span></span><a class=\"ltx_ref ltx_url\" href=\"https://github.com/facebookresearch/fairseq\" title=\"\">https://github.com/facebookresearch/fairseq</a></span></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">Ott et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib32\" title=\"\">2019</a>); Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib48\" title=\"\">2020</a>)</cite> on each type of alignments.\nThe S2UT model takes source speech as input and predicts a sequence of target discrete units.\nThe discrete units are obtained by applying\na k-means model to the <math alttext=\"11^{\\text{th}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><msup><mn>11</mn><mtext>th</mtext></msup><annotation encoding=\"application/x-tex\">11^{\\text{th}}</annotation></semantics></math> layer features of a HuBERT model <cite class=\"ltx_cite ltx_citemacro_cite\">Hsu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib16\" title=\"\">2021</a>)</cite>.\nFor English, we use the mHuBERT from <cite class=\"ltx_cite ltx_citemacro_citet\">Lee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib29\" title=\"\">2022b</a>)</cite>, and for German, we use the Germanic mHuBERT from <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nConsecutive duplicated units are removed.\nOur S2UT model architecture follows exactly <cite class=\"ltx_cite ltx_citemacro_citet\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite>.\nThe architecture details and training hyperparameters are in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A2\" title=\"Appendix B Speech-to-Speech Translation &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Intriguingly, for both directions, Speech Vecalign and speech mining models are competitive with or outperform SpeechMatrix&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite> models, despite the latter being mined from about 24k hours of speech per language, <span class=\"ltx_text ltx_font_italic\">8 times more</span> than our raw data.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>We do not aim for state-of-the-art performance. Our results are not directly comparable to SpeechMatrix. We report SpeechMatrix results only to show the performance gap.</span></span></span>\nFor En-to-De, our Global Mining and Speech Vecalign models achieve improvements of 0.94 and 1.31 BLEU, respectively.\nOur Local Mining model achieves even 1.64 BLEU improvement.\nWe suspect that SpeechMatrix has not removed identical untranslated segments prior to and after mining, which significantly hurts model performance.\nFurther discussion is in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.SS5\" title=\"5.5 Removing Identical Untranslated Segments is Critical &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5.5</span></a>.</p>\n\n",
                "matched_terms": [
                    "hours",
                    "vecalign",
                    "segments",
                    "after",
                    "identical",
                    "untranslated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our Speech Vecalign models outperform both speech mining models in both directions.\nFor En-to-De, the Speech Vecalign model achieves 12.58 BLEU, comparable with our strong Global Mining and Local Mining baselines.\nIn terms of chrF2++, it surpasses Global Mining and Local Mining by 1.69 and 0.26, respectively.\nIt also significantly improves their referenced BLASER 2.0 by 0.08 and 0.03.\nFor De-to-En, Speech Vecalign and Global Mining models achieve comparable BLEU (16.14 vs. 15.96), but Speech Vecalign surpasses Global Mining by 0.57 in chrF2++.\nSpeech Vecalign significantly outperforms Local Mining under all metrics.\nThese results demonstrate that Speech Vecalign produces higher-quality alignments than both speech mining baselines.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "vecalign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We analyze properties of speech mining methods and compare them with Speech Vecalign.\nAlthough we show that speech mining methods produce alignments similar to those of Speech Vecalign, the latter offers advantages of producing longer and less noisy alignments.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "vecalign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, we show that Global Mining mostly <span class=\"ltx_text ltx_font_bold\">locally</span> aligns speech documents.\nWhile Global Mining searches for the best matching segment pairs among roughly 10 million segments, one might expect its alignments to cover the spread of the entire dataset.\nOn the contrary, we find that Global Mining alignments are concentrated within document pairs, each typically containing hundreds to thousands of segments.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "segments",
                    "best",
                    "each",
                    "segment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To quantify this, we examine the 1000-hour Global Mining data and count alignments whose source and target segments come from <span class=\"ltx_text ltx_font_italic\">different</span> document pairs.\nAs shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F3\" title=\"Figure 3 &#8227; 5.1 Speech Mining Mostly Locally Aligns Segments in Time Order &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, fewer than 6% fall into this category, while the majority&#160;(<math alttext=\"&gt;93\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mrow><mn>93</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">&gt;93\\%</annotation></semantics></math>) are within paired documents.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "segments"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, we analyze the time order of alignments produced by both speech mining methods.\nBorrowing the notation from Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS2\" title=\"3.2 Speech Segment Alignment &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, we define two pairs of alignments to be <span class=\"ltx_text ltx_font_italic\">in-order</span> if either <math alttext=\"a_{e}&lt;c_{s},b_{e}&lt;d_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>a</mi><mi>e</mi></msub><mo>&lt;</mo><msub><mi>c</mi><mi>s</mi></msub></mrow><mo>,</mo><mrow><msub><mi>b</mi><mi>e</mi></msub><mo>&lt;</mo><msub><mi>d</mi><mi>s</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">a_{e}&lt;c_{s},b_{e}&lt;d_{s}</annotation></semantics></math> or <math alttext=\"a_{s}&gt;c_{e},b_{s}&gt;d_{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>a</mi><mi>s</mi></msub><mo>&gt;</mo><msub><mi>c</mi><mi>e</mi></msub></mrow><mo>,</mo><mrow><msub><mi>b</mi><mi>s</mi></msub><mo>&gt;</mo><msub><mi>d</mi><mi>e</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">a_{s}&gt;c_{e},b_{s}&gt;d_{e}</annotation></semantics></math>; otherwise, they are <span class=\"ltx_text ltx_font_italic\">out-of-order</span>.\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F3\" title=\"Figure 3 &#8227; 5.1 Speech Mining Mostly Locally Aligns Segments in Time Order &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows that only around 1% alignments are out-of-order for both speech mining methods.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "second"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the observations in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.SS1\" title=\"5.1 Speech Mining Mostly Locally Aligns Segments in Time Order &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> that speech mining produces mostly local, in-order alignments, we analyze the similarity between them and Speech Vecalign alignments.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "vecalign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ the alignment evaluation method<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/thompsonb/vecalign/blob/master/score.py\" title=\"\">https://github.com/thompsonb/vecalign/blob/master/score.py</a>.</span></span></span> from&#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Thompson and Koehn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib43\" title=\"\">2019</a>)</cite>, which computes precision and recall by comparing system alignments to a reference.\nThere are two modes: <span class=\"ltx_text ltx_font_italic\">Strict</span>, which counts only exact matches as true positives, and <span class=\"ltx_text ltx_font_italic\">Lax</span>, which considers an alignment as true positive if both its source and target segment overlap with the reference.\nIf not true positive, an alignment is false positive.\nRecall is computed by swapping the reference and the system alignments.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "segment",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Without loss of generality, we use Speech Vecalign En-De alignments as the reference, and evaluate speech mining ones.\nWe choose 700k highest-scoring alignments from all three methods to ensure a fair comparison.\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.T2\" title=\"Table 2 &#8227; 5.2 Speech Mining Methods Produce Similar Alignments as Speech Vecalign &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows that about 30% of speech mining alignments are exactly the same as those of Speech Vecalign, and about 90% overlap with Speech Vecalign alignments.\nThis high similarity explains why Speech Vecalign and speech mining models have similar performance.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "vecalign",
                    "choose"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As speech mining and Speech Vecalign produce similar alignments, we explore why Speech Vecalign models still perform better.\nA key advantage of Speech Vecalign is that it first produces fine-grained alignments and then constructs alignments with different amounts of context, thanks to the alignment concatenation strategy.\nSpeech mining methods, on the other hand, solely depend on margin-scores and tend to favor shorter alignments.</p>\n\n",
                "matched_terms": [
                    "concatenation",
                    "alignments",
                    "vecalign",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With the best En-to-De models and corresponding data sizes from Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS5\" title=\"4.5 Main Results &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.5</span></a>, Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F4\" title=\"Figure 4 &#8227; 5.3 Speech Vecalign Produces Longer Alignments &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> presents the average sentence-level chrF2++ scores on the test set and the percentage of training alignments for different source speech duration ranges.\nNotably, Speech Vecalign has a large portion of long training samples: the blue bars are highest for durations longer than 12 seconds.\nSpecifically, the average source duration of Speech Vecalign is 8.51 seconds, while Global Mining and Local Mining have average durations of 7.50 and 8.53 seconds, respectively.\nAs a result, the Speech Vecalign model performs better on test samples longer than 10 seconds, while having comparable performance on shorter ones.\nThis highlights that Speech Vecalign is able to produce longer, context-rich alignments which help to improve S2ST model performance.\nInterestingly, Local Mining surpasses the Global Mining model on long inputs, which could be also attributed to its longer training samples.</p>\n\n",
                "matched_terms": [
                    "seconds",
                    "alignments",
                    "vecalign",
                    "best",
                    "duration"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We visualize alignments produced by different methods for the same document pair, which is about 10 minutes long and contains around 200 segments.\nFor reference, we manually created a gold segment-level alignment, with detailed procedure in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A6\" title=\"Appendix F Procedure of Manual Alignments &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>.\nWe illustrate the best 80 alignments for each of the speech mining methods.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "alignments",
                    "segments",
                    "best",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.F5\" title=\"Figure 5 &#8227; 5.3 Speech Vecalign Produces Longer Alignments &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows, Speech Vecalign produces the most fine-grained alignments and is most similar to the gold reference.\nGlobal Mining also performs well, aligning closely with the groundtruth path, whereas Local Mining produces more noise and misses more alignments along the correct path.\nWe hypothesize Local Mining has limited number of segments in a single document pair, making nearest neighbors less effective normalizers in the margin-score computation.</p>\n\n",
                "matched_terms": [
                    "segments",
                    "alignments",
                    "vecalign",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As presented in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S4.SS5\" title=\"4.5 Main Results &#8227; 4 Experiments &amp; Results &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4.5</span></a>, our reproduced speech mining models achieve comparable or even better results than SpeechMatrix models.\nBy listening to samples of SpeechMatrix alignments, we observed many cases where the source and target segments contained identical untranslated content, which is an issue mentioned in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>.\nUsing the method described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS1\" title=\"3.1 Speech Preprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>, we identified approximately 100k out of 630k alignments with untranslated source and target segments, totaling 181 hours.</p>\n\n",
                "matched_terms": [
                    "hours",
                    "alignments",
                    "segments",
                    "identical",
                    "untranslated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the impact of untranslated segments, we trained models on the original SpeechMatrix En-De alignments and on a version with untranslated alignments <span class=\"ltx_text ltx_font_italic\">removed</span>.\nThe training data is chosen with a margin-score threshold of 1.09, following the original setup.\nAs shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.T3\" title=\"Table 3 &#8227; 5.5 Removing Identical Untranslated Segments is Critical &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, the cleaned data produces better models, improving BLEU score by <span class=\"ltx_text ltx_font_bold\">1.00</span> for En-to-De and <span class=\"ltx_text ltx_font_bold\">0.11</span> for De-to-En, despite having 13% less training data.\nThe smaller gain on De-to-En may be due to most untranslated segments being in English, which have smaller impact on into-English translation.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "untranslated",
                    "segments"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We also re-produced our alignment pipelines <span class=\"ltx_text ltx_font_italic\">without</span> removing identical untranslated segments, referred to as &#8220;noisy\" in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.T4\" title=\"Table 4 &#8227; 5.5 Removing Identical Untranslated Segments is Critical &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nWe trained models on 500 hours of this data.\nAlthough these untranslated segments account for less than 1% of the training data, performance degrades noticeably.</p>\n\n",
                "matched_terms": [
                    "removing",
                    "alignment",
                    "hours",
                    "segments",
                    "identical",
                    "untranslated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, the experiments highlight that removing untranslated alignments is essential for S2ST training, corroborating <cite class=\"ltx_cite ltx_citemacro_citet\">Khayrallah and Koehn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib23\" title=\"\">2018</a>)</cite>, who found that the untranslated sentences are most catastrophic in neural machine translation.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "untranslated",
                    "removing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present Speech Vecalign, a parallel speech document alignment method that aligns speech segment embeddings within document pairs and in chronological order.\nWe apply Speech Vecalign to parallel English-German VoxPopuli speech documents and conduct S2ST experiments to demonstrate its superiority over two strong speech mining baselines.\nOur analysis reveals that although speech mining methods primarily align documents locally and in-order, Global Mining falls short of producing long alignments, and Local Mining in particular produces more noise.\nFor long-term future work, we plan to extend Speech Vecalign to other language pairs or other data sources.\nWe can also explore aligning speech and text embeddings to construct S2TT datasets.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "alignments",
                    "vecalign",
                    "short",
                    "segment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech features for identical untranslated segment detection could be improved.</span>\nOur current approach uses filterbank features, which are based on power spectrum, to detect identical untranslated segments.\nHowever, filterbank features are likely to fail for segments that have identical content but differ in signal power.\nAs one of the anonymous reviewers pointed out, cepstral features may be a more robust alternative.</p>\n\n",
                "matched_terms": [
                    "segments",
                    "detection",
                    "identical",
                    "untranslated",
                    "segment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Bilingual text sentence alignment.</span>\nText alignment is very related to speech alignment.\nMethods apply dynamic programming <cite class=\"ltx_cite ltx_citemacro_cite\">Bellman (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib4\" title=\"\">1954</a>)</cite> and mainly differ in the design of scoring functions.\nEarly works <cite class=\"ltx_cite ltx_citemacro_cite\">Brown et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib5\" title=\"\">1991</a>); Gale and Church (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib12\" title=\"\">1993</a>)</cite> are based on sentence lengths.\nLater methods incorporate translations in various ways&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Moore (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib30\" title=\"\">2002</a>); Varga et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib44\" title=\"\">2007</a>); Sennrich and Volk (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib40\" title=\"\">2010</a>); Gomes and Lopes (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib13\" title=\"\">2016</a>)</cite>.\nOur work is inspired by Vecalign <cite class=\"ltx_cite ltx_citemacro_cite\">Thompson and Koehn (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib43\" title=\"\">2019</a>)</cite>, which utilizes margin-based cosine similarities between multilingual sentence embeddings like LASER&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Artetxe and Schwenk (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib3\" title=\"\">2019b</a>); Heffernan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib14\" title=\"\">2022</a>)</cite> and LaBSE <cite class=\"ltx_cite ltx_citemacro_cite\">Feng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib11\" title=\"\">2022</a>)</cite>.\nVecalign is also more efficient than previous methods.\nBy applying fast dynamic time warping <cite class=\"ltx_cite ltx_citemacro_cite\">Salvador and Chan (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib37\" title=\"\">2007</a>)</cite>, it has a linear time and space complexity with respect to the number of input sentences.\nThe recent progress of extending multilingual sentence embeddings to the speech modality <cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib9\" title=\"\">2021</a>); Khurana et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib24\" title=\"\">2022</a>); Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib10\" title=\"\">b</a>)</cite> enables us to align speech segments by their speech embeddings using the same algorithm.</p>\n\n",
                "matched_terms": [
                    "segments",
                    "vecalign",
                    "number",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">S2ST datasets.</span>\nThere are two common ways to automatically build an S2ST dataset: (1)&#160;building alignments from multilingual speech data; (2)&#160;synthesizing speech for text translations from existing speech-to-text translation (S2TT) corpora.\nThe first line of work has human spoken speech on both source and target sides.\nVoxPopuli <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib47\" title=\"\">2021a</a>)</cite> aligns multilingual speech documents based on text transcriptions, yielding 17.3k-hour alignments between 15 source and target languages.\nSpeechMatrix <cite class=\"ltx_cite ltx_citemacro_cite\">Duquenne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib8\" title=\"\">2023a</a>)</cite> applies Global Mining with SpeechLASER embeddings on VoxPopuli. It obtains alignments for 136 language pairs with an average of 1537 hours per direction.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Seamless Communication et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib39\" title=\"\">2023</a>)</cite> apply Global Mining to web-crawled speech data with SONAR embeddings.\n<cite class=\"ltx_cite ltx_citemacro_citet\">Seamless Communication et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib39\" title=\"\">2023</a>)</cite> also mine a SeamlessAlignExpressive dataset with expressively- and semantically-aligned segment pairs, based on a blend of both semantic and prosodic similarity score <cite class=\"ltx_cite ltx_citemacro_cite\">Heffernan et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib15\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "segment",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The second line of work has synthesized speech on the target side.\nFisher <cite class=\"ltx_cite ltx_citemacro_cite\">Post et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib35\" title=\"\">2013</a>)</cite> is a Spanish-English S2TT dataset containing about 170 hours of Spanish telephone conversations and English translations which are used to synthesize English speech.\nCVSS <cite class=\"ltx_cite ltx_citemacro_cite\">Jia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib20\" title=\"\">2022b</a>)</cite> is an S2ST dataset covering utterances from 21 languages to English, obtained by synthesizing the text translations in CoVoST 2 <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib49\" title=\"\">2021b</a>)</cite>.\nBesides automatic methods, FLEURS <cite class=\"ltx_cite ltx_citemacro_cite\">Conneau et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#bib.bib6\" title=\"\">2023</a>)</cite> has collected human read speech covering 102 languages. But it contains only about 12 hours per language and is intended for evaluation.</p>\n\n",
                "matched_terms": [
                    "hours",
                    "second"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Due to limited computing resources, we adopt different training strategies for different purposes.\nThe 500-hour datasets are used for hyperparameter optimization, and larger datasets are used for reporting main results.\nAll models are trained for up to 400k steps, with the first 10,000 steps as a warmup stage.\nFor experiments on a 500-hour dataset, we use a batch size of 320k tokens and apply early-stopping if there is no improvement on the development set for 30 epochs.\nThese models are trained on 4 NVIDIA GeForce GTX 1080 Ti GPUs for approximately 15 days.\nFor larger datasets, we increase the batch size to 640k tokens and early-stopping is not applied.\nThese models are trained on 2 NVIDIA A100-SXM4-80GB GPUs for approximately 15 days.\nThe best checkpoint is selected based on the development set loss.\nAll experiments are conducted in fp32, as we found training with fp16 and amp very unstable.</p>\n\n",
                "matched_terms": [
                    "best",
                    "stage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Segment embedding.</span>\nThis is the most time-consuming step.\nWe use a mixture of NVIDIA GeForce GTX 1080 and 2080 Ti GPUs.\nEmbedding about 6,000 hours of speech (3,000 hours for each language) took approximately 1,100 GPU hours.</p>\n\n",
                "matched_terms": [
                    "segment",
                    "each",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Alignment.</span>\nLocal Mining and Global Mining run on a single GPU.\nThey take about 2 hours.\nSpeech Vecalign runs on a single CPU and takes about 2 hours.</p>\n\n",
                "matched_terms": [
                    "vecalign",
                    "hours",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">There are two hyperparameters that affect training data: (1) the maximum source duration overlap ratio between alignments, <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m1\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math>, which is mentioned in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S3.SS3\" title=\"3.3 Alignment Postprocessing &#8227; 3 Proposed Method: Speech Vecalign &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>, and\n(2) the data size.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "duration",
                    "m​a​x​​o​v​e​r​l​a​pmaxoverlap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m1\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> controls the trade-off between overlapped durations and data quality.\nFor instance, a lower <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m2\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> reduces the overlap but also discards alignments more aggressively.\nOverlapped alignments usually have similar margin-scores, so more high-quality alignments are lost.\nThe data size controls the trade-off between data size and data quality cutoff.\nFor instance, a larger dataset will have a lower quality cutoff, as alignments are selected in descending order of margin-scores.\nIn this section, we optimize the combination of <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m3\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> and data size by training S2UT models on different datasets.\nNote that the raw data stays the same.</p>\n\n",
                "matched_terms": [
                    "alignments",
                    "m​a​x​​o​v​e​r​l​a​pmaxoverlap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first experiment with different values of <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math>.\nWe apply different <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> thresholds during the postprocessing stage, and always choose the best 500 hours as the training data.\nThe optimal value is determined based on development set ASR-BLEU.\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.F6\" title=\"Figure 6 &#8227; D.1 Optimizing &#119898;&#8290;&#119886;&#8290;&#119909;&#8290;_&#8290;&#119900;&#8290;&#119907;&#8290;&#119890;&#8290;&#119903;&#8290;&#119897;&#8290;&#119886;&#8290;&#119901; &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows that 0.8 works best for Speech Vecalign and Local Mining and 0.4 works best for Global Mining.\nThe test set performance is also drawn in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.F6\" title=\"Figure 6 &#8227; D.1 Optimizing &#119898;&#8290;&#119886;&#8290;&#119909;&#8290;_&#8290;&#119900;&#8290;&#119907;&#8290;&#119890;&#8290;&#119903;&#8290;&#119897;&#8290;&#119886;&#8290;&#119901; &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, exhibiting a similar trend.</p>\n\n",
                "matched_terms": [
                    "hours",
                    "vecalign",
                    "stage",
                    "best",
                    "m​a​x​​o​v​e​r​l​a​pmaxoverlap",
                    "choose"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next we optimize the training data size.\nWe fix <math alttext=\"max\\_overlap\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">_</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>v</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">max\\_overlap</annotation></semantics></math> at 0.4 for Global Mining and 0.8 for Speech Vecalign and Local Mining during postprocessing, only lowering the quality cutoff to include more training data.\nThe models are trained on different amounts of data until we find the peak performance.\nResults are shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A4.F7\" title=\"Figure 7 &#8227; D.2 Optimizing Training Data Size &#8227; Appendix D Training Data Optimization &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "vecalign",
                    "m​a​x​​o​v​e​r​l​a​pmaxoverlap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For En-to-De, the best Speech Vecalign model is trained on the 750-hour dataset, achieving 12.58 BLEU.\nIt outperforms the best Global Mining model which achieves 12.21 BLEU.\nThe best Local Mining model achieves 12.91 BLEU.\nHowever, we note that it requires a lot more data than the other two methods to achieve the peak performance.</p>\n\n",
                "matched_terms": [
                    "best",
                    "vecalign"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For De-to-En, the 1000-hour dataset works best for Speech Vecalign while the 750-hour dataset works best for Global Mining.\nLocal Mining achieves the peak performance when the data size is 1250 hours, still requiring more data than the other methods.\nThe Speech Vecalign performs better than both the Global Mining and the Local Mining models.</p>\n\n",
                "matched_terms": [
                    "best",
                    "vecalign",
                    "hours"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We manually select the corresponding words for each speech segment from the obtained transcriptions;</p>\n\n",
                "matched_terms": [
                    "segment",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the same alignment evaluation method as Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#S5.SS2\" title=\"5.2 Speech Mining Methods Produce Similar Alignments as Speech Vecalign &#8227; 5 Analysis &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a>, but we use the manual alignments as the reference.\nThere are 144 raw Speech Vecalign alignments, and we choose the same number of alignments from Global Mining and Local Mining in descending order of margin-scores.\nThe Recall and Precision of raw Speech Vecalign, Local Mining, and Global Mining alignments are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18360v1#A7.T6\" title=\"Table 6 &#8227; Appendix G Evaluation of System Alignments using the Manual Alignments &#8227; Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "alignments",
                    "vecalign",
                    "choose",
                    "number"
                ]
            }
        ]
    }
}