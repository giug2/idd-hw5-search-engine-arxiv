{
    "S2.T1": {
        "source_file": "Nord-Parl-TTS: Finnish and Swedish TTS Dataset from Parliament Speech",
        "caption": "Table 1: Comparison of Nord-Parl-TTS with existing TTS datasets for Finnish and Swedish languages.",
        "body": "Language\nDataset\nData Source\nHours\n\n\nFinnish\nPerso Synteesi [11]\n\nStudio Recording\n20\n\n\nCSS10 [12]\n\nStudio Recording\n10\n\n\nFinSyn [13]\n\nStudio Recording\n60\n\n\nNord-Parl-TTS\nParliament Recordings\n900\n\n\nSwedish\nNo Public Dataset\n\n\nNord-Parl-TTS\nParliament Recordings\n5090",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Language</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Data Source</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Hours</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"4\"><span class=\"ltx_text ltx_font_bold\">Finnish</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Perso Synteesi&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib11\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">11</span></a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Studio Recording</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">20</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">CSS10&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">Studio Recording</td>\n<td class=\"ltx_td ltx_align_center\">10</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">FinSyn&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib13\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">13</span></a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">Studio Recording</td>\n<td class=\"ltx_td ltx_align_center\">60</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Nord-Parl-TTS</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Parliament Recordings</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">900</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Swedish</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"3\">No Public Dataset</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\">Nord-Parl-TTS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\">Parliament Recordings</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\">5090</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "source",
            "studio",
            "public",
            "synteesi",
            "datasets",
            "hours",
            "tts",
            "swedish",
            "parliament",
            "existing",
            "language",
            "css10",
            "recordings",
            "dataset",
            "perso",
            "nordparltts",
            "finsyn",
            "recording",
            "languages",
            "data",
            "comparison",
            "finnish"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">A brief summary of the existing Finnish and Swedish TTS datasets is illustrated in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#S2.T1\" title=\"Table 1 &#8227; 2 Related Work &#8227; Nord-Parl-TTS: Finnish and Swedish TTS Dataset from Parliament Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. To our best knowledge, there is no existing public Swedish TTS dataset. Regarding Finnish,&#160;<span class=\"ltx_text ltx_font_italic\">Perso Synteesi</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib11\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">11</span></a>]</cite> is a 20-hour speech dataset with 30 male and 30 female native speakers; <span class=\"ltx_text ltx_font_italic\">CSS10</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite> contains 10 hours of Finnish audiobooks read by a male speaker; and <span class=\"ltx_text ltx_font_italic\">FinSyn</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib13\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">13</span></a>]</cite> consists of approximately 60 hours of speech in various speaking styles by two female speakers. These datasets are on a small scale and are not suitable for training large speech generation models. Furthermore, <span class=\"ltx_text ltx_font_italic\">Perso Synteesi</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib11\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">11</span></a>]</cite> and <span class=\"ltx_text ltx_font_italic\">FinSyn</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib13\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">13</span></a>]</cite> are not conveniently available from open-sourced data hubs, like <span class=\"ltx_text ltx_font_italic\">OpenSLR<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_upright\">1</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_upright\" href=\"https://www.openslr.org/index.html\" title=\"\">https://www.openslr.org/index.html</a></span></span></span></span> or <span class=\"ltx_text ltx_font_italic\">HuggingFace datasets<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_upright\">2</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_upright\" href=\"https://huggingface.co/datasets\" title=\"\">https://huggingface.co/datasets</a></span></span></span></span>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Text-to-speech (TTS) development is limited by scarcity of high-quality, publicly available speech data for most languages outside a few high-resource languages.\nWe present <span class=\"ltx_text ltx_font_italic\">Nord-Parl-TTS</span>, an open TTS dataset for Finnish and Swedish based on speech found in the wild.\nUsing recordings of Nordic parliamentary proceedings, we extract 900 hours of Finnish and 5090 hours of Swedish speech suitable for TTS training.\nThe dataset is built using an adapted version of the Emilia data processing pipeline and includes unified evaluation sets to support model development and benchmarking.\nBy offering open, large-scale data for Finnish and Swedish, Nord-Parl-TTS narrows the resource gap in TTS between high- and lower-resourced languages.</p>\n\n",
                "matched_terms": [
                    "recordings",
                    "tts",
                    "swedish",
                    "dataset",
                    "hours",
                    "languages",
                    "data",
                    "nordparltts",
                    "finnish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;</span></span>\nText-to-Speech, Dataset, Benchmark, Low-resource Languages, In-the-wild Data</p>\n\n",
                "matched_terms": [
                    "data",
                    "dataset",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Text-to-speech (TTS) converts written text into speech. With current methods, the humanlikeness and intelligibility of the resulting speech depend on the availability of speech datasets.\nWhen trained on extensive, high-quality data, modern TTS systems can generate speech with intelligibility and prosodic variation comparable to human speakers in high-resource languages&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib2\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">2</span></a>]</cite>.\nConventional TTS datasets rely on studio recordings, typically drawn from audiobooks or produced by native speakers and voice actors&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib3\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib4\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib5\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">5</span></a>]</cite>.\nLow-resource languages lack the publicly available recordings and speaker access required to build conventional high-quality TTS datasets. As a result, system development is slow and costly.</p>\n\n",
                "matched_terms": [
                    "recordings",
                    "studio",
                    "tts",
                    "datasets",
                    "languages",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent work on building extensive and diverse datasets from speech found in the wild&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib7\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">7</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib8\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">8</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib9\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">9</span></a>]</cite> directly targets this challenge. In particular, Emilia&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>]</cite> proposed a unified data processing pipeline and released 101k hours of multilingual speech data. This was followed by several works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib7\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">7</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib8\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">8</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib9\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">9</span></a>]</cite> that further extended the data quality and quantity. While these efforts mainly target high-resource languages like English and Mandarin Chinese, languages with lower resources are often overlooked.\nAs a step toward addressing this imbalance in the Nordic region, we present <span class=\"ltx_text ltx_font_italic\">Nord-Parl-TTS</span>, an open in-the-wild TTS dataset for Finnish and Swedish.\nWe adapted the Emilia data processing pipeline and extracted ready-to-use TTS data from parliament speech recordings, resulting in 900 hours of Finnish and 5090 hours of Swedish TTS data. To further support the development of TTS for these languages, we propose unified evaluation sets following&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib10\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">10</span></a>]</cite>. Finally, we conduct benchmark experiments on two open-source TTS systems to demonstrate their applicability for model development and evaluation.</p>\n\n",
                "matched_terms": [
                    "recordings",
                    "tts",
                    "swedish",
                    "parliament",
                    "dataset",
                    "datasets",
                    "hours",
                    "languages",
                    "data",
                    "nordparltts",
                    "finnish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_italic\">Nord-Parl-TTS</span>, an open-source TTS dataset covering 900 hours of Finnish and 5090 hours of Swedish.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "swedish",
                    "dataset",
                    "hours",
                    "nordparltts",
                    "finnish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide unified evaluation sets for Finnish and Swedish TTS.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "swedish",
                    "finnish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We benchmark two representative open-source TTS systems to demonstrate the applicability of the dataset and evaluation sets.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finnish parliament recordings have been transformed into the <span class=\"ltx_text ltx_font_italic\">Finnish Parliament ASR corpus</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib14\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">14</span></a>]</cite>. However, further transforming the ASR corpus into a TTS dataset is not an optimal solution for several reasons. First, the transcripts are corrected for hesitations, repetitions, and slips of the tongue. They are also edited to replace spoken and spontaneous language with equivalent written language for clarity&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib14\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">14</span></a>]</cite>. Second, the ASR corpus is segmented based on the word boundary, meaning that an utterance can start and end in the middle of a sentence. The corrected transcripts make it difficult for TTS models to learn the right pronunciations, and the word-level segmentation negatively affects the sentence-level prosody modeling.</p>\n\n",
                "matched_terms": [
                    "language",
                    "recordings",
                    "tts",
                    "parliament",
                    "dataset",
                    "finnish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Swedish, <span class=\"ltx_text ltx_font_italic\">RixVox-v2<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_upright\">3</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_upright\" href=\"https://huggingface.co/datasets/KBLab/rixvox-v2\" title=\"\">https://huggingface.co/datasets/KBLab/rixvox-v2</a></span></span></span></span> provides a large-scale ASR dataset containing nearly 23000 hours of parliamentary speech and debate. The recordings are standardized to 16&#8201;kHz mono audio, segmented into utterances of up to 30 seconds with sentence-level timestamps.</p>\n\n",
                "matched_terms": [
                    "swedish",
                    "hours",
                    "recordings",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section elaborates on the details of the data processing pipeline and the approaches for curating evaluation datasets for Finnish and Swedish, respectively, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Nord-Parl-TTS: Finnish and Swedish TTS Dataset from Parliament Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "finnish",
                    "data",
                    "swedish",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our Finnish processing pipeline mimics the Emilia pipeline&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>]</cite> with modifications, as shown by blue arrows in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Nord-Parl-TTS: Finnish and Swedish TTS Dataset from Parliament Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. We first extract the audio track from the video and standardize it to a 24&#8201;kHz mono-channel format. It is then separated from noise using a pretrained <span class=\"ltx_text ltx_font_italic\">UVR-MDX-Net Inst 3<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_upright\">4</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_upright\" href=\"https://github.com/TRvlvr/model_repo/releases/tag/all_public_uvr_models\" title=\"\">https://github.com/TRvlvr/model_repo/releases/tag/all_public_uvr_models</a></span></span></span></span> model and diarized using <span class=\"ltx_text ltx_font_italic\">speaker-diarization-3.1</span> from Pyannote&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib15\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a>]</cite>. A fine-grained VAD is conducted on the diarized speech using <span class=\"ltx_text ltx_font_italic\">Silero VAD</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib16\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>]</cite>. Since the <span class=\"ltx_text ltx_font_italic\">faster-whisper-large-v3<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_upright\">5</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_upright\" href=\"https://huggingface.co/Systran/faster-whisper-large-v3\" title=\"\">https://huggingface.co/Systran/faster-whisper-large-v3</a></span></span></span></span> is not performant on Finnish, we decided to use an extra ASR model to validate the transcript.\nSpecifically, we use a Wav2Vec2-large model pretrained and fine-tuned solely on Finnish&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib17\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">17</span></a>]</cite> as the second ASR model to validate the Whisper model&#8217;s transcript based on the assumption that two different models would not make the same mistake.\nTherefore, if the WER between the normalized transcripts of these two models is less than <math alttext=\"5\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">5\\%</annotation></semantics></math>, we think the Whisper&#8217;s transcript is confident and can be considered as a candidate for the final dataset.\nWe choose Whisper&#8217;s transcripts here because both models hold a similar performance, while Whisper will restore capital letters and punctuations, which can be useful for TTS models. Following Emilia&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>]</cite>, the pipeline is finished with the DNSMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite> prediction, with samples that have a DNSMOS P.835 OVRL score less than 3.0 to be excluded. The DNSMOS and duration distributions of the <span class=\"ltx_text ltx_font_italic\">Nord-Parl-TTS</span> Finnish subset are shown in the blue pillars in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#S3.F2.sf1\" title=\"In Figure 2 &#8227; 3 Nord-Parl-TTS &#8227; Nord-Parl-TTS: Finnish and Swedish TTS Dataset from Parliament Speech\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#S3.F2.sf2\" title=\"In Figure 2 &#8227; 3 Nord-Parl-TTS &#8227; Nord-Parl-TTS: Finnish and Swedish TTS Dataset from Parliament Speech\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a>, respectively.</p>\n\n",
                "matched_terms": [
                    "nordparltts",
                    "tts",
                    "finnish",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the evaluation dataset, we curate 500 prompt-target pairs from <span class=\"ltx_text ltx_font_italic\">Perso Synteesi</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib11\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">11</span></a>]</cite>. We ensure a gender balance by sampling 250 prompts from male speakers and 250 prompts from female speakers. To avoid the error and model bias brought by the short sentences. All prompt and target speeches are between 3 seconds and 20 seconds, and the corresponding content has more than 10 characters.</p>\n\n",
                "matched_terms": [
                    "perso",
                    "synteesi",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We process the <span class=\"ltx_text ltx_font_italic\">RixVox-v2</span> for the Swedish dataset. The pipeline is shown in the yellow arrows in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Nord-Parl-TTS: Finnish and Swedish TTS Dataset from Parliament Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. We first resample the audio to 24&#8201;kHz and use the same speech separation model in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#S3.SS1\" title=\"3.1 Finnish &#8227; 3 Nord-Parl-TTS &#8227; Nord-Parl-TTS: Finnish and Swedish TTS Dataset from Parliament Speech\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a> to clean out non-speech tracks. Then we cut the utterance using the existing sentence-level timestamp from the dataset if the gap between two sentences is longer than 2 seconds. The same speaker diarization model in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#S3.SS1\" title=\"3.1 Finnish &#8227; 3 Nord-Parl-TTS &#8227; Nord-Parl-TTS: Finnish and Swedish TTS Dataset from Parliament Speech\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a> is used to exclude segmented utterances with multiple speakers. To evaluate the intelligibility of the utterance, we transcribe the utterance using the Swedish Whisper-large&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a>]</cite> and filter out those utterances that have a Word Error Rate (WER) larger than 10%. We allow a WER of 10% because the sentence-level transcripts and timestamps of <span class=\"ltx_text ltx_font_italic\">RixVox-v2</span> are also derived from ASR models, but they are not perfectly accurate. On the other hand, Swedish Whisper-large is trained on many large-scale Swedish datasets including <span class=\"ltx_text ltx_font_italic\">RixVox-v2</span>, making it more accurate than the original transcripts&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a>]</cite>. Moreover, we wish to filter out unintelligible speech while keeping the scale of the dataset as large as possible. Finally, we filter out utterances with a DNSMOS P.835 OVRL score less than 3.0. This yields a 5090-hour Swedish dataset. The DNSMOS score distribution and the duration distribution of this dataset are shown in the yellow pillars in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#S3.F2.sf1\" title=\"In Figure 2 &#8227; 3 Nord-Parl-TTS &#8227; Nord-Parl-TTS: Finnish and Swedish TTS Dataset from Parliament Speech\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#S3.F2.sf2\" title=\"In Figure 2 &#8227; 3 Nord-Parl-TTS &#8227; Nord-Parl-TTS: Finnish and Swedish TTS Dataset from Parliament Speech\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a>, respectively. We notice that our pipeline for Swedish results in a monotonically increasing duration distribution in the dataset, which is different from existing datasets that are dominated by utterances in 5 to 15 seconds in duration.</p>\n\n",
                "matched_terms": [
                    "existing",
                    "swedish",
                    "datasets",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We curate the Swedish evaluation dataset from CommonVoice&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib20\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></a>]</cite> Swedish 22.0. First, we keep utterances between 3 to 20 seconds with non-empty gender labels. Then we sample from each speaker a maximum of 30 utterances to create the pool of candidates while trying to balance the number of the utterances per gender and per speaker. After this, a native Swedish speaker listens to all these utterances using a bespoke curation tool and filters out utterances with unclear speech, low volume, strong non-native accents, and background noise. Then we form 500 prompt-target pairs as in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#S3.SS1\" title=\"3.1 Finnish &#8227; 3 Nord-Parl-TTS &#8227; Nord-Parl-TTS: Finnish and Swedish TTS Dataset from Parliament Speech\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "swedish",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate the effectiveness of our <span class=\"ltx_text ltx_font_italic\">Nord-Parl-TTS</span>, we train two non-autoregressive (NAR) TTS models, Matcha-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib21\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">21</span></a>]</cite> and F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>]</cite>, in the monolingual setting. Both models are diffusion-based. In particular, Matcha-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib21\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">21</span></a>]</cite> uses monotonic alignment search (MAS)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib22\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">22</span></a>]</cite> to find the optimal alignment between input text and output mel-spectrogram, while F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>]</cite> learns implicit alignment through a stack of Diffusion Transformers (DiT)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>]</cite> instead of explicit alignment modeling. No auto-regressive (AR) TTS models are included since we failed to find official open-source implementations for typical AR TTS models. For Matcha-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib21\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">21</span></a>]</cite>, we follow the implementation in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib24\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">24</span></a>]</cite>, replacing the speaker embedding table with a pretrained <span class=\"ltx_text ltx_font_italic\">SimAMResNet</span> Speaker Encoder from WeSpeaker&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib25\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></a>]</cite> and strengthening the model with Classifier-free Guidance (CFG)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib26\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">26</span></a>]</cite>. We train the Swedish model with phoneme inputs using Phonemizer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib27\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">27</span></a>]</cite> for Swedish G2P. Finnish on the other hand has a near one-to-one phoneme-to-grapheme mapping, so for the Finish model we use the character sequences directly. Both the Finnish and the Swedish models are trained on 1 Nvidia A100 GPU for 500k updates with a batch size of 64.\nFor F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>]</cite>, we follow the <span class=\"ltx_text ltx_font_italic\">F5TTS_v1_Base<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_upright\">6</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_upright\" href=\"https://github.com/SWivid/F5-TTS/blob/main/src/f5_tts/configs/F5TTS_v1_Base.yaml\" title=\"\">https://github.com/SWivid/F5-TTS/blob/main/src/f5_tts/configs/F5TTS_v1_Base.yaml</a></span></span></span></span> configuration. Both Finnish and Swedish models are trained using 24 AMD MI250X GPUs to maintain a global batch size of 308400 mel-spectrogram frames with 1.2M updates, as in the original paper.</p>\n\n",
                "matched_terms": [
                    "nordparltts",
                    "tts",
                    "swedish",
                    "finnish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the evaluation datasets described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#S3\" title=\"3 Nord-Parl-TTS &#8227; Nord-Parl-TTS: Finnish and Swedish TTS Dataset from Parliament Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> to evaluate the performance of the TTS models objectively and subjectively. The CFG strength is set to <math alttext=\"2.0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\"><semantics><mn>2.0</mn><annotation encoding=\"application/x-tex\">2.0</annotation></semantics></math> for both models during inference time. For objective evaluation, we first synthesize the content of the target speech prompted by the prompt speech, then use ASR models to evaluate the Character Error Rate (CER) between the synthetic speech and the ground truth content. We also calculate the Cosine Speaker Similarity (SIM) between the synthetic speech and the prompt speech. We use Wav2Vec2-large&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib17\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">17</span></a>]</cite> for Finnish ASR and Swedish Whisper-large&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a>]</cite> for Swedish ASR. Following&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>]</cite>, we use a WavLM-large-based&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite> speaker verification model for SIM. All evaluations are conducted on a single Nvidia V100 GPU.</p>\n\n",
                "matched_terms": [
                    "finnish",
                    "tts",
                    "swedish",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For subjective evaluations, we evaluate the human-likeness of the synthetic speech using Comparative Mean Opinion Score (CMOS) and speaker similarity using Speaker Mean Opinion Score (SMOS). For CMOS, we ask the participants whether sample A or sample B is more human-like, with a score between -3 to 3 and an increment of 1, where -3 means <span class=\"ltx_text ltx_font_italic\">sample A is definitely more human-like</span>, 0 is <span class=\"ltx_text ltx_font_italic\">unsure</span>, and 3 means <span class=\"ltx_text ltx_font_italic\">sample B is definitely more human-like</span>. One of the two samples is the ground truth speech, and the other is the synthetic speech. For SMOS, we ask how sample B (synthetic speech) sounds like the same speaker as sample A (target speech), and the participants should rate between -2 and 2 with an increment of 1, where -2 represents <span class=\"ltx_text ltx_font_italic\">definitely not the same speaker</span> and 2 represents <span class=\"ltx_text ltx_font_italic\">definitely the same speaker</span>. We map SMOS back to a 1 to 5 scale when calculating the statistics. For both languages, we insert several language skill checks into each questionnaire, which instruct the participants to rate certain scores on certain questions in the testing language.\nOur participants are recruited via <span class=\"ltx_text ltx_font_italic\">Prolific<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_upright\">7</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_upright\" href=\"https://www.prolific.com/\" title=\"\">https://www.prolific.com/</a></span></span></span></span>, a crowd-sourcing platform. For both languages, we screen the participants by their primary language, and with a history of at least 98% acceptance rate. For Finnish, we also recruit students and employees from the university to ensure the desired number of participants. We exclude the submissions that fail the language skill checks from the final results.</p>\n\n",
                "matched_terms": [
                    "language",
                    "finnish",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The objective and subjective evaluation results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#S4.T2\" title=\"Table 2 &#8227; 4 Experiments &#8227; Nord-Parl-TTS: Finnish and Swedish TTS Dataset from Parliament Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. To exclude hallucinated synthetic utterances, we use CER <math alttext=\"&gt;100\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mrow><mn>100</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">&gt;100\\%</annotation></semantics></math> as the threshold for filtering and calculate CER and SIM over the kept utterances. In particular, for F5-TTS-Base, 4 out of 500 utterances in the Finnish evaluation set and 18 out of 500 utterances in the Swedish evaluation set are hallucinated and filtered. For Matcha-TTS, only 1 out of 500 Swedish and 500 Finnish utterances is hallucinated and filtered.</p>\n\n",
                "matched_terms": [
                    "swedish",
                    "finnish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first discuss the objective evaluation results. In terms of CER, Matcha-TTS outforms F5-TTS-Base in both Finnish (<math alttext=\"2.55\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\"><semantics><mrow><mn>2.55</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">2.55\\%</annotation></semantics></math> vs <math alttext=\"6.72\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\"><semantics><mrow><mn>6.72</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">6.72\\%</annotation></semantics></math>) and Swedish (<math alttext=\"4.66\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m3\" intent=\":literal\"><semantics><mrow><mn>4.66</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">4.66\\%</annotation></semantics></math> vs <math alttext=\"13.64\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m4\" intent=\":literal\"><semantics><mrow><mn>13.64</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">13.64\\%</annotation></semantics></math>). Results in hallucination and CER indicate that the explicit alignment modeling reduces hallucination and improves the intelligibility of the synthetic speech, even though Matcha-TTS has fewer parameters, trained with a smaller batch size and fewer updates. For SIM, Matcha-TTS shows better speaker similarity than F5-TTS-Base (<math alttext=\"0.566\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m5\" intent=\":literal\"><semantics><mn>0.566</mn><annotation encoding=\"application/x-tex\">0.566</annotation></semantics></math> vs <math alttext=\"0.538\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m6\" intent=\":literal\"><semantics><mn>0.538</mn><annotation encoding=\"application/x-tex\">0.538</annotation></semantics></math>) in Finnish, by directly conditioning on speaker embeddings. However, prompt-based F5-TTS-Base achieves better speaker similarity on Swedish, with a SIM of 0.53 over Matcha-TTS&#8217;s SIM of 0.442.</p>\n\n",
                "matched_terms": [
                    "swedish",
                    "finnish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For subjective evaluation, we invited 22 participants for Finnish and 20 participants for Swedish. Evaluation results show the models achieve acceptable performances in both languages. Specifically, for Finnish, Matcha-TTS achieves a CMOS of -1.93 and a SMOS of 2.49; F5-TTS-Base outputs Matcha-TTS for 0.5 on CMOS and 0.6 on SMOS. For Swedish, Matcha-TTS achieves a CMOS of -2.25 and a SMOS of 2.27, while F5-TTS-Base achieves a CMOS of -1.46 and a SMOS of 3.41.\nThe evaluation results also demonstrate that an implicit alignment approach like F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>]</cite> improves the human-likeness at the cost of intelligibility.</p>\n\n",
                "matched_terms": [
                    "swedish",
                    "finnish",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present <span class=\"ltx_text ltx_font_italic\">Nord-Parl-TTS</span>, an in-the-wild TTS dataset for Finnish and Swedish derived from parliament speech recordings. Open-sourced TTS models trained on this dataset achieve acceptable synthesis results and proves our dataset is applicable for TTS training.\nIn addition to refining the tooling and making curation tools and data visualization available, our future work includes extending the dataset to other Nordic languages like Danish and Norwegian, providing more benchmark models, and further improving dataset quality.</p>\n\n",
                "matched_terms": [
                    "recordings",
                    "tts",
                    "swedish",
                    "parliament",
                    "dataset",
                    "languages",
                    "data",
                    "nordparltts",
                    "finnish"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "Nord-Parl-TTS: Finnish and Swedish TTS Dataset from Parliament Speech",
        "caption": "Table 2: Objective and subjective evaluation results of TTS systems. The best results of every column in each language are bold. The CMOS and SMOS scores are reported with 95% Confidence Interval (CI).",
        "body": "Language\nModel\nCER (↓\\downarrow)\nSIM (↑\\uparrow)\nCMOS (↑\\uparrow)\nSMOS (↑\\uparrow)\n\n\nFinnish\nMatcha-TTS\n2.55%\n0.566\n-1.93 ±\\pm 0.12\n2.49 ±\\pm 0.14\n\n\nF5-TTS-Base\n6.72%\n0.538\n-1.42 ±\\pm 0.14\n3.12 ±\\pm 0.14\n\n\nSwedish\nMatcha-TTS\n4.66%\n0.442\n-2.25 ±\\pm 0.11\n2.27 ±\\pm 0.14\n\n\nF5-TTS-Base\n13.64%\n0.530\n-1.46 ±\\pm 0.13\n3.41 ±\\pm 0.15",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Language</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">CER (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">SIM (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">CMOS (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">SMOS (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Finnish</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Matcha-TTS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">2.55%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.566</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-1.93 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.49 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m6\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.14</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">F5-TTS-Base</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">6.72%</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.538</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">-1.42 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m7\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.14</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">3.12 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m8\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.14</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">Swedish</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Matcha-TTS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">4.66%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.442</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">-2.25 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m9\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2.27 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m10\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.14</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">F5-TTS-Base</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">13.64%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.530</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">-1.46 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m11\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.13</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\">3.41 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m12\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.15</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "confidence",
            "subjective",
            "↓downarrow",
            "sim",
            "±pm",
            "objective",
            "f5ttsbase",
            "each",
            "tts",
            "swedish",
            "every",
            "cmos",
            "results",
            "column",
            "language",
            "model",
            "evaluation",
            "bold",
            "systems",
            "cer",
            "↑uparrow",
            "interval",
            "reported",
            "matchatts",
            "scores",
            "best",
            "smos",
            "finnish"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The objective and subjective evaluation results are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#S4.T2\" title=\"Table 2 &#8227; 4 Experiments &#8227; Nord-Parl-TTS: Finnish and Swedish TTS Dataset from Parliament Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. To exclude hallucinated synthetic utterances, we use CER <math alttext=\"&gt;100\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mrow><mn>100</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">&gt;100\\%</annotation></semantics></math> as the threshold for filtering and calculate CER and SIM over the kept utterances. In particular, for F5-TTS-Base, 4 out of 500 utterances in the Finnish evaluation set and 18 out of 500 utterances in the Swedish evaluation set are hallucinated and filtered. For Matcha-TTS, only 1 out of 500 Swedish and 500 Finnish utterances is hallucinated and filtered.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Text-to-speech (TTS) development is limited by scarcity of high-quality, publicly available speech data for most languages outside a few high-resource languages.\nWe present <span class=\"ltx_text ltx_font_italic\">Nord-Parl-TTS</span>, an open TTS dataset for Finnish and Swedish based on speech found in the wild.\nUsing recordings of Nordic parliamentary proceedings, we extract 900 hours of Finnish and 5090 hours of Swedish speech suitable for TTS training.\nThe dataset is built using an adapted version of the Emilia data processing pipeline and includes unified evaluation sets to support model development and benchmarking.\nBy offering open, large-scale data for Finnish and Swedish, Nord-Parl-TTS narrows the resource gap in TTS between high- and lower-resourced languages.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tts",
                    "swedish",
                    "evaluation",
                    "finnish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Text-to-speech (TTS) converts written text into speech. With current methods, the humanlikeness and intelligibility of the resulting speech depend on the availability of speech datasets.\nWhen trained on extensive, high-quality data, modern TTS systems can generate speech with intelligibility and prosodic variation comparable to human speakers in high-resource languages&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib2\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">2</span></a>]</cite>.\nConventional TTS datasets rely on studio recordings, typically drawn from audiobooks or produced by native speakers and voice actors&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib3\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib4\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib5\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">5</span></a>]</cite>.\nLow-resource languages lack the publicly available recordings and speaker access required to build conventional high-quality TTS datasets. As a result, system development is slow and costly.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent work on building extensive and diverse datasets from speech found in the wild&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib7\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">7</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib8\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">8</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib9\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">9</span></a>]</cite> directly targets this challenge. In particular, Emilia&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>]</cite> proposed a unified data processing pipeline and released 101k hours of multilingual speech data. This was followed by several works&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib7\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">7</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib8\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">8</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib9\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">9</span></a>]</cite> that further extended the data quality and quantity. While these efforts mainly target high-resource languages like English and Mandarin Chinese, languages with lower resources are often overlooked.\nAs a step toward addressing this imbalance in the Nordic region, we present <span class=\"ltx_text ltx_font_italic\">Nord-Parl-TTS</span>, an open in-the-wild TTS dataset for Finnish and Swedish.\nWe adapted the Emilia data processing pipeline and extracted ready-to-use TTS data from parliament speech recordings, resulting in 900 hours of Finnish and 5090 hours of Swedish TTS data. To further support the development of TTS for these languages, we propose unified evaluation sets following&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib10\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">10</span></a>]</cite>. Finally, we conduct benchmark experiments on two open-source TTS systems to demonstrate their applicability for model development and evaluation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tts",
                    "swedish",
                    "evaluation",
                    "systems",
                    "finnish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce <span class=\"ltx_text ltx_font_italic\">Nord-Parl-TTS</span>, an open-source TTS dataset covering 900 hours of Finnish and 5090 hours of Swedish.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "swedish",
                    "finnish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide unified evaluation sets for Finnish and Swedish TTS.</p>\n\n",
                "matched_terms": [
                    "swedish",
                    "tts",
                    "evaluation",
                    "finnish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We benchmark two representative open-source TTS systems to demonstrate the applicability of the dataset and evaluation sets.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "tts",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A brief summary of the existing Finnish and Swedish TTS datasets is illustrated in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#S2.T1\" title=\"Table 1 &#8227; 2 Related Work &#8227; Nord-Parl-TTS: Finnish and Swedish TTS Dataset from Parliament Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. To our best knowledge, there is no existing public Swedish TTS dataset. Regarding Finnish,&#160;<span class=\"ltx_text ltx_font_italic\">Perso Synteesi</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib11\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">11</span></a>]</cite> is a 20-hour speech dataset with 30 male and 30 female native speakers; <span class=\"ltx_text ltx_font_italic\">CSS10</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib12\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">12</span></a>]</cite> contains 10 hours of Finnish audiobooks read by a male speaker; and <span class=\"ltx_text ltx_font_italic\">FinSyn</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib13\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">13</span></a>]</cite> consists of approximately 60 hours of speech in various speaking styles by two female speakers. These datasets are on a small scale and are not suitable for training large speech generation models. Furthermore, <span class=\"ltx_text ltx_font_italic\">Perso Synteesi</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib11\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">11</span></a>]</cite> and <span class=\"ltx_text ltx_font_italic\">FinSyn</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib13\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">13</span></a>]</cite> are not conveniently available from open-sourced data hubs, like <span class=\"ltx_text ltx_font_italic\">OpenSLR<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_upright\">1</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_upright\" href=\"https://www.openslr.org/index.html\" title=\"\">https://www.openslr.org/index.html</a></span></span></span></span> or <span class=\"ltx_text ltx_font_italic\">HuggingFace datasets<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_upright\">2</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_upright\" href=\"https://huggingface.co/datasets\" title=\"\">https://huggingface.co/datasets</a></span></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "best",
                    "tts",
                    "swedish",
                    "finnish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finnish parliament recordings have been transformed into the <span class=\"ltx_text ltx_font_italic\">Finnish Parliament ASR corpus</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib14\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">14</span></a>]</cite>. However, further transforming the ASR corpus into a TTS dataset is not an optimal solution for several reasons. First, the transcripts are corrected for hesitations, repetitions, and slips of the tongue. They are also edited to replace spoken and spontaneous language with equivalent written language for clarity&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib14\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">14</span></a>]</cite>. Second, the ASR corpus is segmented based on the word boundary, meaning that an utterance can start and end in the middle of a sentence. The corrected transcripts make it difficult for TTS models to learn the right pronunciations, and the word-level segmentation negatively affects the sentence-level prosody modeling.</p>\n\n",
                "matched_terms": [
                    "language",
                    "tts",
                    "finnish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section elaborates on the details of the data processing pipeline and the approaches for curating evaluation datasets for Finnish and Swedish, respectively, as illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Nord-Parl-TTS: Finnish and Swedish TTS Dataset from Parliament Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "swedish",
                    "evaluation",
                    "finnish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our Finnish processing pipeline mimics the Emilia pipeline&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>]</cite> with modifications, as shown by blue arrows in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Nord-Parl-TTS: Finnish and Swedish TTS Dataset from Parliament Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. We first extract the audio track from the video and standardize it to a 24&#8201;kHz mono-channel format. It is then separated from noise using a pretrained <span class=\"ltx_text ltx_font_italic\">UVR-MDX-Net Inst 3<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_upright\">4</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_upright\" href=\"https://github.com/TRvlvr/model_repo/releases/tag/all_public_uvr_models\" title=\"\">https://github.com/TRvlvr/model_repo/releases/tag/all_public_uvr_models</a></span></span></span></span> model and diarized using <span class=\"ltx_text ltx_font_italic\">speaker-diarization-3.1</span> from Pyannote&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib15\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">15</span></a>]</cite>. A fine-grained VAD is conducted on the diarized speech using <span class=\"ltx_text ltx_font_italic\">Silero VAD</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib16\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></a>]</cite>. Since the <span class=\"ltx_text ltx_font_italic\">faster-whisper-large-v3<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_upright\">5</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_upright\" href=\"https://huggingface.co/Systran/faster-whisper-large-v3\" title=\"\">https://huggingface.co/Systran/faster-whisper-large-v3</a></span></span></span></span> is not performant on Finnish, we decided to use an extra ASR model to validate the transcript.\nSpecifically, we use a Wav2Vec2-large model pretrained and fine-tuned solely on Finnish&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib17\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">17</span></a>]</cite> as the second ASR model to validate the Whisper model&#8217;s transcript based on the assumption that two different models would not make the same mistake.\nTherefore, if the WER between the normalized transcripts of these two models is less than <math alttext=\"5\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">5\\%</annotation></semantics></math>, we think the Whisper&#8217;s transcript is confident and can be considered as a candidate for the final dataset.\nWe choose Whisper&#8217;s transcripts here because both models hold a similar performance, while Whisper will restore capital letters and punctuations, which can be useful for TTS models. Following Emilia&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib6\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></a>]</cite>, the pipeline is finished with the DNSMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib18\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">18</span></a>]</cite> prediction, with samples that have a DNSMOS P.835 OVRL score less than 3.0 to be excluded. The DNSMOS and duration distributions of the <span class=\"ltx_text ltx_font_italic\">Nord-Parl-TTS</span> Finnish subset are shown in the blue pillars in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#S3.F2.sf1\" title=\"In Figure 2 &#8227; 3 Nord-Parl-TTS &#8227; Nord-Parl-TTS: Finnish and Swedish TTS Dataset from Parliament Speech\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#S3.F2.sf2\" title=\"In Figure 2 &#8227; 3 Nord-Parl-TTS &#8227; Nord-Parl-TTS: Finnish and Swedish TTS Dataset from Parliament Speech\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a>, respectively.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "model",
                    "finnish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the evaluation dataset, we curate 500 prompt-target pairs from <span class=\"ltx_text ltx_font_italic\">Perso Synteesi</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib11\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">11</span></a>]</cite>. We ensure a gender balance by sampling 250 prompts from male speakers and 250 prompts from female speakers. To avoid the error and model bias brought by the short sentences. All prompt and target speeches are between 3 seconds and 20 seconds, and the corresponding content has more than 10 characters.</p>\n\n",
                "matched_terms": [
                    "model",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We process the <span class=\"ltx_text ltx_font_italic\">RixVox-v2</span> for the Swedish dataset. The pipeline is shown in the yellow arrows in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Nord-Parl-TTS: Finnish and Swedish TTS Dataset from Parliament Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. We first resample the audio to 24&#8201;kHz and use the same speech separation model in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#S3.SS1\" title=\"3.1 Finnish &#8227; 3 Nord-Parl-TTS &#8227; Nord-Parl-TTS: Finnish and Swedish TTS Dataset from Parliament Speech\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a> to clean out non-speech tracks. Then we cut the utterance using the existing sentence-level timestamp from the dataset if the gap between two sentences is longer than 2 seconds. The same speaker diarization model in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#S3.SS1\" title=\"3.1 Finnish &#8227; 3 Nord-Parl-TTS &#8227; Nord-Parl-TTS: Finnish and Swedish TTS Dataset from Parliament Speech\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a> is used to exclude segmented utterances with multiple speakers. To evaluate the intelligibility of the utterance, we transcribe the utterance using the Swedish Whisper-large&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a>]</cite> and filter out those utterances that have a Word Error Rate (WER) larger than 10%. We allow a WER of 10% because the sentence-level transcripts and timestamps of <span class=\"ltx_text ltx_font_italic\">RixVox-v2</span> are also derived from ASR models, but they are not perfectly accurate. On the other hand, Swedish Whisper-large is trained on many large-scale Swedish datasets including <span class=\"ltx_text ltx_font_italic\">RixVox-v2</span>, making it more accurate than the original transcripts&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a>]</cite>. Moreover, we wish to filter out unintelligible speech while keeping the scale of the dataset as large as possible. Finally, we filter out utterances with a DNSMOS P.835 OVRL score less than 3.0. This yields a 5090-hour Swedish dataset. The DNSMOS score distribution and the duration distribution of this dataset are shown in the yellow pillars in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#S3.F2.sf1\" title=\"In Figure 2 &#8227; 3 Nord-Parl-TTS &#8227; Nord-Parl-TTS: Finnish and Swedish TTS Dataset from Parliament Speech\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a> and Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#S3.F2.sf2\" title=\"In Figure 2 &#8227; 3 Nord-Parl-TTS &#8227; Nord-Parl-TTS: Finnish and Swedish TTS Dataset from Parliament Speech\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a>, respectively. We notice that our pipeline for Swedish results in a monotonically increasing duration distribution in the dataset, which is different from existing datasets that are dominated by utterances in 5 to 15 seconds in duration.</p>\n\n",
                "matched_terms": [
                    "model",
                    "swedish",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We curate the Swedish evaluation dataset from CommonVoice&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib20\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">20</span></a>]</cite> Swedish 22.0. First, we keep utterances between 3 to 20 seconds with non-empty gender labels. Then we sample from each speaker a maximum of 30 utterances to create the pool of candidates while trying to balance the number of the utterances per gender and per speaker. After this, a native Swedish speaker listens to all these utterances using a bespoke curation tool and filters out utterances with unclear speech, low volume, strong non-native accents, and background noise. Then we form 500 prompt-target pairs as in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#S3.SS1\" title=\"3.1 Finnish &#8227; 3 Nord-Parl-TTS &#8227; Nord-Parl-TTS: Finnish and Swedish TTS Dataset from Parliament Speech\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "swedish",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate the effectiveness of our <span class=\"ltx_text ltx_font_italic\">Nord-Parl-TTS</span>, we train two non-autoregressive (NAR) TTS models, Matcha-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib21\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">21</span></a>]</cite> and F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>]</cite>, in the monolingual setting. Both models are diffusion-based. In particular, Matcha-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib21\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">21</span></a>]</cite> uses monotonic alignment search (MAS)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib22\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">22</span></a>]</cite> to find the optimal alignment between input text and output mel-spectrogram, while F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>]</cite> learns implicit alignment through a stack of Diffusion Transformers (DiT)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib23\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">23</span></a>]</cite> instead of explicit alignment modeling. No auto-regressive (AR) TTS models are included since we failed to find official open-source implementations for typical AR TTS models. For Matcha-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib21\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">21</span></a>]</cite>, we follow the implementation in&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib24\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">24</span></a>]</cite>, replacing the speaker embedding table with a pretrained <span class=\"ltx_text ltx_font_italic\">SimAMResNet</span> Speaker Encoder from WeSpeaker&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib25\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">25</span></a>]</cite> and strengthening the model with Classifier-free Guidance (CFG)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib26\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">26</span></a>]</cite>. We train the Swedish model with phoneme inputs using Phonemizer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib27\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">27</span></a>]</cite> for Swedish G2P. Finnish on the other hand has a near one-to-one phoneme-to-grapheme mapping, so for the Finish model we use the character sequences directly. Both the Finnish and the Swedish models are trained on 1 Nvidia A100 GPU for 500k updates with a batch size of 64.\nFor F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>]</cite>, we follow the <span class=\"ltx_text ltx_font_italic\">F5TTS_v1_Base<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_upright\">6</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_upright\" href=\"https://github.com/SWivid/F5-TTS/blob/main/src/f5_tts/configs/F5TTS_v1_Base.yaml\" title=\"\">https://github.com/SWivid/F5-TTS/blob/main/src/f5_tts/configs/F5TTS_v1_Base.yaml</a></span></span></span></span> configuration. Both Finnish and Swedish models are trained using 24 AMD MI250X GPUs to maintain a global batch size of 308400 mel-spectrogram frames with 1.2M updates, as in the original paper.</p>\n\n",
                "matched_terms": [
                    "model",
                    "matchatts",
                    "tts",
                    "swedish",
                    "finnish"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the evaluation datasets described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#S3\" title=\"3 Nord-Parl-TTS &#8227; Nord-Parl-TTS: Finnish and Swedish TTS Dataset from Parliament Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> to evaluate the performance of the TTS models objectively and subjectively. The CFG strength is set to <math alttext=\"2.0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\"><semantics><mn>2.0</mn><annotation encoding=\"application/x-tex\">2.0</annotation></semantics></math> for both models during inference time. For objective evaluation, we first synthesize the content of the target speech prompted by the prompt speech, then use ASR models to evaluate the Character Error Rate (CER) between the synthetic speech and the ground truth content. We also calculate the Cosine Speaker Similarity (SIM) between the synthetic speech and the prompt speech. We use Wav2Vec2-large&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib17\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">17</span></a>]</cite> for Finnish ASR and Swedish Whisper-large&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib19\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">19</span></a>]</cite> for Swedish ASR. Following&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>]</cite>, we use a WavLM-large-based&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib28\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">28</span></a>]</cite> speaker verification model for SIM. All evaluations are conducted on a single Nvidia V100 GPU.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tts",
                    "swedish",
                    "evaluation",
                    "sim",
                    "cer",
                    "finnish",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For subjective evaluations, we evaluate the human-likeness of the synthetic speech using Comparative Mean Opinion Score (CMOS) and speaker similarity using Speaker Mean Opinion Score (SMOS). For CMOS, we ask the participants whether sample A or sample B is more human-like, with a score between -3 to 3 and an increment of 1, where -3 means <span class=\"ltx_text ltx_font_italic\">sample A is definitely more human-like</span>, 0 is <span class=\"ltx_text ltx_font_italic\">unsure</span>, and 3 means <span class=\"ltx_text ltx_font_italic\">sample B is definitely more human-like</span>. One of the two samples is the ground truth speech, and the other is the synthetic speech. For SMOS, we ask how sample B (synthetic speech) sounds like the same speaker as sample A (target speech), and the participants should rate between -2 and 2 with an increment of 1, where -2 represents <span class=\"ltx_text ltx_font_italic\">definitely not the same speaker</span> and 2 represents <span class=\"ltx_text ltx_font_italic\">definitely the same speaker</span>. We map SMOS back to a 1 to 5 scale when calculating the statistics. For both languages, we insert several language skill checks into each questionnaire, which instruct the participants to rate certain scores on certain questions in the testing language.\nOur participants are recruited via <span class=\"ltx_text ltx_font_italic\">Prolific<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_upright\">7</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_upright\" href=\"https://www.prolific.com/\" title=\"\">https://www.prolific.com/</a></span></span></span></span>, a crowd-sourcing platform. For both languages, we screen the participants by their primary language, and with a history of at least 98% acceptance rate. For Finnish, we also recruit students and employees from the university to ensure the desired number of participants. We exclude the submissions that fail the language skill checks from the final results.</p>\n\n",
                "matched_terms": [
                    "each",
                    "language",
                    "subjective",
                    "scores",
                    "smos",
                    "cmos",
                    "finnish",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first discuss the objective evaluation results. In terms of CER, Matcha-TTS outforms F5-TTS-Base in both Finnish (<math alttext=\"2.55\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m1\" intent=\":literal\"><semantics><mrow><mn>2.55</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">2.55\\%</annotation></semantics></math> vs <math alttext=\"6.72\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m2\" intent=\":literal\"><semantics><mrow><mn>6.72</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">6.72\\%</annotation></semantics></math>) and Swedish (<math alttext=\"4.66\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m3\" intent=\":literal\"><semantics><mrow><mn>4.66</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">4.66\\%</annotation></semantics></math> vs <math alttext=\"13.64\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m4\" intent=\":literal\"><semantics><mrow><mn>13.64</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">13.64\\%</annotation></semantics></math>). Results in hallucination and CER indicate that the explicit alignment modeling reduces hallucination and improves the intelligibility of the synthetic speech, even though Matcha-TTS has fewer parameters, trained with a smaller batch size and fewer updates. For SIM, Matcha-TTS shows better speaker similarity than F5-TTS-Base (<math alttext=\"0.566\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m5\" intent=\":literal\"><semantics><mn>0.566</mn><annotation encoding=\"application/x-tex\">0.566</annotation></semantics></math> vs <math alttext=\"0.538\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.m6\" intent=\":literal\"><semantics><mn>0.538</mn><annotation encoding=\"application/x-tex\">0.538</annotation></semantics></math>) in Finnish, by directly conditioning on speaker embeddings. However, prompt-based F5-TTS-Base achieves better speaker similarity on Swedish, with a SIM of 0.53 over Matcha-TTS&#8217;s SIM of 0.442.</p>\n\n",
                "matched_terms": [
                    "matchatts",
                    "swedish",
                    "evaluation",
                    "sim",
                    "cer",
                    "results",
                    "finnish",
                    "objective",
                    "f5ttsbase"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For subjective evaluation, we invited 22 participants for Finnish and 20 participants for Swedish. Evaluation results show the models achieve acceptable performances in both languages. Specifically, for Finnish, Matcha-TTS achieves a CMOS of -1.93 and a SMOS of 2.49; F5-TTS-Base outputs Matcha-TTS for 0.5 on CMOS and 0.6 on SMOS. For Swedish, Matcha-TTS achieves a CMOS of -2.25 and a SMOS of 2.27, while F5-TTS-Base achieves a CMOS of -1.46 and a SMOS of 3.41.\nThe evaluation results also demonstrate that an implicit alignment approach like F5-TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17988v1#bib.bib1\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></a>]</cite> improves the human-likeness at the cost of intelligibility.</p>\n\n",
                "matched_terms": [
                    "subjective",
                    "matchatts",
                    "swedish",
                    "evaluation",
                    "smos",
                    "cmos",
                    "finnish",
                    "results",
                    "f5ttsbase"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we present <span class=\"ltx_text ltx_font_italic\">Nord-Parl-TTS</span>, an in-the-wild TTS dataset for Finnish and Swedish derived from parliament speech recordings. Open-sourced TTS models trained on this dataset achieve acceptable synthesis results and proves our dataset is applicable for TTS training.\nIn addition to refining the tooling and making curation tools and data visualization available, our future work includes extending the dataset to other Nordic languages like Danish and Norwegian, providing more benchmark models, and further improving dataset quality.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "swedish",
                    "finnish",
                    "results"
                ]
            }
        ]
    }
}