{
    "S2.SS2.tab1": {
        "source_file": "M-CIF: Multi-Scale Alignment for CIF-Based Non-Autoregressive ASR",
        "caption": "Table 1: WER results of our method, where w/o Char CIF and w/o Phone CIF denote two-scale training without the character or phoneme CIF. M-CIF* denotes the M-CIF mothod applied in Paraformer. LS denotes the setting trained and tested on the LibriSpeech dataset, CV denotes the CommonVoice dataset, and AS2 denotes the AISHELL-2 dataset. The same abbreviations are used throughout the paper.",
        "body": "Method\nParam.\nEN(LS) ↓\\downarrow\nFR(CV) ↓\\downarrow\nDE(CV) ↓\\downarrow\nZH(AS2) ↓\\downarrow\n\n\nclean\nother\nAvg.\n\n\nParaformer\n60.11 M\n5.67\n12.04\n8.86\n21.80\n19.48\n7.06\n\n\nE-Paraformer\n57.54 M\n8.68\n18.76\n13.72\n30.92\n27.16\n15.67\n\n\nOur M-CIF*\n65.39 M\n5.33\n11.76\n8.55\n18.75\n15.27\n7.24\n\n\nw/o Char CIF\n62.75 M\n7.04\n13.73\n\n10.39 (↑\\uparrow 1.84)\n\n\n20.75 (↑\\uparrow 2.00)\n\n\n16.51 (↑\\uparrow 0.98)\n\n-\n\n\nw/o Phone CIF\n62.75 M\n6.61\n12.78\n\n9.70 (↑\\uparrow 1.15)\n\n\n21.71 (↑\\uparrow 2.96)\n\n\n17.07 (↑\\uparrow 1.54)\n\n-",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Param.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">EN(LS)&#160;<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">FR(CV)&#160;<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">DE(CV)&#160;<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ZH(AS2)&#160;<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">clean</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">other</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Avg.</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Paraformer</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">60.11 M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.67</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.04</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.86</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">21.80</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">19.48</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.06</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">E-Paraformer</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">57.54 M</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.68</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">18.76</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.72</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">30.92</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.16</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">15.67</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Our M-CIF*</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">65.39 M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">11.76</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">8.55</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">18.75</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">15.27</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.24</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">w/o Char CIF</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">62.75 M</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.04</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.73</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:11.4pt;padding-right:11.4pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">10.39 (</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 1.84)</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:11.4pt;padding-right:11.4pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">20.75 (</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 2.00)</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:11.4pt;padding-right:11.4pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">16.51 (</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.98)</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">w/o Phone CIF</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">62.75 M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.61</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.78</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:11.4pt;padding-right:11.4pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">9.70 (</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 1.15)</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:11.4pt;padding-right:11.4pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">21.71 (</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 2.96)</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:11.4pt;padding-right:11.4pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">17.07 (</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.m10\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 1.54)</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:11.4pt;padding-right:11.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "char",
            "other",
            "training",
            "zhas2",
            "aishell2",
            "twoscale",
            "librispeech",
            "↓downarrow",
            "throughout",
            "avg",
            "mcif",
            "same",
            "paper",
            "used",
            "our",
            "applied",
            "mothod",
            "where",
            "tested",
            "denote",
            "wer",
            "frcv",
            "param",
            "results",
            "phone",
            "without",
            "cif",
            "dataset",
            "denotes",
            "paraformer",
            "eparaformer",
            "clean",
            "↑uparrow",
            "phoneme",
            "as2",
            "character",
            "abbreviations",
            "setting",
            "decv",
            "method",
            "enls",
            "trained",
            "commonvoice"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The Continuous Integrate-and-Fire (CIF) mechanism provides effective alignment for non-autoregressive (NAR) speech recognition. This mechanism creates a smooth and monotonic mapping from acoustic features to target tokens, achieving performance on Mandarin competitive with other NAR approaches. However, without finer-grained guidance, its stability degrades in some languages such as English and French. In this paper, we propose Multi-scale CIF (<span class=\"ltx_text ltx_font_italic\">M-CIF</span>), which performs multi-level alignment by integrating character and phoneme level supervision progressively distilled into subword representations, thereby enhancing robust acoustic&#8211;text alignment. Experiments show that <span class=\"ltx_text ltx_font_italic\">M-CIF</span> reduces WER compared to the Paraformer baseline, especially on CommonVoice by 4.21% in German and 3.05% in French. To further investigate these gains, we define phonetic confusion errors (<span class=\"ltx_text ltx_font_italic\">PE</span>) and space-related segmentation errors (<span class=\"ltx_text ltx_font_italic\">SE</span>) as evaluation metrics. Analysis of these metrics across different <span class=\"ltx_text ltx_font_italic\">M-CIF</span> settings reveals that the phoneme and character layers are essential for enhancing progressive CIF alignment.</span>\n</p>\n\n",
                "matched_terms": [
                    "other",
                    "phoneme",
                    "character",
                    "wer",
                    "without",
                    "cif",
                    "paraformer",
                    "paper",
                    "commonvoice",
                    "mcif"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Specifically, most CIF applications operate at a coarse granularity, aligning acoustic-text features primarily at the word level</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#bib.bib8\" title=\"\">8</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Activations occur once evidence crosses a threshold, yet words are treated as indivisible units, disregarding their internal syllabic structure. In particular, when encountering densely multi-syllabic words, the lack of finer-grained guidance, such as from phoneme and character-level modeling, makes it difficult to capture the inherent fine-grained acoustic information. For example, Mandarin, an isolating language </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, uses words like &#8220;Beijing&#8221; that consist of two clearly separable monosyllabic characters, rendering the CIF alignment task straightforward. On the contrary, English and French, both synthetic languages </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, have words composed of multiple pronounced units, such as &#8220;unbelievable&#8221;, which contains the prefix &#8220;un-&#8221;, the root &#8220;believe&#8221;, and the suffix &#8220;-able&#8221;. This multi-syllabic structure disrupts the stability of CIF activation alignment,\ninducing identification errors and boundary drift as shown in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; M-CIF: Multi-Scale Alignment for CIF-Based Non-Autoregressive ASR\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Consequently, CIF exhibits a performance gap between synthetic and isolating languages. This observation motivates us to integrate\nmultiscale features into the CIF for enhancing acoustic&#8211;text alignment.</span>\n</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "cif"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we propose </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">M-CIF</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a multi-scale hierarchical framework for synthetic languages. Our method progressively compresses and aligns fine-grained character-level and phoneme-level features into coherent word-level representations in a hierarchical manner, enabling more coordinated integration across scales. Furthermore, scale-matched CTC losses are incorporated at each level to provide more comprehensive supervision. Subsequently, to validate the rationale for introducing phoneme-level and character-level guidance, we quantify and analyze two error types: phonetic confusion errors (</span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">PE</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) and space-related segmentation errors (</span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SE</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). Implemented within Paraformer</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, it delivers an average relative Word Error Rate (WER) reduction of 0.31% on the LibriSpeech test set for English, and up to 4.21% and 3.05% on German and French CommonVoice, respectively.\nOur contributions are as follows:</span>\n</p>\n\n",
                "matched_terms": [
                    "librispeech",
                    "method",
                    "wer",
                    "commonvoice",
                    "mcif",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this section, we present a comparative visualization of the CIF firing process in isolating and synthetic languages. From this analysis, we define and examine two representative error types. Then we introduce the Multi-scale CIF method as a solution to these challenges.</span>\n</p>\n\n",
                "matched_terms": [
                    "method",
                    "cif"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To investigate cross-linguistic differences, we visualize in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; M-CIF: Multi-Scale Alignment for CIF-Based Non-Autoregressive ASR\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> how CIF-predicted weights accumulate to indicate the temporal spans of characters or words. Then we compare these predicted spans with manually annotated ground-truth intervals. The visualizations show that CIF activations align closely with reference word spans in Mandarin, but become irregular and unstable in synthetic languages like English. This instability stems from their multi-syllabic structures and acoustically invisible space delimiter</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which increase the alignment difficulty of CIF and degrade recognition accuracy. Consequently, systematic WER patterns emerge, with phonetic confusion errors (PE) and space-related segmentation errors (SE) particularly evident in the red-marked regions of Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; M-CIF: Multi-Scale Alignment for CIF-Based Non-Autoregressive ASR\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "cif",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address the unstable behavior of CIF in synthetic languages, we propose </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">M-CIF</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a multi-scale framework that alleviates multi-syllabic ambiguity through progressive alignment. As shown in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#S1.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 1 Introduction &#8227; M-CIF: Multi-Scale Alignment for CIF-Based Non-Autoregressive ASR\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">(a), it aligns encoder-derived acoustic representations at the character, phoneme, and word levels, with scale-specific CTC objectives providing auxiliary supervision for stable training.</span>\n</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "character",
                    "training",
                    "cif",
                    "mcif"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">M-CIF Alignment Strategy</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160; Let the encoder output be </span>\n  <math alttext=\"\\mathbf{h}=(h_{1},h_{2},\\ldots,h_{T})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119841;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">h</mi>\n            <mn mathsize=\"0.900em\">1</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">h</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8230;</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">h</mi>\n            <mi mathsize=\"0.900em\">T</mi>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{h}=(h_{1},h_{2},\\ldots,h_{T})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and the target transcription be </span>\n  <math alttext=\"\\mathbf{Y}=(y_{1},y_{2},\\ldots,y_{U})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119832;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">y</mi>\n            <mn mathsize=\"0.900em\">1</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">y</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8230;</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">y</mi>\n            <mi mathsize=\"0.900em\">U</mi>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{Y}=(y_{1},y_{2},\\ldots,y_{U})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. In </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">M-CIF</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the compression is carried out hierarchically through three stages of CIF alignment, operating respectively at the character level, the phoneme level, and the word level. At each stage </span>\n  <math alttext=\"s\\in\\{c,p,w\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <mi mathsize=\"0.900em\">c</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">p</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">w</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">s\\in\\{c,p,w\\}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, alignment is obtained by accumulating the weight </span>\n  <math alttext=\"\\alpha^{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m4\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">&#945;</mi>\n        <mi mathsize=\"0.900em\">s</mi>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\alpha^{s}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> until a threshold </span>\n  <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#946;</mi>\n      <annotation encoding=\"application/x-tex\">\\beta</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is reached, upon which an integrated acoustic embedding is emitted as the input to the next stage, formally defined as:</span>\n</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "character",
                    "cif",
                    "mcif"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In parallel, a multi-scale CTC loss</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is applied before each CIF stage, where a scale-specific weight </span>\n  <math alttext=\"W_{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">W</mi>\n        <mi mathsize=\"0.900em\">s</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">W_{s}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> controls its contribution, thereby providing acoustic supervision at the corresponding granularity. These weights are scheduled across training: supervision begins with stronger emphasis on character-level alignment, gradually shifts toward phoneme-level guidance, and ultimately converges on word-level constraints in the later stages, calculated by:</span>\n</p>\n\n",
                "matched_terms": [
                    "applied",
                    "training",
                    "cif",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Finally, the overall training criterion of </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">M-CIF</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> integrates both objectives, combining the multi-scale quantity constraint with the multi-scale CTC regularization:</span>\n</p>\n\n",
                "matched_terms": [
                    "training",
                    "mcif"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Char level CIF</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160; In synthetic languages such as English and French, character-level CIF decomposes words into characters with </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">|</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> marking boundaries, while in isolating languages like Chinese it operates on processed pinyin. The resulting lengths define the activation targets, with CTC loss applied to stabilize alignment.</span>\n</p>\n\n",
                "matched_terms": [
                    "char",
                    "applied",
                    "cif"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Phoneme level CIF</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160; At the phoneme level, we convert text into phonemic sequences using a G2P tool</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\">\n    <sup class=\"ltx_note_mark\">2</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>The tools can be obtained at https://github.com/Kyubyong/g2p</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and the CMU Pronouncing Dictionary</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\">\n    <sup class=\"ltx_note_mark\">3</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>It is avaliable at http://www.speech.cs.cmu.edu/cgi-bin/cmudict</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Building on character-level compressed acoustic features, CIF activations are constrained by phoneme lengths, with phonemes explicitly serving as targets for CTC training.</span>\n</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "training",
                    "cif"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Word level CIF</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160; At the word level, BPE</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> tokenization is trained on synthetic language corpora with a 10k vocabulary, while isolating languages such as Chinese are segmented at the character level. A word-level CTC constraint is likewise applied before CIF to regularize the compressed acoustic features during training.</span>\n</p>\n\n",
                "matched_terms": [
                    "applied",
                    "character",
                    "training",
                    "cif",
                    "trained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model Architecture</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> &#160;We implement </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">M-CIF</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on the widely adopted Paraformer</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> framework. Paraformer employs a Conformer based encoder</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and a Transformer-based decoder</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, together with a word-level CIF module that provides explicit length prediction and enforces monotonic acoustic-to-text alignment. On top of this, a GLM-based sampler, as illustrated in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#S1.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 1 Introduction &#8227; M-CIF: Multi-Scale Alignment for CIF-Based Non-Autoregressive ASR\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">(b), generates an initial candidate sequence by sampling from the predicted token distribution, which then serves as the starting point for subsequent iterative refinement during decoding.</span>\n</p>\n\n",
                "matched_terms": [
                    "paraformer",
                    "cif",
                    "mcif"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Baseline</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;&#160;We select Paraformer and its variant E-Paraformer</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as our baselines, and integrate the proposed </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">M-CIF</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> framework into Paraformer. Compared to basic Paraformer, which employs the base CIF structure, E-Paraformer further introduces the Parallel Integrate-and-Fire (PIF) mechanism, replacing CIF&#8217;s recursive alignment with a parallel procedure that computes a global attention matrix in one shot. For all models, we employ a 12-layer Conformer encoder and a 12-layer Transformer decoder, each with a hidden size of 256.</span>\n</p>\n\n",
                "matched_terms": [
                    "cif",
                    "eparaformer",
                    "paraformer",
                    "mcif",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;&#160;During the training stage, we employ a hyperparameter scheduling strategy tailored for the multi-scale architecture. CTC losses at different CIF levels are weighted with a scheduled emphasis across stages, while a learning-rate annealing scheme is applied: after 90 epochs, the learning rate is reinitialized to </span>\n  <math alttext=\"6.448\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">6.448</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">5</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">6.448\\times 10^{-5}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and subsequently decayed to promote stable and efficient convergence. To stabilize training on languages like Chinese, where token lengths across structural levels are relatively close, we adopt a three-stage curriculum</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Stage I uses only character-level CTC and length losses; Stage II adds phoneme-level objectives; and Stage III incorporates word-level CTC, length losses, and final decoder cross-entropy. This progressive introduction of objectives effectively stabilizes alignment and ensures reliable convergence.</span>\n</p>\n\n",
                "matched_terms": [
                    "applied",
                    "training",
                    "cif",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For our experiments, all implementations are based on the open-source FunASR</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> toolkit</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\">\n    <sup class=\"ltx_note_mark\">4</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>The tool is avaliable at https://github.com/modelscope/FunASR</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The acoustic features are augmented using SpecAugment</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and training is conducted for 150 epochs on synthetic language dataset and 50 epochs on isolating language dataset with eight NVIDIA 3090 GPUs.</span>\n</p>\n\n",
                "matched_terms": [
                    "training",
                    "our",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Integrating multi-scale CIF into the Paraformer yields consistent improvements across synthetic languages. As shown in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#S2.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 2.2 Multi-scale CIF Strategy &#8227; 2 METHOD &#8227; M-CIF: Multi-Scale Alignment for CIF-Based Non-Autoregressive ASR\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, relative WER reductions of 0.31% are observed on average for the LibriSpeech test sets, together with reductions of 3.05% on the French CommonVoice test set and 4.21% on the German CommonVoice test set. On the contrary, on Chinese corpora this strategy still performs 0.18% WER worse than the baseline, indicating that multi-scale supervision provides limited gains where syllable-based units already impose stable alignment boundaries. Overall, these results demonstrate the performance advantage of the multi-scale CIF architecture in synthetic languages such as English, German and French, effectively reducing WER errors and improving recognition accuracy.</span>\n</p>\n\n",
                "matched_terms": [
                    "librispeech",
                    "cif",
                    "where",
                    "paraformer",
                    "wer",
                    "commonvoice",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We perform ablation experiments by removing the phoneme and character level alignments while keeping other settings unchanged. Our ablation results in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#S2.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 2.2 Multi-scale CIF Strategy &#8227; 2 METHOD &#8227; M-CIF: Multi-Scale Alignment for CIF-Based Non-Autoregressive ASR\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> reveal that removing either the character layer or the phoneme layer consistently increases WER in English, French, and German. This shows that the three-level architecture is indispensable rather than redundant. Each component makes a complementary contribution to overall performance. Based on this, the multi-scale CIF framework performs hierarchical compression&#8211;alignment, where character and phoneme level supervision is progressively distilled into coherent word-level representations. This layered design sharpens alignment by internalizing fine-grained phonological and boundary information, ultimately improving word-level feature and reducing WER in synthetic languages.</span>\n</p>\n\n",
                "matched_terms": [
                    "other",
                    "phoneme",
                    "character",
                    "where",
                    "cif",
                    "wer",
                    "results",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We conduct a detailed comparative analysis based on the ablation results, focusing specifically on PE and SE. As summarized in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#S3.SS2\" style=\"font-size:90%;\" title=\"3.2 Overall Performance &#8227; 3 Experiments &#8227; M-CIF: Multi-Scale Alignment for CIF-Based Non-Autoregressive ASR\">\n    <span class=\"ltx_text ltx_ref_tag\">3.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the Paraformer baseline shows that both error types occur frequently in synthetic languages, indicating that single-level CIF produces unstable and imprecise alignments with abundant PE and SE errors. By contrast, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">M-CIF</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> framework substantially reduces both types of errors, demonstrating its effectiveness in addressing phonological confusions and boundary mis-segmentation in synthetic languages such as English and French with multi-syllabic structures.</span>\n</p>\n\n",
                "matched_terms": [
                    "paraformer",
                    "results",
                    "cif",
                    "mcif"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">PE Metrics</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;&#160;As shown in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#S3.SS2\" style=\"font-size:90%;\" title=\"3.2 Overall Performance &#8227; 3 Experiments &#8227; M-CIF: Multi-Scale Alignment for CIF-Based Non-Autoregressive ASR\">\n    <span class=\"ltx_text ltx_ref_tag\">3.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, on the English clean set and the German and French test sets, removing the phoneme layer leads to a sharper rise in PE rates than removing the character layer. This underscores the stronger corrective role of phoneme-level guidance in mitigating phonetic confusions: it progressively integrates this information into the subword alignment. Furthermore, in German and French, the setting with only phoneme and word layers achieves the lowest PE rates, reflecting that in languages where phonetic confusions strongly correlate with WER degradation, preserving phonological fidelity provides the most effective reduction of such errors.</span>\n</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "character",
                    "setting",
                    "where",
                    "clean",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SE Metrics</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;&#160;Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#S3.SS2\" style=\"font-size:90%;\" title=\"3.2 Overall Performance &#8227; 3 Experiments &#8227; M-CIF: Multi-Scale Alignment for CIF-Based Non-Autoregressive ASR\">\n    <span class=\"ltx_text ltx_ref_tag\">3.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows that the full multi-scale CIF yields the lowest SE rates, strongly demonstrating that reliable word-boundary segmentation in languages like English and German requires the combined effect of orthographic and phonological guidance. Furthermore, SE rates rise markedly more when the character layer is removed than when the phoneme layer is ablated. This confirms that fine-grained orthographic supervision exerts a stronger corrective influence on segmentation errors.</span>\n</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "character",
                    "cif"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To further substantiate </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">M-CIF</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8217;s effectiveness in improving compression&#8211;alignment for synthetic languages such as English, we present a comparative visualization against human-annotated ground-truth timestamps. This visualization shows timestamp alignments across different </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">M-CIF</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> configurations, including ablated variants and the original CIF. As shown in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#S4.F4\" style=\"font-size:90%;\" title=\"Figure 4 &#8227; 4.2 PE and SE Metrics Analysis &#8227; 4 Analysis &#8227; 3.2 Overall Performance &#8227; 3 Experiments &#8227; M-CIF: Multi-Scale Alignment for CIF-Based Non-Autoregressive ASR\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the complete </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">M-CIF</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> configuration aligns most closely with the ground-truth timestamps. This demonstrates that the multi-level design markedly improves alignment fidelity in synthetic languages such as English. Meanwhile, the ablated variants that remove either the phoneme layer or the character layer achieve better alignment than the original CIF but still lag behind the full configuration. These results indicate that incorporating phoneme-level and character-level guidance is essential for stabilizing CIF alignments in synthetic languages.</span>\n</p>\n\n",
                "matched_terms": [
                    "phoneme",
                    "character",
                    "cif",
                    "mcif",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we propose </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">M-CIF</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a multiscale framework for synthetic languages. This method progressively compresses fine-grained character-level and phoneme-level features into word-level representation with scale-matched CTC supervision. Building on this design, it constructs a progressive multi-scale acoustic feature capture process, thereby enhancing robust acoustic&#8211;text alignment. Experiments on English, French, and German show consistent accuracy gains and WER reductions. We further define and analyze phonetic confusion errors (PE) and space-related segmentation errors (SE). Our analysis shows that </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">M-CIF</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8217;s multi-level alignment captures fine-grained features. This mitigates challenges from the multi-syllabic and space-delimited structures of synthetic languages.</span>\n</p>\n\n",
                "matched_terms": [
                    "method",
                    "wer",
                    "our",
                    "mcif"
                ]
            }
        ]
    },
    "S3.SS2.tab1": {
        "source_file": "M-CIF: Multi-Scale Alignment for CIF-Based Non-Autoregressive ASR",
        "caption": "Table 2: Results of PE and SE error rates (values in ‰) for different Paraformer implementations, with θPE=0.6\\theta_{\\text{PE}}=0.6 and θSE=0.5\\theta_{\\text{SE}}=0.5.",
        "body": "EN(LS) ↓\\downarrow\nDE(CV) ↓\\downarrow\nFR(CV) ↓\\downarrow\n\n\nModel\nclean\nother\nAvg.\n\n\n\n\\rowcolorgray!20      PE\n\n\n\nBase\n29.42\n41.04\n35.23\n74.40\n58.91\n\n\nM-CIF*\n27.40\n41.84\n34.62\n68.15\n58.37\n\n\nw/o Char CIF\n31.60\n43.31\n37.46\n67.34\n56.62\n\n\nw/o Phone CIF\n31.85\n40.95\n36.40\n76.07\n57.26\n\n\n\n\\rowcolorgray!20      SE\n\n\n\nBase\n7.37\n12.53\n9.95\n27.14\n24.36\n\n\nM-CIF*\n7.21\n12.02\n9.62\n21.79\n20.54\n\n\nw/o Char CIF\n9.51\n13.89\n11.70\n23.44\n25.23\n\n\nw/o Phone CIF\n8.19\n13.32\n10.76\n23.02\n25.73",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">EN(LS)&#160;<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">DE(CV)&#160;<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">FR(CV)&#160;<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">clean</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">other</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Avg.</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"6\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span><span class=\"ltx_text\" style=\"font-size:90%;\">gray!20 &#8194;&#8202;&#8194;&#8202; </span><span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">PE</span>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Base</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">29.42</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">41.04</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">35.23</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">74.40</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">58.91</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">M-CIF*</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">27.40</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">41.84</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">34.62</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">68.15</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">58.37</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">w/o Char CIF</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.60</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">43.31</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">37.46</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">67.34</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">56.62</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">w/o Phone CIF</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">31.85</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">40.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">36.40</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">76.07</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">57.26</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"6\">\n<span class=\"ltx_ERROR undefined\">\\rowcolor</span><span class=\"ltx_text\" style=\"font-size:90%;\">gray!20 &#8194;&#8202;&#8194;&#8202; </span><span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">SE</span>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Base</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.37</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.53</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.14</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">24.36</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">M-CIF*</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.21</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">12.02</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">9.62</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">21.79</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">20.54</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">w/o Char CIF</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.51</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.89</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">11.70</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">23.44</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">25.23</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">w/o Phone CIF</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.19</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">10.76</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">23.02</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">25.73</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "other",
            "char",
            "θse05thetatextse05",
            "implementations",
            "↓downarrow",
            "avg",
            "mcif",
            "θpe06thetatextpe06",
            "base",
            "frcv",
            "results",
            "model",
            "phone",
            "rowcolorgray20",
            "cif",
            "values",
            "paraformer",
            "clean",
            "rates",
            "decv",
            "different",
            "enls",
            "error"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The Continuous Integrate-and-Fire (CIF) mechanism provides effective alignment for non-autoregressive (NAR) speech recognition. This mechanism creates a smooth and monotonic mapping from acoustic features to target tokens, achieving performance on Mandarin competitive with other NAR approaches. However, without finer-grained guidance, its stability degrades in some languages such as English and French. In this paper, we propose Multi-scale CIF (<span class=\"ltx_text ltx_font_italic\">M-CIF</span>), which performs multi-level alignment by integrating character and phoneme level supervision progressively distilled into subword representations, thereby enhancing robust acoustic&#8211;text alignment. Experiments show that <span class=\"ltx_text ltx_font_italic\">M-CIF</span> reduces WER compared to the Paraformer baseline, especially on CommonVoice by 4.21% in German and 3.05% in French. To further investigate these gains, we define phonetic confusion errors (<span class=\"ltx_text ltx_font_italic\">PE</span>) and space-related segmentation errors (<span class=\"ltx_text ltx_font_italic\">SE</span>) as evaluation metrics. Analysis of these metrics across different <span class=\"ltx_text ltx_font_italic\">M-CIF</span> settings reveals that the phoneme and character layers are essential for enhancing progressive CIF alignment.</span>\n</p>\n\n",
                "matched_terms": [
                    "other",
                    "different",
                    "cif",
                    "paraformer",
                    "mcif"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we propose </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">M-CIF</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a multi-scale hierarchical framework for synthetic languages. Our method progressively compresses and aligns fine-grained character-level and phoneme-level features into coherent word-level representations in a hierarchical manner, enabling more coordinated integration across scales. Furthermore, scale-matched CTC losses are incorporated at each level to provide more comprehensive supervision. Subsequently, to validate the rationale for introducing phoneme-level and character-level guidance, we quantify and analyze two error types: phonetic confusion errors (</span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">PE</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) and space-related segmentation errors (</span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SE</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). Implemented within Paraformer</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, it delivers an average relative Word Error Rate (WER) reduction of 0.31% on the LibriSpeech test set for English, and up to 4.21% and 3.05% on German and French CommonVoice, respectively.\nOur contributions are as follows:</span>\n</p>\n\n",
                "matched_terms": [
                    "error",
                    "mcif"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this section, we present a comparative visualization of the CIF firing process in isolating and synthetic languages. From this analysis, we define and examine two representative error types. Then we introduce the Multi-scale CIF method as a solution to these challenges.</span>\n</p>\n\n",
                "matched_terms": [
                    "error",
                    "cif"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To quantify these errors, we compute their rates by normalizing error counts with respect to the number of reference units. We first define the normalized Levenshtein</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> distance as </span>\n  <math alttext=\"\\text{NLD}(x,y)=\\text{Lev}(x,y)/\\max(|x|,|y|)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mtext mathsize=\"0.900em\">NLD</mtext>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <mi mathsize=\"0.900em\">x</mi>\n            <mo mathsize=\"0.900em\">,</mo>\n            <mi mathsize=\"0.900em\">y</mi>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n        </mrow>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mrow>\n            <mtext mathsize=\"0.900em\">Lev</mtext>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n              <mi mathsize=\"0.900em\">x</mi>\n              <mo mathsize=\"0.900em\">,</mo>\n              <mi mathsize=\"0.900em\">y</mi>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n            </mrow>\n          </mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\" symmetric=\"true\">/</mo>\n          <mrow>\n            <mi mathsize=\"0.900em\">max</mi>\n            <mo>&#8289;</mo>\n            <mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n              <mrow>\n                <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n                <mi mathsize=\"0.900em\">x</mi>\n                <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n              </mrow>\n              <mo mathsize=\"0.900em\">,</mo>\n              <mrow>\n                <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n                <mi mathsize=\"0.900em\">y</mi>\n                <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\">|</mo>\n              </mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n            </mrow>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\text{NLD}(x,y)=\\text{Lev}(x,y)/\\max(|x|,|y|)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. PE are counted when the normalized phoneme distance falls below </span>\n  <math alttext=\"\\theta_{\\text{PE}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#952;</mi>\n        <mtext mathsize=\"0.900em\">PE</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\theta_{\\text{PE}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and SE are counted when the reference and hypothesis show boundary mismatches but their de-spaced strings have a character-level distance below </span>\n  <math alttext=\"\\theta_{\\text{SE}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#952;</mi>\n        <mtext mathsize=\"0.900em\">SE</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\theta_{\\text{SE}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "rates",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address the unstable behavior of CIF in synthetic languages, we propose </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">M-CIF</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a multi-scale framework that alleviates multi-syllabic ambiguity through progressive alignment. As shown in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#S1.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 1 Introduction &#8227; M-CIF: Multi-Scale Alignment for CIF-Based Non-Autoregressive ASR\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">(a), it aligns encoder-derived acoustic representations at the character, phoneme, and word levels, with scale-specific CTC objectives providing auxiliary supervision for stable training.</span>\n</p>\n\n",
                "matched_terms": [
                    "cif",
                    "mcif"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">M-CIF Alignment Strategy</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160; Let the encoder output be </span>\n  <math alttext=\"\\mathbf{h}=(h_{1},h_{2},\\ldots,h_{T})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119841;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">h</mi>\n            <mn mathsize=\"0.900em\">1</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">h</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8230;</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">h</mi>\n            <mi mathsize=\"0.900em\">T</mi>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{h}=(h_{1},h_{2},\\ldots,h_{T})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and the target transcription be </span>\n  <math alttext=\"\\mathbf{Y}=(y_{1},y_{2},\\ldots,y_{U})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#119832;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">y</mi>\n            <mn mathsize=\"0.900em\">1</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">y</mi>\n            <mn mathsize=\"0.900em\">2</mn>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8230;</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">y</mi>\n            <mi mathsize=\"0.900em\">U</mi>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathbf{Y}=(y_{1},y_{2},\\ldots,y_{U})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. In </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">M-CIF</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the compression is carried out hierarchically through three stages of CIF alignment, operating respectively at the character level, the phoneme level, and the word level. At each stage </span>\n  <math alttext=\"s\\in\\{c,p,w\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <mi mathsize=\"0.900em\">c</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">p</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">w</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">s\\in\\{c,p,w\\}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, alignment is obtained by accumulating the weight </span>\n  <math alttext=\"\\alpha^{s}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m4\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">&#945;</mi>\n        <mi mathsize=\"0.900em\">s</mi>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\alpha^{s}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> until a threshold </span>\n  <math alttext=\"\\beta\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#946;</mi>\n      <annotation encoding=\"application/x-tex\">\\beta</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is reached, upon which an integrated acoustic embedding is emitted as the input to the next stage, formally defined as:</span>\n</p>\n\n",
                "matched_terms": [
                    "cif",
                    "mcif"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Char level CIF</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160; In synthetic languages such as English and French, character-level CIF decomposes words into characters with </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">|</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> marking boundaries, while in isolating languages like Chinese it operates on processed pinyin. The resulting lengths define the activation targets, with CTC loss applied to stabilize alignment.</span>\n</p>\n\n",
                "matched_terms": [
                    "char",
                    "cif"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model Architecture</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> &#160;We implement </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">M-CIF</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> on the widely adopted Paraformer</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> framework. Paraformer employs a Conformer based encoder</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and a Transformer-based decoder</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, together with a word-level CIF module that provides explicit length prediction and enforces monotonic acoustic-to-text alignment. On top of this, a GLM-based sampler, as illustrated in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#S1.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 1 Introduction &#8227; M-CIF: Multi-Scale Alignment for CIF-Based Non-Autoregressive ASR\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">(b), generates an initial candidate sequence by sampling from the predicted token distribution, which then serves as the starting point for subsequent iterative refinement during decoding.</span>\n</p>\n\n",
                "matched_terms": [
                    "paraformer",
                    "model",
                    "cif",
                    "mcif"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Baseline</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;&#160;We select Paraformer and its variant E-Paraformer</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as our baselines, and integrate the proposed </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">M-CIF</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> framework into Paraformer. Compared to basic Paraformer, which employs the base CIF structure, E-Paraformer further introduces the Parallel Integrate-and-Fire (PIF) mechanism, replacing CIF&#8217;s recursive alignment with a parallel procedure that computes a global attention matrix in one shot. For all models, we employ a 12-layer Conformer encoder and a 12-layer Transformer decoder, each with a hidden size of 256.</span>\n</p>\n\n",
                "matched_terms": [
                    "base",
                    "paraformer",
                    "cif",
                    "mcif"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;&#160;During the training stage, we employ a hyperparameter scheduling strategy tailored for the multi-scale architecture. CTC losses at different CIF levels are weighted with a scheduled emphasis across stages, while a learning-rate annealing scheme is applied: after 90 epochs, the learning rate is reinitialized to </span>\n  <math alttext=\"6.448\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">6.448</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">5</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">6.448\\times 10^{-5}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and subsequently decayed to promote stable and efficient convergence. To stabilize training on languages like Chinese, where token lengths across structural levels are relatively close, we adopt a three-stage curriculum</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Stage I uses only character-level CTC and length losses; Stage II adds phoneme-level objectives; and Stage III incorporates word-level CTC, length losses, and final decoder cross-entropy. This progressive introduction of objectives effectively stabilizes alignment and ensures reliable convergence.</span>\n</p>\n\n",
                "matched_terms": [
                    "different",
                    "cif"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Integrating multi-scale CIF into the Paraformer yields consistent improvements across synthetic languages. As shown in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#S2.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 2.2 Multi-scale CIF Strategy &#8227; 2 METHOD &#8227; M-CIF: Multi-Scale Alignment for CIF-Based Non-Autoregressive ASR\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, relative WER reductions of 0.31% are observed on average for the LibriSpeech test sets, together with reductions of 3.05% on the French CommonVoice test set and 4.21% on the German CommonVoice test set. On the contrary, on Chinese corpora this strategy still performs 0.18% WER worse than the baseline, indicating that multi-scale supervision provides limited gains where syllable-based units already impose stable alignment boundaries. Overall, these results demonstrate the performance advantage of the multi-scale CIF architecture in synthetic languages such as English, German and French, effectively reducing WER errors and improving recognition accuracy.</span>\n</p>\n\n",
                "matched_terms": [
                    "paraformer",
                    "results",
                    "cif"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We perform ablation experiments by removing the phoneme and character level alignments while keeping other settings unchanged. Our ablation results in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#S2.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 2.2 Multi-scale CIF Strategy &#8227; 2 METHOD &#8227; M-CIF: Multi-Scale Alignment for CIF-Based Non-Autoregressive ASR\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> reveal that removing either the character layer or the phoneme layer consistently increases WER in English, French, and German. This shows that the three-level architecture is indispensable rather than redundant. Each component makes a complementary contribution to overall performance. Based on this, the multi-scale CIF framework performs hierarchical compression&#8211;alignment, where character and phoneme level supervision is progressively distilled into coherent word-level representations. This layered design sharpens alignment by internalizing fine-grained phonological and boundary information, ultimately improving word-level feature and reducing WER in synthetic languages.</span>\n</p>\n\n",
                "matched_terms": [
                    "other",
                    "results",
                    "cif"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We conduct a detailed comparative analysis based on the ablation results, focusing specifically on PE and SE. As summarized in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#S3.SS2\" style=\"font-size:90%;\" title=\"3.2 Overall Performance &#8227; 3 Experiments &#8227; M-CIF: Multi-Scale Alignment for CIF-Based Non-Autoregressive ASR\">\n    <span class=\"ltx_text ltx_ref_tag\">3.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the Paraformer baseline shows that both error types occur frequently in synthetic languages, indicating that single-level CIF produces unstable and imprecise alignments with abundant PE and SE errors. By contrast, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">M-CIF</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> framework substantially reduces both types of errors, demonstrating its effectiveness in addressing phonological confusions and boundary mis-segmentation in synthetic languages such as English and French with multi-syllabic structures.</span>\n</p>\n\n",
                "matched_terms": [
                    "cif",
                    "paraformer",
                    "mcif",
                    "results",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">PE Metrics</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;&#160;As shown in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#S3.SS2\" style=\"font-size:90%;\" title=\"3.2 Overall Performance &#8227; 3 Experiments &#8227; M-CIF: Multi-Scale Alignment for CIF-Based Non-Autoregressive ASR\">\n    <span class=\"ltx_text ltx_ref_tag\">3.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, on the English clean set and the German and French test sets, removing the phoneme layer leads to a sharper rise in PE rates than removing the character layer. This underscores the stronger corrective role of phoneme-level guidance in mitigating phonetic confusions: it progressively integrates this information into the subword alignment. Furthermore, in German and French, the setting with only phoneme and word layers achieves the lowest PE rates, reflecting that in languages where phonetic confusions strongly correlate with WER degradation, preserving phonological fidelity provides the most effective reduction of such errors.</span>\n</p>\n\n",
                "matched_terms": [
                    "rates",
                    "clean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SE Metrics</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#160;&#160;Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#S3.SS2\" style=\"font-size:90%;\" title=\"3.2 Overall Performance &#8227; 3 Experiments &#8227; M-CIF: Multi-Scale Alignment for CIF-Based Non-Autoregressive ASR\">\n    <span class=\"ltx_text ltx_ref_tag\">3.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows that the full multi-scale CIF yields the lowest SE rates, strongly demonstrating that reliable word-boundary segmentation in languages like English and German requires the combined effect of orthographic and phonological guidance. Furthermore, SE rates rise markedly more when the character layer is removed than when the phoneme layer is ablated. This confirms that fine-grained orthographic supervision exerts a stronger corrective influence on segmentation errors.</span>\n</p>\n\n",
                "matched_terms": [
                    "rates",
                    "cif"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To further substantiate </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">M-CIF</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8217;s effectiveness in improving compression&#8211;alignment for synthetic languages such as English, we present a comparative visualization against human-annotated ground-truth timestamps. This visualization shows timestamp alignments across different </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">M-CIF</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> configurations, including ablated variants and the original CIF. As shown in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.22172v1#S4.F4\" style=\"font-size:90%;\" title=\"Figure 4 &#8227; 4.2 PE and SE Metrics Analysis &#8227; 4 Analysis &#8227; 3.2 Overall Performance &#8227; 3 Experiments &#8227; M-CIF: Multi-Scale Alignment for CIF-Based Non-Autoregressive ASR\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the complete </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">M-CIF</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> configuration aligns most closely with the ground-truth timestamps. This demonstrates that the multi-level design markedly improves alignment fidelity in synthetic languages such as English. Meanwhile, the ablated variants that remove either the phoneme layer or the character layer achieve better alignment than the original CIF but still lag behind the full configuration. These results indicate that incorporating phoneme-level and character-level guidance is essential for stabilizing CIF alignments in synthetic languages.</span>\n</p>\n\n",
                "matched_terms": [
                    "different",
                    "results",
                    "cif",
                    "mcif"
                ]
            }
        ]
    }
}