{
    "S2.T1": {
        "source_file": "End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs",
        "caption": "Table 1: Details of each model, with its corresponding Encoder and Decoder components",
        "body": "Encoder\nDecoder\n\n\nAdapter\n\n\n\n\n\n\n\n\nHuBERT\n\n\nGemma 7B\n\n\nCTC collapse\n\n\n\n\nGemma 2 9B\n\n\n\nLlama 2 7B\n\n\n\nMistral 7B v0.1\n\n\n\n\n\nWhisper enc.\n\n\nGemma 7B\n\n\n5x5 Convolution\n\n\n\n\nGemma 2 9B\n\n\n\nLlama 2 7B\n\n\n\nMistral 7B v0.1",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">Encoder</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">Decoder</span></th>\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Adapter</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" rowspan=\"4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.5pt;\">HuBERT</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Gemma 7B</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">CTC collapse</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Gemma 2 9B</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Llama 2 7B</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Mistral 7B v0.1</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t\" rowspan=\"4\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.5pt;\">Whisper enc.</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Gemma 7B</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\">5x5 Convolution</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Gemma 2 9B</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Llama 2 7B</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Mistral 7B v0.1</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_b\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"/>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "components",
            "collapse",
            "v01",
            "convolution",
            "details",
            "llama",
            "adapter",
            "each",
            "encoder",
            "decoder",
            "enc",
            "mistral",
            "gemma",
            "its",
            "hubert",
            "whisper",
            "corresponding",
            "5x5",
            "ctc",
            "model"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We experimented with four different pre-trained LLMs available on HuggingFace, namely <em class=\"ltx_emph ltx_font_italic\">Gemma 7B</em> (<span class=\"ltx_text ltx_font_typewriter\">gemma-7b</span>),\n<em class=\"ltx_emph ltx_font_italic\">Gemma 2 9B</em> (<span class=\"ltx_text ltx_font_typewriter\">gemma-2-9b</span>),\n<em class=\"ltx_emph ltx_font_italic\">Llama 2 7B</em> (<span class=\"ltx_text ltx_font_typewriter\">Llama-2-7b-hf</span>),\nand <em class=\"ltx_emph ltx_font_italic\">Mistral 7B v0.1</em> (<span class=\"ltx_text ltx_font_typewriter\">Mistral-7B-v0.1</span>).\nDetails about each variation are described in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#S2.T1\" title=\"In 2.5 LLMs &#8227; 2 Methods and Dataset &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Speech Translation (ST) is a machine translation task that involves converting speech signals from one language to the corresponding text in another language;\nthis task has two different approaches, namely the traditional <em class=\"ltx_emph ltx_font_italic\">cascade</em> and the more recent <em class=\"ltx_emph ltx_font_italic\">end-to-end</em>. This paper explores a combined end-to-end architecture of pre-trained speech encoders and Large Language Models (LLMs) for performing both Automatic Speech Recognition (ASR) and ST simultaneously. Experiments with the English-to-German language pair show that our best model not only can achieve better translation results than SeamlessM4T <cite class=\"ltx_cite ltx_citemacro_citep\">(Communication et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib5\" title=\"\">2023</a>)</cite>, a large foundational end-to-end, multi-modal translation model, but can also match the performance of a cascaded system with Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib24\" title=\"\">2022</a>)</cite> and NLLB <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib27\" title=\"\">2022</a>)</cite>, with up to a score gain of 8% in <math alttext=\"\\text{COMET}^{\\text{DA}}_{22}\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><msubsup><mtext>COMET</mtext><mn>22</mn><mtext>DA</mtext></msubsup><annotation encoding=\"application/x-tex\">\\text{COMET}^{\\text{DA}}_{22}</annotation></semantics></math> metric.</p>\n\n",
                "matched_terms": [
                    "model",
                    "corresponding",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">End-to-end Speech Translation is a growing research direction that aims to ignore the intermediate ASR step to directly translate the audio input into its corresponding text in another language. This approach simplifies the overall architecture, which has been shown to match the performance of the cascaded counterpart <cite class=\"ltx_cite ltx_citemacro_citep\">(B&#233;rard et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib3\" title=\"\">2018</a>; Liu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib15\" title=\"\">2019</a>; Gaido et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib8\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "corresponding",
                    "its"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The overall architecture is illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#S2.F1\" title=\"In 2.1 Architecture &#8227; 2 Methods and Dataset &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>. For each training sample, given the speech signal, its corresponding transcript, and the translated text, the speech hidden features are obtained using a speech encoder, including HuBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib10\" title=\"\">2021</a>)</cite> and Whisper encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib24\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "whisper",
                    "its",
                    "hubert",
                    "corresponding",
                    "each",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopted HuBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib10\" title=\"\">2021</a>)</cite> and Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib24\" title=\"\">2022</a>)</cite> as the speech encoders, utilizing their capability of extracting high-quality representation from audio data. We used the <span class=\"ltx_text ltx_font_typewriter\">hubert-large-ls960-ft</span>\nvariation, which was trained on 60,000 hours of data from the LibriLight <cite class=\"ltx_cite ltx_citemacro_citep\">(Kahn et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib11\" title=\"\">2020</a>)</cite> corpus, then fine-tuned on 960 hours of data from the LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib20\" title=\"\">2015a</a>)</cite> corpus. For Whisper-based models, we only used the encoder part of the pre-trained <span class=\"ltx_text ltx_font_typewriter\">whisper-large-v3-turbo</span>\nto extract the audio hidden features.</p>\n\n",
                "matched_terms": [
                    "encoder",
                    "hubert",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For HuBERT-based models, we followed the work of <cite class=\"ltx_cite ltx_citemacro_citet\">Gaido et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib7\" title=\"\">2021</a>)</cite>, and compressed the feature sequence by taking an average of vectors whose repeated labels were obtained from the followed Connectionist Temporal Classification (CTC) layer. <cite class=\"ltx_cite ltx_citemacro_citet\">Wu et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib28\" title=\"\">2023</a>)</cite> illustrated that speech feature sequence compression with CTC gave better results than the traditional collapsing approach with convolution layers in the speech translation task. Hence, in our pipeline, from the obtained labels predicted by CTC, we merged the vectors with repeating labels by averaging their corresponding values.</p>\n\n",
                "matched_terms": [
                    "ctc",
                    "corresponding",
                    "convolution"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compared our architecture with two out-of-the-box baselines: a cascaded pipeline of Whisper\n(<span class=\"ltx_text ltx_font_typewriter\">whisper-large-v3-turbo</span>; <cite class=\"ltx_cite ltx_citemacro_citep\">Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib24\" title=\"\">2022</a></cite>) producing the transcript and NLLB\n(<span class=\"ltx_text ltx_font_typewriter\">nllb-200-3.3B</span>; <cite class=\"ltx_cite ltx_citemacro_citep\">Team et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib27\" title=\"\">2022</a></cite>) translating the transcript, along with SeamlessM4T\n(<span class=\"ltx_text ltx_font_typewriter\">seamless-m4t-v2-large</span>; <cite class=\"ltx_cite ltx_citemacro_citep\">Communication et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib5\" title=\"\">2023</a></cite>) - an end-to-end, multi-modal translation model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#S3.T2\" title=\"In 3.2 ASR Results &#8227; 3 Evaluation &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a> details the ASR evaluation results against the four test sets. We reported the <span class=\"ltx_text ltx_font_smallcaps\">WER</span> score after applying the &#8220;LPW&#8221; pre-processing strategy available in <span class=\"ltx_text ltx_font_typewriter\">SLTev</span>, which first lowercased every character, removed all punctuation, then used the built-in <span class=\"ltx_text ltx_font_typewriter\">mwerSegmenter</span> tool to resegment the output transcripts. Due to some bugs when processing the IWSLT&#8217;21 test set (tst2021), <span class=\"ltx_text ltx_font_typewriter\">mwerSegmenter</span> failed to run during evaluation, hence we could not obtain the results. It can be seen that models with Gemma 2 9B as the decoder have the best result among the four LLMs, albeit still lagging behind the performance of Whisper.</p>\n\n",
                "matched_terms": [
                    "gemma",
                    "decoder",
                    "whisper",
                    "details"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Similarly, the models with Gemma 2 9B still have the best evaluation score among the four fine-tuned LLMs. In combination with the Whisper encoder, it even surpassed the performance of the cascaded system of Whisper + NLLB in most of the test sets and metrics.</p>\n\n",
                "matched_terms": [
                    "gemma",
                    "encoder",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Try replacing the CTC collapsing procedure with a length adapter of convolution layers for the HuBERT encoder. Try other modal adapter methods, like Q-Former.</p>\n\n",
                "matched_terms": [
                    "hubert",
                    "convolution",
                    "adapter",
                    "ctc",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we leveraged pre-trained speech encoders and LLMs and connected them to become an end-to-end architecture for speech translation. The overall result is expected: for the English-to-German direction, even though our models performed better than the end-to-end SeamlessM4T model all of the time, there was still a gap compared to the performance of the cascaded Whisper + NLLB pipeline. It suggests that cascaded models are still the state-of-the-art approach in the speech translation task; this is also confirmed according to <cite class=\"ltx_cite ltx_citemacro_citet\">Ahmad et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib1\" title=\"\">2024</a>)</cite>, in which all systems submitted to the Offline Track of IWSLT&#8217;24 were cascaded systems.</p>\n\n",
                "matched_terms": [
                    "model",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, considering the size of the LLMs, our models were inferior regarding inference speed, compared to the two baselines. Our models also managed to surpass the performance of the cascaded system in the translation task; however, the differences were not too substantial. In addition, despite being a much smaller model, Whisper alone still excels at speech recognition. This raises a question: <span class=\"ltx_text ltx_font_italic\">\"Can end-to-end speech translation systems be smaller in size, while still keeping the robustness in translation, especially for the rising need to be used in mobile devices?\"</span></p>\n\n",
                "matched_terms": [
                    "model",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All models were fine-tuned using 4-bit QLoRA <cite class=\"ltx_cite ltx_citemacro_citep\">(Dettmers et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib6\" title=\"\">2023</a>)</cite> adapters in <span class=\"ltx_text ltx_font_typewriter\">bfloat16</span> precision, with the following LoRA parameters: rank of <math alttext=\"r=8\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">r=8</annotation></semantics></math>, alpha of <math alttext=\"\\alpha=8\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=8</annotation></semantics></math>. For the models with HuBERT as the encoder, because of the manual CTC collapsing procedure, we could only process one example at a time, hence the batch size was set to <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m3\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>; while for those with Whisper, the batch size was set to <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m4\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>. Other training hyperparameters included the learning rate of <math alttext=\"1e-4\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m5\" intent=\":literal\"><semantics><mrow><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>&#8722;</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">1e-4</annotation></semantics></math> with <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m6\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math> warmup steps, and an AdamW optimizer <cite class=\"ltx_cite ltx_citemacro_cite\">Loshchilov and Hutter (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib17\" title=\"\">2019</a>)</cite> with a cosine scheduler <cite class=\"ltx_cite ltx_citemacro_cite\">Loshchilov and Hutter (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib16\" title=\"\">2017</a>)</cite>. All HuBERT-encoder models were trained for <math alttext=\"500,000\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m7\" intent=\":literal\"><semantics><mrow><mn>500</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding=\"application/x-tex\">500,000</annotation></semantics></math> steps, while Whisper-encoder models were trained for <math alttext=\"100,000\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m8\" intent=\":literal\"><semantics><mrow><mn>100</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding=\"application/x-tex\">100,000</annotation></semantics></math> steps.</p>\n\n",
                "matched_terms": [
                    "ctc",
                    "encoder",
                    "hubert",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During training, we added three new tokens to feed into the LLMs, namely &#8220;<span class=\"ltx_text ltx_font_typewriter\">&lt;&gt;audio&lt;&gt;</span>&#8221;, &#8220;<span class=\"ltx_text ltx_font_typewriter\">&lt;&gt;transcript&lt;&gt;</span>&#8221;, and &#8220;<span class=\"ltx_text ltx_font_typewriter\">&lt;&gt;translation&lt;&gt;</span>&#8221;, which acted as separators between the extracted audio features, the ASR transcript, and the corresponding translation, respectively. For each sample, the training data is formatted as follows: &#8220;<span class=\"ltx_text ltx_font_typewriter\">&lt;bos&gt; &lt;&gt;audio&lt;&gt; {audio features} &lt;&gt;transcript&lt;&gt; {transcript} &lt;&gt;translation&lt;&gt; {translation} &lt;eos&gt;</span>&#8221;. The cross-entropy loss was computed only for the tokens following &#8220;<span class=\"ltx_text ltx_font_typewriter\">&lt;&gt;transcript&lt;&gt;</span>&#8221;. Each model&#8217;s training loss details are illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#A1.F3.sf1\" title=\"In Figure 3 &#8227; Appendix A Training and Inference Details &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">Figures</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3(a)</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#A1.F3.sf2\" title=\"Figure 3(b) &#8227; Figure 3 &#8227; Appendix A Training and Inference Details &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">3(b)</span></a>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "corresponding",
                    "details"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During inference, for each audio data, the LLMs were prompted using the following format: &#8220;<span class=\"ltx_text ltx_font_typewriter\">&lt;bos&gt; &lt;&gt;audio&lt;&gt; {audio features} &lt;&gt;transcript&lt;&gt;</span>&#8221;, then generated the transcript and the corresponding translated text in an auto-regressive manner.\nWe performed inference using the beam search algorithm, with a beam size of 2 for all models. All evaluation results,\nare described in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#S3.SS2\" title=\"3.2 ASR Results &#8227; 3 Evaluation &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">Sections</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.2</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#S3.SS3\" title=\"3.3 Offline ST Results &#8227; 3 Evaluation &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "corresponding"
                ]
            }
        ]
    },
    "S3.T2": {
        "source_file": "End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs",
        "caption": "Table 2: ASR evaluation results (WER)",
        "body": "Model\nMuST-C\nIWSLT\nLibriSpeech\n\n\n\n\n\n\n\n\n\ntst-COMMON v2\n\n\n\n\ntst-COMMON v3\n\n\n\n\ntst2022\n\n\n\n\ntest-clean\n\n\n\n\ntest-other\n\n\n\n\nWhisper\n\n\n6.7%\\mathbf{6.7\\%}\n\n\n\n\n7.7%\\mathbf{7.7\\%}\n\n\n\n\n11.8%\\mathbf{11.8\\%}\n\n\n\n\n4.1%\\mathbf{4.1\\%}\n\n\n\n\n7.2%\\mathbf{7.2\\%}\n\n\n\n\n\n\nHuBERT + Gemma 2 9B\n\n\n\n\n11.111.1%\n\n\n\n\n12.512.5%\n\n\n\n\n21.921.9%\n\n\n\n\n8.48.4%\n\n\n\n\n13.113.1%\n\n\n\n\n\n\nHuBERT + Gemma 7B\n\n\n\n\n12.912.9%\n\n\n\n\n14.514.5%\n\n\n\n\n30.730.7%\n\n\n\n\n11.711.7%\n\n\n\n\n17.417.4%\n\n\n\n\n\n\nHuBERT + Llama 2 7B\n\n\n\n\n11.111.1%\n\n\n\n\n12.612.6%\n\n\n\n\n22.922.9%\n\n\n\n\n8.78.7%\n\n\n\n\n13.213.2%\n\n\n\n\n\n\nHuBERT + Mistral 7B v0.1\n\n\n\n\n11.111.1%\n\n\n\n\n12.412.4%\n\n\n\n\n22.922.9%\n\n\n\n\n8.58.5%\n\n\n\n\n13.313.3%\n\n\n\n\n\n\nWhisper enc. + Gemma 2 9B\n\n\n\n\n8.28.2%\n\n\n\n\n8.18.1%\n\n\n\n\n22.622.6%\n\n\n\n\n8.08.0%\n\n\n\n\n13.713.7%\n\n\n\n\n\n\nWhisper enc. + Gemma 7B\n\n\n\n\n8.68.6%\n\n\n\n\n10.410.4%\n\n\n\n\n25.125.1%\n\n\n\n\n11.711.7%\n\n\n\n\n18.818.8%\n\n\n\n\n\n\nWhisper enc. + Llama 2 7B\n\n\n\n\n10.510.5%\n\n\n\n\n12.812.8%\n\n\n\n\n22.522.5%\n\n\n\n\n9.29.2%\n\n\n\n\n14.814.8%\n\n\n\n\n\n\nWhisper enc. + Mistral 7B v0.1\n\n\n\n\n9.09.0%\n\n\n\n\n10.210.2%\n\n\n\n\n23.723.7%\n\n\n\n\n8.28.2%\n\n\n\n\n14.514.5%",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_align_top ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">MuST-C</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">IWSLT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">LibriSpeech</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:108.4pt;\"/>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">tst-COMMON v2</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">tst-COMMON v3</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">tst2022</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">test-clean</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">test-other</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Whisper</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"\\mathbf{6.7\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m1\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">6.7</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">\\mathbf{6.7\\%}</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"\\mathbf{7.7\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m2\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">7.7</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">\\mathbf{7.7\\%}</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"\\mathbf{11.8\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m3\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">11.8</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">\\mathbf{11.8\\%}</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"\\mathbf{4.1\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m4\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">4.1</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">\\mathbf{4.1\\%}</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"\\mathbf{7.2\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m5\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">7.2</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">\\mathbf{7.2\\%}</annotation></semantics></math></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:108.4pt;\">HuBERT + Gemma 2 9B</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"11.1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m6\" intent=\":literal\"><semantics><mn>11.1</mn><annotation encoding=\"application/x-tex\">11.1</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"12.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m7\" intent=\":literal\"><semantics><mn>12.5</mn><annotation encoding=\"application/x-tex\">12.5</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"21.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m8\" intent=\":literal\"><semantics><mn>21.9</mn><annotation encoding=\"application/x-tex\">21.9</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"8.4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m9\" intent=\":literal\"><semantics><mn>8.4</mn><annotation encoding=\"application/x-tex\">8.4</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"13.1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m10\" intent=\":literal\"><semantics><mn>13.1</mn><annotation encoding=\"application/x-tex\">13.1</annotation></semantics></math>%</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:108.4pt;\">HuBERT + Gemma 7B</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"12.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m11\" intent=\":literal\"><semantics><mn>12.9</mn><annotation encoding=\"application/x-tex\">12.9</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"14.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m12\" intent=\":literal\"><semantics><mn>14.5</mn><annotation encoding=\"application/x-tex\">14.5</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"30.7\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m13\" intent=\":literal\"><semantics><mn>30.7</mn><annotation encoding=\"application/x-tex\">30.7</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"11.7\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m14\" intent=\":literal\"><semantics><mn>11.7</mn><annotation encoding=\"application/x-tex\">11.7</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"17.4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m15\" intent=\":literal\"><semantics><mn>17.4</mn><annotation encoding=\"application/x-tex\">17.4</annotation></semantics></math>%</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:108.4pt;\">HuBERT + Llama 2 7B</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"11.1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m16\" intent=\":literal\"><semantics><mn>11.1</mn><annotation encoding=\"application/x-tex\">11.1</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"12.6\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m17\" intent=\":literal\"><semantics><mn>12.6</mn><annotation encoding=\"application/x-tex\">12.6</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"22.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m18\" intent=\":literal\"><semantics><mn>22.9</mn><annotation encoding=\"application/x-tex\">22.9</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"8.7\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m19\" intent=\":literal\"><semantics><mn>8.7</mn><annotation encoding=\"application/x-tex\">8.7</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"13.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m20\" intent=\":literal\"><semantics><mn>13.2</mn><annotation encoding=\"application/x-tex\">13.2</annotation></semantics></math>%</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:108.4pt;\">HuBERT + Mistral 7B v0.1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"11.1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m21\" intent=\":literal\"><semantics><mn>11.1</mn><annotation encoding=\"application/x-tex\">11.1</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"12.4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m22\" intent=\":literal\"><semantics><mn>12.4</mn><annotation encoding=\"application/x-tex\">12.4</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"22.9\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m23\" intent=\":literal\"><semantics><mn>22.9</mn><annotation encoding=\"application/x-tex\">22.9</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"8.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m24\" intent=\":literal\"><semantics><mn>8.5</mn><annotation encoding=\"application/x-tex\">8.5</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"13.3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m25\" intent=\":literal\"><semantics><mn>13.3</mn><annotation encoding=\"application/x-tex\">13.3</annotation></semantics></math>%</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:108.4pt;\">Whisper enc. + Gemma 2 9B</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"8.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m26\" intent=\":literal\"><semantics><mn>8.2</mn><annotation encoding=\"application/x-tex\">8.2</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"8.1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m27\" intent=\":literal\"><semantics><mn>8.1</mn><annotation encoding=\"application/x-tex\">8.1</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"22.6\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m28\" intent=\":literal\"><semantics><mn>22.6</mn><annotation encoding=\"application/x-tex\">22.6</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"8.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m29\" intent=\":literal\"><semantics><mn>8.0</mn><annotation encoding=\"application/x-tex\">8.0</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"13.7\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m30\" intent=\":literal\"><semantics><mn>13.7</mn><annotation encoding=\"application/x-tex\">13.7</annotation></semantics></math>%</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:108.4pt;\">Whisper enc. + Gemma 7B</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"8.6\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m31\" intent=\":literal\"><semantics><mn>8.6</mn><annotation encoding=\"application/x-tex\">8.6</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"10.4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m32\" intent=\":literal\"><semantics><mn>10.4</mn><annotation encoding=\"application/x-tex\">10.4</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"25.1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m33\" intent=\":literal\"><semantics><mn>25.1</mn><annotation encoding=\"application/x-tex\">25.1</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"11.7\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m34\" intent=\":literal\"><semantics><mn>11.7</mn><annotation encoding=\"application/x-tex\">11.7</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"18.8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m35\" intent=\":literal\"><semantics><mn>18.8</mn><annotation encoding=\"application/x-tex\">18.8</annotation></semantics></math>%</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:108.4pt;\">Whisper enc. + Llama 2 7B</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"10.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m36\" intent=\":literal\"><semantics><mn>10.5</mn><annotation encoding=\"application/x-tex\">10.5</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"12.8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m37\" intent=\":literal\"><semantics><mn>12.8</mn><annotation encoding=\"application/x-tex\">12.8</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"22.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m38\" intent=\":literal\"><semantics><mn>22.5</mn><annotation encoding=\"application/x-tex\">22.5</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"9.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m39\" intent=\":literal\"><semantics><mn>9.2</mn><annotation encoding=\"application/x-tex\">9.2</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"14.8\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m40\" intent=\":literal\"><semantics><mn>14.8</mn><annotation encoding=\"application/x-tex\">14.8</annotation></semantics></math>%</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:108.4pt;\">Whisper enc. + Mistral 7B v0.1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"9.0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m41\" intent=\":literal\"><semantics><mn>9.0</mn><annotation encoding=\"application/x-tex\">9.0</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"10.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m42\" intent=\":literal\"><semantics><mn>10.2</mn><annotation encoding=\"application/x-tex\">10.2</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"23.7\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m43\" intent=\":literal\"><semantics><mn>23.7</mn><annotation encoding=\"application/x-tex\">23.7</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"8.2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m44\" intent=\":literal\"><semantics><mn>8.2</mn><annotation encoding=\"application/x-tex\">8.2</annotation></semantics></math>%</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><math alttext=\"14.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m45\" intent=\":literal\"><semantics><mn>14.5</mn><annotation encoding=\"application/x-tex\">14.5</annotation></semantics></math>%</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "testother",
            "wer",
            "77mathbf77",
            "tstcommon",
            "v01",
            "evaluation",
            "asr",
            "67mathbf67",
            "results",
            "llama",
            "41mathbf41",
            "enc",
            "72mathbf72",
            "testclean",
            "118mathbf118",
            "mistral",
            "mustc",
            "gemma",
            "whisper",
            "hubert",
            "librispeech",
            "iwslt",
            "tst2022",
            "model"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#S3.T2\" title=\"In 3.2 ASR Results &#8227; 3 Evaluation &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a> details the ASR evaluation results against the four test sets. We reported the <span class=\"ltx_text ltx_font_smallcaps\">WER</span> score after applying the &#8220;LPW&#8221; pre-processing strategy available in <span class=\"ltx_text ltx_font_typewriter\">SLTev</span>, which first lowercased every character, removed all punctuation, then used the built-in <span class=\"ltx_text ltx_font_typewriter\">mwerSegmenter</span> tool to resegment the output transcripts. Due to some bugs when processing the IWSLT&#8217;21 test set (tst2021), <span class=\"ltx_text ltx_font_typewriter\">mwerSegmenter</span> failed to run during evaluation, hence we could not obtain the results. It can be seen that models with Gemma 2 9B as the decoder have the best result among the four LLMs, albeit still lagging behind the performance of Whisper.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Speech Translation (ST) is a machine translation task that involves converting speech signals from one language to the corresponding text in another language;\nthis task has two different approaches, namely the traditional <em class=\"ltx_emph ltx_font_italic\">cascade</em> and the more recent <em class=\"ltx_emph ltx_font_italic\">end-to-end</em>. This paper explores a combined end-to-end architecture of pre-trained speech encoders and Large Language Models (LLMs) for performing both Automatic Speech Recognition (ASR) and ST simultaneously. Experiments with the English-to-German language pair show that our best model not only can achieve better translation results than SeamlessM4T <cite class=\"ltx_cite ltx_citemacro_citep\">(Communication et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib5\" title=\"\">2023</a>)</cite>, a large foundational end-to-end, multi-modal translation model, but can also match the performance of a cascaded system with Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib24\" title=\"\">2022</a>)</cite> and NLLB <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib27\" title=\"\">2022</a>)</cite>, with up to a score gain of 8% in <math alttext=\"\\text{COMET}^{\\text{DA}}_{22}\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><msubsup><mtext>COMET</mtext><mn>22</mn><mtext>DA</mtext></msubsup><annotation encoding=\"application/x-tex\">\\text{COMET}^{\\text{DA}}_{22}</annotation></semantics></math> metric.</p>\n\n",
                "matched_terms": [
                    "results",
                    "asr",
                    "model",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by recent contributions in speech representation learning and LLMs, we aim to investigate an end-to-end architecture that simultaneously performs both ASR and ST. This architecture combines the high-quality audio representation from the pre-trained acoustic models with the excellent performance of LLMs to serve as an end-to-end speech translation system,\nwhile still having the ability to transcribe from the audio signal. Our proposed model, after being fine-tuned with the Quantized Low-Rank Adaptation (QLoRA; <cite class=\"ltx_cite ltx_citemacro_citep\">Dettmers et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib6\" title=\"\">2023</a></cite>) technique, achieves a robust translation performance, comparable to a cascaded system, which is still a state-of-the-art approach for this task.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#S3\" title=\"3 Evaluation &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a> provides the ASR and ST evaluation results of the model in different public test sets, and compares them to some baselines from out-of-the-box models.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results",
                    "asr",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The overall architecture is illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#S2.F1\" title=\"In 2.1 Architecture &#8227; 2 Methods and Dataset &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>. For each training sample, given the speech signal, its corresponding transcript, and the translated text, the speech hidden features are obtained using a speech encoder, including HuBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib10\" title=\"\">2021</a>)</cite> and Whisper encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib24\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "hubert",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopted HuBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib10\" title=\"\">2021</a>)</cite> and Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib24\" title=\"\">2022</a>)</cite> as the speech encoders, utilizing their capability of extracting high-quality representation from audio data. We used the <span class=\"ltx_text ltx_font_typewriter\">hubert-large-ls960-ft</span>\nvariation, which was trained on 60,000 hours of data from the LibriLight <cite class=\"ltx_cite ltx_citemacro_citep\">(Kahn et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib11\" title=\"\">2020</a>)</cite> corpus, then fine-tuned on 960 hours of data from the LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib20\" title=\"\">2015a</a>)</cite> corpus. For Whisper-based models, we only used the encoder part of the pre-trained <span class=\"ltx_text ltx_font_typewriter\">whisper-large-v3-turbo</span>\nto extract the audio hidden features.</p>\n\n",
                "matched_terms": [
                    "hubert",
                    "whisper",
                    "librispeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We experimented with four different pre-trained LLMs available on HuggingFace, namely <em class=\"ltx_emph ltx_font_italic\">Gemma 7B</em> (<span class=\"ltx_text ltx_font_typewriter\">gemma-7b</span>),\n<em class=\"ltx_emph ltx_font_italic\">Gemma 2 9B</em> (<span class=\"ltx_text ltx_font_typewriter\">gemma-2-9b</span>),\n<em class=\"ltx_emph ltx_font_italic\">Llama 2 7B</em> (<span class=\"ltx_text ltx_font_typewriter\">Llama-2-7b-hf</span>),\nand <em class=\"ltx_emph ltx_font_italic\">Mistral 7B v0.1</em> (<span class=\"ltx_text ltx_font_typewriter\">Mistral-7B-v0.1</span>).\nDetails about each variation are described in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#S2.T1\" title=\"In 2.5 LLMs &#8227; 2 Methods and Dataset &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "gemma",
                    "mistral",
                    "v01",
                    "llama"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, MuST-C also provides two public test sets, both named <span class=\"ltx_text ltx_font_typewriter\">tst-COMMON</span> in version 2.0 and 3.0. We also used the test sets from the Offline Track of IWSLT&#8217;21\nand &#8217;22.\nIn addition, to evaluate ASR performance, we used two test sets from the LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib21\" title=\"\">2015b</a>)</cite> dataset, namely <span class=\"ltx_text ltx_font_typewriter\">test-clean</span> and <span class=\"ltx_text ltx_font_typewriter\">test-other</span>, both of which are the standard datasets for this task. As all models can perform both ASR and ST simultaneously, evaluation results for both tasks are described in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#S3.SS2\" title=\"3.2 ASR Results &#8227; 3 Evaluation &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">Sections</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.2</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#S3.SS3\" title=\"3.3 Offline ST Results &#8227; 3 Evaluation &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>, respectively.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "testother",
                    "evaluation",
                    "librispeech",
                    "asr",
                    "testclean",
                    "results",
                    "tstcommon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the evaluation purpose, we used the <span class=\"ltx_text ltx_font_typewriter\">SLTev</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Ansari et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib2\" title=\"\">2021</a>)</cite> library,\nbecause it supports both MT and ASR evaluation in one package, using <span class=\"ltx_text ltx_font_typewriter\">sacreBLEU</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Post, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib23\" title=\"\">2018</a>)</cite> to calculate <span class=\"ltx_text ltx_font_smallcaps\">BLEU</span> score. However, since <span class=\"ltx_text ltx_font_typewriter\">SLTev</span> does not report any <span class=\"ltx_text ltx_font_smallcaps\">COMET</span>-family metrics, we had to change the structure of the sentence with <span class=\"ltx_text ltx_font_typewriter\">mwerSegmenter</span>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www-i6.informatik.rwth-aachen.de/web/Software/mwerSegmenter.tar.gz\" title=\"\">https://www-i6.informatik.rwth-aachen.de/web/Software/mwerSegmenter.tar.gz</a></span></span></span>\nto automatically resegment the models&#8217; output according to the reference,\nbefore evaluating with the <span class=\"ltx_text ltx_font_typewriter\">unbabel-comet</span>\npackage. The evaluation was done using <span class=\"ltx_text ltx_font_typewriter\">python-3.11.5</span>, <span class=\"ltx_text ltx_font_typewriter\">SLTev-1.2.3</span>, and <span class=\"ltx_text ltx_font_typewriter\">unbabel-comet-2.2.2</span>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compared our architecture with two out-of-the-box baselines: a cascaded pipeline of Whisper\n(<span class=\"ltx_text ltx_font_typewriter\">whisper-large-v3-turbo</span>; <cite class=\"ltx_cite ltx_citemacro_citep\">Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib24\" title=\"\">2022</a></cite>) producing the transcript and NLLB\n(<span class=\"ltx_text ltx_font_typewriter\">nllb-200-3.3B</span>; <cite class=\"ltx_cite ltx_citemacro_citep\">Team et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib27\" title=\"\">2022</a></cite>) translating the transcript, along with SeamlessM4T\n(<span class=\"ltx_text ltx_font_typewriter\">seamless-m4t-v2-large</span>; <cite class=\"ltx_cite ltx_citemacro_citep\">Communication et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib5\" title=\"\">2023</a></cite>) - an end-to-end, multi-modal translation model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Similarly, the models with Gemma 2 9B still have the best evaluation score among the four fine-tuned LLMs. In combination with the Whisper encoder, it even surpassed the performance of the cascaded system of Whisper + NLLB in most of the test sets and metrics.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "gemma",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we leveraged pre-trained speech encoders and LLMs and connected them to become an end-to-end architecture for speech translation. The overall result is expected: for the English-to-German direction, even though our models performed better than the end-to-end SeamlessM4T model all of the time, there was still a gap compared to the performance of the cascaded Whisper + NLLB pipeline. It suggests that cascaded models are still the state-of-the-art approach in the speech translation task; this is also confirmed according to <cite class=\"ltx_cite ltx_citemacro_citet\">Ahmad et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib1\" title=\"\">2024</a>)</cite>, in which all systems submitted to the Offline Track of IWSLT&#8217;24 were cascaded systems.</p>\n\n",
                "matched_terms": [
                    "model",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, considering the size of the LLMs, our models were inferior regarding inference speed, compared to the two baselines. Our models also managed to surpass the performance of the cascaded system in the translation task; however, the differences were not too substantial. In addition, despite being a much smaller model, Whisper alone still excels at speech recognition. This raises a question: <span class=\"ltx_text ltx_font_italic\">\"Can end-to-end speech translation systems be smaller in size, while still keeping the robustness in translation, especially for the rising need to be used in mobile devices?\"</span></p>\n\n",
                "matched_terms": [
                    "model",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All models were fine-tuned using 4-bit QLoRA <cite class=\"ltx_cite ltx_citemacro_citep\">(Dettmers et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib6\" title=\"\">2023</a>)</cite> adapters in <span class=\"ltx_text ltx_font_typewriter\">bfloat16</span> precision, with the following LoRA parameters: rank of <math alttext=\"r=8\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">r=8</annotation></semantics></math>, alpha of <math alttext=\"\\alpha=8\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=8</annotation></semantics></math>. For the models with HuBERT as the encoder, because of the manual CTC collapsing procedure, we could only process one example at a time, hence the batch size was set to <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m3\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>; while for those with Whisper, the batch size was set to <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m4\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>. Other training hyperparameters included the learning rate of <math alttext=\"1e-4\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m5\" intent=\":literal\"><semantics><mrow><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>&#8722;</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">1e-4</annotation></semantics></math> with <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m6\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math> warmup steps, and an AdamW optimizer <cite class=\"ltx_cite ltx_citemacro_cite\">Loshchilov and Hutter (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib17\" title=\"\">2019</a>)</cite> with a cosine scheduler <cite class=\"ltx_cite ltx_citemacro_cite\">Loshchilov and Hutter (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib16\" title=\"\">2017</a>)</cite>. All HuBERT-encoder models were trained for <math alttext=\"500,000\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m7\" intent=\":literal\"><semantics><mrow><mn>500</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding=\"application/x-tex\">500,000</annotation></semantics></math> steps, while Whisper-encoder models were trained for <math alttext=\"100,000\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m8\" intent=\":literal\"><semantics><mrow><mn>100</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding=\"application/x-tex\">100,000</annotation></semantics></math> steps.</p>\n\n",
                "matched_terms": [
                    "hubert",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During inference, for each audio data, the LLMs were prompted using the following format: &#8220;<span class=\"ltx_text ltx_font_typewriter\">&lt;bos&gt; &lt;&gt;audio&lt;&gt; {audio features} &lt;&gt;transcript&lt;&gt;</span>&#8221;, then generated the transcript and the corresponding translated text in an auto-regressive manner.\nWe performed inference using the beam search algorithm, with a beam size of 2 for all models. All evaluation results,\nare described in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#S3.SS2\" title=\"3.2 ASR Results &#8227; 3 Evaluation &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">Sections</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.2</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#S3.SS3\" title=\"3.3 Offline ST Results &#8227; 3 Evaluation &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results"
                ]
            }
        ]
    },
    "S3.T3": {
        "source_file": "End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs",
        "caption": "Table 3: Offline ST en2de BLEU results, with both docAsWhole and mwerSegmenter scores, respectively",
        "body": "Model\nMuST-C\nIWSLT\n\n\ntst-COMMON v2\ntst-COMMON v3\ntst2021\ntst2022\n\n\n\n\nCascaded Whisper + NLLB\n\n39.8439.84 / 31.0631.06\n\n\n40.3040.30 / 31.6031.60\n\n\n43.84\\mathbf{43.84} / -\n\n41.86\\mathbf{41.86} / 30.48\\mathbf{30.48}\n\n\n\nSeamlessM4T\n\n32.6232.62 / 22.9822.98\n\n\n33.3633.36 / 23.5923.59\n\n\n35.9735.97 / -\n\n34.0834.08 / 22.6822.68\n\n\n\nHuBERT + Gemma 2 9B\n\n37.9837.98 / 28.1528.15\n\n\n37.5037.50 / 27.5927.59\n\n\n37.5937.59 / -\n\n37.0437.04 / 25.8625.86\n\n\n\nHuBERT + Gemma 7B\n\n36.2036.20 / 25.8925.89\n\n\n36.2436.24 / 26.0226.02\n\n\n33.0033.00 / -\n\n34.2734.27 / 22.9822.98\n\n\n\nHuBERT + Llama 2 7B\n\n36.5236.52 / 26.4226.42\n\n\n35.9335.93 / 25.8925.89\n\n\n35.6635.66 / -\n\n35.1335.13 / 23.8823.88\n\n\n\nHuBERT + Mistral 7B v0.1\n\n36.9136.91 / 26.9026.90\n\n\n36.9436.94 / 27.0527.05\n\n\n36.2936.29 / -\n\n36.0936.09 / 25.0725.07\n\n\n\nWhisper enc. + Gemma 2 9B\n\n41.33\\mathbf{41.33} / 31.98\\mathbf{31.98}\n\n\n41.16\\mathbf{41.16} / 31.72\\mathbf{31.72}\n\n\n40.7640.76 / -\n\n39.6439.64 / 29.1829.18\n\n\n\nWhisper enc. + Gemma 7B\n\n38.6238.62 / 28.5528.55\n\n\n38.8138.81 / 28.8128.81\n\n\n37.0237.02 / -\n\n37.5837.58 / 26.2926.29\n\n\n\nWhisper enc. + Llama 2 7B\n\n38.9538.95 / 29.1729.17\n\n\n38.7938.79 / 28.9428.94\n\n\n37.1837.18 / -\n\n36.9436.94 / 26.1826.18\n\n\n\nWhisper enc. + Mistral 7B v0.1\n\n39.5239.52 / 30.0330.03\n\n\n39.2839.28 / 29.5929.59\n\n\n38.6038.60 / -\n\n37.5537.55 / 26.6426.64",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\" rowspan=\"2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">MuST-C</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">IWSLT</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">tst-COMMON v2</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">tst-COMMON v3</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">tst2021</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">tst2022</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Cascaded Whisper + NLLB</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"39.84\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m1\" intent=\":literal\"><semantics><mn>39.84</mn><annotation encoding=\"application/x-tex\">39.84</annotation></semantics></math> / <math alttext=\"31.06\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m2\" intent=\":literal\"><semantics><mn>31.06</mn><annotation encoding=\"application/x-tex\">31.06</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"40.30\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m3\" intent=\":literal\"><semantics><mn>40.30</mn><annotation encoding=\"application/x-tex\">40.30</annotation></semantics></math> / <math alttext=\"31.60\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m4\" intent=\":literal\"><semantics><mn>31.60</mn><annotation encoding=\"application/x-tex\">31.60</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"\\mathbf{43.84}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m5\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">43.84</mn><annotation encoding=\"application/x-tex\">\\mathbf{43.84}</annotation></semantics></math> / -</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"\\mathbf{41.86}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m6\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">41.86</mn><annotation encoding=\"application/x-tex\">\\mathbf{41.86}</annotation></semantics></math> / <math alttext=\"\\mathbf{30.48}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m7\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">30.48</mn><annotation encoding=\"application/x-tex\">\\mathbf{30.48}</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">SeamlessM4T</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"32.62\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m8\" intent=\":literal\"><semantics><mn>32.62</mn><annotation encoding=\"application/x-tex\">32.62</annotation></semantics></math> / <math alttext=\"22.98\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m9\" intent=\":literal\"><semantics><mn>22.98</mn><annotation encoding=\"application/x-tex\">22.98</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"33.36\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m10\" intent=\":literal\"><semantics><mn>33.36</mn><annotation encoding=\"application/x-tex\">33.36</annotation></semantics></math> / <math alttext=\"23.59\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m11\" intent=\":literal\"><semantics><mn>23.59</mn><annotation encoding=\"application/x-tex\">23.59</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"35.97\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m12\" intent=\":literal\"><semantics><mn>35.97</mn><annotation encoding=\"application/x-tex\">35.97</annotation></semantics></math> / -</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"34.08\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m13\" intent=\":literal\"><semantics><mn>34.08</mn><annotation encoding=\"application/x-tex\">34.08</annotation></semantics></math> / <math alttext=\"22.68\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m14\" intent=\":literal\"><semantics><mn>22.68</mn><annotation encoding=\"application/x-tex\">22.68</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">HuBERT + Gemma 2 9B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"37.98\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m15\" intent=\":literal\"><semantics><mn>37.98</mn><annotation encoding=\"application/x-tex\">37.98</annotation></semantics></math> / <math alttext=\"28.15\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m16\" intent=\":literal\"><semantics><mn>28.15</mn><annotation encoding=\"application/x-tex\">28.15</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"37.50\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m17\" intent=\":literal\"><semantics><mn>37.50</mn><annotation encoding=\"application/x-tex\">37.50</annotation></semantics></math> / <math alttext=\"27.59\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m18\" intent=\":literal\"><semantics><mn>27.59</mn><annotation encoding=\"application/x-tex\">27.59</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"37.59\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m19\" intent=\":literal\"><semantics><mn>37.59</mn><annotation encoding=\"application/x-tex\">37.59</annotation></semantics></math> / -</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"37.04\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m20\" intent=\":literal\"><semantics><mn>37.04</mn><annotation encoding=\"application/x-tex\">37.04</annotation></semantics></math> / <math alttext=\"25.86\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m21\" intent=\":literal\"><semantics><mn>25.86</mn><annotation encoding=\"application/x-tex\">25.86</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">HuBERT + Gemma 7B</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"36.20\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m22\" intent=\":literal\"><semantics><mn>36.20</mn><annotation encoding=\"application/x-tex\">36.20</annotation></semantics></math> / <math alttext=\"25.89\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m23\" intent=\":literal\"><semantics><mn>25.89</mn><annotation encoding=\"application/x-tex\">25.89</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"36.24\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m24\" intent=\":literal\"><semantics><mn>36.24</mn><annotation encoding=\"application/x-tex\">36.24</annotation></semantics></math> / <math alttext=\"26.02\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m25\" intent=\":literal\"><semantics><mn>26.02</mn><annotation encoding=\"application/x-tex\">26.02</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"33.00\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m26\" intent=\":literal\"><semantics><mn>33.00</mn><annotation encoding=\"application/x-tex\">33.00</annotation></semantics></math> / -</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"34.27\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m27\" intent=\":literal\"><semantics><mn>34.27</mn><annotation encoding=\"application/x-tex\">34.27</annotation></semantics></math> / <math alttext=\"22.98\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m28\" intent=\":literal\"><semantics><mn>22.98</mn><annotation encoding=\"application/x-tex\">22.98</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">HuBERT + Llama 2 7B</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"36.52\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m29\" intent=\":literal\"><semantics><mn>36.52</mn><annotation encoding=\"application/x-tex\">36.52</annotation></semantics></math> / <math alttext=\"26.42\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m30\" intent=\":literal\"><semantics><mn>26.42</mn><annotation encoding=\"application/x-tex\">26.42</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"35.93\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m31\" intent=\":literal\"><semantics><mn>35.93</mn><annotation encoding=\"application/x-tex\">35.93</annotation></semantics></math> / <math alttext=\"25.89\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m32\" intent=\":literal\"><semantics><mn>25.89</mn><annotation encoding=\"application/x-tex\">25.89</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"35.66\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m33\" intent=\":literal\"><semantics><mn>35.66</mn><annotation encoding=\"application/x-tex\">35.66</annotation></semantics></math> / -</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"35.13\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m34\" intent=\":literal\"><semantics><mn>35.13</mn><annotation encoding=\"application/x-tex\">35.13</annotation></semantics></math> / <math alttext=\"23.88\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m35\" intent=\":literal\"><semantics><mn>23.88</mn><annotation encoding=\"application/x-tex\">23.88</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">HuBERT + Mistral 7B v0.1</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"36.91\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m36\" intent=\":literal\"><semantics><mn>36.91</mn><annotation encoding=\"application/x-tex\">36.91</annotation></semantics></math> / <math alttext=\"26.90\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m37\" intent=\":literal\"><semantics><mn>26.90</mn><annotation encoding=\"application/x-tex\">26.90</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"36.94\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m38\" intent=\":literal\"><semantics><mn>36.94</mn><annotation encoding=\"application/x-tex\">36.94</annotation></semantics></math> / <math alttext=\"27.05\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m39\" intent=\":literal\"><semantics><mn>27.05</mn><annotation encoding=\"application/x-tex\">27.05</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"36.29\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m40\" intent=\":literal\"><semantics><mn>36.29</mn><annotation encoding=\"application/x-tex\">36.29</annotation></semantics></math> / -</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"36.09\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m41\" intent=\":literal\"><semantics><mn>36.09</mn><annotation encoding=\"application/x-tex\">36.09</annotation></semantics></math> / <math alttext=\"25.07\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m42\" intent=\":literal\"><semantics><mn>25.07</mn><annotation encoding=\"application/x-tex\">25.07</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Whisper enc. + Gemma 2 9B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"\\mathbf{41.33}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m43\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">41.33</mn><annotation encoding=\"application/x-tex\">\\mathbf{41.33}</annotation></semantics></math> / <math alttext=\"\\mathbf{31.98}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m44\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">31.98</mn><annotation encoding=\"application/x-tex\">\\mathbf{31.98}</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"\\mathbf{41.16}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m45\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">41.16</mn><annotation encoding=\"application/x-tex\">\\mathbf{41.16}</annotation></semantics></math> / <math alttext=\"\\mathbf{31.72}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m46\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">31.72</mn><annotation encoding=\"application/x-tex\">\\mathbf{31.72}</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"40.76\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m47\" intent=\":literal\"><semantics><mn>40.76</mn><annotation encoding=\"application/x-tex\">40.76</annotation></semantics></math> / -</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"39.64\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m48\" intent=\":literal\"><semantics><mn>39.64</mn><annotation encoding=\"application/x-tex\">39.64</annotation></semantics></math> / <math alttext=\"29.18\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m49\" intent=\":literal\"><semantics><mn>29.18</mn><annotation encoding=\"application/x-tex\">29.18</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Whisper enc. + Gemma 7B</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"38.62\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m50\" intent=\":literal\"><semantics><mn>38.62</mn><annotation encoding=\"application/x-tex\">38.62</annotation></semantics></math> / <math alttext=\"28.55\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m51\" intent=\":literal\"><semantics><mn>28.55</mn><annotation encoding=\"application/x-tex\">28.55</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"38.81\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m52\" intent=\":literal\"><semantics><mn>38.81</mn><annotation encoding=\"application/x-tex\">38.81</annotation></semantics></math> / <math alttext=\"28.81\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m53\" intent=\":literal\"><semantics><mn>28.81</mn><annotation encoding=\"application/x-tex\">28.81</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"37.02\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m54\" intent=\":literal\"><semantics><mn>37.02</mn><annotation encoding=\"application/x-tex\">37.02</annotation></semantics></math> / -</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"37.58\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m55\" intent=\":literal\"><semantics><mn>37.58</mn><annotation encoding=\"application/x-tex\">37.58</annotation></semantics></math> / <math alttext=\"26.29\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m56\" intent=\":literal\"><semantics><mn>26.29</mn><annotation encoding=\"application/x-tex\">26.29</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Whisper enc. + Llama 2 7B</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"38.95\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m57\" intent=\":literal\"><semantics><mn>38.95</mn><annotation encoding=\"application/x-tex\">38.95</annotation></semantics></math> / <math alttext=\"29.17\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m58\" intent=\":literal\"><semantics><mn>29.17</mn><annotation encoding=\"application/x-tex\">29.17</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"38.79\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m59\" intent=\":literal\"><semantics><mn>38.79</mn><annotation encoding=\"application/x-tex\">38.79</annotation></semantics></math> / <math alttext=\"28.94\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m60\" intent=\":literal\"><semantics><mn>28.94</mn><annotation encoding=\"application/x-tex\">28.94</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"37.18\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m61\" intent=\":literal\"><semantics><mn>37.18</mn><annotation encoding=\"application/x-tex\">37.18</annotation></semantics></math> / -</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"36.94\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m62\" intent=\":literal\"><semantics><mn>36.94</mn><annotation encoding=\"application/x-tex\">36.94</annotation></semantics></math> / <math alttext=\"26.18\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m63\" intent=\":literal\"><semantics><mn>26.18</mn><annotation encoding=\"application/x-tex\">26.18</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Whisper enc. + Mistral 7B v0.1</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"39.52\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m64\" intent=\":literal\"><semantics><mn>39.52</mn><annotation encoding=\"application/x-tex\">39.52</annotation></semantics></math> / <math alttext=\"30.03\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m65\" intent=\":literal\"><semantics><mn>30.03</mn><annotation encoding=\"application/x-tex\">30.03</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"39.28\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m66\" intent=\":literal\"><semantics><mn>39.28</mn><annotation encoding=\"application/x-tex\">39.28</annotation></semantics></math> / <math alttext=\"29.59\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m67\" intent=\":literal\"><semantics><mn>29.59</mn><annotation encoding=\"application/x-tex\">29.59</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"38.60\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m68\" intent=\":literal\"><semantics><mn>38.60</mn><annotation encoding=\"application/x-tex\">38.60</annotation></semantics></math> / -</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"37.55\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m69\" intent=\":literal\"><semantics><mn>37.55</mn><annotation encoding=\"application/x-tex\">37.55</annotation></semantics></math> / <math alttext=\"26.64\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m70\" intent=\":literal\"><semantics><mn>26.64</mn><annotation encoding=\"application/x-tex\">26.64</annotation></semantics></math>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "both",
            "respectively",
            "3198mathbf3198",
            "en2de",
            "seamlessm4t",
            "tstcommon",
            "tst2021",
            "v01",
            "4133mathbf4133",
            "docaswhole",
            "3048mathbf3048",
            "mwersegmenter",
            "llama",
            "results",
            "cascaded",
            "scores",
            "4384mathbf4384",
            "nllb",
            "enc",
            "mistral",
            "mustc",
            "gemma",
            "whisper",
            "4186mathbf4186",
            "hubert",
            "3172mathbf3172",
            "iwslt",
            "offline",
            "tst2022",
            "bleu",
            "4116mathbf4116",
            "model"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#S3.T3\" title=\"In 3.3 Offline ST Results &#8227; 3 Evaluation &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#S3.T4\" title=\"Table 4 &#8227; 3.3 Offline ST Results &#8227; 3 Evaluation &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> report the <span class=\"ltx_text ltx_font_smallcaps\">BLEU</span> and <span class=\"ltx_text ltx_font_smallcaps\">COMET</span>-family scores, respectively, on the four test sets. For evaluating with <span class=\"ltx_text ltx_font_smallcaps\">BLEU</span>, we included both <span class=\"ltx_text ltx_font_typewriter\">docAsWhole</span> score, which concatenated all reference segments and candidate complete segments as two documents, and <span class=\"ltx_text ltx_font_typewriter\">mwerSegmenter</span> score, which resegments complete candidate segments according to reference segments to minimize WER. Similar to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#S3.SS2\" title=\"3.2 ASR Results &#8227; 3 Evaluation &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, <span class=\"ltx_text ltx_font_typewriter\">mwerSegmenter</span> scores for IWSLT&#8217;21 test set could not be obtained, hence we did not include them.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Speech Translation (ST) is a machine translation task that involves converting speech signals from one language to the corresponding text in another language;\nthis task has two different approaches, namely the traditional <em class=\"ltx_emph ltx_font_italic\">cascade</em> and the more recent <em class=\"ltx_emph ltx_font_italic\">end-to-end</em>. This paper explores a combined end-to-end architecture of pre-trained speech encoders and Large Language Models (LLMs) for performing both Automatic Speech Recognition (ASR) and ST simultaneously. Experiments with the English-to-German language pair show that our best model not only can achieve better translation results than SeamlessM4T <cite class=\"ltx_cite ltx_citemacro_citep\">(Communication et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib5\" title=\"\">2023</a>)</cite>, a large foundational end-to-end, multi-modal translation model, but can also match the performance of a cascaded system with Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib24\" title=\"\">2022</a>)</cite> and NLLB <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib27\" title=\"\">2022</a>)</cite>, with up to a score gain of 8% in <math alttext=\"\\text{COMET}^{\\text{DA}}_{22}\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><msubsup><mtext>COMET</mtext><mn>22</mn><mtext>DA</mtext></msubsup><annotation encoding=\"application/x-tex\">\\text{COMET}^{\\text{DA}}_{22}</annotation></semantics></math> metric.</p>\n\n",
                "matched_terms": [
                    "both",
                    "whisper",
                    "nllb",
                    "results",
                    "cascaded",
                    "seamlessm4t",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by recent contributions in speech representation learning and LLMs, we aim to investigate an end-to-end architecture that simultaneously performs both ASR and ST. This architecture combines the high-quality audio representation from the pre-trained acoustic models with the excellent performance of LLMs to serve as an end-to-end speech translation system,\nwhile still having the ability to transcribe from the audio signal. Our proposed model, after being fine-tuned with the Quantized Low-Rank Adaptation (QLoRA; <cite class=\"ltx_cite ltx_citemacro_citep\">Dettmers et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib6\" title=\"\">2023</a></cite>) technique, achieves a robust translation performance, comparable to a cascaded system, which is still a state-of-the-art approach for this task.</p>\n\n",
                "matched_terms": [
                    "both",
                    "model",
                    "cascaded"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#S3\" title=\"3 Evaluation &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a> provides the ASR and ST evaluation results of the model in different public test sets, and compares them to some baselines from out-of-the-box models.</p>\n\n",
                "matched_terms": [
                    "results",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The overall architecture is illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#S2.F1\" title=\"In 2.1 Architecture &#8227; 2 Methods and Dataset &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>. For each training sample, given the speech signal, its corresponding transcript, and the translated text, the speech hidden features are obtained using a speech encoder, including HuBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib10\" title=\"\">2021</a>)</cite> and Whisper encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib24\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "hubert",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopted HuBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib10\" title=\"\">2021</a>)</cite> and Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib24\" title=\"\">2022</a>)</cite> as the speech encoders, utilizing their capability of extracting high-quality representation from audio data. We used the <span class=\"ltx_text ltx_font_typewriter\">hubert-large-ls960-ft</span>\nvariation, which was trained on 60,000 hours of data from the LibriLight <cite class=\"ltx_cite ltx_citemacro_citep\">(Kahn et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib11\" title=\"\">2020</a>)</cite> corpus, then fine-tuned on 960 hours of data from the LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib20\" title=\"\">2015a</a>)</cite> corpus. For Whisper-based models, we only used the encoder part of the pre-trained <span class=\"ltx_text ltx_font_typewriter\">whisper-large-v3-turbo</span>\nto extract the audio hidden features.</p>\n\n",
                "matched_terms": [
                    "hubert",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We experimented with four different pre-trained LLMs available on HuggingFace, namely <em class=\"ltx_emph ltx_font_italic\">Gemma 7B</em> (<span class=\"ltx_text ltx_font_typewriter\">gemma-7b</span>),\n<em class=\"ltx_emph ltx_font_italic\">Gemma 2 9B</em> (<span class=\"ltx_text ltx_font_typewriter\">gemma-2-9b</span>),\n<em class=\"ltx_emph ltx_font_italic\">Llama 2 7B</em> (<span class=\"ltx_text ltx_font_typewriter\">Llama-2-7b-hf</span>),\nand <em class=\"ltx_emph ltx_font_italic\">Mistral 7B v0.1</em> (<span class=\"ltx_text ltx_font_typewriter\">Mistral-7B-v0.1</span>).\nDetails about each variation are described in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#S2.T1\" title=\"In 2.5 LLMs &#8227; 2 Methods and Dataset &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "gemma",
                    "mistral",
                    "v01",
                    "llama"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, MuST-C also provides two public test sets, both named <span class=\"ltx_text ltx_font_typewriter\">tst-COMMON</span> in version 2.0 and 3.0. We also used the test sets from the Offline Track of IWSLT&#8217;21\nand &#8217;22.\nIn addition, to evaluate ASR performance, we used two test sets from the LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib21\" title=\"\">2015b</a>)</cite> dataset, namely <span class=\"ltx_text ltx_font_typewriter\">test-clean</span> and <span class=\"ltx_text ltx_font_typewriter\">test-other</span>, both of which are the standard datasets for this task. As all models can perform both ASR and ST simultaneously, evaluation results for both tasks are described in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#S3.SS2\" title=\"3.2 ASR Results &#8227; 3 Evaluation &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">Sections</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.2</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#S3.SS3\" title=\"3.3 Offline ST Results &#8227; 3 Evaluation &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>, respectively.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "both",
                    "respectively",
                    "offline",
                    "results",
                    "tstcommon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the Offline Speech Translation task, we evaluated all models using standard metrics, namely <span class=\"ltx_text ltx_font_smallcaps\">BLEU</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Papineni et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib22\" title=\"\">2002</a>)</cite>, <math alttext=\"\\text{COMET}^{\\text{DA}}_{22}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msubsup><mtext>COMET</mtext><mn>22</mn><mtext>DA</mtext></msubsup><annotation encoding=\"application/x-tex\">\\text{COMET}^{\\text{DA}}_{22}</annotation></semantics></math> <cite class=\"ltx_cite ltx_citemacro_citep\">(Rei et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib25\" title=\"\">2022a</a>)</cite>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/Unbabel/wmt22-comet-da\" title=\"\">https://huggingface.co/Unbabel/wmt22-comet-da</a></span></span></span> and <math alttext=\"\\text{COMET}^{\\text{KIWI-DA}}_{22}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msubsup><mtext>COMET</mtext><mn>22</mn><mtext>KIWI-DA</mtext></msubsup><annotation encoding=\"application/x-tex\">\\text{COMET}^{\\text{KIWI-DA}}_{22}</annotation></semantics></math> <cite class=\"ltx_cite ltx_citemacro_citep\">(Rei et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib26\" title=\"\">2022b</a>)</cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/Unbabel/wmt22-cometkiwi-da\" title=\"\">https://huggingface.co/Unbabel/wmt22-cometkiwi-da</a></span></span></span> For the Automatic Speech Recognition Task, we used <span class=\"ltx_text ltx_font_smallcaps\">WER</span>, the standard metric for speech recognition.</p>\n\n",
                "matched_terms": [
                    "bleu",
                    "offline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the evaluation purpose, we used the <span class=\"ltx_text ltx_font_typewriter\">SLTev</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Ansari et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib2\" title=\"\">2021</a>)</cite> library,\nbecause it supports both MT and ASR evaluation in one package, using <span class=\"ltx_text ltx_font_typewriter\">sacreBLEU</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Post, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib23\" title=\"\">2018</a>)</cite> to calculate <span class=\"ltx_text ltx_font_smallcaps\">BLEU</span> score. However, since <span class=\"ltx_text ltx_font_typewriter\">SLTev</span> does not report any <span class=\"ltx_text ltx_font_smallcaps\">COMET</span>-family metrics, we had to change the structure of the sentence with <span class=\"ltx_text ltx_font_typewriter\">mwerSegmenter</span>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www-i6.informatik.rwth-aachen.de/web/Software/mwerSegmenter.tar.gz\" title=\"\">https://www-i6.informatik.rwth-aachen.de/web/Software/mwerSegmenter.tar.gz</a></span></span></span>\nto automatically resegment the models&#8217; output according to the reference,\nbefore evaluating with the <span class=\"ltx_text ltx_font_typewriter\">unbabel-comet</span>\npackage. The evaluation was done using <span class=\"ltx_text ltx_font_typewriter\">python-3.11.5</span>, <span class=\"ltx_text ltx_font_typewriter\">SLTev-1.2.3</span>, and <span class=\"ltx_text ltx_font_typewriter\">unbabel-comet-2.2.2</span>.</p>\n\n",
                "matched_terms": [
                    "both",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compared our architecture with two out-of-the-box baselines: a cascaded pipeline of Whisper\n(<span class=\"ltx_text ltx_font_typewriter\">whisper-large-v3-turbo</span>; <cite class=\"ltx_cite ltx_citemacro_citep\">Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib24\" title=\"\">2022</a></cite>) producing the transcript and NLLB\n(<span class=\"ltx_text ltx_font_typewriter\">nllb-200-3.3B</span>; <cite class=\"ltx_cite ltx_citemacro_citep\">Team et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib27\" title=\"\">2022</a></cite>) translating the transcript, along with SeamlessM4T\n(<span class=\"ltx_text ltx_font_typewriter\">seamless-m4t-v2-large</span>; <cite class=\"ltx_cite ltx_citemacro_citep\">Communication et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib5\" title=\"\">2023</a></cite>) - an end-to-end, multi-modal translation model.</p>\n\n",
                "matched_terms": [
                    "whisper",
                    "nllb",
                    "cascaded",
                    "seamlessm4t",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#S3.T2\" title=\"In 3.2 ASR Results &#8227; 3 Evaluation &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a> details the ASR evaluation results against the four test sets. We reported the <span class=\"ltx_text ltx_font_smallcaps\">WER</span> score after applying the &#8220;LPW&#8221; pre-processing strategy available in <span class=\"ltx_text ltx_font_typewriter\">SLTev</span>, which first lowercased every character, removed all punctuation, then used the built-in <span class=\"ltx_text ltx_font_typewriter\">mwerSegmenter</span> tool to resegment the output transcripts. Due to some bugs when processing the IWSLT&#8217;21 test set (tst2021), <span class=\"ltx_text ltx_font_typewriter\">mwerSegmenter</span> failed to run during evaluation, hence we could not obtain the results. It can be seen that models with Gemma 2 9B as the decoder have the best result among the four LLMs, albeit still lagging behind the performance of Whisper.</p>\n\n",
                "matched_terms": [
                    "gemma",
                    "whisper",
                    "mwersegmenter",
                    "results",
                    "tst2021"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Similarly, the models with Gemma 2 9B still have the best evaluation score among the four fine-tuned LLMs. In combination with the Whisper encoder, it even surpassed the performance of the cascaded system of Whisper + NLLB in most of the test sets and metrics.</p>\n\n",
                "matched_terms": [
                    "gemma",
                    "cascaded",
                    "nllb",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we leveraged pre-trained speech encoders and LLMs and connected them to become an end-to-end architecture for speech translation. The overall result is expected: for the English-to-German direction, even though our models performed better than the end-to-end SeamlessM4T model all of the time, there was still a gap compared to the performance of the cascaded Whisper + NLLB pipeline. It suggests that cascaded models are still the state-of-the-art approach in the speech translation task; this is also confirmed according to <cite class=\"ltx_cite ltx_citemacro_citet\">Ahmad et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib1\" title=\"\">2024</a>)</cite>, in which all systems submitted to the Offline Track of IWSLT&#8217;24 were cascaded systems.</p>\n\n",
                "matched_terms": [
                    "whisper",
                    "nllb",
                    "offline",
                    "cascaded",
                    "seamlessm4t",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, considering the size of the LLMs, our models were inferior regarding inference speed, compared to the two baselines. Our models also managed to surpass the performance of the cascaded system in the translation task; however, the differences were not too substantial. In addition, despite being a much smaller model, Whisper alone still excels at speech recognition. This raises a question: <span class=\"ltx_text ltx_font_italic\">\"Can end-to-end speech translation systems be smaller in size, while still keeping the robustness in translation, especially for the rising need to be used in mobile devices?\"</span></p>\n\n",
                "matched_terms": [
                    "cascaded",
                    "model",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All models were fine-tuned using 4-bit QLoRA <cite class=\"ltx_cite ltx_citemacro_citep\">(Dettmers et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib6\" title=\"\">2023</a>)</cite> adapters in <span class=\"ltx_text ltx_font_typewriter\">bfloat16</span> precision, with the following LoRA parameters: rank of <math alttext=\"r=8\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">r=8</annotation></semantics></math>, alpha of <math alttext=\"\\alpha=8\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=8</annotation></semantics></math>. For the models with HuBERT as the encoder, because of the manual CTC collapsing procedure, we could only process one example at a time, hence the batch size was set to <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m3\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>; while for those with Whisper, the batch size was set to <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m4\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>. Other training hyperparameters included the learning rate of <math alttext=\"1e-4\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m5\" intent=\":literal\"><semantics><mrow><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>&#8722;</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">1e-4</annotation></semantics></math> with <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m6\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math> warmup steps, and an AdamW optimizer <cite class=\"ltx_cite ltx_citemacro_cite\">Loshchilov and Hutter (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib17\" title=\"\">2019</a>)</cite> with a cosine scheduler <cite class=\"ltx_cite ltx_citemacro_cite\">Loshchilov and Hutter (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib16\" title=\"\">2017</a>)</cite>. All HuBERT-encoder models were trained for <math alttext=\"500,000\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m7\" intent=\":literal\"><semantics><mrow><mn>500</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding=\"application/x-tex\">500,000</annotation></semantics></math> steps, while Whisper-encoder models were trained for <math alttext=\"100,000\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m8\" intent=\":literal\"><semantics><mrow><mn>100</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding=\"application/x-tex\">100,000</annotation></semantics></math> steps.</p>\n\n",
                "matched_terms": [
                    "hubert",
                    "whisper"
                ]
            }
        ]
    },
    "S3.T4": {
        "source_file": "End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs",
        "caption": "Table 4: Offline ST en2de COMET22DA\\text{COMET}^{\\text{DA}}_{22} and COMET22KIWI-DA\\text{COMET}^{\\text{KIWI-DA}}_{22} results, respectively",
        "body": "Model\nMuST-C\nIWSLT\n\n\n\n\ntst-COMMON v2\n\n\n\n\ntst-COMMON v3\n\n\ntst2021\ntst2022\n\n\n\n\n\n\nCascaded Whisper + NLLB\n\n\n\n\n83.0083.00 / 79.9879.98\n\n\n\n\n82.4982.49 / 80.5380.53\n\n\n\n64.4764.47 / 58.2358.23\n\n\n65.3265.32 / 59.2759.27\n\n\n\n\n\nSeamlessM4T\n\n\n\n\n76.7276.72 / 73.4973.49\n\n\n\n\n76.4276.42 / 74.0374.03\n\n\n\n59.6359.63 / 53.9253.92\n\n\n60.3460.34 / 54.9354.93\n\n\n\n\n\nHuBERT + Gemma 2 9B\n\n\n\n\n80.9880.98 / 77.4277.42\n\n\n\n\n80.1780.17 / 77.4577.45\n\n\n\n67.6367.63 / 60.3460.34\n\n\n67.1167.11 / 59.6859.68\n\n\n\n\n\nHuBERT + Gemma 7B\n\n\n\n\n79.6479.64 / 75.5275.52\n\n\n\n\n78.8578.85 / 75.5375.53\n\n\n\n65.2265.22 / 57.5157.51\n\n\n64.7764.77 / 57.2357.23\n\n\n\n\n\nHuBERT + Llama 2 7B\n\n\n\n\n79.8879.88 / 76.3076.30\n\n\n\n\n79.0879.08 / 76.3276.32\n\n\n\n66.5466.54 / 59.2759.27\n\n\n65.7065.70 / 58.7058.70\n\n\n\n\n\nHuBERT + Mistral 7B v0.1\n\n\n\n\n80.1280.12 / 76.9276.92\n\n\n\n\n79.4579.45 / 76.9276.92\n\n\n\n66.9766.97 / 59.7359.73\n\n\n66.6266.62 / 59.8559.85\n\n\n\n\n\nWhisper enc. + Gemma 2 9B\n\n\n\n\n84.22\\mathbf{84.22} / 81.15\\mathbf{81.15}\n\n\n\n\n83.65\\mathbf{83.65} / 81.29\\mathbf{81.29}\n\n\n\n70.51\\mathbf{70.51} / 62.80\\mathbf{62.80}\n\n\n70.34\\mathbf{70.34} / 63.27\\mathbf{63.27}\n\n\n\n\n\nWhisper enc. + Gemma 7B\n\n\n\n\n82.5582.55 / 79.6979.69\n\n\n\n\n82.1582.15 / 79.8879.88\n\n\n\n67.6367.63 / 60.0660.06\n\n\n68.2468.24 / 60.9160.91\n\n\n\n\n\nWhisper enc. + Llama 2 7B\n\n\n\n\n82.8482.84 / 80.0980.09\n\n\n\n\n82.1482.14 / 80.0580.05\n\n\n\n68.8268.82 / 61.8261.82\n\n\n68.6468.64 / 61.9161.91\n\n\n\n\n\nWhisper enc. + Mistral 7B v0.1\n\n\n\n\n83.1383.13 / 80.2480.24\n\n\n\n\n82.4382.43 / 80.3780.37\n\n\n\n69.7369.73 / 62.4062.40\n\n\n68.8668.86 / 61.7961.79",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_t\" rowspan=\"2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">MuST-C</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">IWSLT</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:65.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\">tst-COMMON v2</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:65.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_align_center\">tst-COMMON v3</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">tst2021</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">tst2022</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:130.1pt;\">Cascaded Whisper + NLLB</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:65.0pt;\"><math alttext=\"83.00\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m1\" intent=\":literal\"><semantics><mn>83.00</mn><annotation encoding=\"application/x-tex\">83.00</annotation></semantics></math> / <math alttext=\"79.98\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m2\" intent=\":literal\"><semantics><mn>79.98</mn><annotation encoding=\"application/x-tex\">79.98</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:65.0pt;\"><math alttext=\"82.49\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m3\" intent=\":literal\"><semantics><mn>82.49</mn><annotation encoding=\"application/x-tex\">82.49</annotation></semantics></math> / <math alttext=\"80.53\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m4\" intent=\":literal\"><semantics><mn>80.53</mn><annotation encoding=\"application/x-tex\">80.53</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"64.47\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m5\" intent=\":literal\"><semantics><mn>64.47</mn><annotation encoding=\"application/x-tex\">64.47</annotation></semantics></math> / <math alttext=\"58.23\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m6\" intent=\":literal\"><semantics><mn>58.23</mn><annotation encoding=\"application/x-tex\">58.23</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"65.32\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m7\" intent=\":literal\"><semantics><mn>65.32</mn><annotation encoding=\"application/x-tex\">65.32</annotation></semantics></math> / <math alttext=\"59.27\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m8\" intent=\":literal\"><semantics><mn>59.27</mn><annotation encoding=\"application/x-tex\">59.27</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:130.1pt;\">SeamlessM4T</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:65.0pt;\"><math alttext=\"76.72\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m9\" intent=\":literal\"><semantics><mn>76.72</mn><annotation encoding=\"application/x-tex\">76.72</annotation></semantics></math> / <math alttext=\"73.49\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m10\" intent=\":literal\"><semantics><mn>73.49</mn><annotation encoding=\"application/x-tex\">73.49</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:65.0pt;\"><math alttext=\"76.42\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m11\" intent=\":literal\"><semantics><mn>76.42</mn><annotation encoding=\"application/x-tex\">76.42</annotation></semantics></math> / <math alttext=\"74.03\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m12\" intent=\":literal\"><semantics><mn>74.03</mn><annotation encoding=\"application/x-tex\">74.03</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"59.63\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m13\" intent=\":literal\"><semantics><mn>59.63</mn><annotation encoding=\"application/x-tex\">59.63</annotation></semantics></math> / <math alttext=\"53.92\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m14\" intent=\":literal\"><semantics><mn>53.92</mn><annotation encoding=\"application/x-tex\">53.92</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"60.34\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m15\" intent=\":literal\"><semantics><mn>60.34</mn><annotation encoding=\"application/x-tex\">60.34</annotation></semantics></math> / <math alttext=\"54.93\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m16\" intent=\":literal\"><semantics><mn>54.93</mn><annotation encoding=\"application/x-tex\">54.93</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:130.1pt;\">HuBERT + Gemma 2 9B</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:65.0pt;\"><math alttext=\"80.98\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m17\" intent=\":literal\"><semantics><mn>80.98</mn><annotation encoding=\"application/x-tex\">80.98</annotation></semantics></math> / <math alttext=\"77.42\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m18\" intent=\":literal\"><semantics><mn>77.42</mn><annotation encoding=\"application/x-tex\">77.42</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:65.0pt;\"><math alttext=\"80.17\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m19\" intent=\":literal\"><semantics><mn>80.17</mn><annotation encoding=\"application/x-tex\">80.17</annotation></semantics></math> / <math alttext=\"77.45\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m20\" intent=\":literal\"><semantics><mn>77.45</mn><annotation encoding=\"application/x-tex\">77.45</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"67.63\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m21\" intent=\":literal\"><semantics><mn>67.63</mn><annotation encoding=\"application/x-tex\">67.63</annotation></semantics></math> / <math alttext=\"60.34\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m22\" intent=\":literal\"><semantics><mn>60.34</mn><annotation encoding=\"application/x-tex\">60.34</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"67.11\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m23\" intent=\":literal\"><semantics><mn>67.11</mn><annotation encoding=\"application/x-tex\">67.11</annotation></semantics></math> / <math alttext=\"59.68\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m24\" intent=\":literal\"><semantics><mn>59.68</mn><annotation encoding=\"application/x-tex\">59.68</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:130.1pt;\">HuBERT + Gemma 7B</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:65.0pt;\"><math alttext=\"79.64\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m25\" intent=\":literal\"><semantics><mn>79.64</mn><annotation encoding=\"application/x-tex\">79.64</annotation></semantics></math> / <math alttext=\"75.52\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m26\" intent=\":literal\"><semantics><mn>75.52</mn><annotation encoding=\"application/x-tex\">75.52</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:65.0pt;\"><math alttext=\"78.85\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m27\" intent=\":literal\"><semantics><mn>78.85</mn><annotation encoding=\"application/x-tex\">78.85</annotation></semantics></math> / <math alttext=\"75.53\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m28\" intent=\":literal\"><semantics><mn>75.53</mn><annotation encoding=\"application/x-tex\">75.53</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"65.22\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m29\" intent=\":literal\"><semantics><mn>65.22</mn><annotation encoding=\"application/x-tex\">65.22</annotation></semantics></math> / <math alttext=\"57.51\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m30\" intent=\":literal\"><semantics><mn>57.51</mn><annotation encoding=\"application/x-tex\">57.51</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"64.77\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m31\" intent=\":literal\"><semantics><mn>64.77</mn><annotation encoding=\"application/x-tex\">64.77</annotation></semantics></math> / <math alttext=\"57.23\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m32\" intent=\":literal\"><semantics><mn>57.23</mn><annotation encoding=\"application/x-tex\">57.23</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:130.1pt;\">HuBERT + Llama 2 7B</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:65.0pt;\"><math alttext=\"79.88\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m33\" intent=\":literal\"><semantics><mn>79.88</mn><annotation encoding=\"application/x-tex\">79.88</annotation></semantics></math> / <math alttext=\"76.30\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m34\" intent=\":literal\"><semantics><mn>76.30</mn><annotation encoding=\"application/x-tex\">76.30</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:65.0pt;\"><math alttext=\"79.08\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m35\" intent=\":literal\"><semantics><mn>79.08</mn><annotation encoding=\"application/x-tex\">79.08</annotation></semantics></math> / <math alttext=\"76.32\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m36\" intent=\":literal\"><semantics><mn>76.32</mn><annotation encoding=\"application/x-tex\">76.32</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"66.54\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m37\" intent=\":literal\"><semantics><mn>66.54</mn><annotation encoding=\"application/x-tex\">66.54</annotation></semantics></math> / <math alttext=\"59.27\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m38\" intent=\":literal\"><semantics><mn>59.27</mn><annotation encoding=\"application/x-tex\">59.27</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"65.70\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m39\" intent=\":literal\"><semantics><mn>65.70</mn><annotation encoding=\"application/x-tex\">65.70</annotation></semantics></math> / <math alttext=\"58.70\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m40\" intent=\":literal\"><semantics><mn>58.70</mn><annotation encoding=\"application/x-tex\">58.70</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:130.1pt;\">HuBERT + Mistral 7B v0.1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:65.0pt;\"><math alttext=\"80.12\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m41\" intent=\":literal\"><semantics><mn>80.12</mn><annotation encoding=\"application/x-tex\">80.12</annotation></semantics></math> / <math alttext=\"76.92\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m42\" intent=\":literal\"><semantics><mn>76.92</mn><annotation encoding=\"application/x-tex\">76.92</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:65.0pt;\"><math alttext=\"79.45\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m43\" intent=\":literal\"><semantics><mn>79.45</mn><annotation encoding=\"application/x-tex\">79.45</annotation></semantics></math> / <math alttext=\"76.92\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m44\" intent=\":literal\"><semantics><mn>76.92</mn><annotation encoding=\"application/x-tex\">76.92</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"66.97\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m45\" intent=\":literal\"><semantics><mn>66.97</mn><annotation encoding=\"application/x-tex\">66.97</annotation></semantics></math> / <math alttext=\"59.73\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m46\" intent=\":literal\"><semantics><mn>59.73</mn><annotation encoding=\"application/x-tex\">59.73</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"66.62\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m47\" intent=\":literal\"><semantics><mn>66.62</mn><annotation encoding=\"application/x-tex\">66.62</annotation></semantics></math> / <math alttext=\"59.85\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m48\" intent=\":literal\"><semantics><mn>59.85</mn><annotation encoding=\"application/x-tex\">59.85</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:130.1pt;\">Whisper enc. + Gemma 2 9B</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:65.0pt;\"><math alttext=\"\\mathbf{84.22}\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m49\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">84.22</mn><annotation encoding=\"application/x-tex\">\\mathbf{84.22}</annotation></semantics></math> / <math alttext=\"\\mathbf{81.15}\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m50\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">81.15</mn><annotation encoding=\"application/x-tex\">\\mathbf{81.15}</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:65.0pt;\"><math alttext=\"\\mathbf{83.65}\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m51\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">83.65</mn><annotation encoding=\"application/x-tex\">\\mathbf{83.65}</annotation></semantics></math> / <math alttext=\"\\mathbf{81.29}\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m52\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">81.29</mn><annotation encoding=\"application/x-tex\">\\mathbf{81.29}</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"\\mathbf{70.51}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m53\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">70.51</mn><annotation encoding=\"application/x-tex\">\\mathbf{70.51}</annotation></semantics></math> / <math alttext=\"\\mathbf{62.80}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m54\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">62.80</mn><annotation encoding=\"application/x-tex\">\\mathbf{62.80}</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"\\mathbf{70.34}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m55\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">70.34</mn><annotation encoding=\"application/x-tex\">\\mathbf{70.34}</annotation></semantics></math> / <math alttext=\"\\mathbf{63.27}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m56\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">63.27</mn><annotation encoding=\"application/x-tex\">\\mathbf{63.27}</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:130.1pt;\">Whisper enc. + Gemma 7B</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:65.0pt;\"><math alttext=\"82.55\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m57\" intent=\":literal\"><semantics><mn>82.55</mn><annotation encoding=\"application/x-tex\">82.55</annotation></semantics></math> / <math alttext=\"79.69\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m58\" intent=\":literal\"><semantics><mn>79.69</mn><annotation encoding=\"application/x-tex\">79.69</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:65.0pt;\"><math alttext=\"82.15\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m59\" intent=\":literal\"><semantics><mn>82.15</mn><annotation encoding=\"application/x-tex\">82.15</annotation></semantics></math> / <math alttext=\"79.88\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m60\" intent=\":literal\"><semantics><mn>79.88</mn><annotation encoding=\"application/x-tex\">79.88</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"67.63\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m61\" intent=\":literal\"><semantics><mn>67.63</mn><annotation encoding=\"application/x-tex\">67.63</annotation></semantics></math> / <math alttext=\"60.06\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m62\" intent=\":literal\"><semantics><mn>60.06</mn><annotation encoding=\"application/x-tex\">60.06</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"68.24\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m63\" intent=\":literal\"><semantics><mn>68.24</mn><annotation encoding=\"application/x-tex\">68.24</annotation></semantics></math> / <math alttext=\"60.91\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m64\" intent=\":literal\"><semantics><mn>60.91</mn><annotation encoding=\"application/x-tex\">60.91</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:130.1pt;\">Whisper enc. + Llama 2 7B</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:65.0pt;\"><math alttext=\"82.84\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m65\" intent=\":literal\"><semantics><mn>82.84</mn><annotation encoding=\"application/x-tex\">82.84</annotation></semantics></math> / <math alttext=\"80.09\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m66\" intent=\":literal\"><semantics><mn>80.09</mn><annotation encoding=\"application/x-tex\">80.09</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:65.0pt;\"><math alttext=\"82.14\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m67\" intent=\":literal\"><semantics><mn>82.14</mn><annotation encoding=\"application/x-tex\">82.14</annotation></semantics></math> / <math alttext=\"80.05\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m68\" intent=\":literal\"><semantics><mn>80.05</mn><annotation encoding=\"application/x-tex\">80.05</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"68.82\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m69\" intent=\":literal\"><semantics><mn>68.82</mn><annotation encoding=\"application/x-tex\">68.82</annotation></semantics></math> / <math alttext=\"61.82\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m70\" intent=\":literal\"><semantics><mn>61.82</mn><annotation encoding=\"application/x-tex\">61.82</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"68.64\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m71\" intent=\":literal\"><semantics><mn>68.64</mn><annotation encoding=\"application/x-tex\">68.64</annotation></semantics></math> / <math alttext=\"61.91\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m72\" intent=\":literal\"><semantics><mn>61.91</mn><annotation encoding=\"application/x-tex\">61.91</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:130.1pt;\">Whisper enc. + Mistral 7B v0.1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:65.0pt;\"><math alttext=\"83.13\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m73\" intent=\":literal\"><semantics><mn>83.13</mn><annotation encoding=\"application/x-tex\">83.13</annotation></semantics></math> / <math alttext=\"80.24\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m74\" intent=\":literal\"><semantics><mn>80.24</mn><annotation encoding=\"application/x-tex\">80.24</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:65.0pt;\"><math alttext=\"82.43\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m75\" intent=\":literal\"><semantics><mn>82.43</mn><annotation encoding=\"application/x-tex\">82.43</annotation></semantics></math> / <math alttext=\"80.37\" class=\"ltx_centering\" display=\"inline\" id=\"S3.T4.m76\" intent=\":literal\"><semantics><mn>80.37</mn><annotation encoding=\"application/x-tex\">80.37</annotation></semantics></math></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"69.73\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m77\" intent=\":literal\"><semantics><mn>69.73</mn><annotation encoding=\"application/x-tex\">69.73</annotation></semantics></math> / <math alttext=\"62.40\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m78\" intent=\":literal\"><semantics><mn>62.40</mn><annotation encoding=\"application/x-tex\">62.40</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<math alttext=\"68.86\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m79\" intent=\":literal\"><semantics><mn>68.86</mn><annotation encoding=\"application/x-tex\">68.86</annotation></semantics></math> / <math alttext=\"61.79\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T4.m80\" intent=\":literal\"><semantics><mn>61.79</mn><annotation encoding=\"application/x-tex\">61.79</annotation></semantics></math>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "comet22datextcomettextda22",
            "respectively",
            "7051mathbf7051",
            "6280mathbf6280",
            "en2de",
            "seamlessm4t",
            "8365mathbf8365",
            "tstcommon",
            "tst2021",
            "comet22kiwidatextcomettextkiwida22",
            "v01",
            "llama",
            "results",
            "cascaded",
            "nllb",
            "enc",
            "7034mathbf7034",
            "8115mathbf8115",
            "8129mathbf8129",
            "mistral",
            "mustc",
            "gemma",
            "whisper",
            "hubert",
            "8422mathbf8422",
            "iwslt",
            "offline",
            "tst2022",
            "6327mathbf6327",
            "model"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#S3.T3\" title=\"In 3.3 Offline ST Results &#8227; 3 Evaluation &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">Tables</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#S3.T4\" title=\"Table 4 &#8227; 3.3 Offline ST Results &#8227; 3 Evaluation &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> report the <span class=\"ltx_text ltx_font_smallcaps\">BLEU</span> and <span class=\"ltx_text ltx_font_smallcaps\">COMET</span>-family scores, respectively, on the four test sets. For evaluating with <span class=\"ltx_text ltx_font_smallcaps\">BLEU</span>, we included both <span class=\"ltx_text ltx_font_typewriter\">docAsWhole</span> score, which concatenated all reference segments and candidate complete segments as two documents, and <span class=\"ltx_text ltx_font_typewriter\">mwerSegmenter</span> score, which resegments complete candidate segments according to reference segments to minimize WER. Similar to <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#S3.SS2\" title=\"3.2 ASR Results &#8227; 3 Evaluation &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, <span class=\"ltx_text ltx_font_typewriter\">mwerSegmenter</span> scores for IWSLT&#8217;21 test set could not be obtained, hence we did not include them.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Speech Translation (ST) is a machine translation task that involves converting speech signals from one language to the corresponding text in another language;\nthis task has two different approaches, namely the traditional <em class=\"ltx_emph ltx_font_italic\">cascade</em> and the more recent <em class=\"ltx_emph ltx_font_italic\">end-to-end</em>. This paper explores a combined end-to-end architecture of pre-trained speech encoders and Large Language Models (LLMs) for performing both Automatic Speech Recognition (ASR) and ST simultaneously. Experiments with the English-to-German language pair show that our best model not only can achieve better translation results than SeamlessM4T <cite class=\"ltx_cite ltx_citemacro_citep\">(Communication et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib5\" title=\"\">2023</a>)</cite>, a large foundational end-to-end, multi-modal translation model, but can also match the performance of a cascaded system with Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib24\" title=\"\">2022</a>)</cite> and NLLB <cite class=\"ltx_cite ltx_citemacro_citep\">(Team et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib27\" title=\"\">2022</a>)</cite>, with up to a score gain of 8% in <math alttext=\"\\text{COMET}^{\\text{DA}}_{22}\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><msubsup><mtext>COMET</mtext><mn>22</mn><mtext>DA</mtext></msubsup><annotation encoding=\"application/x-tex\">\\text{COMET}^{\\text{DA}}_{22}</annotation></semantics></math> metric.</p>\n\n",
                "matched_terms": [
                    "comet22datextcomettextda22",
                    "whisper",
                    "nllb",
                    "results",
                    "cascaded",
                    "seamlessm4t",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Motivated by recent contributions in speech representation learning and LLMs, we aim to investigate an end-to-end architecture that simultaneously performs both ASR and ST. This architecture combines the high-quality audio representation from the pre-trained acoustic models with the excellent performance of LLMs to serve as an end-to-end speech translation system,\nwhile still having the ability to transcribe from the audio signal. Our proposed model, after being fine-tuned with the Quantized Low-Rank Adaptation (QLoRA; <cite class=\"ltx_cite ltx_citemacro_citep\">Dettmers et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib6\" title=\"\">2023</a></cite>) technique, achieves a robust translation performance, comparable to a cascaded system, which is still a state-of-the-art approach for this task.</p>\n\n",
                "matched_terms": [
                    "cascaded",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#S3\" title=\"3 Evaluation &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3</span></a> provides the ASR and ST evaluation results of the model in different public test sets, and compares them to some baselines from out-of-the-box models.</p>\n\n",
                "matched_terms": [
                    "results",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The overall architecture is illustrated in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#S2.F1\" title=\"In 2.1 Architecture &#8227; 2 Methods and Dataset &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>. For each training sample, given the speech signal, its corresponding transcript, and the translated text, the speech hidden features are obtained using a speech encoder, including HuBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib10\" title=\"\">2021</a>)</cite> and Whisper encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib24\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "hubert",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopted HuBERT <cite class=\"ltx_cite ltx_citemacro_citep\">(Hsu et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib10\" title=\"\">2021</a>)</cite> and Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib24\" title=\"\">2022</a>)</cite> as the speech encoders, utilizing their capability of extracting high-quality representation from audio data. We used the <span class=\"ltx_text ltx_font_typewriter\">hubert-large-ls960-ft</span>\nvariation, which was trained on 60,000 hours of data from the LibriLight <cite class=\"ltx_cite ltx_citemacro_citep\">(Kahn et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib11\" title=\"\">2020</a>)</cite> corpus, then fine-tuned on 960 hours of data from the LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib20\" title=\"\">2015a</a>)</cite> corpus. For Whisper-based models, we only used the encoder part of the pre-trained <span class=\"ltx_text ltx_font_typewriter\">whisper-large-v3-turbo</span>\nto extract the audio hidden features.</p>\n\n",
                "matched_terms": [
                    "hubert",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We experimented with four different pre-trained LLMs available on HuggingFace, namely <em class=\"ltx_emph ltx_font_italic\">Gemma 7B</em> (<span class=\"ltx_text ltx_font_typewriter\">gemma-7b</span>),\n<em class=\"ltx_emph ltx_font_italic\">Gemma 2 9B</em> (<span class=\"ltx_text ltx_font_typewriter\">gemma-2-9b</span>),\n<em class=\"ltx_emph ltx_font_italic\">Llama 2 7B</em> (<span class=\"ltx_text ltx_font_typewriter\">Llama-2-7b-hf</span>),\nand <em class=\"ltx_emph ltx_font_italic\">Mistral 7B v0.1</em> (<span class=\"ltx_text ltx_font_typewriter\">Mistral-7B-v0.1</span>).\nDetails about each variation are described in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#S2.T1\" title=\"In 2.5 LLMs &#8227; 2 Methods and Dataset &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "gemma",
                    "mistral",
                    "v01",
                    "llama"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, MuST-C also provides two public test sets, both named <span class=\"ltx_text ltx_font_typewriter\">tst-COMMON</span> in version 2.0 and 3.0. We also used the test sets from the Offline Track of IWSLT&#8217;21\nand &#8217;22.\nIn addition, to evaluate ASR performance, we used two test sets from the LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib21\" title=\"\">2015b</a>)</cite> dataset, namely <span class=\"ltx_text ltx_font_typewriter\">test-clean</span> and <span class=\"ltx_text ltx_font_typewriter\">test-other</span>, both of which are the standard datasets for this task. As all models can perform both ASR and ST simultaneously, evaluation results for both tasks are described in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#S3.SS2\" title=\"3.2 ASR Results &#8227; 3 Evaluation &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">Sections</span>&#732;<span class=\"ltx_text ltx_ref_tag\">3.2</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#S3.SS3\" title=\"3.3 Offline ST Results &#8227; 3 Evaluation &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>, respectively.</p>\n\n",
                "matched_terms": [
                    "mustc",
                    "respectively",
                    "offline",
                    "results",
                    "tstcommon"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the Offline Speech Translation task, we evaluated all models using standard metrics, namely <span class=\"ltx_text ltx_font_smallcaps\">BLEU</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Papineni et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib22\" title=\"\">2002</a>)</cite>, <math alttext=\"\\text{COMET}^{\\text{DA}}_{22}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msubsup><mtext>COMET</mtext><mn>22</mn><mtext>DA</mtext></msubsup><annotation encoding=\"application/x-tex\">\\text{COMET}^{\\text{DA}}_{22}</annotation></semantics></math> <cite class=\"ltx_cite ltx_citemacro_citep\">(Rei et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib25\" title=\"\">2022a</a>)</cite>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/Unbabel/wmt22-comet-da\" title=\"\">https://huggingface.co/Unbabel/wmt22-comet-da</a></span></span></span> and <math alttext=\"\\text{COMET}^{\\text{KIWI-DA}}_{22}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msubsup><mtext>COMET</mtext><mn>22</mn><mtext>KIWI-DA</mtext></msubsup><annotation encoding=\"application/x-tex\">\\text{COMET}^{\\text{KIWI-DA}}_{22}</annotation></semantics></math> <cite class=\"ltx_cite ltx_citemacro_citep\">(Rei et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib26\" title=\"\">2022b</a>)</cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/Unbabel/wmt22-cometkiwi-da\" title=\"\">https://huggingface.co/Unbabel/wmt22-cometkiwi-da</a></span></span></span> For the Automatic Speech Recognition Task, we used <span class=\"ltx_text ltx_font_smallcaps\">WER</span>, the standard metric for speech recognition.</p>\n\n",
                "matched_terms": [
                    "comet22kiwidatextcomettextkiwida22",
                    "comet22datextcomettextda22",
                    "offline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compared our architecture with two out-of-the-box baselines: a cascaded pipeline of Whisper\n(<span class=\"ltx_text ltx_font_typewriter\">whisper-large-v3-turbo</span>; <cite class=\"ltx_cite ltx_citemacro_citep\">Radford et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib24\" title=\"\">2022</a></cite>) producing the transcript and NLLB\n(<span class=\"ltx_text ltx_font_typewriter\">nllb-200-3.3B</span>; <cite class=\"ltx_cite ltx_citemacro_citep\">Team et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib27\" title=\"\">2022</a></cite>) translating the transcript, along with SeamlessM4T\n(<span class=\"ltx_text ltx_font_typewriter\">seamless-m4t-v2-large</span>; <cite class=\"ltx_cite ltx_citemacro_citep\">Communication et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib5\" title=\"\">2023</a></cite>) - an end-to-end, multi-modal translation model.</p>\n\n",
                "matched_terms": [
                    "whisper",
                    "nllb",
                    "cascaded",
                    "seamlessm4t",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#S3.T2\" title=\"In 3.2 ASR Results &#8227; 3 Evaluation &#8227; End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#732;<span class=\"ltx_text ltx_ref_tag\">2</span></a> details the ASR evaluation results against the four test sets. We reported the <span class=\"ltx_text ltx_font_smallcaps\">WER</span> score after applying the &#8220;LPW&#8221; pre-processing strategy available in <span class=\"ltx_text ltx_font_typewriter\">SLTev</span>, which first lowercased every character, removed all punctuation, then used the built-in <span class=\"ltx_text ltx_font_typewriter\">mwerSegmenter</span> tool to resegment the output transcripts. Due to some bugs when processing the IWSLT&#8217;21 test set (tst2021), <span class=\"ltx_text ltx_font_typewriter\">mwerSegmenter</span> failed to run during evaluation, hence we could not obtain the results. It can be seen that models with Gemma 2 9B as the decoder have the best result among the four LLMs, albeit still lagging behind the performance of Whisper.</p>\n\n",
                "matched_terms": [
                    "results",
                    "tst2021",
                    "gemma",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Similarly, the models with Gemma 2 9B still have the best evaluation score among the four fine-tuned LLMs. In combination with the Whisper encoder, it even surpassed the performance of the cascaded system of Whisper + NLLB in most of the test sets and metrics.</p>\n\n",
                "matched_terms": [
                    "gemma",
                    "cascaded",
                    "nllb",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we leveraged pre-trained speech encoders and LLMs and connected them to become an end-to-end architecture for speech translation. The overall result is expected: for the English-to-German direction, even though our models performed better than the end-to-end SeamlessM4T model all of the time, there was still a gap compared to the performance of the cascaded Whisper + NLLB pipeline. It suggests that cascaded models are still the state-of-the-art approach in the speech translation task; this is also confirmed according to <cite class=\"ltx_cite ltx_citemacro_citet\">Ahmad et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib1\" title=\"\">2024</a>)</cite>, in which all systems submitted to the Offline Track of IWSLT&#8217;24 were cascaded systems.</p>\n\n",
                "matched_terms": [
                    "whisper",
                    "nllb",
                    "offline",
                    "cascaded",
                    "seamlessm4t",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, considering the size of the LLMs, our models were inferior regarding inference speed, compared to the two baselines. Our models also managed to surpass the performance of the cascaded system in the translation task; however, the differences were not too substantial. In addition, despite being a much smaller model, Whisper alone still excels at speech recognition. This raises a question: <span class=\"ltx_text ltx_font_italic\">\"Can end-to-end speech translation systems be smaller in size, while still keeping the robustness in translation, especially for the rising need to be used in mobile devices?\"</span></p>\n\n",
                "matched_terms": [
                    "cascaded",
                    "model",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All models were fine-tuned using 4-bit QLoRA <cite class=\"ltx_cite ltx_citemacro_citep\">(Dettmers et al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib6\" title=\"\">2023</a>)</cite> adapters in <span class=\"ltx_text ltx_font_typewriter\">bfloat16</span> precision, with the following LoRA parameters: rank of <math alttext=\"r=8\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">r=8</annotation></semantics></math>, alpha of <math alttext=\"\\alpha=8\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=8</annotation></semantics></math>. For the models with HuBERT as the encoder, because of the manual CTC collapsing procedure, we could only process one example at a time, hence the batch size was set to <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m3\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>; while for those with Whisper, the batch size was set to <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m4\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>. Other training hyperparameters included the learning rate of <math alttext=\"1e-4\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m5\" intent=\":literal\"><semantics><mrow><mrow><mn>1</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>&#8722;</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">1e-4</annotation></semantics></math> with <math alttext=\"10\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m6\" intent=\":literal\"><semantics><mn>10</mn><annotation encoding=\"application/x-tex\">10</annotation></semantics></math> warmup steps, and an AdamW optimizer <cite class=\"ltx_cite ltx_citemacro_cite\">Loshchilov and Hutter (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib17\" title=\"\">2019</a>)</cite> with a cosine scheduler <cite class=\"ltx_cite ltx_citemacro_cite\">Loshchilov and Hutter (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.10329v1#bib.bib16\" title=\"\">2017</a>)</cite>. All HuBERT-encoder models were trained for <math alttext=\"500,000\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m7\" intent=\":literal\"><semantics><mrow><mn>500</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding=\"application/x-tex\">500,000</annotation></semantics></math> steps, while Whisper-encoder models were trained for <math alttext=\"100,000\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m8\" intent=\":literal\"><semantics><mrow><mn>100</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding=\"application/x-tex\">100,000</annotation></semantics></math> steps.</p>\n\n",
                "matched_terms": [
                    "hubert",
                    "whisper"
                ]
            }
        ]
    }
}