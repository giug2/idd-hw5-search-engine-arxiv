{
    "S5.T1": {
        "caption": "Table 1: Pearson œÅ\\rho correlation between layer-wise (‚Ñì\\ell) and head-wise (hh) cross-attention and the explanations for the monolingual ASR model on English (base). The layer/head average (‚Ñì\\ell-/hh-AVG) correlation is computed between the averaged cross-attention across layers/head (ùêÇùêÄ¬Ø(‚Ñì)\\widebar{\\text{{\\color[rgb]{0.75390625,0.390625,0.9609375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.75390625,0.390625,0.9609375}$\\mathbf{CA}$}}}^{(\\ell)}/ùêÇùêÄ¬Øh\\widebar{\\text{{\\color[rgb]{0.75390625,0.390625,0.9609375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.75390625,0.390625,0.9609375}$\\mathbf{CA}$}}}_{h}) and the input explanations ùêíùêå\\mathbf{SM}XX. Bold indicates the highest correlation, underline indicates the highest layer-wise and head-wise correlation. Low to high values are green, to yellow, to pink.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold\">Layer/Head</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><math alttext=\"h=1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m1\" intent=\":literal\"><semantics><mrow><mi>h</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">h=1</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><math alttext=\"h=2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m2\" intent=\":literal\"><semantics><mrow><mi>h</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">h=2</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><math alttext=\"h=3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m3\" intent=\":literal\"><semantics><mrow><mi>h</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">h=3</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><math alttext=\"h=4\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m4\" intent=\":literal\"><semantics><mrow><mi>h</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">h=4</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><math alttext=\"h=5\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m5\" intent=\":literal\"><semantics><mrow><mi>h</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">h=5</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><math alttext=\"h=6\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m6\" intent=\":literal\"><semantics><mrow><mi>h</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">h=6</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><math alttext=\"h=7\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m7\" intent=\":literal\"><semantics><mrow><mi>h</mi><mo>=</mo><mn>7</mn></mrow><annotation encoding=\"application/x-tex\">h=7</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><math alttext=\"h=8\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m8\" intent=\":literal\"><semantics><mrow><mi>h</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">h=8</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m9\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math><span class=\"ltx_text ltx_font_bold\">-AVG</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><math alttext=\"\\ell=1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m10\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=1</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#DAF4D7;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#DAF4D7;\">0.076</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#D0F4DE;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D0F4DE;\">-0.021</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#DCF4D5;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#DCF4D5;\">0.096</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#D6F4DA;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D6F4DA;\">0.037</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#D6F4D9;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D6F4D9;\">0.042</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#D7F4D9;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D7F4D9;\">0.053</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#D0F4DE;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D0F4DE;\">-0.020</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"--ltx-bg-color:#DCF4D5;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_border_r_dashed\" style=\"--ltx-bg-color:#DCF4D5;\">0.094</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#DEF4D4;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#DEF4D4;\">0.111</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><math alttext=\"\\ell=2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m11\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=2</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#D3F4DC;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D3F4DC;\">0.013</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#DFF4D3;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#DFF4D3;\">0.122</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#D6F4DA;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D6F4DA;\">0.039</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E4F4CF;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E4F4CF;\">0.171</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#DFF4D3;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#DFF4D3;\">0.123</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#DBF4D6;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#DBF4D6;\">0.089</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#DFF4D3;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#DFF4D3;\">0.119</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"--ltx-bg-color:#D6F4D9;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_border_r_dashed\" style=\"--ltx-bg-color:#D6F4D9;\">0.041</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E5F4CE;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E5F4CE;\">0.178</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><math alttext=\"\\ell=3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m12\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=3</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FDD7C0;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDD7C0;\">0.455</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FCEEBD;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCEEBD;\">0.404</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#F7F5C1;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F5C1;\">0.348</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FBF5BE;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FBF5BE;\">0.386</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#EEF5C7;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#EEF5C7;\">0.263</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E0F4D2;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E0F4D2;\">0.136</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#EDF5C9;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#EDF5C9;\">0.248</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"--ltx-bg-color:#ECF5C9;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_border_r_dashed\" style=\"--ltx-bg-color:#ECF5C9;\">0.246</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FCDDC0;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCDDC0;\">0.443</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><math alttext=\"\\ell=4\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m13\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=4</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FCD8C0;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCD8C0;\">0.452</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#F7F5C1;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F5C1;\">0.344</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#EAF5CA;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#EAF5CA;\">0.227</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FCEEBE;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCEEBE;\">0.405</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#F4F5C3;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F4F5C3;\">0.314</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FDBDC3;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDBDC3;\">0.512</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FCEABE;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCEABE;\">0.414</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"--ltx-bg-color:#FDC4C2;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_border_r_dashed\" style=\"--ltx-bg-color:#FDC4C2;\">0.495</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FEADC5;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FEADC5;\">0.546</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><math alttext=\"\\ell=5\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m14\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=5</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FDBEC3;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDBEC3;\">0.508</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FDD3C1;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDD3C1;\">0.463</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FDD2C1;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDD2C1;\">0.466</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FEB8C4;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FEB8C4;\">0.521</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FDBAC4;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDBAC4;\">0.518</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FAF5BF;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FAF5BF;\">0.374</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FDC1C3;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDC1C3;\">0.502</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"--ltx-bg-color:#FDC9C2;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_border_r_dashed\" style=\"--ltx-bg-color:#FDC9C2;\">0.485</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FE9EC7;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FE9EC7;\">0.578</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:7.0pt;padding-right:7.0pt;\"><math alttext=\"\\ell=6\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m15\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=6</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FBF5BE;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FBF5BE;\">0.377</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FCF3BD;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCF3BD;\">0.394</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FDBFC3;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDBFC3;\">0.508</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FCF6BD;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCF6BD;\">0.386</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FDBAC4;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDBAC4;\">0.517</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FAF5BF;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FAF5BF;\">0.371</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FCE5BF;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCE5BF;\">0.424</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"--ltx-bg-color:#F5F5C3;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_border_r_dashed\" style=\"--ltx-bg-color:#F5F5C3;\">0.322</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FF99C8;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"--ltx-bg-color:#FF99C8;\">0.588</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-left:7.0pt;padding-right:7.0pt;\">\n<math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m16\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math><span class=\"ltx_text ltx_font_bold\">-AVG</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"--ltx-bg-color:#FEADC5;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FEADC5;\">0.546</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"--ltx-bg-color:#FDD6C0;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDD6C0;\">0.456</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"--ltx-bg-color:#FDD7C0;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDD7C0;\">0.455</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"--ltx-bg-color:#FEA0C7;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"--ltx-bg-color:#FEA0C7;\">0.574</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"--ltx-bg-color:#FEB5C4;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FEB5C4;\">0.529</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"--ltx-bg-color:#FEB0C5;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FEB0C5;\">0.541</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"--ltx-bg-color:#FEB6C4;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FEB6C4;\">0.525</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\" style=\"--ltx-bg-color:#FDC2C3;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text ltx_border_r_dashed\" style=\"--ltx-bg-color:#FDC2C3;\">0.500</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"--ltx-bg-color:#FE9FC7;padding-left:7.0pt;padding-right:7.0pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FE9FC7;\">0.577</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "ùêíùêåmathbfsmxx",
            "h3h3",
            "pearson",
            "high",
            "‚Ñì1ell1",
            "layershead",
            "hhavg",
            "green",
            "‚Ñì2ell2",
            "h2h2",
            "underline",
            "headwise",
            "monolingual",
            "base",
            "layerhead",
            "low",
            "average",
            "h1h1",
            "computed",
            "between",
            "input",
            "ùêÇùêÄ¬Ø‚Ñìwidebartextcolorrgb075390625039062509609375definecolornamedpgfstrokecolorrgb075390625039062509609375mathbfcaellùêÇùêÄ¬Øhwidebartextcolorrgb075390625039062509609375definecolornamedpgfstrokecolorrgb075390625039062509609375mathbfcah",
            "layerwise",
            "‚Ñìellhhavg",
            "across",
            "english",
            "h7h7",
            "averaged",
            "bold",
            "pink",
            "h4h4",
            "‚Ñì4ell4",
            "asr",
            "indicates",
            "‚Ñì6ell6",
            "h5h5",
            "‚Ñìell",
            "crossattention",
            "values",
            "h6h6",
            "correlation",
            "œÅrho",
            "yellow",
            "model",
            "highest",
            "explanations",
            "‚Ñì3ell3",
            "‚Ñìellavg",
            "h8h8",
            "‚Ñì5ell5"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.T1\" title=\"Table 1 &#8227; 5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reports the correlation scores for the monolingual English ASR model (<span class=\"ltx_text ltx_font_typewriter\">base</span>), considering cross-attention at the head level, layer level, and in aggregated form.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Cross-attention is a core mechanism in encoder-decoder architectures, widespread in many fields, including speech-to-text (S2T) processing. Its scores have been repurposed for various downstream applications&#8211;such as timestamp estimation and audio-text alignment&#8211;under the assumption that they reflect the dependencies between input speech representation and the generated text. While the explanatory nature of attention mechanisms has been widely debated in the broader NLP literature, this assumption remains largely unexplored within the speech domain. To address this gap, we assess the explanatory power of cross-attention in S2T models by comparing its scores to input saliency maps derived from feature attribution. Our analysis spans monolingual and multilingual, single-task and multi-task models at multiple scales, and shows that attention scores moderately to strongly align with saliency-based explanations, particularly when aggregated across heads and layers. However, it also shows that cross-attention captures only about 50% of the input relevance and, in the best case, only partially reflects how the decoder attends to the encoder&#8217;s representations&#8211;accounting for just 52-75% of the saliency. These findings uncover fundamental limitations in interpreting cross-attention as an explanatory proxy, suggesting that it offers an informative yet incomplete view of the factors driving predictions in S2T models.</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "across",
                    "explanations",
                    "between",
                    "input",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-attention <cite class=\"ltx_cite ltx_citemacro_citep\">(Bahdanau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib6\" title=\"\">2015</a>)</cite> is the core mechanism of the encoder-decoder Transformer architecture <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib85\" title=\"\">2017</a>)</cite>, a model that has become foundational across numerous AI domains <cite class=\"ltx_cite ltx_citemacro_citep\">(Galassi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib25\" title=\"\">2021</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib49\" title=\"\">2022</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib45\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib92\" title=\"\">2024b</a>; Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib51\" title=\"\">2024</a>)</cite>, including natural language processing (NLP).\nDesigned for modeling dependencies between the generated output sequence and the input representations, the cross-attention scores&#8211;derived from the attention mechanism&#8211;have been leveraged in various NLP tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib32\" title=\"\">2020</a>; Zhang &amp; Kim, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib103\" title=\"\">2023</a>)</cite>, such as\nsource-target textual alignment <cite class=\"ltx_cite ltx_citemacro_citep\">(Garg et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib26\" title=\"\">2019</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib12\" title=\"\">2020</a>)</cite>, co-reference resolution <cite class=\"ltx_cite ltx_citemacro_citep\">(Voita et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib87\" title=\"\">2018</a>)</cite>, and word sense disambiguation <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib83\" title=\"\">2018</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "model",
                    "between",
                    "input",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In speech-to-text (S2T) modeling, cross-attention scores have been widely repurposed for diverse downstream applications such as audio-text alignment <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib104\" title=\"\">2020</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib46\" title=\"\">2020</a>)</cite>, speaker identification <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib38\" title=\"\">2019</a>)</cite>, timestamp estimation <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib47\" title=\"\">2022</a>; Louradour, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib50\" title=\"\">2023</a>; Zusag et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib106\" title=\"\">2024</a>)</cite>, and guiding simultaneous automatic speech recognition (ASR) and speech translation (ST) <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib91\" title=\"\">2024a</a>; Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib65\" title=\"\">2023a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib66\" title=\"\">b</a>)</cite>.\nThese applications rely on the implicit assumption that\ncross-attention reliably indicates what the model attends to in the input signal during output generation. However, despite its widespread use, this assumption has\nnever been verified.\nA key concern is that cross-attention operates over the encoder&#8217;s output sequence&#8211;rather than directly on the raw audio&#8211;which may have been reorganized or mixed with contextual information. This phenomenon, known as context mixing <cite class=\"ltx_cite ltx_citemacro_citep\">(Mohebbi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib60\" title=\"\">2023b</a>)</cite>, can potentially obscure the alignment between cross-attention weights and the original input signal.\nSimilar concerns have been extensively debated in the NLP community, where the reliability of attention mechanisms as explanations has been both challenged and defended, leading to conflicting perspectives and empirical evidence <cite class=\"ltx_cite ltx_citemacro_citep\">(Serrano &amp; Smith, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib78\" title=\"\">2019</a>; Jain &amp; Wallace, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib35\" title=\"\">2019</a>; Wiegreffe &amp; Pinter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib93\" title=\"\">2019</a>; Bastings &amp; Filippova, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib9\" title=\"\">2020</a>)</cite>. In contrast, this question remains largely underexplored in the speech domain.\nExisting work on explainability in S2T has primarily focused on self-attention <cite class=\"ltx_cite ltx_citemacro_citep\">(Shim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib80\" title=\"\">2022</a>; Audhkhasi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib5\" title=\"\">2022</a>; A&#160;Shams et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib1\" title=\"\">2024</a>)</cite>, or on empirically measuring the effects of context mixing <cite class=\"ltx_cite ltx_citemacro_citep\">(Mohebbi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib59\" title=\"\">2023a</a>)</cite>, without directly assessing the explanatory potential of cross-attention mechanisms.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr",
                    "indicates",
                    "input",
                    "explanations",
                    "between",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this gap, we present the first systematic\nanalysis of cross-attention as a proxy for input-output dependencies in S2T models.\nOur study serves two main objectives: <span class=\"ltx_text ltx_font_italic\">i)</span> assessing the validity of using cross-attention as a surrogate for input-output alignment,\nand <span class=\"ltx_text ltx_font_italic\">ii)</span> evaluating whether it provides insights comparable to formal explainability methods such as feature attribution&#8211;while being more lightweight and less computationally expensive to obtain <cite class=\"ltx_cite ltx_citemacro_citep\">(Samek et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib77\" title=\"\">2021</a>; Madsen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib53\" title=\"\">2022</a>)</cite>. We compare cross-attention scores with input saliency maps derived from SPES <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>, the current state-of-the-art feature-attribution method in S2T, to determine the extent to which\ncross-attention\ncaptures\nwhich input features are relevant for models&#8217; predictions.\nIn addition, we compute saliency maps on\nencoder outputs and compare them with cross-attention scores to evaluate whether cross-attention fully explains how the decoder uses encoded representations, avoiding potential discrepancies\nof context mixing.\nOur\nanalysis spans ASR and ST tasks across monolingual, multilingual, and multitask settings using\nstate-of-the-art speech processing\narchitectures <cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib29\" title=\"\">2020</a>)</cite> at multiple scales.\nWith consistent trends across different settings, we find\nthat cross-attention exhibits moderate to strong correlations with input saliency maps and aligns more closely with encoder output representations, suggesting an influence of context mixing. However, our results also indicate that the overall explanatory power of cross-attention is limited&#8211;accounting for only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>50% of input relevance and, at best, 52-75% of encoder output saliency.\nOur findings uncover fundamental limitations in interpreting cross-attention as an explanatory proxy, suggesting that it provides an informative yet incomplete view of the factors driving predictions in S2T models.\n</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "across",
                    "asr",
                    "input",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Explainability in Speech-to-Text.</span>\nExplainable AI (XAI) has emerged to make model behavior more interpretable to humans, thereby supporting informed decision-making and responsible deployment <cite class=\"ltx_cite ltx_citemacro_citep\">(Barredo Arrieta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib8\" title=\"\">2020</a>)</cite>.\nWhile XAI research has seen a rapid growth in the last years across multiple modalities, including vision and language <cite class=\"ltx_cite ltx_citemacro_citep\">(Sharma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib79\" title=\"\">2024</a>)</cite>, progress in the speech domain has lagged.\nThis gap arises from the inherent complexities of speech processing, including the multidimensional nature of speech signals across time and frequency, and the variability in output sequence length <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib95\" title=\"\">2024</a>)</cite>.\nDespite these challenges, growing concerns about trustworthiness\nare driving explainability efforts in speech classification <cite class=\"ltx_cite ltx_citemacro_citep\">(Becker et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib10\" title=\"\">2024</a>; Pastor et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib68\" title=\"\">2024</a>)</cite> and S2T generation <cite class=\"ltx_cite ltx_citemacro_citep\">(Mandel, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib54\" title=\"\">2016</a>; Kavaki &amp; Mandel, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib37\" title=\"\">2020</a>; Trinh &amp; Mandel, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib84\" title=\"\">2020</a>; Markert et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib55\" title=\"\">2021</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib94\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib95\" title=\"\">2024</a>; Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>. Most of these works rely on perturbation-based methods\nthat assess how input modifications affect model predictions <cite class=\"ltx_cite ltx_citemacro_citep\">(Covert et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib16\" title=\"\">2021b</a>; Ivanovs et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib34\" title=\"\">2021</a>)</cite>.\nAmong these,\n<cite class=\"ltx_cite ltx_citemacro_citet\">Fucci et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite> recently proposed\na technique for autoregressive S2T models that identifies regions of the spectrogram that most influence predictions to generate saliency maps.\nHowever, XAI methods are generally computationally expensive&#8211;especially perturbation-based approaches applied to large models <cite class=\"ltx_cite ltx_citemacro_citep\">(Luo &amp; Specia, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib52\" title=\"\">2024</a>; Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib102\" title=\"\">2025</a>)</cite>&#8211;which motivates exploring whether cross-attention, already computed at inference time, could serve as a lightweight alternative in a landscape still lacking efficient explainability tools for speech-based models.</p>\n\n",
                "matched_terms": [
                    "across",
                    "model",
                    "computed",
                    "input",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Attention as Explanation.</span>\nAttention mechanisms have been widely used to probe model behavior in text-based NLP, as attention scores often align with human intuitions about relevance and salience <cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib13\" title=\"\">2019</a>; Ferrando et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib21\" title=\"\">2024</a>)</cite>.\nEarly studies proposed norm-based analyses to improve the interpretability of attention weights <cite class=\"ltx_cite ltx_citemacro_citep\">(Kobayashi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib39\" title=\"\">2020</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib40\" title=\"\">2021</a>; Mohebbi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib58\" title=\"\">2021</a>; Ferrando et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib20\" title=\"\">2022b</a>)</cite>, while others suggested aggregating attention across layers and heads to quantify input-output influence more systematically <cite class=\"ltx_cite ltx_citemacro_citep\">(Abnar &amp; Zuidema, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib2\" title=\"\">2020</a>; Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib101\" title=\"\">2021</a>; Chefer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib11\" title=\"\">2021</a>)</cite>.\nWhile some have raised concerns about whether attention reliably reflects which inputs are actually responsible for outputs <cite class=\"ltx_cite ltx_citemacro_citep\">(Jain &amp; Wallace, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib35\" title=\"\">2019</a>; Serrano &amp; Smith, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib78\" title=\"\">2019</a>; Bastings &amp; Filippova, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib9\" title=\"\">2020</a>)</cite>, others have proposed conditions under which attention can meaningfully explain model behavior <cite class=\"ltx_cite ltx_citemacro_citep\">(Wiegreffe &amp; Pinter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib93\" title=\"\">2019</a>)</cite>. More recent work highlights that attention aggregation may obscure localized, token-specific interactions <cite class=\"ltx_cite ltx_citemacro_citep\">(Modarressi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib57\" title=\"\">2023</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib100\" title=\"\">2023</a>; Oh &amp; Schuler, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib63\" title=\"\">2023</a>)</cite>, motivating hybrid approaches that combine attention with other XAI techniques, such as attribution methods <cite class=\"ltx_cite ltx_citemacro_citep\">(Modarressi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib56\" title=\"\">2022</a>)</cite>, or the use of attention as a regularization signal during interpretability-driven training <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib97\" title=\"\">2024</a>)</cite>.\nDespite the ongoing efforts, most research has focused on self-attention within encoders, with limited attention to feed-forward dynamics <cite class=\"ltx_cite ltx_citemacro_citep\">(Kobayashi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib41\" title=\"\">2024</a>)</cite> and even less to encoder-decoder models. A few studies have investigated attention in encoder-decoder architectures <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib62\" title=\"\">2021</a>)</cite>, including in machine translation <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib105\" title=\"\">2024</a>)</cite>, but cross-attention remains largely underexplored in the speech domain and has been\nabsent from the broader <span class=\"ltx_text ltx_inline-quote ltx_outerquote\">&#8220;attention as explanation&#8221;</span> debate in NLP. Our work seeks to bridge this gap by bringing cross-attention of S2T models into this broader conversation, aiming to assess\nwhether\nit can serve as a reliable explanation&#8211;and where its limitations emerge.\n</p>\n\n",
                "matched_terms": [
                    "across",
                    "model",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We assess the extent to which cross-attention scores (<math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math>) explain how the model looks at\ninput features when generating a token\nby comparing them to the saliency map on the input <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math>, obtained with the state-of-the art feature-attribution method for S2T, SPES <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "input",
                    "model",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, to assess whether cross-attention more accurately reflects how the decoder accesses encoded representations&#8211;rather than capturing the model&#8217;s full input-output behavior&#8211;we compare <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> with the encoder-output saliency map <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m2\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>H</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}</annotation></semantics></math>.\nBy analyzing how the correlation between <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> and <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m4\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>H</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}</annotation></semantics></math> deviates from that with <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m5\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math>,\nwe can indirectly quantify the impact of context mixing in the resulting explanations.\nA visual overview of this setup is provided in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S3.F1\" title=\"Figure 1 &#8227; 3 Methodology &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "explanations",
                    "correlation",
                    "crossattention",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In S2T models, the cross-attention mechanism enables each decoder token to integrate relevant portions of the encoded speech features, thereby conditioning generation on the entire input.</p>\n\n",
                "matched_terms": [
                    "input",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At each decoder layer <math alttext=\"\\ell\\in\\{1,\\ldots,L\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\ell\\in\\{1,\\ldots,L\\}</annotation></semantics></math>, cross-attention scores are computed via dot-product attention <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib28\" title=\"\">2014</a>)</cite> between the decoder&#8217;s current hidden states <math alttext=\"\\mathbf{B}^{(\\ell)}\\in\\mathbb{R}^{I\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><msup><mi>&#119809;</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{B}^{(\\ell)}\\in\\mathbb{R}^{I\\times D}</annotation></semantics></math> and the encoder outputs <math alttext=\"\\mathbf{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m3\" intent=\":literal\"><semantics><mi>&#119815;</mi><annotation encoding=\"application/x-tex\">\\mathbf{H}</annotation></semantics></math>. Specifically, the decoder states are linearly projected to queries <math alttext=\"\\mathbf{Q}^{(\\ell)}=\\mathbf{B}^{(\\ell)}\\mathbf{W}_{Q}^{(\\ell)}\\in\\mathbb{R}^{I\\times d_{k}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m4\" intent=\":literal\"><semantics><mrow><msup><mi>&#119824;</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><mrow><msup><mi>&#119809;</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#119830;</mi><mi>Q</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Q}^{(\\ell)}=\\mathbf{B}^{(\\ell)}\\mathbf{W}_{Q}^{(\\ell)}\\in\\mathbb{R}^{I\\times d_{k}}</annotation></semantics></math>, while the encoder outputs are projected to keys <math alttext=\"\\mathbf{K}^{(\\ell)}=\\mathbf{H}\\mathbf{W}_{K}^{(\\ell)}\\in\\mathbb{R}^{T^{\\prime}\\times d_{k}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m5\" intent=\":literal\"><semantics><mrow><msup><mi>&#119818;</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><msubsup><mi>&#119815;&#119830;</mi><mi>K</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{K}^{(\\ell)}=\\mathbf{H}\\mathbf{W}_{K}^{(\\ell)}\\in\\mathbb{R}^{T^{\\prime}\\times d_{k}}</annotation></semantics></math>\nusing learned projection matrices <math alttext=\"\\mathbf{W}_{Q}^{(\\ell)},\\mathbf{W}_{K}^{(\\ell)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m6\" intent=\":literal\"><semantics><mrow><msubsup><mi>&#119830;</mi><mi>Q</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>&#119830;</mi><mi>K</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{W}_{Q}^{(\\ell)},\\mathbf{W}_{K}^{(\\ell)}</annotation></semantics></math>. The resulting cross-attention matrix <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m7\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> is:</p>\n\n",
                "matched_terms": [
                    "computed",
                    "crossattention",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Extracting\nthe full set of scores provides a fine-grained view of how each output token in the generated hypothesis attends to the encoder&#8217;s representations across all layers and heads.\nTo derive a single layer-wise or head-wise attention distribution, we compute the mean of the attention matrices over a subset <math alttext=\"\\mathcal{S}\\subseteq\\{1,\\dots,L\\}\\times\\{1,\\dots,H\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>&#8838;</mo><mrow><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo rspace=\"0.055em\" stretchy=\"false\">}</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>H</mi><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}\\subseteq\\{1,\\dots,L\\}\\times\\{1,\\dots,H\\}</annotation></semantics></math> of layers and heads:</p>\n\n",
                "matched_terms": [
                    "layerwise",
                    "across",
                    "headwise"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By selecting different index sets <math alttext=\"\\mathcal{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><annotation encoding=\"application/x-tex\">\\mathcal{S}</annotation></semantics></math>, this formulation yields layer-wise, head-wise, or global averages. For example, setting <math alttext=\"\\mathcal{S}=\\{(\\ell,h):h=1,\\dots,H\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m3\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo>,</mo><mi>h</mi><mo rspace=\"0.278em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.278em\">:</mo><mrow><mi>h</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>H</mi></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}=\\{(\\ell,h):h=1,\\dots,H\\}</annotation></semantics></math> gives the average across heads at a given layer <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>; <math alttext=\"\\mathcal{S}=\\{(\\ell,h):\\ell=1,\\dots,L\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m5\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo>,</mo><mi>h</mi><mo rspace=\"0.278em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.278em\">:</mo><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}=\\{(\\ell,h):\\ell=1,\\dots,L\\}</annotation></semantics></math> averages across layers for head <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m6\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math>; and <math alttext=\"\\mathcal{S}=\\{1,\\dots,L\\}\\times\\{1,\\dots,H\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m7\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>=</mo><mrow><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo rspace=\"0.055em\" stretchy=\"false\">}</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>H</mi><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}=\\{1,\\dots,L\\}\\times\\{1,\\dots,H\\}</annotation></semantics></math> computes the full average.\nThis averaged attention provides a more aggregated view of the model&#8217;s attention patterns at a specific layer or attention head, summarizing how the model attends to the input speech over time.</p>\n\n",
                "matched_terms": [
                    "layerwise",
                    "across",
                    "average",
                    "model",
                    "averaged",
                    "input",
                    "‚Ñìell",
                    "headwise"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Let again <math alttext=\"\\mathbf{X}\\in\\mathbb{R}^{T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}\\in\\mathbb{R}^{T\\times F}</annotation></semantics></math> denote a mel-spectrogram input, where <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> is the number of time frames and <math alttext=\"F\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mi>F</mi><annotation encoding=\"application/x-tex\">F</annotation></semantics></math> the number of frequency bins, and <math alttext=\"\\mathbf{y}=(y_{0},y_{1},\\ldots,y_{I})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#119858;</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mn>0</mn></msub><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>y</mi><mi>I</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{y}=(y_{0},y_{1},\\ldots,y_{I})</annotation></semantics></math> the sequence of length <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math> of the autoregressively-generated tokens predicted based on the input and the previously generated tokens <math alttext=\"y_{&lt;i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><msub><mi>y</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">y_{&lt;i}</annotation></semantics></math>.\nTo attribute the prediction of each token <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math> to specific parts of the input spectrogram, we adopt SPES&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>,\nthe state-of-the-art feature-attribution method designed for autoregressive S2T modeling. SPES assigns a saliency score to each time-frequency\nelement\nof <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><mi>&#119831;</mi><annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation></semantics></math>, producing a saliency map <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{X}\\in\\mathbb{R}^{T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m9\" intent=\":literal\"><semantics><mrow><msubsup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>i</mi><mi>X</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{X}\\in\\mathbb{R}^{T\\times F}</annotation></semantics></math> for each token <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m10\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math>, where higher values indicate greater relevance of the corresponding time-frequency regions. SPES operates by clustering spectrogram elements based on energy profiles&#8211;capturing acoustic\ncomponents such as harmonics and background noise&#8211;and estimating the influence of each cluster by perturbing it with probability <math alttext=\"p_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m11\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">p_{X}</annotation></semantics></math>, repeated <math alttext=\"N_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m12\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">N_{X}</annotation></semantics></math> times. The effect of each perturbation&#8211;i.e., masking parts of the input with 0 values&#8211;is measured by computing the Kullback-Leibler (KL) divergence <cite class=\"ltx_cite ltx_citemacro_citep\">(Kullback &amp; Leibler, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib44\" title=\"\">1951</a>)</cite> between the model&#8217;s original output distribution <math alttext=\"P(y_{i}\\mid y_{&lt;i},\\mathbf{X})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m13\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>&#8739;</mo><mrow><msub><mi>y</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><mo>,</mo><mi>&#119831;</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(y_{i}\\mid y_{&lt;i},\\mathbf{X})</annotation></semantics></math> and the distribution resulting from the perturbed input <math alttext=\"P^{(n)}(y_{i}\\mid y_{&lt;i},\\tilde{\\mathbf{X}}^{(n)})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m14\" intent=\":literal\"><semantics><mrow><msup><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>&#8739;</mo><mrow><msub><mi>y</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><mo>,</mo><msup><mover accent=\"true\"><mi>&#119831;</mi><mo>~</mo></mover><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P^{(n)}(y_{i}\\mid y_{&lt;i},\\tilde{\\mathbf{X}}^{(n)})</annotation></semantics></math> at time <math alttext=\"n\\in\\{1,\\ldots,N_{X}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m15\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>N</mi><mi>X</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">n\\in\\{1,\\ldots,N_{X}\\}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "input",
                    "values",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further examine the influence of the encoder&#8217;s internal representations on the prediction of each output token.\nLet again <math alttext=\"\\mathbf{H}=\\mathrm{Encoder}(\\mathbf{X})\\in\\mathbb{R}^{T^{\\prime}\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119815;</mi><mo>=</mo><mrow><mi>Encoder</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119831;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{H}=\\mathrm{Encoder}(\\mathbf{X})\\in\\mathbb{R}^{T^{\\prime}\\times D}</annotation></semantics></math> denote the sequence of encoder hidden states or <span class=\"ltx_text ltx_font_italic\">encoder output</span>, where <math alttext=\"T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><msup><mi>T</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">T^{\\prime}</annotation></semantics></math> is the subsampled time dimension and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> is the hidden dimension.\nTo assess the importance of the encoder output representations, we compute token-specific saliency maps <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{H}\\in\\mathbb{R}^{T^{\\prime}\\times 1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m4\" intent=\":literal\"><semantics><mrow><msubsup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>i</mi><mi>H</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{H}\\in\\mathbb{R}^{T^{\\prime}\\times 1}</annotation></semantics></math>, where each entry reflects the contribution of the corresponding hidden state to the generation of <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m5\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math>.\nEach encoder state <math alttext=\"\\mathbf{H}=(\\mathbf{h}_{1},\\ldots,\\mathbf{h}_{T^{\\prime}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#119815;</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119841;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119841;</mi><msup><mi>T</mi><mo>&#8242;</mo></msup></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{H}=(\\mathbf{h}_{1},\\ldots,\\mathbf{h}_{T^{\\prime}})</annotation></semantics></math> is perturbed&#8211;i.e., all its features are set to 0&#8211;independently with probability <math alttext=\"p_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m7\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">p_{H}</annotation></semantics></math>, and the process is repeated <math alttext=\"N_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m8\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">N_{H}</annotation></semantics></math> times. The KL divergence is computed for each perturbation between the original and perturbed output distributions:</p>\n\n",
                "matched_terms": [
                    "computed",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since our focus lies in the temporal dynamics of the input <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mi>&#119831;</mi><annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation></semantics></math>, we aggregate the 3D saliency scores <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\\in\\mathbb{R}^{I\\times T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\\in\\mathbb{R}^{I\\times T\\times F}</annotation></semantics></math> across the frequency dimension and downsample the time axis to produce a compressed representation <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\\in\\mathbb{R}^{I\\times T^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>T</mi><mo>&#8242;</mo></msup></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\\in\\mathbb{R}^{I\\times T^{\\prime}}</annotation></semantics></math> compatible with the cross-attention granularity, where <math alttext=\"T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m4\" intent=\":literal\"><semantics><msup><mi>T</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">T^{\\prime}</annotation></semantics></math> corresponds to the number of encoder time steps. The aggregation is performed by taking the maximum saliency value over the frequency axis and within each corresponding time window. The resulting saliency map of each token <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{X}\\in\\mathbb{R}^{T^{\\prime}\\times 1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><msubsup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>i</mi><mi>X</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{X}\\in\\mathbb{R}^{T^{\\prime}\\times 1}</annotation></semantics></math> reflects the temporal relevance of the input spectrogram with respect to the generation of token <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m6\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math>. Complementary experiments on the choice of the aggregation function are presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A1\" title=\"Appendix A Effect of Aggregation Functions on Input Explanations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.\nBoth <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m7\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> and <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m8\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math> representations are normalized before computing the correlation scores, and the beginning and end of sentence are removed as they are not relevant for the analysis.\nThe <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m9\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> matrix is normalized frame-wise using mean-variance normalization to mitigate the impact of potential attention sinks at initial or final tokens&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib13\" title=\"\">2019</a>; Ferrando et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib19\" title=\"\">2022a</a>; Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib65\" title=\"\">2023a</a>; Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib96\" title=\"\">2024</a>)</cite> on the correlation computation.\nBoth <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m10\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math> and <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m11\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>H</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}</annotation></semantics></math> are normalized along the token dimension using the strategy proposed by <cite class=\"ltx_cite ltx_citemacro_citet\">Fucci et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>, as saliency scores can vary widely across tokens due to differences in the original output distributions used to compute the KL divergence.</p>\n\n",
                "matched_terms": [
                    "input",
                    "across",
                    "correlation",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following prior work on cross-attention matrices <cite class=\"ltx_cite ltx_citemacro_citep\">(Vig &amp; Belinkov, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib86\" title=\"\">2019</a>)</cite> and explainable AI <cite class=\"ltx_cite ltx_citemacro_citep\">(Eberle et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib18\" title=\"\">2023</a>)</cite>, we use Pearson correlation\nto quantify the relationship between cross-attention scores and saliency-based explanations.\nPearson correlation is preferred over Kendall and Spearman because saliency scores are continuous, and their magnitude&#8211;not just ranking&#8211;is crucial. Rank-based measures are overly sensitive to small fluctuations among non-important features with near-zero scores, while Pearson better captures whether features are identified as important (high score) or not (low score).\nSpecifically, given the two representations <math alttext=\"\\text{{\\color[rgb]{0.75390625,0.390625,0.9609375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.75390625,0.390625,0.9609375}$\\mathbf{CA}$}},\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}\\in\\mathbb{R}^{I\\times T^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><mo>,</mo><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>T</mi><mo>&#8242;</mo></msup></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.75390625,0.390625,0.9609375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.75390625,0.390625,0.9609375}$\\mathbf{CA}$}},\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}\\in\\mathbb{R}^{I\\times T^{\\prime}}</annotation></semantics></math>, we compute the Pearson correlation coefficient <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math> to assess the similarity of their attribution patterns across output tokens and time steps. We first flatten each matrix into a vector of size <math alttext=\"I\\cdot T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8901;</mo><msup><mi>T</mi><mo>&#8242;</mo></msup></mrow><annotation encoding=\"application/x-tex\">I\\cdot T^{\\prime}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "across",
                    "correlation",
                    "œÅrho",
                    "low",
                    "pearson",
                    "high",
                    "explanations",
                    "between",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Then, the Pearson correlation coefficient <math alttext=\"\\rho\\in[-1,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><mi>&#961;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\rho\\in[-1,1]</annotation></semantics></math> is computed as:</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "computed",
                    "pearson"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This scalar value quantifies the linear relationship between the two saliency maps, with values closer to 1 indicating a strong positive correlation, and values near 0 indicating no correlation.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "values",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To avoid potential data contamination issues <cite class=\"ltx_cite ltx_citemacro_citep\">(Sainz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib76\" title=\"\">2023</a>)</cite>, we train from scratch a monolingual ASR model and two-sized multitask (ASR and ST) and multilingual (English and Italian) models. Details about training data and process are presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A2\" title=\"Appendix B Training Settings &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.\nBeing the only non-synthetic dataset supporting both tasks and language directions, we select EuroParl-ST <cite class=\"ltx_cite ltx_citemacro_citep\">(Iranzo-S&#225;nchez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib33\" title=\"\">2020</a>)</cite> as the test set for our analyses. The test set covers both <span class=\"ltx_text ltx_font_italic\">en</span> and <span class=\"ltx_text ltx_font_italic\">it</span> ASR, and <span class=\"ltx_text ltx_font_italic\">en-it</span> and <span class=\"ltx_text ltx_font_italic\">it-en</span> ST.\nThe <span class=\"ltx_text ltx_font_italic\">it/it-en</span> section consists of 1,686 segments, for a total of approximately 6 hours of audio, while the <span class=\"ltx_text ltx_font_italic\">en/en-it</span> section contains 1,130 segments, for a total of approximately 3 hours of audio.</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "asr",
                    "model",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The models analyzed in the paper are all composed of a Conformer encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib29\" title=\"\">2020</a>)</cite> and a Transformer decoder, as Conformer is the current state-of-the-art architecture for S2T processing <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib30\" title=\"\">2021</a>; Srivastava et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib81\" title=\"\">2022</a>; Li &amp; Doddipatla, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib48\" title=\"\">2023</a>)</cite>.\nThe monolingual ASR model (<span class=\"ltx_text ltx_font_typewriter\">base</span>) is composed of 12 encoder layers and 6 decoder layers. Each layer has 8 attention heads, 512 as embedding dimension, and FFNs dimension of 2,048. The vocabulary is built using a SentencePiece unigram model <cite class=\"ltx_cite ltx_citemacro_citep\">(Kudo &amp; Richardson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib42\" title=\"\">2018</a>)</cite> with size 8,000 trained on <span class=\"ltx_text ltx_font_italic\">en</span> transcripts. The resulting number of parameters is 125M.\nThe multitask and multilingual models are of two sizes, <span class=\"ltx_text ltx_font_typewriter\">small</span> and <span class=\"ltx_text ltx_font_typewriter\">large</span>, the first having 12 encoder layers and 6 decoder layers and the latter having 24 encoder layers and 12 decoder layers. In both sizes, each layer has 16 attention heads, an embedding dimension of 1,024, and an FFN dimension of 4,096. The vocabulary is built using a SentencePiece unigram model with size 16,000 trained on <span class=\"ltx_text ltx_font_italic\">en</span> and <span class=\"ltx_text ltx_font_italic\">it</span> transcripts. Two extra tokens&#8211;<span class=\"ltx_text ltx_font_typewriter\">&lt;lang:en&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;lang:it&gt;</span>&#8211;are added to indicate whether the target text is in <span class=\"ltx_text ltx_font_italic\">en</span> or <span class=\"ltx_text ltx_font_italic\">it</span>. The resulting number of parameters is 474M for the <span class=\"ltx_text ltx_font_typewriter\">small</span> model and 878M for the <span class=\"ltx_text ltx_font_typewriter\">large</span> model.\nIn all models, the Conformer encoder is preceded by two 1D convolutional layers with stride 2 and kernel size 5, resulting in a fixed subsampling factor <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> of 4. The kernel size of the Conformer convolutional module is 31 for both the point- and depth-wise convolutions. The input audio is represented by 80 Mel-filterbank features extracted every 10 ms with a window of 25 ms.</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "base",
                    "model",
                    "asr",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Hypothesis and Cross-Attention Generation.</span>\nFor the hypothesis generation, we use beam search with a beam size of 5 and a no-repeat n-gram size of 5. The attention scores are extracted from layers or heads during the output generation. The ASR and ST quality scores of the hypotheses are presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A3\" title=\"Appendix C Quality Metrics for the Reported Models &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nThe inference is performed using a single NVIDIA A40 GPU (40GB RAM) with a batch size of 40,000 tokens and takes <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>2.5 minutes for <span class=\"ltx_text ltx_font_typewriter\">base</span>, <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>3-5.5 minutes for <span class=\"ltx_text ltx_font_typewriter\">small</span>, and <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>3-6.5 for <span class=\"ltx_text ltx_font_typewriter\">large</span>, depending on the source language.</p>\n\n",
                "matched_terms": [
                    "base",
                    "crossattention",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Explanation Heatmaps Generation.</span>\nFollowing the best configuration obtained in SPES <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>, we adopt the Morphological Fragmental Perturbation Pyramid <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib99\" title=\"\">2021</a>)</cite> for clustering, which relies on Simple Linear Iterative Clustering or SLIC <cite class=\"ltx_cite ltx_citemacro_citep\">(Achanta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib3\" title=\"\">2012</a>)</cite>, a k-means-based algorithm that groups elements according to spectral patterns. We use the default parameters; the threshold length in seconds is\n7.50s, the\n<a class=\"ltx_ref ltx_href\" href=\"https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.slic\" title=\"\">SLIC</a>\nsigma is 0, the compactness is 0.1, and the number of patches per second for the MFPP technique is [400, 500, 600].\nFor the choice of <math alttext=\"p_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">p_{X}</annotation></semantics></math> and <math alttext=\"N_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">N_{X}</annotation></semantics></math>, we refer to the parameters used in <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>, setting <math alttext=\"p_{X}=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>X</mi></msub><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">p_{X}=0.5</annotation></semantics></math> and <math alttext=\"N_{X}=20,000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><msub><mi>N</mi><mi>X</mi></msub><mo>=</mo><mrow><mn>20</mn><mo>,</mo><mn>000</mn></mrow></mrow><annotation encoding=\"application/x-tex\">N_{X}=20,000</annotation></semantics></math>. The quality of the input explanations is presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A3\" title=\"Appendix C Quality Metrics for the Reported Models &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>. For the choice of <math alttext=\"p_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m5\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">p_{H}</annotation></semantics></math> and <math alttext=\"N_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m6\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">N_{H}</annotation></semantics></math>, we use the same number of iterations of <math alttext=\"N_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m7\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">N_{X}</annotation></semantics></math>, i.e., <math alttext=\"N_{H}=20,000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m8\" intent=\":literal\"><semantics><mrow><msub><mi>N</mi><mi>H</mi></msub><mo>=</mo><mrow><mn>20</mn><mo>,</mo><mn>000</mn></mrow></mrow><annotation encoding=\"application/x-tex\">N_{H}=20,000</annotation></semantics></math>, while the optimal occlusion probability <math alttext=\"p_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m9\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">p_{H}</annotation></semantics></math> is determined over the dev set, resulting in <math alttext=\"p_{H}=0.7\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m10\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>H</mi></msub><mo>=</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">p_{H}=0.7</annotation></semantics></math>, whose experiments are reported in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A4\" title=\"Appendix D Effect of Occlusion Probability on Encoder Output Explanations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.\nThe inference is performed using a single NVIDIA A40 GPU (40GB RAM)\nand takes <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m11\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>27 hours for <span class=\"ltx_text ltx_font_typewriter\">base</span>, <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m12\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>3-4 days for <span class=\"ltx_text ltx_font_typewriter\">small</span> and <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m13\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>6-8 days for <span class=\"ltx_text ltx_font_typewriter\">large</span>, depending on the source language.</p>\n\n",
                "matched_terms": [
                    "base",
                    "explanations",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Correlation Computation.</span>\nThe Pearson <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">r</annotation></semantics></math> correlation score is computed using the <span class=\"ltx_text ltx_font_typewriter\">scipy</span> implementation<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html\" title=\"\">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html</a></span></span></span> and averaging across samples in the test set.</p>\n\n",
                "matched_terms": [
                    "across",
                    "correlation",
                    "computed",
                    "pearson"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we compare <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> with input saliency maps <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m3\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></sup>, which serve as an external reference for measuring input relevance.\nSpecifically, in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS1\" title=\"5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a>, we analyze the\n<span class=\"ltx_text ltx_font_typewriter\">base</span> model across all levels of granularity. Then, in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS2\" title=\"5.1.2 Multitask and Multilingual Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.2</span></a>, we extend the analysis to additional models (<span class=\"ltx_text ltx_font_typewriter\">small</span> and <span class=\"ltx_text ltx_font_typewriter\">large</span>), languages (<span class=\"ltx_text ltx_font_italic\">en</span> and <span class=\"ltx_text ltx_font_italic\">it</span>), and tasks (ASR and ST).</p>\n\n",
                "matched_terms": [
                    "ùêíùêåmathbfsmxx",
                    "across",
                    "base",
                    "model",
                    "asr",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At the individual head level, correlations with saliency maps are generally low. This suggests that attention heads, when taken in isolation, only partially capture the model&#8217;s dependency on the input and often encode noisy or inconsistent relevance signals. However, not all heads are equal: some, especially in the upper layers (layers 4-6), exhibit relatively stronger correlations. Notably, <span class=\"ltx_text ltx_font_bold\">averaging across heads consistently outperforms selecting individual heads</span>, suggesting that, despite head-level sparsity and weak individual correlations, the collective information captured across heads reflects input relevance more effectively.\nMoving from heads to layers, we find a clearer picture. Averaging attention scores across all heads within each layer boosts correlation substantially, with layer 6 standing out as the most aligned with the saliency maps. This is followed closely by layer 5 and the average across all layers, indicating that the <span class=\"ltx_text ltx_font_bold\">last layers exhibit the highest alignment with input relevance</span>. These results reinforce the idea that deeper layers encode higher-level semantic or task-relevant features, a trend previously observed in Transformer-based models <cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib13\" title=\"\">2019</a>)</cite>.\nInterestingly, while averaging across heads improves alignment, averaging across both heads and layers does not yield the overall best result, even if values are close. This indicates that not all layers contribute equally and that indiscriminate aggregation can dilute the relevance signal.</p>\n\n",
                "matched_terms": [
                    "across",
                    "correlation",
                    "low",
                    "average",
                    "highest",
                    "indicates",
                    "input",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, the results show that appropriately selected and aggregated cross-attention scores exhibit only a <span class=\"ltx_text ltx_font_italic\">moderate</span> to <span class=\"ltx_text ltx_font_italic\">strong</span> correlation with input saliency maps, reaching values up to 0.588. This provides an initial indication of the\nlimited explanatory power\nof cross-attention weights, which we further examine under multilingual and multitask conditions in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS2\" title=\"5.1.2 Multitask and Multilingual Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "input",
                    "correlation",
                    "crossattention",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the impact of multilingual and multitask training on the correlation between cross-attention scores and saliency maps, we evaluate the <span class=\"ltx_text ltx_font_typewriter\">small</span> and <span class=\"ltx_text ltx_font_typewriter\">large</span> models. Layer-wise results are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.T2\" title=\"Table 2 &#8227; 5.1.2 Multitask and Multilingual Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, while head-wise results are omitted due to the noisy behavior observed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS1\" title=\"5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "layerwise",
                    "correlation",
                    "between",
                    "crossattention",
                    "headwise"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across all configurations, we observe that <span class=\"ltx_text ltx_font_italic\">en</span> ASR yields the highest correlation values, outperforming even the monolingual <span class=\"ltx_text ltx_font_typewriter\">base</span> model (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS1\" title=\"5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a>). This suggests that large-scale multilingual training enhances the alignment between cross-attention and saliency maps, likely due to the improved generalization capacity of the model. In contrast, <span class=\"ltx_text ltx_font_italic\">en-it</span> ST shows a drop in correlation, which is expected given the increased complexity of ST compared to ASR.\nWhen considering <span class=\"ltx_text ltx_font_italic\">it</span> as the source language, we observe a similar pattern: ASR correlations are consistently higher than ST, yet remain below their <span class=\"ltx_text ltx_font_italic\">en</span> counterparts. This discrepancy aligns with the data distribution in training, where <span class=\"ltx_text ltx_font_italic\">en</span> accounts for 84% of the data versus 16% for <span class=\"ltx_text ltx_font_italic\">it</span>, resulting in more robust representations for <span class=\"ltx_text ltx_font_italic\">en</span>.\nAt the layer level, we find consistent evidence that the last decoder layers yield stronger correlations, reaffirming the trends observed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS1\" title=\"5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a>.\nThe specific optimal layer varies with model size: layer 5 performs best in <span class=\"ltx_text ltx_font_typewriter\">small</span>, while layers 8-10 achieve the highest correlations in <span class=\"ltx_text ltx_font_typewriter\">large</span>.\nNevertheless, correlation values across the last\nlayers remain very close, suggesting that their cross-attention scores\nprovide the most robust alignment with saliency maps across both tasks and languages. This trend is further supported by downstream application results, where the final layers have shown the best token-level performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib65\" title=\"\">2023a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib66\" title=\"\">b</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib91\" title=\"\">2024a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "across",
                    "correlation",
                    "base",
                    "model",
                    "highest",
                    "asr",
                    "between",
                    "crossattention",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Averaging attention scores across layers further improves the correlation with saliency maps in\nalmost\nall configurations. The only exceptions are <span class=\"ltx_text ltx_font_italic\">en</span> and <span class=\"ltx_text ltx_font_italic\">it</span> ASR in <span class=\"ltx_text ltx_font_typewriter\">small</span>, where selective-layer extraction offers a marginal improvement (0.006 for English, 0.001 for Italian).\nTherefore, similarly to what we observed in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS1\" title=\"5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a>, averaging attention across heads and layers consistently yields the best or near-best correlation with <span class=\"ltx_text ltx_font_italic\">moderate</span> to <span class=\"ltx_text ltx_font_italic\">strong</span> correlation with input saliency maps, even considering large-scale models trained in multitask and multilingual settings.\nNonetheless, this alignment accounts for only 49-63% of the total input relevance, indicating that <span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#88185B;\">cross-attention falls short of fully accounting for the S2T models&#8217; behavior</span>.\nSince this limitation may stem from the phenomenon of context mixing, in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS2\" title=\"5.2 What Is the Impact of Context Mixing? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a> we analyze the correlation between cross-attention and encoder output&#8211;representations that have already undergone transformation by the encoder&#8211;to better isolate the true explanatory power of cross-attention.</p>\n\n",
                "matched_terms": [
                    "across",
                    "correlation",
                    "english",
                    "asr",
                    "between",
                    "input",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1\" title=\"5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> focused on input relevance, we now investigate whether <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math>\naligns more closely with encoder output saliency maps.\nA higher correlation with encoder output representations would support the hypothesis that discrepancies between cross-attention and input saliency arise from context mixing, due to the reorganization of information within the encoder.\nTo this end,\nwe compare <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> with encoder output saliency maps <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m4\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></sup>, which attribute relevance to the encoder hidden states for each output token (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S3.SS2\" title=\"3.2 Feature Attribution for Speech-to-Text &#8227; 3 Methodology &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>). Layer-wise results for all models are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.T3\" title=\"Table 3 &#8227; 5.2 What Is the Impact of Context Mixing? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "layerwise",
                    "correlation",
                    "between",
                    "input",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Even when examining encoder output representations, we observe trends consistent with those identified in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1\" title=\"5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>. Specifically, when averaged across decoder layers, cross-attention scores consistently provide the strongest or nearly optimal correlation with saliency maps, with the last decoder layers offering more representative explanations than the first ones across all models. As expected, correlation with encoder output representations consistently yields higher scores than those obtained from input representations, with absolute <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math> differences ranging from 0.03 to 0.18, quantifying the influence of context mixing effects to 6.6-16.7%.\nThe increased correlation is also visually evident in the example shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.F2\" title=\"Figure 2 &#8227; 5.2 What Is the Impact of Context Mixing? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, where <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> aligns more closely with the relevance scores from <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m4\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></sup> than with those from <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m5\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m6\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></sup>.\nHowever, despite being unaffected by context mixing, the correlation between <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m7\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> and <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m8\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m9\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></sup> remains limited&#8211;capturing only 52-75% of the relevance.\n<span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#88185B;\">This\ngap underscores the inherent limitations in relying solely on cross-attention as an explanation mechanism, reinforcing its role as an informative but <span class=\"ltx_text ltx_ulem_uline\">incomplete</span> proxy for explainability in S2T models</span>&#8211;not only for input-level saliency, but even at the encoder-output level, where cross-attention directly operates.</p>\n\n",
                "matched_terms": [
                    "ùêíùêåmathbfsmxx",
                    "across",
                    "correlation",
                    "œÅrho",
                    "averaged",
                    "explanations",
                    "between",
                    "input",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Discussion.</span>\nOur results demonstrate that, although <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> scores moderately correlate with aggregated <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m2\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math> (with a correlation peaking around 0.45-0.55 in the best-performing settings), they consistently fall short of capturing the full input relevance&#8211;even when context mixing effects are factored out. To directly assess explanation quality, we compute the deletion metric (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A1\" title=\"Appendix A Effect of Aggregation Functions on Input Explanations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>) on the <span class=\"ltx_text ltx_font_typewriter\">base</span> model, finding that <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> achieves 41.2, compared to 52.9 for frequency-aggregated <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m4\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math> and 91.3 for full-resolution maps.\nThis gap underscores that <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m5\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> discards fine-grained time-frequency cues and produces weaker attributions, even under identical aggregation. As further discussed in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A6\" title=\"Appendix F Limitations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>, our analysis is bounded by the use of SPES as the attribution baseline, but the consistent underperformance across correlation and deletion confirms that <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m6\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> offers, at best, an incomplete picture of model behavior. These results also carry implications for downstream tasks. In applications such as timestamp prediction, prior work often relies on attention from a single decoder layer or head <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib91\" title=\"\">2024a</a>; Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib65\" title=\"\">2023a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib66\" title=\"\">b</a>; Zusag et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib106\" title=\"\">2024</a>)</cite>. Our analysis suggests that averaging across layers and, especially, across heads provides a closer match to saliency behavior and could improve these methods.\nBuilding on past success with attention regularization in ASR (e.g., imposing monotonicity as in <cite class=\"ltx_cite ltx_citemacro_citet\">Zhao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib104\" title=\"\">2020</a></cite>), similar training-time strategies&#8211;such as auxiliary losses that align attention with saliency&#8211;could further benefit downstream applications, enhancing both interpretability and task performance.\nIn summary, <span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"--ltx-fg-color:#88185B;\"><math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m7\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> should not be treated as a stand-alone XAI tool</span>. It provides lightweight cues that may complement attribution-based methods, but it cannot replace them. Reframing <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m8\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> as an auxiliary rather than a proxy recalibrates expectations and grounds future work on more faithful and effective approaches to explainability in S2T models.</p>\n\n",
                "matched_terms": [
                    "across",
                    "correlation",
                    "base",
                    "model",
                    "asr",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conclusions.</span>\nWe presented the first systematic analysis of cross-attention in S2T through the lens of explainable AI, comparing it to saliency maps across tasks, languages, and model scales. Cross-attention moderately to strongly aligns with saliency&#8211;especially when averaged across heads and layers&#8211;but captures only about half of the input relevance. Even when disentangling the effect of context mixing by analyzing encoder outputs, it explains just 52-75% of saliency. This gap reveals intrinsic limits of cross-attention as an explanation mechanism: it offers informative cues but only a partial view of the factors driving S2T predictions.</p>\n\n",
                "matched_terms": [
                    "across",
                    "model",
                    "averaged",
                    "input",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Broader Implications.</span> Explainability in S2T systems has tangible implications for AI transparency, especially in high-stakes settings such as healthcare, legal transcription, and educational accessibility. Our findings provide insights about the usage of cross-attention as a tool for identifying how models relate output predictions to input regions, which can support auditing, debugging, and fair deployment. However, there is a risk that misinterpreted attention visualizations may be overtrusted by non-expert users, reinforcing false confidence in system behavior <cite class=\"ltx_cite ltx_citemacro_citep\">(Rudin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib75\" title=\"\">2019</a>)</cite>. Moreover, our language choices and focus on high-resource speech still reflect global imbalances in language technology access <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib36\" title=\"\">2020</a>)</cite>. Future work should extend this analysis to low-resource and underrepresented languages to promote broader inclusion.</p>\n\n",
                "matched_terms": [
                    "input",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To properly obtain input-level explanations comparable with the dimensions of cross-attention scores (i.e., making <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\\in\\mathbb{R}^{I\\times T^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m1\" intent=\":literal\"><semantics><mrow><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>T</mi><mo>&#8242;</mo></msup></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\\in\\mathbb{R}^{I\\times T^{\\prime}}</annotation></semantics></math>), we explore the effect of different aggregation strategies over the time and frequency dimensions.</p>\n\n",
                "matched_terms": [
                    "explanations",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A1.T4\" title=\"Table 4 &#8227; Appendix A Effect of Aggregation Functions on Input Explanations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> reports the deletion scores and, for completeness, the Pearson <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m1\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math> correlations between the cross attention scores <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> and the saliency maps <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m4\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></sup> for the representations aggregated following three strategies:</p>\n\n",
                "matched_terms": [
                    "ùêíùêåmathbfsmxx",
                    "œÅrho",
                    "pearson",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2D average pooling</span>, applied over the entire time-frequency plane to obtain its <span class=\"ltx_text ltx_font_italic\">average</span> value and computed through <span class=\"ltx_text ltx_font_typewriter\">adaptive_avg_pool2d<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">5</span></span><a class=\"ltx_ref ltx_url\" href=\"https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.adaptive_avg_pool2d.html\" title=\"\">https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.adaptive_avg_pool2d.html</a></span></span></span></span>;</p>\n\n",
                "matched_terms": [
                    "computed",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2-step pooling (1D maximum + 1D average)</span>, where max pooling is applied along the frequency axis, followed by averaging over time, and computed by applying <span class=\"ltx_text ltx_font_typewriter\">max_pool1d<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">6</span></span><a class=\"ltx_ref ltx_url\" href=\"https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.max_pool1d.html\" title=\"\">https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.max_pool1d.html</a></span></span></span></span> and <span class=\"ltx_text ltx_font_typewriter\">avg_pool1d</span>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.avg_pool1d.html\" title=\"\">https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.avg_pool1d.html</a></span></span></span> respectively;</p>\n\n",
                "matched_terms": [
                    "computed",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The aggregation functions were selected to contrast methods that either isolate the most relevant features (with maximum pooling) or represent their mean relevance (with average pooling). Similarly, the 2-step approach has been tried to first isolate relevance patterns in the frequency domain, a dimension that is not present in cross-attention representation, and then average across the time dimension to match the downsampled time resolution of the cross-attention scores.</p>\n\n",
                "matched_terms": [
                    "across",
                    "average",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Among the tested methods, we observe that the 2D maximum pooling aggregation (2D max) yields the best quality explanations, obtaining the highest deletion score, while the 2D average pooling (2D avg) is the worst, with the lowest deletion score. Looking at the correlations, we notice that they follow the same trend of deletion scores, with the 2D max yielding the best <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m1\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math>. In particular, 2D avg consistently has the lowest correlations compared to the 2D max, particularly in the last layers (e.g., 0.457 against 0.572 at layer 5). Regarding the 2-step pooling approach, we not only observe an improved deletion score but also better correlation scores compared to 2D avg, especially from layer 3 onward, approaching the best performance with a layer-average correlation of 0.565. Nevertheless, the explanation quality is still lower compared to 2D max (i.e., 55.18 against 57.04), which also achieves the highest correlations at nearly every layer, peaking at 0.582 in layer 6, and yielding the best overall correlation among the averaged cross-attention across layers (i.e., 0.572).</p>\n\n",
                "matched_terms": [
                    "across",
                    "correlation",
                    "œÅrho",
                    "average",
                    "averaged",
                    "highest",
                    "explanations",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the monolingual ASR model, we leverage the speech-to-text English data available for the IWSLT 2024 evaluation campaign (offline task),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://iwslt.org/2024/offline\" title=\"\">https://iwslt.org/2024/offline</a></span></span></span> namely: CommonVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Ardila et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib4\" title=\"\">2020</a>)</cite>, CoVoST v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib90\" title=\"\">2021b</a>)</cite>, Europarl-ST <cite class=\"ltx_cite ltx_citemacro_citep\">(Iranzo-S&#225;nchez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib33\" title=\"\">2020</a>)</cite>, LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib64\" title=\"\">2015</a>)</cite>, MuST-C v1 <cite class=\"ltx_cite ltx_citemacro_citep\">(Di&#160;Gangi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib17\" title=\"\">2019</a>)</cite>, TEDLIUM v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Hernandez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib31\" title=\"\">2018</a>)</cite>, and VoxPopuli ASR <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib89\" title=\"\">2021a</a>)</cite>. The resulting training set is about 3k hours of speech.</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "asr",
                    "model",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the multitask (ASR and ST) multilingual large-scale models, we leverage more than 150k hours of open-source speech<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>Speech, transcripts, and translations released under an open-source license such as CC-0 and CC-BY 4.0.</span></span></span> in English (<span class=\"ltx_text ltx_font_italic\">en</span>) and Italian (<span class=\"ltx_text ltx_font_italic\">it</span>), namely: CommonVoice, CoVoST v2, FLEURS <cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib14\" title=\"\">2023</a>)</cite>, MOSEL <cite class=\"ltx_cite ltx_citemacro_citep\">(Gaido et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib23\" title=\"\">2024a</a>)</cite>, MLS <cite class=\"ltx_cite ltx_citemacro_citep\">(Pratap et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib71\" title=\"\">2020</a>)</cite>, and YouTube-Commons<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hf.co/datasets/PleIAs/YouTube-Commons\" title=\"\">https://hf.co/datasets/PleIAs/YouTube-Commons</a></span></span></span> (from which 14.2k hours of <span class=\"ltx_text ltx_font_italic\">en</span> and 1.8k for <span class=\"ltx_text ltx_font_italic\">it</span> have been extracted). For datasets missing the translations, we generated them using <span class=\"ltx_text ltx_font_typewriter\">MADLAD-400 3B-MT</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Kudugunta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib43\" title=\"\">2023</a>)</cite>. This setting allows us to verify our analysis with a large-scale setting similar to the scale of a popular model such as OWSM <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib69\" title=\"\">2023</a>)</cite> and 2 times that of NVIDIA Canary <cite class=\"ltx_cite ltx_citemacro_citep\">(Puvvada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib72\" title=\"\">2024</a>)</cite> while having complete control of data used during training, ensuring that data contamination issues are avoided completely.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "model",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train all models using a combination of three losses: <em class=\"ltx_emph ltx_font_italic\">i)</em> a label-smoothed cross-entropy loss (<math alttext=\"\\mathcal{L}_{\\text{CE}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>CE</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{CE}}</annotation></semantics></math>) applied to the decoder output using the target text as the reference (transcripts for ASR and translations for ST), <em class=\"ltx_emph ltx_font_italic\">ii)</em> a CTC loss <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib27\" title=\"\">2006</a>)</cite> computed using transcripts as reference (<math alttext=\"\\mathcal{L}_{\\text{CTCsrc}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>CTCsrc</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{CTCsrc}}</annotation></semantics></math>) on the output of the 8<sup class=\"ltx_sup\">th</sup> encoder layer for <span class=\"ltx_text ltx_font_typewriter\">base</span> and <span class=\"ltx_text ltx_font_typewriter\">small</span> and the 16<sup class=\"ltx_sup\">th</sup> for <span class=\"ltx_text ltx_font_typewriter\">medium</span>, <em class=\"ltx_emph ltx_font_italic\">iii)</em> a CTC loss on the final encoder output (<math alttext=\"\\mathcal{L}_{\\text{CTCtgt}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>CTCtgt</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{CTCtgt}}</annotation></semantics></math>) applied to predict the target text <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib98\" title=\"\">2023</a>)</cite>.\nThe final loss is the weighted sum of the above-mentioned losses:</p>\n\n",
                "matched_terms": [
                    "base",
                    "computed",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The monolingual ASR <span class=\"ltx_text ltx_font_typewriter\">base</span> model is trained on all 3k hours of ASR data for 200k steps using Noam as the learning rate scheduler <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib85\" title=\"\">2017</a>)</cite> with a peak of 2e-3 and 25,000 warm-up steps.</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "base",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The multitask and multilingual models are trained using a two-stage approach, where the model is pre-trained first on ASR data only (ASR pre-training) and then trained on both ASR and ST data (ASR+ST training). For the ASR pre-training, the learning rate scheduler adopted for the <span class=\"ltx_text ltx_font_typewriter\">small</span> model is the same as the <span class=\"ltx_text ltx_font_typewriter\">base</span> model.\nFor the <span class=\"ltx_text ltx_font_typewriter\">medium</span> model, we adopted a piece-wise warm-up on the Noam scheduler to avoid divergence issues <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib70\" title=\"\">2024</a>)</cite>, with the learning rate first increasing linearly to 2e-5 for 25k steps and then to 2e-4 for an additional 25k steps, followed by the standard inverse square root function.\nFor the ASR+ST training, we sample the ASR target with probability 0.5 and use the ST target otherwise following the same settings of ASR pre-training, except for the learning rate that is set to a constant value of 1e-4 for <span class=\"ltx_text ltx_font_typewriter\">small</span> and 1e-5 for <span class=\"ltx_text ltx_font_typewriter\">medium</span>, following the same downscale of the ASR pre-taining. Both training stages lasted 1M steps, corresponding to <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>6 epochs over the training data.</p>\n\n",
                "matched_terms": [
                    "base",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the <span class=\"ltx_text ltx_font_typewriter\">base</span> model, the trainings are executed on 4 NVIDIA A100 GPUs (64GB RAM) with a mini-batch of 40,000 tokens, an update frequency of 2, and averaging the last 7 checkpoints obtained from the training.\nFor the multitask and multitlingual models, we use mini-batches of 10,000 tokens for the <span class=\"ltx_text ltx_font_typewriter\">small</span> and 4,500 for the <span class=\"ltx_text ltx_font_typewriter\">medium</span> with an update frequency of, respectively, 2 and 6 on 16 NVIDIA A100 GPUs (64GB RAM), save checkpoints every 1,000 steps and average the last 25 checkpoints to obtain the final models.</p>\n\n",
                "matched_terms": [
                    "base",
                    "average",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The quality of the ASR hypotheses is evaluated with the WER metric using the jiWER library<span class=\"ltx_note ltx_role_footnote\" id=\"footnote12\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_tag ltx_tag_note\">12</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://pypi.org/project/jiwer/\" title=\"\">https://pypi.org/project/jiwer/</a></span></span></span> and applying the Whisper text normalizer<span class=\"ltx_note ltx_role_footnote\" id=\"footnote13\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_tag ltx_tag_note\">13</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://pypi.org/project/whisper-normalizer/\" title=\"\">https://pypi.org/project/whisper-normalizer/</a></span></span></span>.\nThe quality of the ST hypotheses is evaluated using COMET <cite class=\"ltx_cite ltx_citemacro_citep\">(Rei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib74\" title=\"\">2020</a>)</cite> version 2.2.4, with the default model.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\">14</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hf.co/Unbabel/wmt22-comet-da\" title=\"\">https://hf.co/Unbabel/wmt22-comet-da</a></span></span></span>\nThe quality of the explanations is obtained by measuring both\n<span class=\"ltx_text ltx_font_italic\">deletion</span> and <span class=\"ltx_text ltx_font_italic\">size</span> metrics\navailable in the SPES repository, using <span class=\"ltx_text ltx_font_typewriter\">wer_max</span> as the scorer for ASR and <span class=\"ltx_text ltx_font_typewriter\">bleu</span> for ST, as described in <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "explanations",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For comparison, in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A3.T5\" title=\"Table 5 &#8227; Appendix C Quality Metrics for the Reported Models &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we also report results obtained from popular large-scale models, namely Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib73\" title=\"\">2023</a>)</cite>, OWSM v3.1 <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib70\" title=\"\">2024</a>)</cite>, and SeamlessM4T <cite class=\"ltx_cite ltx_citemacro_citep\">(Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib7\" title=\"\">2023</a>)</cite>. Looking at the transcription/translation quality performance, we observe that both the monolingual <span class=\"ltx_text ltx_font_typewriter\">base</span> model and the multitask multilingual <span class=\"ltx_text ltx_font_typewriter\">small</span> and <span class=\"ltx_text ltx_font_typewriter\">large</span> models are mostly able to achieve competitive results, even outperforming the well-known models in two cases (<span class=\"ltx_text ltx_font_italic\">en</span> ASR for <span class=\"ltx_text ltx_font_typewriter\">base</span> and <span class=\"ltx_text ltx_font_italic\">en-it</span> ST for <span class=\"ltx_text ltx_font_typewriter\">large</span>). While our models and OWSM v3.1 strive to be on par on <span class=\"ltx_text ltx_font_italic\">it</span> with models with closed training data (Whisper and Seamless), they are able to close the gap on <span class=\"ltx_text ltx_font_italic\">en</span>, most probably given a larger availability of public training data. Moreover, the highest performance of <span class=\"ltx_text ltx_font_typewriter\">base</span> on <span class=\"ltx_text ltx_font_italic\">en</span> ASR compared to the <span class=\"ltx_text ltx_font_typewriter\">small</span> and <span class=\"ltx_text ltx_font_typewriter\">large</span> can be attributed to both the specialization of the model and the presence of the EuroParl-ST training set in the training data.</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "base",
                    "model",
                    "highest",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moving to the explanation quality, we observe that both deletion and size scores are comparable across all three models analyzed in the paper and coherent with values obtained in the original SPES paper <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite> on different benchmarks and models. Overall, the deletion scores for ASR are close to the highest possible value (i.e., 100), especially on <span class=\"ltx_text ltx_font_italic\">it</span>, where 97% is achieved. Similarly, the deletion scores for ST are close to 0, indicating that the quality of explanations is very high. The size scores are all close, ranging between 28.2 and 30.6 among models, languages, and tasks, indicating a good compactness of the explanations.</p>\n\n",
                "matched_terms": [
                    "across",
                    "high",
                    "highest",
                    "asr",
                    "between",
                    "explanations",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To properly choose the occlusion probability (<math alttext=\"p_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m1\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">p_{H}</annotation></semantics></math>) for the encoder output explanations <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m3\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></sup>, we conducted experiments by varying this probability in the set of <math alttext=\"\\{0.1,0.3,0.5,0.7,0.9\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>0.1</mn><mo>,</mo><mn>0.3</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>0.7</mn><mo>,</mo><mn>0.9</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{0.1,0.3,0.5,0.7,0.9\\}</annotation></semantics></math>, similarly to what has been done for determining the input occlusion probability (<math alttext=\"p_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m5\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">p_{X}</annotation></semantics></math>) in SPES <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "input",
                    "explanations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A4.T6\" title=\"Table 6 &#8227; Appendix D Effect of Occlusion Probability on Encoder Output Explanations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we report the <span class=\"ltx_text ltx_font_italic\">deletion</span>\nmetric computed on the dev set and, for completeness, the results of the Pearson <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m1\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math> correlation with cross-attention <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math>.\nAnalogously to\nthe deletion metric computed on the input saliency maps <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>, we compute the deletion on the encoder output saliency maps by iteratively replacing\nportions of the encoder output sequence <math alttext=\"\\mathbf{H}=\\{\\mathbf{h}_{1},\\ldots,\\mathbf{h}_{I}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#119815;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>&#119841;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119841;</mi><mi>I</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{H}=\\{\\mathbf{h}_{1},\\ldots,\\mathbf{h}_{I}\\}</annotation></semantics></math> with zero vectors, and removing 5% of the most important time frames at each step based on their saliency.\nFrame importance is determined using saliency maps <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m4\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>H</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}</annotation></semantics></math> aggregated at the sentence level.\nThe output quality is evaluated using the same <span class=\"ltx_text ltx_font_typewriter\">wer_max</span> scorer from the SPES repository. Lastly, we compute the area under the curve of the WER progression to quantify the faithfulness of the explanation.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "œÅrho",
                    "pearson",
                    "computed",
                    "input",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the results, we can notice that higher occlusion probabilities yield not only a better deletion score but also an increased correlation with <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p3.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math>. The overall best correlation is achieved when averaging across all layers, and layer 5 achieves the best layer-specific correlation values, a phenomenon that remains coherent even when varying the occlusion probability.\nInterestingly, the deletion scores and the <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p3.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math>-<math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p3.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p3.m4\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></sup> correlations always follow the same trend, with the best values achieved with <math alttext=\"p_{H}=0.7\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p3.m5\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>H</mi></msub><mo>=</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">p_{H}=0.7</annotation></semantics></math>, which we used in all experiments reported in the main paper.</p>\n\n",
                "matched_terms": [
                    "across",
                    "correlation",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Examples of different saliency maps and cross-attention representations obtained with the <span class=\"ltx_text ltx_font_typewriter\">large</span> model are presented in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We notice similar relevance patterns in the paired samples&#8211;i.e., the samples having the same source language (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>a-f, and Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>g-l)&#8211;even if involving different tasks. We observe a reordering phenomenon from the English audio <span class=\"ltx_text ltx_inline-quote ltx_outerquote\">&#8220;<span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#0000FF;\">cheap<span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\"> <span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">money</span></span></span>&#8221;</span> and its Italian textual counterpart <span class=\"ltx_text ltx_inline-quote ltx_outerquote\">&#8220;<span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#FF0000;\">denaro<span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\"> <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">a buon mercato</span></span></span>&#8221;</span>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote15\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_tag ltx_tag_note\">15</span>Same colors reflect the same concepts.</span></span></span> which is reflected in the saliency maps (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>d and e) and also captured by cross-attention (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>f). We also observe that there are some patterns captured by the attention that are not reflected in the input. For instance, in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>f, the first words (<span class=\"ltx_text ltx_inline-quote ltx_outerquote\">&#8220;<span class=\"ltx_text ltx_font_italic\">&#200; solo</span>&#8221;</span>) attend&#8211;albeit with relatively low scores&#8211;to the audio frames between 75 and 85, while this pattern this pattern is absent in the relevance scores of both the encoder output and the input. Consistent with the findings discussed throughout the paper, this example illustrates that while attention generally follows the saliency patterns identified by feature attribution, some discrepancies persist.</p>\n\n",
                "matched_terms": [
                    "english",
                    "low",
                    "between",
                    "input",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work provides an in-depth analysis of cross-attention explainability in encoder-decoder S2T models. While it yields actionable insights, some limitations should be acknowledged. First, our experimental scope is restricted to ASR and ST. Although these tasks are central to S2T-based AI <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib73\" title=\"\">2023</a>; Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib7\" title=\"\">2023</a>)</cite>, we do not evaluate other downstream tasks such as spoken question answering or speech summarization, which may involve different dynamics in decoder attention. Second, our multilingual analysis is limited to English (a Germanic language) and Italian (a Romance language), due to the high computational cost of large-scale model training across a broader set of languages. Third, we focus on models trained from scratch but do not include architectures based on Speech Foundation Models (SFMs) paired with large language models (LLMs), often referred to as SpeechLLM&#8211;a recent growing area of interest in S2T modeling <cite class=\"ltx_cite ltx_citemacro_citep\">(Gaido et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib24\" title=\"\">2024b</a>)</cite>. As our analysis focuses on evaluation, our goal was to completely avoid data contamination issues <cite class=\"ltx_cite ltx_citemacro_citep\">(Sainz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib76\" title=\"\">2023</a>)</cite>, which is a problem affecting almost every SFM and SpeechLLM architectures currently available, as we have no control over their training data, and, for this reason, we decided to retrain the models from scratch.\nFourth, our analysis relies on SPES to compute reference explanations, acknowledging that, as an empirical method, it may introduce some margin of error. However, in the absence of a gold or human reference&#8211;which is unattainable in practice&#8211;we adopt SPES as a <span class=\"ltx_text ltx_font_italic\">silver</span> reference, since it represents the state of the art in explainability for speech-to-text. We further validate this choice in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A3\" title=\"Appendix C Quality Metrics for the Reported Models &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>, showing that SPES achieves very high quality explanations (deletion scores <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>90 on ASR and <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m2\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>3 on ST), making it a more faithful option than less robust alternatives from the generic XAI field such as gradient norms <cite class=\"ltx_cite ltx_citemacro_citep\">(Covert et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib15\" title=\"\">2021a</a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "across",
                    "english",
                    "model",
                    "high",
                    "asr",
                    "explanations",
                    "crossattention"
                ]
            }
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Person œÅ\\rho correlation between layer-wise cross-attention ùêÇùêÄ¬Ø(‚Ñì)\\widebar{\\text{{\\color[rgb]{0.75390625,0.390625,0.9609375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.75390625,0.390625,0.9609375}$\\mathbf{CA}$}}}^{(\\ell)} and the input explanations ùêíùêå\\mathbf{SM}XX for the multitask (ASR and ST) and multilingual (English and Italian) models (small and large). Bold indicates the highest overall correlation, underline indicates the highest correlation across layers. Low to high values are yellow to aqua for ASR, and to red for ST.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\" colspan=\"2\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"14\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_bold\">Target Language</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" colspan=\"2\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"14\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_italic\">en</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" colspan=\"2\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_bold\">Lang.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><math alttext=\"\\ell=1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m1\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=1</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><math alttext=\"\\ell=2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m2\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=2</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><math alttext=\"\\ell=3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m3\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=3</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><math alttext=\"\\ell=4\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m4\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=4</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><math alttext=\"\\ell=5\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m5\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=5</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><math alttext=\"\\ell=6\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m6\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=6</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><math alttext=\"\\ell=7\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m7\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>7</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=7</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><math alttext=\"\\ell=8\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m8\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=8</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><math alttext=\"\\ell=9\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m9\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>9</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=9</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><math alttext=\"\\ell=10\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m10\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=10</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><math alttext=\"\\ell=11\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m11\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>11</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=11</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><math alttext=\"\\ell=12\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m12\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>12</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=12</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\">\n<math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m13\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math><span class=\"ltx_text ltx_font_bold\">-AVG</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_typewriter\">small</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FDFCDC;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDFCDC;\">0.142</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#D3F2D9;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D3F2D9;\">0.205</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#3BCBCC;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#3BCBCC;\">0.428</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#1B96BA;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"--ltx-bg-color:#1B96BA;\">0.639</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#1A97BB;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"--ltx-bg-color:#1A97BB;\">0.639</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#159EBD;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#159EBD;\">0.614</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"6\" style=\"padding-left:2.7pt;padding-right:2.7pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#1998BB;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#1998BB;\">0.633</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_italic\">en</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_typewriter\">large</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#F7FBDC;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7FBDC;\">0.151</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#F0F9DB;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F0F9DB;\">0.162</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#CCF0D8;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#CCF0D8;\">0.214</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#85DED2;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#85DED2;\">0.320</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#9AE3D4;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#9AE3D4;\">0.289</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#38CACB;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#38CACB;\">0.434</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#0EA8C0;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#0EA8C0;\">0.581</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"--ltx-bg-color:#11A3BF;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#11A3BF;\">0.597</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"--ltx-bg-color:#149FBD;padding-left:2.7pt;padding-right:2.7pt;\">\n<span class=\"ltx_ERROR undefined\">\\ultab</span><span class=\"ltx_text\" style=\"--ltx-bg-color:#149FBD;\">0.611</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"--ltx-bg-color:#11A3BF;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#11A3BF;\">0.597</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"--ltx-bg-color:#07B1C3;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#07B1C3;\">0.551</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"--ltx-bg-color:#0AAEC2;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#0AAEC2;\">0.561</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#169CBC;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#169CBC;\">0.621</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_typewriter\">small</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FDFCDC;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDFCDC;\">0.147</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FDF4D4;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDF4D4;\">0.193</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FDDDBB;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDDDBB;\">0.327</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#F7A18F;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7A18F;\">0.476</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#F79F8D;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"--ltx-bg-color:#F79F8D;\">0.482</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#F8A692;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F8A692;\">0.465</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"6\" style=\"padding-left:2.7pt;padding-right:2.7pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#F69D8C;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F69D8C;\">0.485</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_italic\">it</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_typewriter\">large</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FDFBDB;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDFBDB;\">0.151</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FDF9D9;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDF9D9;\">0.164</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FDEFCE;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDEFCE;\">0.223</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FDE2C0;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDE2C0;\">0.300</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FDE4C3;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDE4C3;\">0.285</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FCCAAC;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCCAAC;\">0.383</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#F8AC97;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F8AC97;\">0.451</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"--ltx-bg-color:#F7A592;padding-left:2.7pt;padding-right:2.7pt;\">\n<span class=\"ltx_ERROR undefined\">\\ultab</span><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7A592;\">0.467</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"--ltx-bg-color:#F8A894;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F8A894;\">0.461</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"--ltx-bg-color:#F8A894;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F8A894;\">0.461</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"--ltx-bg-color:#FAB59E;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FAB59E;\">0.430</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"--ltx-bg-color:#FAB59D;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FAB59D;\">0.431</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#F69A8A;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F69A8A;\">0.492</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"14\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_italic\">it</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_typewriter\">small</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FDF8D7;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDF8D7;\">0.173</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FDF2D2;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDF2D2;\">0.203</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FDDAB8;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDDAB8;\">0.344</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#F38279;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F38279;\">0.547</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#F28077;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"--ltx-bg-color:#F28077;\">0.550</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#F3857B;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F3857B;\">0.539</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"6\" style=\"padding-left:2.7pt;padding-right:2.7pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#F38178;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F38178;\">0.549</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_italic\">en</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_typewriter\">large</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FDF8D8;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDF8D8;\">0.168</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FDF7D7;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDF7D7;\">0.176</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FDEDCC;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDEDCC;\">0.235</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FDE2C0;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDE2C0;\">0.300</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FDE1BF;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDE1BF;\">0.306</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FBBDA3;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FBBDA3;\">0.413</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#F59083;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F59083;\">0.514</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"--ltx-bg-color:#F48B7F;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F48B7F;\">0.526</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"--ltx-bg-color:#F48A7E;padding-left:2.7pt;padding-right:2.7pt;\">\n<span class=\"ltx_ERROR undefined\">\\ultab</span><span class=\"ltx_text\" style=\"--ltx-bg-color:#F48A7E;\">0.529</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"--ltx-bg-color:#F48A7E;padding-left:2.7pt;padding-right:2.7pt;\">\n<span class=\"ltx_ERROR undefined\">\\ultab</span><span class=\"ltx_text\" style=\"--ltx-bg-color:#F48A7E;\">0.529</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" style=\"--ltx-bg-color:#F59183;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F59183;\">0.513</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"--ltx-bg-color:#F59082;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F59082;\">0.516</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#F28077;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#F28077;\">0.551</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_typewriter\">small</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FCFCDC;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCFCDC;\">0.145</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#D0F1D9;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D0F1D9;\">0.209</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#60D4CF;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#60D4CF;\">0.374</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#02B8C5;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#02B8C5;\">0.527</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#03B6C5;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"--ltx-bg-color:#03B6C5;\">0.532</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#02B9C6;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#02B9C6;\">0.525</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"6\" style=\"padding-left:2.7pt;padding-right:2.7pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#05B4C4;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#05B4C4;\">0.539</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.9pt;height:81.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:81.4pt;transform:translate(-36.3pt,-36.3pt) rotate(-90deg) ;\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Source language</span></p>\n</span></div>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_italic\">it</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_typewriter\">large</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"--ltx-bg-color:#ECF8DB;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ECF8DB;\">0.169</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"--ltx-bg-color:#F3FADC;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F3FADC;\">0.157</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"--ltx-bg-color:#CCF0D8;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#CCF0D8;\">0.215</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"--ltx-bg-color:#82DDD2;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#82DDD2;\">0.324</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"--ltx-bg-color:#95E2D3;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#95E2D3;\">0.297</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"--ltx-bg-color:#4ACECD;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#4ACECD;\">0.407</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"--ltx-bg-color:#0ABEC7;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#0ABEC7;\">0.501</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" style=\"--ltx-bg-color:#08BEC7;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#08BEC7;\">0.503</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" style=\"--ltx-bg-color:#00BBC6;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#00BBC6;\">0.516</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" style=\"--ltx-bg-color:#00BBC6;padding-left:2.7pt;padding-right:2.7pt;\">\n<span class=\"ltx_ERROR undefined\">\\ultab</span><span class=\"ltx_text\" style=\"--ltx-bg-color:#00BBC6;\">0.518</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" style=\"--ltx-bg-color:#19C2C9;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#19C2C9;\">0.479</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\" style=\"--ltx-bg-color:#17C1C8;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#17C1C8;\">0.482</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"--ltx-bg-color:#06B3C4;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#06B3C4;\">0.544</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "ùêíùêåmathbfsmxx",
            "models",
            "multilingual",
            "overall",
            "high",
            "‚Ñì1ell1",
            "ultab0529",
            "source",
            "‚Ñì2ell2",
            "italian",
            "underline",
            "‚Ñì10ell10",
            "‚Ñì9ell9",
            "low",
            "target",
            "small",
            "lang",
            "ultab0611",
            "between",
            "input",
            "layerwise",
            "aqua",
            "across",
            "ùêÇùêÄ¬Ø‚Ñìwidebartextcolorrgb075390625039062509609375definecolornamedpgfstrokecolorrgb075390625039062509609375mathbfcaell",
            "english",
            "language",
            "‚Ñì12ell12",
            "bold",
            "‚Ñì11ell11",
            "‚Ñì4ell4",
            "asr",
            "indicates",
            "‚Ñì6ell6",
            "‚Ñì7ell7",
            "ultab0467",
            "crossattention",
            "layers",
            "values",
            "‚Ñì8ell8",
            "correlation",
            "œÅrho",
            "red",
            "multitask",
            "yellow",
            "model",
            "large",
            "highest",
            "explanations",
            "‚Ñì3ell3",
            "ultab0518",
            "‚Ñìellavg",
            "‚Ñì5ell5",
            "person"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To assess the impact of multilingual and multitask training on the correlation between cross-attention scores and saliency maps, we evaluate the <span class=\"ltx_text ltx_font_typewriter\">small</span> and <span class=\"ltx_text ltx_font_typewriter\">large</span> models. Layer-wise results are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.T2\" title=\"Table 2 &#8227; 5.1.2 Multitask and Multilingual Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, while head-wise results are omitted due to the noisy behavior observed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS1\" title=\"5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Cross-attention is a core mechanism in encoder-decoder architectures, widespread in many fields, including speech-to-text (S2T) processing. Its scores have been repurposed for various downstream applications&#8211;such as timestamp estimation and audio-text alignment&#8211;under the assumption that they reflect the dependencies between input speech representation and the generated text. While the explanatory nature of attention mechanisms has been widely debated in the broader NLP literature, this assumption remains largely unexplored within the speech domain. To address this gap, we assess the explanatory power of cross-attention in S2T models by comparing its scores to input saliency maps derived from feature attribution. Our analysis spans monolingual and multilingual, single-task and multi-task models at multiple scales, and shows that attention scores moderately to strongly align with saliency-based explanations, particularly when aggregated across heads and layers. However, it also shows that cross-attention captures only about 50% of the input relevance and, in the best case, only partially reflects how the decoder attends to the encoder&#8217;s representations&#8211;accounting for just 52-75% of the saliency. These findings uncover fundamental limitations in interpreting cross-attention as an explanatory proxy, suggesting that it offers an informative yet incomplete view of the factors driving predictions in S2T models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "multitask",
                    "multilingual",
                    "layers",
                    "explanations",
                    "between",
                    "input",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-attention <cite class=\"ltx_cite ltx_citemacro_citep\">(Bahdanau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib6\" title=\"\">2015</a>)</cite> is the core mechanism of the encoder-decoder Transformer architecture <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib85\" title=\"\">2017</a>)</cite>, a model that has become foundational across numerous AI domains <cite class=\"ltx_cite ltx_citemacro_citep\">(Galassi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib25\" title=\"\">2021</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib49\" title=\"\">2022</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib45\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib92\" title=\"\">2024b</a>; Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib51\" title=\"\">2024</a>)</cite>, including natural language processing (NLP).\nDesigned for modeling dependencies between the generated output sequence and the input representations, the cross-attention scores&#8211;derived from the attention mechanism&#8211;have been leveraged in various NLP tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib32\" title=\"\">2020</a>; Zhang &amp; Kim, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib103\" title=\"\">2023</a>)</cite>, such as\nsource-target textual alignment <cite class=\"ltx_cite ltx_citemacro_citep\">(Garg et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib26\" title=\"\">2019</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib12\" title=\"\">2020</a>)</cite>, co-reference resolution <cite class=\"ltx_cite ltx_citemacro_citep\">(Voita et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib87\" title=\"\">2018</a>)</cite>, and word sense disambiguation <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib83\" title=\"\">2018</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "language",
                    "model",
                    "between",
                    "input",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In speech-to-text (S2T) modeling, cross-attention scores have been widely repurposed for diverse downstream applications such as audio-text alignment <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib104\" title=\"\">2020</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib46\" title=\"\">2020</a>)</cite>, speaker identification <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib38\" title=\"\">2019</a>)</cite>, timestamp estimation <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib47\" title=\"\">2022</a>; Louradour, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib50\" title=\"\">2023</a>; Zusag et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib106\" title=\"\">2024</a>)</cite>, and guiding simultaneous automatic speech recognition (ASR) and speech translation (ST) <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib91\" title=\"\">2024a</a>; Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib65\" title=\"\">2023a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib66\" title=\"\">b</a>)</cite>.\nThese applications rely on the implicit assumption that\ncross-attention reliably indicates what the model attends to in the input signal during output generation. However, despite its widespread use, this assumption has\nnever been verified.\nA key concern is that cross-attention operates over the encoder&#8217;s output sequence&#8211;rather than directly on the raw audio&#8211;which may have been reorganized or mixed with contextual information. This phenomenon, known as context mixing <cite class=\"ltx_cite ltx_citemacro_citep\">(Mohebbi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib60\" title=\"\">2023b</a>)</cite>, can potentially obscure the alignment between cross-attention weights and the original input signal.\nSimilar concerns have been extensively debated in the NLP community, where the reliability of attention mechanisms as explanations has been both challenged and defended, leading to conflicting perspectives and empirical evidence <cite class=\"ltx_cite ltx_citemacro_citep\">(Serrano &amp; Smith, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib78\" title=\"\">2019</a>; Jain &amp; Wallace, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib35\" title=\"\">2019</a>; Wiegreffe &amp; Pinter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib93\" title=\"\">2019</a>; Bastings &amp; Filippova, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib9\" title=\"\">2020</a>)</cite>. In contrast, this question remains largely underexplored in the speech domain.\nExisting work on explainability in S2T has primarily focused on self-attention <cite class=\"ltx_cite ltx_citemacro_citep\">(Shim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib80\" title=\"\">2022</a>; Audhkhasi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib5\" title=\"\">2022</a>; A&#160;Shams et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib1\" title=\"\">2024</a>)</cite>, or on empirically measuring the effects of context mixing <cite class=\"ltx_cite ltx_citemacro_citep\">(Mohebbi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib59\" title=\"\">2023a</a>)</cite>, without directly assessing the explanatory potential of cross-attention mechanisms.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr",
                    "indicates",
                    "input",
                    "explanations",
                    "between",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this gap, we present the first systematic\nanalysis of cross-attention as a proxy for input-output dependencies in S2T models.\nOur study serves two main objectives: <span class=\"ltx_text ltx_font_italic\">i)</span> assessing the validity of using cross-attention as a surrogate for input-output alignment,\nand <span class=\"ltx_text ltx_font_italic\">ii)</span> evaluating whether it provides insights comparable to formal explainability methods such as feature attribution&#8211;while being more lightweight and less computationally expensive to obtain <cite class=\"ltx_cite ltx_citemacro_citep\">(Samek et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib77\" title=\"\">2021</a>; Madsen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib53\" title=\"\">2022</a>)</cite>. We compare cross-attention scores with input saliency maps derived from SPES <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>, the current state-of-the-art feature-attribution method in S2T, to determine the extent to which\ncross-attention\ncaptures\nwhich input features are relevant for models&#8217; predictions.\nIn addition, we compute saliency maps on\nencoder outputs and compare them with cross-attention scores to evaluate whether cross-attention fully explains how the decoder uses encoded representations, avoiding potential discrepancies\nof context mixing.\nOur\nanalysis spans ASR and ST tasks across monolingual, multilingual, and multitask settings using\nstate-of-the-art speech processing\narchitectures <cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib29\" title=\"\">2020</a>)</cite> at multiple scales.\nWith consistent trends across different settings, we find\nthat cross-attention exhibits moderate to strong correlations with input saliency maps and aligns more closely with encoder output representations, suggesting an influence of context mixing. However, our results also indicate that the overall explanatory power of cross-attention is limited&#8211;accounting for only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>50% of input relevance and, at best, 52-75% of encoder output saliency.\nOur findings uncover fundamental limitations in interpreting cross-attention as an explanatory proxy, suggesting that it provides an informative yet incomplete view of the factors driving predictions in S2T models.\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "multitask",
                    "multilingual",
                    "overall",
                    "asr",
                    "input",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Explainability in Speech-to-Text.</span>\nExplainable AI (XAI) has emerged to make model behavior more interpretable to humans, thereby supporting informed decision-making and responsible deployment <cite class=\"ltx_cite ltx_citemacro_citep\">(Barredo Arrieta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib8\" title=\"\">2020</a>)</cite>.\nWhile XAI research has seen a rapid growth in the last years across multiple modalities, including vision and language <cite class=\"ltx_cite ltx_citemacro_citep\">(Sharma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib79\" title=\"\">2024</a>)</cite>, progress in the speech domain has lagged.\nThis gap arises from the inherent complexities of speech processing, including the multidimensional nature of speech signals across time and frequency, and the variability in output sequence length <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib95\" title=\"\">2024</a>)</cite>.\nDespite these challenges, growing concerns about trustworthiness\nare driving explainability efforts in speech classification <cite class=\"ltx_cite ltx_citemacro_citep\">(Becker et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib10\" title=\"\">2024</a>; Pastor et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib68\" title=\"\">2024</a>)</cite> and S2T generation <cite class=\"ltx_cite ltx_citemacro_citep\">(Mandel, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib54\" title=\"\">2016</a>; Kavaki &amp; Mandel, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib37\" title=\"\">2020</a>; Trinh &amp; Mandel, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib84\" title=\"\">2020</a>; Markert et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib55\" title=\"\">2021</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib94\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib95\" title=\"\">2024</a>; Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>. Most of these works rely on perturbation-based methods\nthat assess how input modifications affect model predictions <cite class=\"ltx_cite ltx_citemacro_citep\">(Covert et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib16\" title=\"\">2021b</a>; Ivanovs et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib34\" title=\"\">2021</a>)</cite>.\nAmong these,\n<cite class=\"ltx_cite ltx_citemacro_citet\">Fucci et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite> recently proposed\na technique for autoregressive S2T models that identifies regions of the spectrogram that most influence predictions to generate saliency maps.\nHowever, XAI methods are generally computationally expensive&#8211;especially perturbation-based approaches applied to large models <cite class=\"ltx_cite ltx_citemacro_citep\">(Luo &amp; Specia, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib52\" title=\"\">2024</a>; Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib102\" title=\"\">2025</a>)</cite>&#8211;which motivates exploring whether cross-attention, already computed at inference time, could serve as a lightweight alternative in a landscape still lacking efficient explainability tools for speech-based models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "language",
                    "model",
                    "large",
                    "input",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Attention as Explanation.</span>\nAttention mechanisms have been widely used to probe model behavior in text-based NLP, as attention scores often align with human intuitions about relevance and salience <cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib13\" title=\"\">2019</a>; Ferrando et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib21\" title=\"\">2024</a>)</cite>.\nEarly studies proposed norm-based analyses to improve the interpretability of attention weights <cite class=\"ltx_cite ltx_citemacro_citep\">(Kobayashi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib39\" title=\"\">2020</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib40\" title=\"\">2021</a>; Mohebbi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib58\" title=\"\">2021</a>; Ferrando et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib20\" title=\"\">2022b</a>)</cite>, while others suggested aggregating attention across layers and heads to quantify input-output influence more systematically <cite class=\"ltx_cite ltx_citemacro_citep\">(Abnar &amp; Zuidema, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib2\" title=\"\">2020</a>; Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib101\" title=\"\">2021</a>; Chefer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib11\" title=\"\">2021</a>)</cite>.\nWhile some have raised concerns about whether attention reliably reflects which inputs are actually responsible for outputs <cite class=\"ltx_cite ltx_citemacro_citep\">(Jain &amp; Wallace, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib35\" title=\"\">2019</a>; Serrano &amp; Smith, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib78\" title=\"\">2019</a>; Bastings &amp; Filippova, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib9\" title=\"\">2020</a>)</cite>, others have proposed conditions under which attention can meaningfully explain model behavior <cite class=\"ltx_cite ltx_citemacro_citep\">(Wiegreffe &amp; Pinter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib93\" title=\"\">2019</a>)</cite>. More recent work highlights that attention aggregation may obscure localized, token-specific interactions <cite class=\"ltx_cite ltx_citemacro_citep\">(Modarressi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib57\" title=\"\">2023</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib100\" title=\"\">2023</a>; Oh &amp; Schuler, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib63\" title=\"\">2023</a>)</cite>, motivating hybrid approaches that combine attention with other XAI techniques, such as attribution methods <cite class=\"ltx_cite ltx_citemacro_citep\">(Modarressi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib56\" title=\"\">2022</a>)</cite>, or the use of attention as a regularization signal during interpretability-driven training <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib97\" title=\"\">2024</a>)</cite>.\nDespite the ongoing efforts, most research has focused on self-attention within encoders, with limited attention to feed-forward dynamics <cite class=\"ltx_cite ltx_citemacro_citep\">(Kobayashi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib41\" title=\"\">2024</a>)</cite> and even less to encoder-decoder models. A few studies have investigated attention in encoder-decoder architectures <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib62\" title=\"\">2021</a>)</cite>, including in machine translation <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib105\" title=\"\">2024</a>)</cite>, but cross-attention remains largely underexplored in the speech domain and has been\nabsent from the broader <span class=\"ltx_text ltx_inline-quote ltx_outerquote\">&#8220;attention as explanation&#8221;</span> debate in NLP. Our work seeks to bridge this gap by bringing cross-attention of S2T models into this broader conversation, aiming to assess\nwhether\nit can serve as a reliable explanation&#8211;and where its limitations emerge.\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "crossattention",
                    "model",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We assess the extent to which cross-attention scores (<math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math>) explain how the model looks at\ninput features when generating a token\nby comparing them to the saliency map on the input <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math>, obtained with the state-of-the art feature-attribution method for S2T, SPES <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "input",
                    "model",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, to assess whether cross-attention more accurately reflects how the decoder accesses encoded representations&#8211;rather than capturing the model&#8217;s full input-output behavior&#8211;we compare <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> with the encoder-output saliency map <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m2\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>H</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}</annotation></semantics></math>.\nBy analyzing how the correlation between <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> and <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m4\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>H</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}</annotation></semantics></math> deviates from that with <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m5\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math>,\nwe can indirectly quantify the impact of context mixing in the resulting explanations.\nA visual overview of this setup is provided in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S3.F1\" title=\"Figure 1 &#8227; 3 Methodology &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "explanations",
                    "correlation",
                    "crossattention",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In S2T models, the cross-attention mechanism enables each decoder token to integrate relevant portions of the encoded speech features, thereby conditioning generation on the entire input.</p>\n\n",
                "matched_terms": [
                    "input",
                    "models",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At each decoder layer <math alttext=\"\\ell\\in\\{1,\\ldots,L\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\ell\\in\\{1,\\ldots,L\\}</annotation></semantics></math>, cross-attention scores are computed via dot-product attention <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib28\" title=\"\">2014</a>)</cite> between the decoder&#8217;s current hidden states <math alttext=\"\\mathbf{B}^{(\\ell)}\\in\\mathbb{R}^{I\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><msup><mi>&#119809;</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{B}^{(\\ell)}\\in\\mathbb{R}^{I\\times D}</annotation></semantics></math> and the encoder outputs <math alttext=\"\\mathbf{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m3\" intent=\":literal\"><semantics><mi>&#119815;</mi><annotation encoding=\"application/x-tex\">\\mathbf{H}</annotation></semantics></math>. Specifically, the decoder states are linearly projected to queries <math alttext=\"\\mathbf{Q}^{(\\ell)}=\\mathbf{B}^{(\\ell)}\\mathbf{W}_{Q}^{(\\ell)}\\in\\mathbb{R}^{I\\times d_{k}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m4\" intent=\":literal\"><semantics><mrow><msup><mi>&#119824;</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><mrow><msup><mi>&#119809;</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#119830;</mi><mi>Q</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Q}^{(\\ell)}=\\mathbf{B}^{(\\ell)}\\mathbf{W}_{Q}^{(\\ell)}\\in\\mathbb{R}^{I\\times d_{k}}</annotation></semantics></math>, while the encoder outputs are projected to keys <math alttext=\"\\mathbf{K}^{(\\ell)}=\\mathbf{H}\\mathbf{W}_{K}^{(\\ell)}\\in\\mathbb{R}^{T^{\\prime}\\times d_{k}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m5\" intent=\":literal\"><semantics><mrow><msup><mi>&#119818;</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><msubsup><mi>&#119815;&#119830;</mi><mi>K</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{K}^{(\\ell)}=\\mathbf{H}\\mathbf{W}_{K}^{(\\ell)}\\in\\mathbb{R}^{T^{\\prime}\\times d_{k}}</annotation></semantics></math>\nusing learned projection matrices <math alttext=\"\\mathbf{W}_{Q}^{(\\ell)},\\mathbf{W}_{K}^{(\\ell)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m6\" intent=\":literal\"><semantics><mrow><msubsup><mi>&#119830;</mi><mi>Q</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>&#119830;</mi><mi>K</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{W}_{Q}^{(\\ell)},\\mathbf{W}_{K}^{(\\ell)}</annotation></semantics></math>. The resulting cross-attention matrix <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m7\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> is:</p>\n\n",
                "matched_terms": [
                    "crossattention",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Extracting\nthe full set of scores provides a fine-grained view of how each output token in the generated hypothesis attends to the encoder&#8217;s representations across all layers and heads.\nTo derive a single layer-wise or head-wise attention distribution, we compute the mean of the attention matrices over a subset <math alttext=\"\\mathcal{S}\\subseteq\\{1,\\dots,L\\}\\times\\{1,\\dots,H\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>&#8838;</mo><mrow><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo rspace=\"0.055em\" stretchy=\"false\">}</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>H</mi><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}\\subseteq\\{1,\\dots,L\\}\\times\\{1,\\dots,H\\}</annotation></semantics></math> of layers and heads:</p>\n\n",
                "matched_terms": [
                    "layerwise",
                    "across",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By selecting different index sets <math alttext=\"\\mathcal{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><annotation encoding=\"application/x-tex\">\\mathcal{S}</annotation></semantics></math>, this formulation yields layer-wise, head-wise, or global averages. For example, setting <math alttext=\"\\mathcal{S}=\\{(\\ell,h):h=1,\\dots,H\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m3\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo>,</mo><mi>h</mi><mo rspace=\"0.278em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.278em\">:</mo><mrow><mi>h</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>H</mi></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}=\\{(\\ell,h):h=1,\\dots,H\\}</annotation></semantics></math> gives the average across heads at a given layer <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>; <math alttext=\"\\mathcal{S}=\\{(\\ell,h):\\ell=1,\\dots,L\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m5\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo>,</mo><mi>h</mi><mo rspace=\"0.278em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.278em\">:</mo><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}=\\{(\\ell,h):\\ell=1,\\dots,L\\}</annotation></semantics></math> averages across layers for head <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m6\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math>; and <math alttext=\"\\mathcal{S}=\\{1,\\dots,L\\}\\times\\{1,\\dots,H\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m7\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>=</mo><mrow><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo rspace=\"0.055em\" stretchy=\"false\">}</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>H</mi><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}=\\{1,\\dots,L\\}\\times\\{1,\\dots,H\\}</annotation></semantics></math> computes the full average.\nThis averaged attention provides a more aggregated view of the model&#8217;s attention patterns at a specific layer or attention head, summarizing how the model attends to the input speech over time.</p>\n\n",
                "matched_terms": [
                    "layerwise",
                    "across",
                    "model",
                    "input",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand how S2T models associate individual output tokens with specific regions of the raw speech input or of the model&#8217;s inner representations (e.g., the encoder output), we employ <span class=\"ltx_text ltx_font_italic\">feature-attribution</span> techniques that produce token-level <span class=\"ltx_text ltx_font_italic\">saliency maps</span>. These maps quantify the relevance of different portions of the input sequence in determining the model&#8217;s predictions.</p>\n\n",
                "matched_terms": [
                    "input",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Let again <math alttext=\"\\mathbf{X}\\in\\mathbb{R}^{T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}\\in\\mathbb{R}^{T\\times F}</annotation></semantics></math> denote a mel-spectrogram input, where <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> is the number of time frames and <math alttext=\"F\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mi>F</mi><annotation encoding=\"application/x-tex\">F</annotation></semantics></math> the number of frequency bins, and <math alttext=\"\\mathbf{y}=(y_{0},y_{1},\\ldots,y_{I})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#119858;</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mn>0</mn></msub><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>y</mi><mi>I</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{y}=(y_{0},y_{1},\\ldots,y_{I})</annotation></semantics></math> the sequence of length <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math> of the autoregressively-generated tokens predicted based on the input and the previously generated tokens <math alttext=\"y_{&lt;i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><msub><mi>y</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">y_{&lt;i}</annotation></semantics></math>.\nTo attribute the prediction of each token <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math> to specific parts of the input spectrogram, we adopt SPES&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>,\nthe state-of-the-art feature-attribution method designed for autoregressive S2T modeling. SPES assigns a saliency score to each time-frequency\nelement\nof <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><mi>&#119831;</mi><annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation></semantics></math>, producing a saliency map <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{X}\\in\\mathbb{R}^{T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m9\" intent=\":literal\"><semantics><mrow><msubsup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>i</mi><mi>X</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{X}\\in\\mathbb{R}^{T\\times F}</annotation></semantics></math> for each token <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m10\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math>, where higher values indicate greater relevance of the corresponding time-frequency regions. SPES operates by clustering spectrogram elements based on energy profiles&#8211;capturing acoustic\ncomponents such as harmonics and background noise&#8211;and estimating the influence of each cluster by perturbing it with probability <math alttext=\"p_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m11\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">p_{X}</annotation></semantics></math>, repeated <math alttext=\"N_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m12\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">N_{X}</annotation></semantics></math> times. The effect of each perturbation&#8211;i.e., masking parts of the input with 0 values&#8211;is measured by computing the Kullback-Leibler (KL) divergence <cite class=\"ltx_cite ltx_citemacro_citep\">(Kullback &amp; Leibler, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib44\" title=\"\">1951</a>)</cite> between the model&#8217;s original output distribution <math alttext=\"P(y_{i}\\mid y_{&lt;i},\\mathbf{X})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m13\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>&#8739;</mo><mrow><msub><mi>y</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><mo>,</mo><mi>&#119831;</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(y_{i}\\mid y_{&lt;i},\\mathbf{X})</annotation></semantics></math> and the distribution resulting from the perturbed input <math alttext=\"P^{(n)}(y_{i}\\mid y_{&lt;i},\\tilde{\\mathbf{X}}^{(n)})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m14\" intent=\":literal\"><semantics><mrow><msup><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>&#8739;</mo><mrow><msub><mi>y</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><mo>,</mo><msup><mover accent=\"true\"><mi>&#119831;</mi><mo>~</mo></mover><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P^{(n)}(y_{i}\\mid y_{&lt;i},\\tilde{\\mathbf{X}}^{(n)})</annotation></semantics></math> at time <math alttext=\"n\\in\\{1,\\ldots,N_{X}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m15\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>N</mi><mi>X</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">n\\in\\{1,\\ldots,N_{X}\\}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "input",
                    "values",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since our focus lies in the temporal dynamics of the input <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mi>&#119831;</mi><annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation></semantics></math>, we aggregate the 3D saliency scores <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\\in\\mathbb{R}^{I\\times T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\\in\\mathbb{R}^{I\\times T\\times F}</annotation></semantics></math> across the frequency dimension and downsample the time axis to produce a compressed representation <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\\in\\mathbb{R}^{I\\times T^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>T</mi><mo>&#8242;</mo></msup></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\\in\\mathbb{R}^{I\\times T^{\\prime}}</annotation></semantics></math> compatible with the cross-attention granularity, where <math alttext=\"T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m4\" intent=\":literal\"><semantics><msup><mi>T</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">T^{\\prime}</annotation></semantics></math> corresponds to the number of encoder time steps. The aggregation is performed by taking the maximum saliency value over the frequency axis and within each corresponding time window. The resulting saliency map of each token <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{X}\\in\\mathbb{R}^{T^{\\prime}\\times 1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><msubsup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>i</mi><mi>X</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{X}\\in\\mathbb{R}^{T^{\\prime}\\times 1}</annotation></semantics></math> reflects the temporal relevance of the input spectrogram with respect to the generation of token <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m6\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math>. Complementary experiments on the choice of the aggregation function are presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A1\" title=\"Appendix A Effect of Aggregation Functions on Input Explanations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.\nBoth <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m7\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> and <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m8\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math> representations are normalized before computing the correlation scores, and the beginning and end of sentence are removed as they are not relevant for the analysis.\nThe <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m9\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> matrix is normalized frame-wise using mean-variance normalization to mitigate the impact of potential attention sinks at initial or final tokens&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib13\" title=\"\">2019</a>; Ferrando et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib19\" title=\"\">2022a</a>; Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib65\" title=\"\">2023a</a>; Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib96\" title=\"\">2024</a>)</cite> on the correlation computation.\nBoth <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m10\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math> and <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m11\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>H</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}</annotation></semantics></math> are normalized along the token dimension using the strategy proposed by <cite class=\"ltx_cite ltx_citemacro_citet\">Fucci et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>, as saliency scores can vary widely across tokens due to differences in the original output distributions used to compute the KL divergence.</p>\n\n",
                "matched_terms": [
                    "input",
                    "across",
                    "correlation",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following prior work on cross-attention matrices <cite class=\"ltx_cite ltx_citemacro_citep\">(Vig &amp; Belinkov, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib86\" title=\"\">2019</a>)</cite> and explainable AI <cite class=\"ltx_cite ltx_citemacro_citep\">(Eberle et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib18\" title=\"\">2023</a>)</cite>, we use Pearson correlation\nto quantify the relationship between cross-attention scores and saliency-based explanations.\nPearson correlation is preferred over Kendall and Spearman because saliency scores are continuous, and their magnitude&#8211;not just ranking&#8211;is crucial. Rank-based measures are overly sensitive to small fluctuations among non-important features with near-zero scores, while Pearson better captures whether features are identified as important (high score) or not (low score).\nSpecifically, given the two representations <math alttext=\"\\text{{\\color[rgb]{0.75390625,0.390625,0.9609375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.75390625,0.390625,0.9609375}$\\mathbf{CA}$}},\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}\\in\\mathbb{R}^{I\\times T^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><mo>,</mo><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>T</mi><mo>&#8242;</mo></msup></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.75390625,0.390625,0.9609375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.75390625,0.390625,0.9609375}$\\mathbf{CA}$}},\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}\\in\\mathbb{R}^{I\\times T^{\\prime}}</annotation></semantics></math>, we compute the Pearson correlation coefficient <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math> to assess the similarity of their attribution patterns across output tokens and time steps. We first flatten each matrix into a vector of size <math alttext=\"I\\cdot T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8901;</mo><msup><mi>T</mi><mo>&#8242;</mo></msup></mrow><annotation encoding=\"application/x-tex\">I\\cdot T^{\\prime}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "across",
                    "correlation",
                    "œÅrho",
                    "low",
                    "high",
                    "small",
                    "explanations",
                    "between",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This scalar value quantifies the linear relationship between the two saliency maps, with values closer to 1 indicating a strong positive correlation, and values near 0 indicating no correlation.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "values",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To avoid potential data contamination issues <cite class=\"ltx_cite ltx_citemacro_citep\">(Sainz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib76\" title=\"\">2023</a>)</cite>, we train from scratch a monolingual ASR model and two-sized multitask (ASR and ST) and multilingual (English and Italian) models. Details about training data and process are presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A2\" title=\"Appendix B Training Settings &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.\nBeing the only non-synthetic dataset supporting both tasks and language directions, we select EuroParl-ST <cite class=\"ltx_cite ltx_citemacro_citep\">(Iranzo-S&#225;nchez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib33\" title=\"\">2020</a>)</cite> as the test set for our analyses. The test set covers both <span class=\"ltx_text ltx_font_italic\">en</span> and <span class=\"ltx_text ltx_font_italic\">it</span> ASR, and <span class=\"ltx_text ltx_font_italic\">en-it</span> and <span class=\"ltx_text ltx_font_italic\">it-en</span> ST.\nThe <span class=\"ltx_text ltx_font_italic\">it/it-en</span> section consists of 1,686 segments, for a total of approximately 6 hours of audio, while the <span class=\"ltx_text ltx_font_italic\">en/en-it</span> section contains 1,130 segments, for a total of approximately 3 hours of audio.</p>\n\n",
                "matched_terms": [
                    "models",
                    "english",
                    "language",
                    "multitask",
                    "multilingual",
                    "model",
                    "asr",
                    "italian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The models analyzed in the paper are all composed of a Conformer encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib29\" title=\"\">2020</a>)</cite> and a Transformer decoder, as Conformer is the current state-of-the-art architecture for S2T processing <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib30\" title=\"\">2021</a>; Srivastava et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib81\" title=\"\">2022</a>; Li &amp; Doddipatla, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib48\" title=\"\">2023</a>)</cite>.\nThe monolingual ASR model (<span class=\"ltx_text ltx_font_typewriter\">base</span>) is composed of 12 encoder layers and 6 decoder layers. Each layer has 8 attention heads, 512 as embedding dimension, and FFNs dimension of 2,048. The vocabulary is built using a SentencePiece unigram model <cite class=\"ltx_cite ltx_citemacro_citep\">(Kudo &amp; Richardson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib42\" title=\"\">2018</a>)</cite> with size 8,000 trained on <span class=\"ltx_text ltx_font_italic\">en</span> transcripts. The resulting number of parameters is 125M.\nThe multitask and multilingual models are of two sizes, <span class=\"ltx_text ltx_font_typewriter\">small</span> and <span class=\"ltx_text ltx_font_typewriter\">large</span>, the first having 12 encoder layers and 6 decoder layers and the latter having 24 encoder layers and 12 decoder layers. In both sizes, each layer has 16 attention heads, an embedding dimension of 1,024, and an FFN dimension of 4,096. The vocabulary is built using a SentencePiece unigram model with size 16,000 trained on <span class=\"ltx_text ltx_font_italic\">en</span> and <span class=\"ltx_text ltx_font_italic\">it</span> transcripts. Two extra tokens&#8211;<span class=\"ltx_text ltx_font_typewriter\">&lt;lang:en&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;lang:it&gt;</span>&#8211;are added to indicate whether the target text is in <span class=\"ltx_text ltx_font_italic\">en</span> or <span class=\"ltx_text ltx_font_italic\">it</span>. The resulting number of parameters is 474M for the <span class=\"ltx_text ltx_font_typewriter\">small</span> model and 878M for the <span class=\"ltx_text ltx_font_typewriter\">large</span> model.\nIn all models, the Conformer encoder is preceded by two 1D convolutional layers with stride 2 and kernel size 5, resulting in a fixed subsampling factor <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> of 4. The kernel size of the Conformer convolutional module is 31 for both the point- and depth-wise convolutions. The input audio is represented by 80 Mel-filterbank features extracted every 10 ms with a window of 25 ms.</p>\n\n",
                "matched_terms": [
                    "models",
                    "multitask",
                    "multilingual",
                    "model",
                    "large",
                    "target",
                    "small",
                    "asr",
                    "input",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Hypothesis and Cross-Attention Generation.</span>\nFor the hypothesis generation, we use beam search with a beam size of 5 and a no-repeat n-gram size of 5. The attention scores are extracted from layers or heads during the output generation. The ASR and ST quality scores of the hypotheses are presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A3\" title=\"Appendix C Quality Metrics for the Reported Models &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nThe inference is performed using a single NVIDIA A40 GPU (40GB RAM) with a batch size of 40,000 tokens and takes <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>2.5 minutes for <span class=\"ltx_text ltx_font_typewriter\">base</span>, <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>3-5.5 minutes for <span class=\"ltx_text ltx_font_typewriter\">small</span>, and <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>3-6.5 for <span class=\"ltx_text ltx_font_typewriter\">large</span>, depending on the source language.</p>\n\n",
                "matched_terms": [
                    "crossattention",
                    "language",
                    "large",
                    "source",
                    "small",
                    "asr",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Explanation Heatmaps Generation.</span>\nFollowing the best configuration obtained in SPES <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>, we adopt the Morphological Fragmental Perturbation Pyramid <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib99\" title=\"\">2021</a>)</cite> for clustering, which relies on Simple Linear Iterative Clustering or SLIC <cite class=\"ltx_cite ltx_citemacro_citep\">(Achanta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib3\" title=\"\">2012</a>)</cite>, a k-means-based algorithm that groups elements according to spectral patterns. We use the default parameters; the threshold length in seconds is\n7.50s, the\n<a class=\"ltx_ref ltx_href\" href=\"https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.slic\" title=\"\">SLIC</a>\nsigma is 0, the compactness is 0.1, and the number of patches per second for the MFPP technique is [400, 500, 600].\nFor the choice of <math alttext=\"p_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">p_{X}</annotation></semantics></math> and <math alttext=\"N_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">N_{X}</annotation></semantics></math>, we refer to the parameters used in <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>, setting <math alttext=\"p_{X}=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>X</mi></msub><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">p_{X}=0.5</annotation></semantics></math> and <math alttext=\"N_{X}=20,000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><msub><mi>N</mi><mi>X</mi></msub><mo>=</mo><mrow><mn>20</mn><mo>,</mo><mn>000</mn></mrow></mrow><annotation encoding=\"application/x-tex\">N_{X}=20,000</annotation></semantics></math>. The quality of the input explanations is presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A3\" title=\"Appendix C Quality Metrics for the Reported Models &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>. For the choice of <math alttext=\"p_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m5\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">p_{H}</annotation></semantics></math> and <math alttext=\"N_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m6\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">N_{H}</annotation></semantics></math>, we use the same number of iterations of <math alttext=\"N_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m7\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">N_{X}</annotation></semantics></math>, i.e., <math alttext=\"N_{H}=20,000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m8\" intent=\":literal\"><semantics><mrow><msub><mi>N</mi><mi>H</mi></msub><mo>=</mo><mrow><mn>20</mn><mo>,</mo><mn>000</mn></mrow></mrow><annotation encoding=\"application/x-tex\">N_{H}=20,000</annotation></semantics></math>, while the optimal occlusion probability <math alttext=\"p_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m9\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">p_{H}</annotation></semantics></math> is determined over the dev set, resulting in <math alttext=\"p_{H}=0.7\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m10\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>H</mi></msub><mo>=</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">p_{H}=0.7</annotation></semantics></math>, whose experiments are reported in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A4\" title=\"Appendix D Effect of Occlusion Probability on Encoder Output Explanations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.\nThe inference is performed using a single NVIDIA A40 GPU (40GB RAM)\nand takes <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m11\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>27 hours for <span class=\"ltx_text ltx_font_typewriter\">base</span>, <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m12\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>3-4 days for <span class=\"ltx_text ltx_font_typewriter\">small</span> and <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m13\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>6-8 days for <span class=\"ltx_text ltx_font_typewriter\">large</span>, depending on the source language.</p>\n\n",
                "matched_terms": [
                    "language",
                    "large",
                    "source",
                    "small",
                    "explanations",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Correlation Computation.</span>\nThe Pearson <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">r</annotation></semantics></math> correlation score is computed using the <span class=\"ltx_text ltx_font_typewriter\">scipy</span> implementation<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html\" title=\"\">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html</a></span></span></span> and averaging across samples in the test set.</p>\n\n",
                "matched_terms": [
                    "across",
                    "correlation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we compare <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> with input saliency maps <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m3\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></sup>, which serve as an external reference for measuring input relevance.\nSpecifically, in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS1\" title=\"5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a>, we analyze the\n<span class=\"ltx_text ltx_font_typewriter\">base</span> model across all levels of granularity. Then, in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS2\" title=\"5.1.2 Multitask and Multilingual Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.2</span></a>, we extend the analysis to additional models (<span class=\"ltx_text ltx_font_typewriter\">small</span> and <span class=\"ltx_text ltx_font_typewriter\">large</span>), languages (<span class=\"ltx_text ltx_font_italic\">en</span> and <span class=\"ltx_text ltx_font_italic\">it</span>), and tasks (ASR and ST).</p>\n\n",
                "matched_terms": [
                    "ùêíùêåmathbfsmxx",
                    "across",
                    "models",
                    "model",
                    "large",
                    "small",
                    "asr",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.T1\" title=\"Table 1 &#8227; 5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reports the correlation scores for the monolingual English ASR model (<span class=\"ltx_text ltx_font_typewriter\">base</span>), considering cross-attention at the head level, layer level, and in aggregated form.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "english",
                    "model",
                    "asr",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At the individual head level, correlations with saliency maps are generally low. This suggests that attention heads, when taken in isolation, only partially capture the model&#8217;s dependency on the input and often encode noisy or inconsistent relevance signals. However, not all heads are equal: some, especially in the upper layers (layers 4-6), exhibit relatively stronger correlations. Notably, <span class=\"ltx_text ltx_font_bold\">averaging across heads consistently outperforms selecting individual heads</span>, suggesting that, despite head-level sparsity and weak individual correlations, the collective information captured across heads reflects input relevance more effectively.\nMoving from heads to layers, we find a clearer picture. Averaging attention scores across all heads within each layer boosts correlation substantially, with layer 6 standing out as the most aligned with the saliency maps. This is followed closely by layer 5 and the average across all layers, indicating that the <span class=\"ltx_text ltx_font_bold\">last layers exhibit the highest alignment with input relevance</span>. These results reinforce the idea that deeper layers encode higher-level semantic or task-relevant features, a trend previously observed in Transformer-based models <cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib13\" title=\"\">2019</a>)</cite>.\nInterestingly, while averaging across heads improves alignment, averaging across both heads and layers does not yield the overall best result, even if values are close. This indicates that not all layers contribute equally and that indiscriminate aggregation can dilute the relevance signal.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "correlation",
                    "low",
                    "overall",
                    "highest",
                    "indicates",
                    "input",
                    "layers",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, the results show that appropriately selected and aggregated cross-attention scores exhibit only a <span class=\"ltx_text ltx_font_italic\">moderate</span> to <span class=\"ltx_text ltx_font_italic\">strong</span> correlation with input saliency maps, reaching values up to 0.588. This provides an initial indication of the\nlimited explanatory power\nof cross-attention weights, which we further examine under multilingual and multitask conditions in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS2\" title=\"5.1.2 Multitask and Multilingual Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "multitask",
                    "multilingual",
                    "overall",
                    "input",
                    "crossattention",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across all configurations, we observe that <span class=\"ltx_text ltx_font_italic\">en</span> ASR yields the highest correlation values, outperforming even the monolingual <span class=\"ltx_text ltx_font_typewriter\">base</span> model (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS1\" title=\"5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a>). This suggests that large-scale multilingual training enhances the alignment between cross-attention and saliency maps, likely due to the improved generalization capacity of the model. In contrast, <span class=\"ltx_text ltx_font_italic\">en-it</span> ST shows a drop in correlation, which is expected given the increased complexity of ST compared to ASR.\nWhen considering <span class=\"ltx_text ltx_font_italic\">it</span> as the source language, we observe a similar pattern: ASR correlations are consistently higher than ST, yet remain below their <span class=\"ltx_text ltx_font_italic\">en</span> counterparts. This discrepancy aligns with the data distribution in training, where <span class=\"ltx_text ltx_font_italic\">en</span> accounts for 84% of the data versus 16% for <span class=\"ltx_text ltx_font_italic\">it</span>, resulting in more robust representations for <span class=\"ltx_text ltx_font_italic\">en</span>.\nAt the layer level, we find consistent evidence that the last decoder layers yield stronger correlations, reaffirming the trends observed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS1\" title=\"5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a>.\nThe specific optimal layer varies with model size: layer 5 performs best in <span class=\"ltx_text ltx_font_typewriter\">small</span>, while layers 8-10 achieve the highest correlations in <span class=\"ltx_text ltx_font_typewriter\">large</span>.\nNevertheless, correlation values across the last\nlayers remain very close, suggesting that their cross-attention scores\nprovide the most robust alignment with saliency maps across both tasks and languages. This trend is further supported by downstream application results, where the final layers have shown the best token-level performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib65\" title=\"\">2023a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib66\" title=\"\">b</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib91\" title=\"\">2024a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "correlation",
                    "language",
                    "multilingual",
                    "model",
                    "large",
                    "highest",
                    "source",
                    "small",
                    "asr",
                    "between",
                    "layers",
                    "crossattention",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Averaging attention scores across layers further improves the correlation with saliency maps in\nalmost\nall configurations. The only exceptions are <span class=\"ltx_text ltx_font_italic\">en</span> and <span class=\"ltx_text ltx_font_italic\">it</span> ASR in <span class=\"ltx_text ltx_font_typewriter\">small</span>, where selective-layer extraction offers a marginal improvement (0.006 for English, 0.001 for Italian).\nTherefore, similarly to what we observed in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS1\" title=\"5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a>, averaging attention across heads and layers consistently yields the best or near-best correlation with <span class=\"ltx_text ltx_font_italic\">moderate</span> to <span class=\"ltx_text ltx_font_italic\">strong</span> correlation with input saliency maps, even considering large-scale models trained in multitask and multilingual settings.\nNonetheless, this alignment accounts for only 49-63% of the total input relevance, indicating that <span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#88185B;\">cross-attention falls short of fully accounting for the S2T models&#8217; behavior</span>.\nSince this limitation may stem from the phenomenon of context mixing, in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS2\" title=\"5.2 What Is the Impact of Context Mixing? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a> we analyze the correlation between cross-attention and encoder output&#8211;representations that have already undergone transformation by the encoder&#8211;to better isolate the true explanatory power of cross-attention.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "correlation",
                    "english",
                    "multitask",
                    "multilingual",
                    "small",
                    "layers",
                    "asr",
                    "between",
                    "input",
                    "italian",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1\" title=\"5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> focused on input relevance, we now investigate whether <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math>\naligns more closely with encoder output saliency maps.\nA higher correlation with encoder output representations would support the hypothesis that discrepancies between cross-attention and input saliency arise from context mixing, due to the reorganization of information within the encoder.\nTo this end,\nwe compare <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> with encoder output saliency maps <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m4\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></sup>, which attribute relevance to the encoder hidden states for each output token (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S3.SS2\" title=\"3.2 Feature Attribution for Speech-to-Text &#8227; 3 Methodology &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>). Layer-wise results for all models are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.T3\" title=\"Table 3 &#8227; 5.2 What Is the Impact of Context Mixing? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "layerwise",
                    "models",
                    "correlation",
                    "between",
                    "input",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Even when examining encoder output representations, we observe trends consistent with those identified in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1\" title=\"5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>. Specifically, when averaged across decoder layers, cross-attention scores consistently provide the strongest or nearly optimal correlation with saliency maps, with the last decoder layers offering more representative explanations than the first ones across all models. As expected, correlation with encoder output representations consistently yields higher scores than those obtained from input representations, with absolute <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math> differences ranging from 0.03 to 0.18, quantifying the influence of context mixing effects to 6.6-16.7%.\nThe increased correlation is also visually evident in the example shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.F2\" title=\"Figure 2 &#8227; 5.2 What Is the Impact of Context Mixing? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, where <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> aligns more closely with the relevance scores from <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m4\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></sup> than with those from <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m5\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m6\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></sup>.\nHowever, despite being unaffected by context mixing, the correlation between <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m7\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> and <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m8\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m9\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></sup> remains limited&#8211;capturing only 52-75% of the relevance.\n<span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#88185B;\">This\ngap underscores the inherent limitations in relying solely on cross-attention as an explanation mechanism, reinforcing its role as an informative but <span class=\"ltx_text ltx_ulem_uline\">incomplete</span> proxy for explainability in S2T models</span>&#8211;not only for input-level saliency, but even at the encoder-output level, where cross-attention directly operates.</p>\n\n",
                "matched_terms": [
                    "ùêíùêåmathbfsmxx",
                    "across",
                    "models",
                    "correlation",
                    "œÅrho",
                    "layers",
                    "explanations",
                    "between",
                    "input",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Discussion.</span>\nOur results demonstrate that, although <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> scores moderately correlate with aggregated <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m2\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math> (with a correlation peaking around 0.45-0.55 in the best-performing settings), they consistently fall short of capturing the full input relevance&#8211;even when context mixing effects are factored out. To directly assess explanation quality, we compute the deletion metric (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A1\" title=\"Appendix A Effect of Aggregation Functions on Input Explanations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>) on the <span class=\"ltx_text ltx_font_typewriter\">base</span> model, finding that <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> achieves 41.2, compared to 52.9 for frequency-aggregated <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m4\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math> and 91.3 for full-resolution maps.\nThis gap underscores that <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m5\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> discards fine-grained time-frequency cues and produces weaker attributions, even under identical aggregation. As further discussed in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A6\" title=\"Appendix F Limitations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>, our analysis is bounded by the use of SPES as the attribution baseline, but the consistent underperformance across correlation and deletion confirms that <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m6\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> offers, at best, an incomplete picture of model behavior. These results also carry implications for downstream tasks. In applications such as timestamp prediction, prior work often relies on attention from a single decoder layer or head <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib91\" title=\"\">2024a</a>; Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib65\" title=\"\">2023a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib66\" title=\"\">b</a>; Zusag et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib106\" title=\"\">2024</a>)</cite>. Our analysis suggests that averaging across layers and, especially, across heads provides a closer match to saliency behavior and could improve these methods.\nBuilding on past success with attention regularization in ASR (e.g., imposing monotonicity as in <cite class=\"ltx_cite ltx_citemacro_citet\">Zhao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib104\" title=\"\">2020</a></cite>), similar training-time strategies&#8211;such as auxiliary losses that align attention with saliency&#8211;could further benefit downstream applications, enhancing both interpretability and task performance.\nIn summary, <span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"--ltx-fg-color:#88185B;\"><math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m7\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> should not be treated as a stand-alone XAI tool</span>. It provides lightweight cues that may complement attribution-based methods, but it cannot replace them. Reframing <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m8\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> as an auxiliary rather than a proxy recalibrates expectations and grounds future work on more faithful and effective approaches to explainability in S2T models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "correlation",
                    "model",
                    "asr",
                    "input",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conclusions.</span>\nWe presented the first systematic analysis of cross-attention in S2T through the lens of explainable AI, comparing it to saliency maps across tasks, languages, and model scales. Cross-attention moderately to strongly aligns with saliency&#8211;especially when averaged across heads and layers&#8211;but captures only about half of the input relevance. Even when disentangling the effect of context mixing by analyzing encoder outputs, it explains just 52-75% of saliency. This gap reveals intrinsic limits of cross-attention as an explanation mechanism: it offers informative cues but only a partial view of the factors driving S2T predictions.</p>\n\n",
                "matched_terms": [
                    "input",
                    "across",
                    "model",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Broader Implications.</span> Explainability in S2T systems has tangible implications for AI transparency, especially in high-stakes settings such as healthcare, legal transcription, and educational accessibility. Our findings provide insights about the usage of cross-attention as a tool for identifying how models relate output predictions to input regions, which can support auditing, debugging, and fair deployment. However, there is a risk that misinterpreted attention visualizations may be overtrusted by non-expert users, reinforcing false confidence in system behavior <cite class=\"ltx_cite ltx_citemacro_citep\">(Rudin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib75\" title=\"\">2019</a>)</cite>. Moreover, our language choices and focus on high-resource speech still reflect global imbalances in language technology access <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib36\" title=\"\">2020</a>)</cite>. Future work should extend this analysis to low-resource and underrepresented languages to promote broader inclusion.</p>\n\n",
                "matched_terms": [
                    "input",
                    "models",
                    "language",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Use of Large Language Models.</span> For the writing process, ChatGPT was employed exclusively to correct grammar in content authored by humans.</p>\n\n",
                "matched_terms": [
                    "language",
                    "models",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the reproducibility of our results, we described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S4\" title=\"4 Experimental Settings &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> all the details regarding our model training, training and evaluation data, and evaluation procedure. Moreover, we relied only on openly available data and on open source code<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/hlt-mt/FBK-fairseq/blob/master/fbk_works/XAI_FEATURE_ATTRIBUTION.md\" title=\"\">https://github.com/hlt-mt/FBK-fairseq/blob/master/fbk_works/XAI_FEATURE_ATTRIBUTION.md</a></span></span></span> for the generation of the saliency maps. Lastly, all models (described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S4\" title=\"4 Experimental Settings &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), code, attention scores, and explanation artifacts will be released under the Apache 2.0 (code) and CC-BY 4.0 (all other materials) licenses upon paper acceptance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "source"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To properly obtain input-level explanations comparable with the dimensions of cross-attention scores (i.e., making <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\\in\\mathbb{R}^{I\\times T^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m1\" intent=\":literal\"><semantics><mrow><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>T</mi><mo>&#8242;</mo></msup></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\\in\\mathbb{R}^{I\\times T^{\\prime}}</annotation></semantics></math>), we explore the effect of different aggregation strategies over the time and frequency dimensions.</p>\n\n",
                "matched_terms": [
                    "explanations",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A1.T4\" title=\"Table 4 &#8227; Appendix A Effect of Aggregation Functions on Input Explanations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> reports the deletion scores and, for completeness, the Pearson <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m1\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math> correlations between the cross attention scores <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> and the saliency maps <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m4\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></sup> for the representations aggregated following three strategies:</p>\n\n",
                "matched_terms": [
                    "ùêíùêåmathbfsmxx",
                    "œÅrho",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The aggregation functions were selected to contrast methods that either isolate the most relevant features (with maximum pooling) or represent their mean relevance (with average pooling). Similarly, the 2-step approach has been tried to first isolate relevance patterns in the frequency domain, a dimension that is not present in cross-attention representation, and then average across the time dimension to match the downsampled time resolution of the cross-attention scores.</p>\n\n",
                "matched_terms": [
                    "across",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Among the tested methods, we observe that the 2D maximum pooling aggregation (2D max) yields the best quality explanations, obtaining the highest deletion score, while the 2D average pooling (2D avg) is the worst, with the lowest deletion score. Looking at the correlations, we notice that they follow the same trend of deletion scores, with the 2D max yielding the best <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m1\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math>. In particular, 2D avg consistently has the lowest correlations compared to the 2D max, particularly in the last layers (e.g., 0.457 against 0.572 at layer 5). Regarding the 2-step pooling approach, we not only observe an improved deletion score but also better correlation scores compared to 2D avg, especially from layer 3 onward, approaching the best performance with a layer-average correlation of 0.565. Nevertheless, the explanation quality is still lower compared to 2D max (i.e., 55.18 against 57.04), which also achieves the highest correlations at nearly every layer, peaking at 0.582 in layer 6, and yielding the best overall correlation among the averaged cross-attention across layers (i.e., 0.572).</p>\n\n",
                "matched_terms": [
                    "crossattention",
                    "across",
                    "correlation",
                    "œÅrho",
                    "overall",
                    "highest",
                    "explanations",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the monolingual ASR model, we leverage the speech-to-text English data available for the IWSLT 2024 evaluation campaign (offline task),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://iwslt.org/2024/offline\" title=\"\">https://iwslt.org/2024/offline</a></span></span></span> namely: CommonVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Ardila et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib4\" title=\"\">2020</a>)</cite>, CoVoST v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib90\" title=\"\">2021b</a>)</cite>, Europarl-ST <cite class=\"ltx_cite ltx_citemacro_citep\">(Iranzo-S&#225;nchez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib33\" title=\"\">2020</a>)</cite>, LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib64\" title=\"\">2015</a>)</cite>, MuST-C v1 <cite class=\"ltx_cite ltx_citemacro_citep\">(Di&#160;Gangi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib17\" title=\"\">2019</a>)</cite>, TEDLIUM v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Hernandez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib31\" title=\"\">2018</a>)</cite>, and VoxPopuli ASR <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib89\" title=\"\">2021a</a>)</cite>. The resulting training set is about 3k hours of speech.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "model",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the multitask (ASR and ST) multilingual large-scale models, we leverage more than 150k hours of open-source speech<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>Speech, transcripts, and translations released under an open-source license such as CC-0 and CC-BY 4.0.</span></span></span> in English (<span class=\"ltx_text ltx_font_italic\">en</span>) and Italian (<span class=\"ltx_text ltx_font_italic\">it</span>), namely: CommonVoice, CoVoST v2, FLEURS <cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib14\" title=\"\">2023</a>)</cite>, MOSEL <cite class=\"ltx_cite ltx_citemacro_citep\">(Gaido et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib23\" title=\"\">2024a</a>)</cite>, MLS <cite class=\"ltx_cite ltx_citemacro_citep\">(Pratap et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib71\" title=\"\">2020</a>)</cite>, and YouTube-Commons<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hf.co/datasets/PleIAs/YouTube-Commons\" title=\"\">https://hf.co/datasets/PleIAs/YouTube-Commons</a></span></span></span> (from which 14.2k hours of <span class=\"ltx_text ltx_font_italic\">en</span> and 1.8k for <span class=\"ltx_text ltx_font_italic\">it</span> have been extracted). For datasets missing the translations, we generated them using <span class=\"ltx_text ltx_font_typewriter\">MADLAD-400 3B-MT</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Kudugunta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib43\" title=\"\">2023</a>)</cite>. This setting allows us to verify our analysis with a large-scale setting similar to the scale of a popular model such as OWSM <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib69\" title=\"\">2023</a>)</cite> and 2 times that of NVIDIA Canary <cite class=\"ltx_cite ltx_citemacro_citep\">(Puvvada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib72\" title=\"\">2024</a>)</cite> while having complete control of data used during training, ensuring that data contamination issues are avoided completely.</p>\n\n",
                "matched_terms": [
                    "models",
                    "english",
                    "multitask",
                    "multilingual",
                    "model",
                    "asr",
                    "italian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train all models using a combination of three losses: <em class=\"ltx_emph ltx_font_italic\">i)</em> a label-smoothed cross-entropy loss (<math alttext=\"\\mathcal{L}_{\\text{CE}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>CE</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{CE}}</annotation></semantics></math>) applied to the decoder output using the target text as the reference (transcripts for ASR and translations for ST), <em class=\"ltx_emph ltx_font_italic\">ii)</em> a CTC loss <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib27\" title=\"\">2006</a>)</cite> computed using transcripts as reference (<math alttext=\"\\mathcal{L}_{\\text{CTCsrc}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>CTCsrc</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{CTCsrc}}</annotation></semantics></math>) on the output of the 8<sup class=\"ltx_sup\">th</sup> encoder layer for <span class=\"ltx_text ltx_font_typewriter\">base</span> and <span class=\"ltx_text ltx_font_typewriter\">small</span> and the 16<sup class=\"ltx_sup\">th</sup> for <span class=\"ltx_text ltx_font_typewriter\">medium</span>, <em class=\"ltx_emph ltx_font_italic\">iii)</em> a CTC loss on the final encoder output (<math alttext=\"\\mathcal{L}_{\\text{CTCtgt}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>CTCtgt</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{CTCtgt}}</annotation></semantics></math>) applied to predict the target text <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib98\" title=\"\">2023</a>)</cite>.\nThe final loss is the weighted sum of the above-mentioned losses:</p>\n\n",
                "matched_terms": [
                    "models",
                    "target",
                    "small",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The monolingual ASR <span class=\"ltx_text ltx_font_typewriter\">base</span> model is trained on all 3k hours of ASR data for 200k steps using Noam as the learning rate scheduler <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib85\" title=\"\">2017</a>)</cite> with a peak of 2e-3 and 25,000 warm-up steps.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The multitask and multilingual models are trained using a two-stage approach, where the model is pre-trained first on ASR data only (ASR pre-training) and then trained on both ASR and ST data (ASR+ST training). For the ASR pre-training, the learning rate scheduler adopted for the <span class=\"ltx_text ltx_font_typewriter\">small</span> model is the same as the <span class=\"ltx_text ltx_font_typewriter\">base</span> model.\nFor the <span class=\"ltx_text ltx_font_typewriter\">medium</span> model, we adopted a piece-wise warm-up on the Noam scheduler to avoid divergence issues <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib70\" title=\"\">2024</a>)</cite>, with the learning rate first increasing linearly to 2e-5 for 25k steps and then to 2e-4 for an additional 25k steps, followed by the standard inverse square root function.\nFor the ASR+ST training, we sample the ASR target with probability 0.5 and use the ST target otherwise following the same settings of ASR pre-training, except for the learning rate that is set to a constant value of 1e-4 for <span class=\"ltx_text ltx_font_typewriter\">small</span> and 1e-5 for <span class=\"ltx_text ltx_font_typewriter\">medium</span>, following the same downscale of the ASR pre-taining. Both training stages lasted 1M steps, corresponding to <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>6 epochs over the training data.</p>\n\n",
                "matched_terms": [
                    "models",
                    "multitask",
                    "multilingual",
                    "model",
                    "target",
                    "small",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the <span class=\"ltx_text ltx_font_typewriter\">base</span> model, the trainings are executed on 4 NVIDIA A100 GPUs (64GB RAM) with a mini-batch of 40,000 tokens, an update frequency of 2, and averaging the last 7 checkpoints obtained from the training.\nFor the multitask and multitlingual models, we use mini-batches of 10,000 tokens for the <span class=\"ltx_text ltx_font_typewriter\">small</span> and 4,500 for the <span class=\"ltx_text ltx_font_typewriter\">medium</span> with an update frequency of, respectively, 2 and 6 on 16 NVIDIA A100 GPUs (64GB RAM), save checkpoints every 1,000 steps and average the last 25 checkpoints to obtain the final models.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "small",
                    "multitask"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The quality of the ASR hypotheses is evaluated with the WER metric using the jiWER library<span class=\"ltx_note ltx_role_footnote\" id=\"footnote12\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_tag ltx_tag_note\">12</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://pypi.org/project/jiwer/\" title=\"\">https://pypi.org/project/jiwer/</a></span></span></span> and applying the Whisper text normalizer<span class=\"ltx_note ltx_role_footnote\" id=\"footnote13\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_tag ltx_tag_note\">13</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://pypi.org/project/whisper-normalizer/\" title=\"\">https://pypi.org/project/whisper-normalizer/</a></span></span></span>.\nThe quality of the ST hypotheses is evaluated using COMET <cite class=\"ltx_cite ltx_citemacro_citep\">(Rei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib74\" title=\"\">2020</a>)</cite> version 2.2.4, with the default model.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\">14</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hf.co/Unbabel/wmt22-comet-da\" title=\"\">https://hf.co/Unbabel/wmt22-comet-da</a></span></span></span>\nThe quality of the explanations is obtained by measuring both\n<span class=\"ltx_text ltx_font_italic\">deletion</span> and <span class=\"ltx_text ltx_font_italic\">size</span> metrics\navailable in the SPES repository, using <span class=\"ltx_text ltx_font_typewriter\">wer_max</span> as the scorer for ASR and <span class=\"ltx_text ltx_font_typewriter\">bleu</span> for ST, as described in <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "explanations",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For comparison, in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A3.T5\" title=\"Table 5 &#8227; Appendix C Quality Metrics for the Reported Models &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we also report results obtained from popular large-scale models, namely Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib73\" title=\"\">2023</a>)</cite>, OWSM v3.1 <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib70\" title=\"\">2024</a>)</cite>, and SeamlessM4T <cite class=\"ltx_cite ltx_citemacro_citep\">(Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib7\" title=\"\">2023</a>)</cite>. Looking at the transcription/translation quality performance, we observe that both the monolingual <span class=\"ltx_text ltx_font_typewriter\">base</span> model and the multitask multilingual <span class=\"ltx_text ltx_font_typewriter\">small</span> and <span class=\"ltx_text ltx_font_typewriter\">large</span> models are mostly able to achieve competitive results, even outperforming the well-known models in two cases (<span class=\"ltx_text ltx_font_italic\">en</span> ASR for <span class=\"ltx_text ltx_font_typewriter\">base</span> and <span class=\"ltx_text ltx_font_italic\">en-it</span> ST for <span class=\"ltx_text ltx_font_typewriter\">large</span>). While our models and OWSM v3.1 strive to be on par on <span class=\"ltx_text ltx_font_italic\">it</span> with models with closed training data (Whisper and Seamless), they are able to close the gap on <span class=\"ltx_text ltx_font_italic\">en</span>, most probably given a larger availability of public training data. Moreover, the highest performance of <span class=\"ltx_text ltx_font_typewriter\">base</span> on <span class=\"ltx_text ltx_font_italic\">en</span> ASR compared to the <span class=\"ltx_text ltx_font_typewriter\">small</span> and <span class=\"ltx_text ltx_font_typewriter\">large</span> can be attributed to both the specialization of the model and the presence of the EuroParl-ST training set in the training data.</p>\n\n",
                "matched_terms": [
                    "models",
                    "multitask",
                    "multilingual",
                    "model",
                    "large",
                    "highest",
                    "small",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moving to the explanation quality, we observe that both deletion and size scores are comparable across all three models analyzed in the paper and coherent with values obtained in the original SPES paper <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite> on different benchmarks and models. Overall, the deletion scores for ASR are close to the highest possible value (i.e., 100), especially on <span class=\"ltx_text ltx_font_italic\">it</span>, where 97% is achieved. Similarly, the deletion scores for ST are close to 0, indicating that the quality of explanations is very high. The size scores are all close, ranging between 28.2 and 30.6 among models, languages, and tasks, indicating a good compactness of the explanations.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "overall",
                    "high",
                    "highest",
                    "asr",
                    "between",
                    "explanations",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To properly choose the occlusion probability (<math alttext=\"p_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m1\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">p_{H}</annotation></semantics></math>) for the encoder output explanations <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m3\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></sup>, we conducted experiments by varying this probability in the set of <math alttext=\"\\{0.1,0.3,0.5,0.7,0.9\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>0.1</mn><mo>,</mo><mn>0.3</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>0.7</mn><mo>,</mo><mn>0.9</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{0.1,0.3,0.5,0.7,0.9\\}</annotation></semantics></math>, similarly to what has been done for determining the input occlusion probability (<math alttext=\"p_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m5\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">p_{X}</annotation></semantics></math>) in SPES <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "input",
                    "explanations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A4.T6\" title=\"Table 6 &#8227; Appendix D Effect of Occlusion Probability on Encoder Output Explanations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we report the <span class=\"ltx_text ltx_font_italic\">deletion</span>\nmetric computed on the dev set and, for completeness, the results of the Pearson <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m1\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math> correlation with cross-attention <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math>.\nAnalogously to\nthe deletion metric computed on the input saliency maps <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>, we compute the deletion on the encoder output saliency maps by iteratively replacing\nportions of the encoder output sequence <math alttext=\"\\mathbf{H}=\\{\\mathbf{h}_{1},\\ldots,\\mathbf{h}_{I}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#119815;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>&#119841;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119841;</mi><mi>I</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{H}=\\{\\mathbf{h}_{1},\\ldots,\\mathbf{h}_{I}\\}</annotation></semantics></math> with zero vectors, and removing 5% of the most important time frames at each step based on their saliency.\nFrame importance is determined using saliency maps <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m4\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>H</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}</annotation></semantics></math> aggregated at the sentence level.\nThe output quality is evaluated using the same <span class=\"ltx_text ltx_font_typewriter\">wer_max</span> scorer from the SPES repository. Lastly, we compute the area under the curve of the WER progression to quantify the faithfulness of the explanation.</p>\n\n",
                "matched_terms": [
                    "input",
                    "correlation",
                    "œÅrho",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the results, we can notice that higher occlusion probabilities yield not only a better deletion score but also an increased correlation with <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p3.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math>. The overall best correlation is achieved when averaging across all layers, and layer 5 achieves the best layer-specific correlation values, a phenomenon that remains coherent even when varying the occlusion probability.\nInterestingly, the deletion scores and the <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p3.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math>-<math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p3.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p3.m4\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></sup> correlations always follow the same trend, with the best values achieved with <math alttext=\"p_{H}=0.7\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p3.m5\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>H</mi></msub><mo>=</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">p_{H}=0.7</annotation></semantics></math>, which we used in all experiments reported in the main paper.</p>\n\n",
                "matched_terms": [
                    "across",
                    "correlation",
                    "overall",
                    "layers",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Examples of different saliency maps and cross-attention representations obtained with the <span class=\"ltx_text ltx_font_typewriter\">large</span> model are presented in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "crossattention",
                    "model",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We notice similar relevance patterns in the paired samples&#8211;i.e., the samples having the same source language (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>a-f, and Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>g-l)&#8211;even if involving different tasks. We observe a reordering phenomenon from the English audio <span class=\"ltx_text ltx_inline-quote ltx_outerquote\">&#8220;<span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#0000FF;\">cheap<span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\"> <span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">money</span></span></span>&#8221;</span> and its Italian textual counterpart <span class=\"ltx_text ltx_inline-quote ltx_outerquote\">&#8220;<span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#FF0000;\">denaro<span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\"> <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">a buon mercato</span></span></span>&#8221;</span>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote15\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_tag ltx_tag_note\">15</span>Same colors reflect the same concepts.</span></span></span> which is reflected in the saliency maps (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>d and e) and also captured by cross-attention (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>f). We also observe that there are some patterns captured by the attention that are not reflected in the input. For instance, in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>f, the first words (<span class=\"ltx_text ltx_inline-quote ltx_outerquote\">&#8220;<span class=\"ltx_text ltx_font_italic\">&#200; solo</span>&#8221;</span>) attend&#8211;albeit with relatively low scores&#8211;to the audio frames between 75 and 85, while this pattern this pattern is absent in the relevance scores of both the encoder output and the input. Consistent with the findings discussed throughout the paper, this example illustrates that while attention generally follows the saliency patterns identified by feature attribution, some discrepancies persist.</p>\n\n",
                "matched_terms": [
                    "english",
                    "language",
                    "low",
                    "source",
                    "between",
                    "input",
                    "italian",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work provides an in-depth analysis of cross-attention explainability in encoder-decoder S2T models. While it yields actionable insights, some limitations should be acknowledged. First, our experimental scope is restricted to ASR and ST. Although these tasks are central to S2T-based AI <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib73\" title=\"\">2023</a>; Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib7\" title=\"\">2023</a>)</cite>, we do not evaluate other downstream tasks such as spoken question answering or speech summarization, which may involve different dynamics in decoder attention. Second, our multilingual analysis is limited to English (a Germanic language) and Italian (a Romance language), due to the high computational cost of large-scale model training across a broader set of languages. Third, we focus on models trained from scratch but do not include architectures based on Speech Foundation Models (SFMs) paired with large language models (LLMs), often referred to as SpeechLLM&#8211;a recent growing area of interest in S2T modeling <cite class=\"ltx_cite ltx_citemacro_citep\">(Gaido et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib24\" title=\"\">2024b</a>)</cite>. As our analysis focuses on evaluation, our goal was to completely avoid data contamination issues <cite class=\"ltx_cite ltx_citemacro_citep\">(Sainz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib76\" title=\"\">2023</a>)</cite>, which is a problem affecting almost every SFM and SpeechLLM architectures currently available, as we have no control over their training data, and, for this reason, we decided to retrain the models from scratch.\nFourth, our analysis relies on SPES to compute reference explanations, acknowledging that, as an empirical method, it may introduce some margin of error. However, in the absence of a gold or human reference&#8211;which is unattainable in practice&#8211;we adopt SPES as a <span class=\"ltx_text ltx_font_italic\">silver</span> reference, since it represents the state of the art in explainability for speech-to-text. We further validate this choice in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A3\" title=\"Appendix C Quality Metrics for the Reported Models &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>, showing that SPES achieves very high quality explanations (deletion scores <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>90 on ASR and <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m2\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>3 on ST), making it a more faithful option than less robust alternatives from the generic XAI field such as gradient norms <cite class=\"ltx_cite ltx_citemacro_citep\">(Covert et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib15\" title=\"\">2021a</a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "english",
                    "language",
                    "multilingual",
                    "model",
                    "large",
                    "high",
                    "asr",
                    "explanations",
                    "italian",
                    "crossattention"
                ]
            }
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Person œÅ\\rho correlation between layer-wise cross-attention ùêÇùêÄ¬Ø(‚Ñì)\\widebar{\\text{{\\color[rgb]{0.75390625,0.390625,0.9609375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.75390625,0.390625,0.9609375}$\\mathbf{CA}$}}}^{(\\ell)} and the encoder output explanations ùêíùêå\\mathbf{SM}HH for all models (base, small and large). Bold indicates the highest overall correlation, underline indicates the highest correlation across layers. Low to high values for ASR are yellow to orange, and ST are light cyan to dark cyan.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"14\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_bold\">Target Language</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column\" colspan=\"2\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"14\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_italic\">en</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" colspan=\"2\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_bold\">Lang.</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><math alttext=\"\\ell=1\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m1\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=1</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><math alttext=\"\\ell=2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m2\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=2</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><math alttext=\"\\ell=3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m3\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=3</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><math alttext=\"\\ell=4\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m4\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=4</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><math alttext=\"\\ell=5\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m5\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=5</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><math alttext=\"\\ell=6\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m6\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=6</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><math alttext=\"\\ell=7\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m7\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>7</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=7</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><math alttext=\"\\ell=8\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m8\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=8</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><math alttext=\"\\ell=9\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m9\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>9</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=9</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><math alttext=\"\\ell=10\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m10\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=10</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><math alttext=\"\\ell=11\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m11\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>11</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=11</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><math alttext=\"\\ell=12\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m12\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>12</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=12</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\">\n<math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.m13\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math><span class=\"ltx_text ltx_font_bold\">-AVG</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_typewriter\">base</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FDFCDC;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDFCDC;\">0.105</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FDF5C5;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDF5C5;\">0.179</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FFC541;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFC541;\">0.602</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FFA022;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFA022;\">0.708</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FF9316;padding-left:2.7pt;padding-right:2.7pt;\">\n<span class=\"ltx_ERROR undefined\">\\ultab</span><span class=\"ltx_text\" style=\"--ltx-bg-color:#FF9316;\">0.745</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FFA526;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFA526;\">0.693</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"6\" style=\"padding-left:2.7pt;padding-right:2.7pt;\">-</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"--ltx-bg-color:#FF9014;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FF9014;\">0.752</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<td class=\"ltx_td ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_typewriter\">small</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FDF9D3;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDF9D3;\">0.137</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FDF0B8;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDF0B8;\">0.220</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FED463;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FED463;\">0.492</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FF9F20;padding-left:2.7pt;padding-right:2.7pt;\">\n<span class=\"ltx_ERROR undefined\">\\ultab</span><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FF9F20;\">0.712</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FFA123;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFA123;\">0.704</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FFB02F;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFB02F;\">0.664</span></td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"6\" style=\"padding-left:2.7pt;padding-right:2.7pt;\">-</td>\n<td class=\"ltx_td ltx_align_left\" style=\"--ltx-bg-color:#FFA122;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFA122;\">0.706</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#FFFFFF;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"--ltx-bg-color:#FFFFFF;\">en</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_typewriter\">large</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FDF7CB;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDF7CB;\">0.161</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FDF9D1;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDF9D1;\">0.143</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FDF0B7;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDF0B7;\">0.224</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FEE38F;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FEE38F;\">0.352</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FDE69B;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDE69B;\">0.316</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FED568;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FED568;\">0.476</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FFB433;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFB433;\">0.651</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FFA525;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFA525;\">0.695</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FFA425;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFA425;\">0.697</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FFA324;padding-left:2.7pt;padding-right:2.7pt;\">\n<span class=\"ltx_ERROR undefined\">\\ultab</span><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFA324;\">0.699</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FFBC39;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFBC39;\">0.629</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#FFB835;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFB835;\">0.641</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"--ltx-bg-color:#FF9D1F;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FF9D1F;\">0.718</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_typewriter\">small</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#BEE9FE;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#BEE9FE;\">0.140</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#B4E3F9;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#B4E3F9;\">0.196</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#96D0EA;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#96D0EA;\">0.362</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#75C7E3;padding-left:2.7pt;padding-right:2.7pt;\">\n<span class=\"ltx_ERROR undefined\">\\ultab</span><span class=\"ltx_text\" style=\"--ltx-bg-color:#75C7E3;\">0.502</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#77C7E4;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#77C7E4;\">0.495</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#7EC8E5;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#7EC8E5;\">0.467</span></td>\n<td class=\"ltx_td ltx_border_t\" colspan=\"6\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"--ltx-bg-color:#70C6E3;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#70C6E3;\">0.519</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_italic\">it</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_typewriter\">large</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#C0EAFF;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#C0EAFF;\">0.131</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#C0EAFF;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#C0EAFF;\">0.128</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#B2E1F8;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#B2E1F8;\">0.207</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#A1D6F0;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#A1D6F0;\">0.304</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#A4D8F1;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#A4D8F1;\">0.287</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#8ECAE6;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#8ECAE6;\">0.407</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#72C7E3;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#72C7E3;\">0.511</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#67C5E2;padding-left:2.7pt;padding-right:2.7pt;\">\n<span class=\"ltx_ERROR undefined\">\\ultab</span><span class=\"ltx_text\" style=\"--ltx-bg-color:#67C5E2;\">0.553</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#6BC6E2;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#6BC6E2;\">0.541</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#69C5E2;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#69C5E2;\">0.545</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#78C7E4;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#78C7E4;\">0.491</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#73C7E3;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#73C7E3;\">0.509</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"--ltx-bg-color:#52C2DF;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#52C2DF;\">0.633</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<td class=\"ltx_td ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"14\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_italic\">it</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_typewriter\">small</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#B8E5FB;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#B8E5FB;\">0.176</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#ADDEF6;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ADDEF6;\">0.235</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#86C9E5;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#86C9E5;\">0.439</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#4CC2DE;padding-left:2.7pt;padding-right:2.7pt;\">\n<span class=\"ltx_ERROR undefined\">\\ultab</span><span class=\"ltx_text\" style=\"--ltx-bg-color:#4CC2DE;\">0.655</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#4DC2DF;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#4DC2DF;\">0.650</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#59C3E0;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#59C3E0;\">0.606</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"6\" style=\"padding-left:2.7pt;padding-right:2.7pt;\">-</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"--ltx-bg-color:#4BC1DE;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#4BC1DE;\">0.659</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_italic\">en</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_typewriter\">large</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#B6E4FA;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#B6E4FA;\">0.184</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#BBE7FD;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#BBE7FD;\">0.160</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#ACDDF5;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#ACDDF5;\">0.241</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#99D1EC;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#99D1EC;\">0.348</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#97D0EB;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#97D0EB;\">0.357</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#7AC8E4;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#7AC8E4;\">0.484</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#51C2DF;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#51C2DF;\">0.637</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#49C1DE;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#49C1DE;\">0.667</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#4AC1DE;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#4AC1DE;\">0.661</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#49C1DE;padding-left:2.7pt;padding-right:2.7pt;\">\n<span class=\"ltx_ERROR undefined\">\\ultab</span><span class=\"ltx_text\" style=\"--ltx-bg-color:#49C1DE;\">0.668</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#54C3DF;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#54C3DF;\">0.625</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#54C3DF;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#54C3DF;\">0.625</span></td>\n<td class=\"ltx_td ltx_align_left\" style=\"--ltx-bg-color:#44C0DD;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#44C0DD;\">0.683</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_typewriter\">small</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FDFAD4;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDFAD4;\">0.133</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FDF3BF;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDF3BF;\">0.199</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FEDE81;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FEDE81;\">0.397</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FECB48;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FECB48;\">0.579</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FECA45;padding-left:2.7pt;padding-right:2.7pt;\">\n<span class=\"ltx_ERROR undefined\">\\ultab</span><span class=\"ltx_text\" style=\"--ltx-bg-color:#FECA45;\">0.588</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FECD4F;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FECD4F;\">0.555</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"6\" style=\"padding-left:2.7pt;padding-right:2.7pt;\">-</td>\n<td class=\"ltx_td ltx_align_left\" style=\"--ltx-bg-color:#FFC843;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFC843;\">0.594</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:8.9pt;height:81.4pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:81.4pt;transform:translate(-36.3pt,-36.3pt) rotate(-90deg) ;\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Source language</span></p>\n</span></div>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_italic\">it</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\" style=\"padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text ltx_font_typewriter\">large</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"--ltx-bg-color:#FDF8CE;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDF8CE;\">0.152</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"--ltx-bg-color:#FDFBD8;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDFBD8;\">0.119</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"--ltx-bg-color:#FDF4C3;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDF4C3;\">0.187</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"--ltx-bg-color:#FDE698;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDE698;\">0.322</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"--ltx-bg-color:#FDE89E;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FDE89E;\">0.303</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"--ltx-bg-color:#FEDB79;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FEDB79;\">0.423</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"--ltx-bg-color:#FECD4E;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FECD4E;\">0.560</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"--ltx-bg-color:#FFC440;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFC440;\">0.605</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"--ltx-bg-color:#FFC440;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FFC440;\">0.606</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"--ltx-bg-color:#FFBF3C;padding-left:2.7pt;padding-right:2.7pt;\">\n<span class=\"ltx_ERROR undefined\">\\ultab</span><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#FFBF3C;\">0.619</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"--ltx-bg-color:#FECB4A;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FECB4A;\">0.573</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" style=\"--ltx-bg-color:#FECC4C;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FECC4C;\">0.565</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" style=\"--ltx-bg-color:#FECB4A;padding-left:2.7pt;padding-right:2.7pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FECB4A;\">0.573</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "ultab0553",
            "ultab0619",
            "output",
            "‚Ñìellavg",
            "overall",
            "high",
            "‚Ñì1ell1",
            "source",
            "ultab0712",
            "ultab0502",
            "‚Ñì2ell2",
            "underline",
            "ultab0588",
            "‚Ñì10ell10",
            "base",
            "‚Ñì9ell9",
            "low",
            "ultab0745",
            "target",
            "small",
            "encoder",
            "lang",
            "between",
            "ultab0668",
            "ultab0699",
            "layerwise",
            "across",
            "ùêÇùêÄ¬Ø‚Ñìwidebartextcolorrgb075390625039062509609375definecolornamedpgfstrokecolorrgb075390625039062509609375mathbfcaell",
            "ùêíùêåmathbfsmhh",
            "language",
            "‚Ñì12ell12",
            "bold",
            "‚Ñì11ell11",
            "‚Ñì4ell4",
            "asr",
            "indicates",
            "‚Ñì6ell6",
            "‚Ñì7ell7",
            "crossattention",
            "layers",
            "values",
            "‚Ñì8ell8",
            "correlation",
            "ultab0655",
            "œÅrho",
            "light",
            "yellow",
            "model",
            "orange",
            "large",
            "highest",
            "all",
            "explanations",
            "cyan",
            "dark",
            "‚Ñì3ell3",
            "‚Ñì5ell5",
            "person"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">While Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1\" title=\"5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> focused on input relevance, we now investigate whether <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math>\naligns more closely with encoder output saliency maps.\nA higher correlation with encoder output representations would support the hypothesis that discrepancies between cross-attention and input saliency arise from context mixing, due to the reorganization of information within the encoder.\nTo this end,\nwe compare <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> with encoder output saliency maps <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m4\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></sup>, which attribute relevance to the encoder hidden states for each output token (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S3.SS2\" title=\"3.2 Feature Attribution for Speech-to-Text &#8227; 3 Methodology &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>). Layer-wise results for all models are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.T3\" title=\"Table 3 &#8227; 5.2 What Is the Impact of Context Mixing? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Cross-attention is a core mechanism in encoder-decoder architectures, widespread in many fields, including speech-to-text (S2T) processing. Its scores have been repurposed for various downstream applications&#8211;such as timestamp estimation and audio-text alignment&#8211;under the assumption that they reflect the dependencies between input speech representation and the generated text. While the explanatory nature of attention mechanisms has been widely debated in the broader NLP literature, this assumption remains largely unexplored within the speech domain. To address this gap, we assess the explanatory power of cross-attention in S2T models by comparing its scores to input saliency maps derived from feature attribution. Our analysis spans monolingual and multilingual, single-task and multi-task models at multiple scales, and shows that attention scores moderately to strongly align with saliency-based explanations, particularly when aggregated across heads and layers. However, it also shows that cross-attention captures only about 50% of the input relevance and, in the best case, only partially reflects how the decoder attends to the encoder&#8217;s representations&#8211;accounting for just 52-75% of the saliency. These findings uncover fundamental limitations in interpreting cross-attention as an explanatory proxy, suggesting that it offers an informative yet incomplete view of the factors driving predictions in S2T models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "crossattention",
                    "explanations",
                    "between",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-attention <cite class=\"ltx_cite ltx_citemacro_citep\">(Bahdanau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib6\" title=\"\">2015</a>)</cite> is the core mechanism of the encoder-decoder Transformer architecture <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib85\" title=\"\">2017</a>)</cite>, a model that has become foundational across numerous AI domains <cite class=\"ltx_cite ltx_citemacro_citep\">(Galassi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib25\" title=\"\">2021</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib49\" title=\"\">2022</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib45\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib92\" title=\"\">2024b</a>; Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib51\" title=\"\">2024</a>)</cite>, including natural language processing (NLP).\nDesigned for modeling dependencies between the generated output sequence and the input representations, the cross-attention scores&#8211;derived from the attention mechanism&#8211;have been leveraged in various NLP tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib32\" title=\"\">2020</a>; Zhang &amp; Kim, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib103\" title=\"\">2023</a>)</cite>, such as\nsource-target textual alignment <cite class=\"ltx_cite ltx_citemacro_citep\">(Garg et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib26\" title=\"\">2019</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib12\" title=\"\">2020</a>)</cite>, co-reference resolution <cite class=\"ltx_cite ltx_citemacro_citep\">(Voita et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib87\" title=\"\">2018</a>)</cite>, and word sense disambiguation <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib83\" title=\"\">2018</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "language",
                    "output",
                    "model",
                    "between",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In speech-to-text (S2T) modeling, cross-attention scores have been widely repurposed for diverse downstream applications such as audio-text alignment <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib104\" title=\"\">2020</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib46\" title=\"\">2020</a>)</cite>, speaker identification <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib38\" title=\"\">2019</a>)</cite>, timestamp estimation <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib47\" title=\"\">2022</a>; Louradour, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib50\" title=\"\">2023</a>; Zusag et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib106\" title=\"\">2024</a>)</cite>, and guiding simultaneous automatic speech recognition (ASR) and speech translation (ST) <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib91\" title=\"\">2024a</a>; Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib65\" title=\"\">2023a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib66\" title=\"\">b</a>)</cite>.\nThese applications rely on the implicit assumption that\ncross-attention reliably indicates what the model attends to in the input signal during output generation. However, despite its widespread use, this assumption has\nnever been verified.\nA key concern is that cross-attention operates over the encoder&#8217;s output sequence&#8211;rather than directly on the raw audio&#8211;which may have been reorganized or mixed with contextual information. This phenomenon, known as context mixing <cite class=\"ltx_cite ltx_citemacro_citep\">(Mohebbi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib60\" title=\"\">2023b</a>)</cite>, can potentially obscure the alignment between cross-attention weights and the original input signal.\nSimilar concerns have been extensively debated in the NLP community, where the reliability of attention mechanisms as explanations has been both challenged and defended, leading to conflicting perspectives and empirical evidence <cite class=\"ltx_cite ltx_citemacro_citep\">(Serrano &amp; Smith, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib78\" title=\"\">2019</a>; Jain &amp; Wallace, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib35\" title=\"\">2019</a>; Wiegreffe &amp; Pinter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib93\" title=\"\">2019</a>; Bastings &amp; Filippova, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib9\" title=\"\">2020</a>)</cite>. In contrast, this question remains largely underexplored in the speech domain.\nExisting work on explainability in S2T has primarily focused on self-attention <cite class=\"ltx_cite ltx_citemacro_citep\">(Shim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib80\" title=\"\">2022</a>; Audhkhasi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib5\" title=\"\">2022</a>; A&#160;Shams et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib1\" title=\"\">2024</a>)</cite>, or on empirically measuring the effects of context mixing <cite class=\"ltx_cite ltx_citemacro_citep\">(Mohebbi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib59\" title=\"\">2023a</a>)</cite>, without directly assessing the explanatory potential of cross-attention mechanisms.</p>\n\n",
                "matched_terms": [
                    "output",
                    "model",
                    "asr",
                    "indicates",
                    "explanations",
                    "between",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this gap, we present the first systematic\nanalysis of cross-attention as a proxy for input-output dependencies in S2T models.\nOur study serves two main objectives: <span class=\"ltx_text ltx_font_italic\">i)</span> assessing the validity of using cross-attention as a surrogate for input-output alignment,\nand <span class=\"ltx_text ltx_font_italic\">ii)</span> evaluating whether it provides insights comparable to formal explainability methods such as feature attribution&#8211;while being more lightweight and less computationally expensive to obtain <cite class=\"ltx_cite ltx_citemacro_citep\">(Samek et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib77\" title=\"\">2021</a>; Madsen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib53\" title=\"\">2022</a>)</cite>. We compare cross-attention scores with input saliency maps derived from SPES <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>, the current state-of-the-art feature-attribution method in S2T, to determine the extent to which\ncross-attention\ncaptures\nwhich input features are relevant for models&#8217; predictions.\nIn addition, we compute saliency maps on\nencoder outputs and compare them with cross-attention scores to evaluate whether cross-attention fully explains how the decoder uses encoded representations, avoiding potential discrepancies\nof context mixing.\nOur\nanalysis spans ASR and ST tasks across monolingual, multilingual, and multitask settings using\nstate-of-the-art speech processing\narchitectures <cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib29\" title=\"\">2020</a>)</cite> at multiple scales.\nWith consistent trends across different settings, we find\nthat cross-attention exhibits moderate to strong correlations with input saliency maps and aligns more closely with encoder output representations, suggesting an influence of context mixing. However, our results also indicate that the overall explanatory power of cross-attention is limited&#8211;accounting for only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>50% of input relevance and, at best, 52-75% of encoder output saliency.\nOur findings uncover fundamental limitations in interpreting cross-attention as an explanatory proxy, suggesting that it provides an informative yet incomplete view of the factors driving predictions in S2T models.\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "output",
                    "overall",
                    "encoder",
                    "asr",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Explainability in Speech-to-Text.</span>\nExplainable AI (XAI) has emerged to make model behavior more interpretable to humans, thereby supporting informed decision-making and responsible deployment <cite class=\"ltx_cite ltx_citemacro_citep\">(Barredo Arrieta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib8\" title=\"\">2020</a>)</cite>.\nWhile XAI research has seen a rapid growth in the last years across multiple modalities, including vision and language <cite class=\"ltx_cite ltx_citemacro_citep\">(Sharma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib79\" title=\"\">2024</a>)</cite>, progress in the speech domain has lagged.\nThis gap arises from the inherent complexities of speech processing, including the multidimensional nature of speech signals across time and frequency, and the variability in output sequence length <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib95\" title=\"\">2024</a>)</cite>.\nDespite these challenges, growing concerns about trustworthiness\nare driving explainability efforts in speech classification <cite class=\"ltx_cite ltx_citemacro_citep\">(Becker et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib10\" title=\"\">2024</a>; Pastor et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib68\" title=\"\">2024</a>)</cite> and S2T generation <cite class=\"ltx_cite ltx_citemacro_citep\">(Mandel, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib54\" title=\"\">2016</a>; Kavaki &amp; Mandel, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib37\" title=\"\">2020</a>; Trinh &amp; Mandel, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib84\" title=\"\">2020</a>; Markert et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib55\" title=\"\">2021</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib94\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib95\" title=\"\">2024</a>; Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>. Most of these works rely on perturbation-based methods\nthat assess how input modifications affect model predictions <cite class=\"ltx_cite ltx_citemacro_citep\">(Covert et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib16\" title=\"\">2021b</a>; Ivanovs et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib34\" title=\"\">2021</a>)</cite>.\nAmong these,\n<cite class=\"ltx_cite ltx_citemacro_citet\">Fucci et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite> recently proposed\na technique for autoregressive S2T models that identifies regions of the spectrogram that most influence predictions to generate saliency maps.\nHowever, XAI methods are generally computationally expensive&#8211;especially perturbation-based approaches applied to large models <cite class=\"ltx_cite ltx_citemacro_citep\">(Luo &amp; Specia, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib52\" title=\"\">2024</a>; Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib102\" title=\"\">2025</a>)</cite>&#8211;which motivates exploring whether cross-attention, already computed at inference time, could serve as a lightweight alternative in a landscape still lacking efficient explainability tools for speech-based models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "language",
                    "output",
                    "model",
                    "large",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Attention as Explanation.</span>\nAttention mechanisms have been widely used to probe model behavior in text-based NLP, as attention scores often align with human intuitions about relevance and salience <cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib13\" title=\"\">2019</a>; Ferrando et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib21\" title=\"\">2024</a>)</cite>.\nEarly studies proposed norm-based analyses to improve the interpretability of attention weights <cite class=\"ltx_cite ltx_citemacro_citep\">(Kobayashi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib39\" title=\"\">2020</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib40\" title=\"\">2021</a>; Mohebbi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib58\" title=\"\">2021</a>; Ferrando et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib20\" title=\"\">2022b</a>)</cite>, while others suggested aggregating attention across layers and heads to quantify input-output influence more systematically <cite class=\"ltx_cite ltx_citemacro_citep\">(Abnar &amp; Zuidema, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib2\" title=\"\">2020</a>; Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib101\" title=\"\">2021</a>; Chefer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib11\" title=\"\">2021</a>)</cite>.\nWhile some have raised concerns about whether attention reliably reflects which inputs are actually responsible for outputs <cite class=\"ltx_cite ltx_citemacro_citep\">(Jain &amp; Wallace, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib35\" title=\"\">2019</a>; Serrano &amp; Smith, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib78\" title=\"\">2019</a>; Bastings &amp; Filippova, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib9\" title=\"\">2020</a>)</cite>, others have proposed conditions under which attention can meaningfully explain model behavior <cite class=\"ltx_cite ltx_citemacro_citep\">(Wiegreffe &amp; Pinter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib93\" title=\"\">2019</a>)</cite>. More recent work highlights that attention aggregation may obscure localized, token-specific interactions <cite class=\"ltx_cite ltx_citemacro_citep\">(Modarressi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib57\" title=\"\">2023</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib100\" title=\"\">2023</a>; Oh &amp; Schuler, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib63\" title=\"\">2023</a>)</cite>, motivating hybrid approaches that combine attention with other XAI techniques, such as attribution methods <cite class=\"ltx_cite ltx_citemacro_citep\">(Modarressi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib56\" title=\"\">2022</a>)</cite>, or the use of attention as a regularization signal during interpretability-driven training <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib97\" title=\"\">2024</a>)</cite>.\nDespite the ongoing efforts, most research has focused on self-attention within encoders, with limited attention to feed-forward dynamics <cite class=\"ltx_cite ltx_citemacro_citep\">(Kobayashi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib41\" title=\"\">2024</a>)</cite> and even less to encoder-decoder models. A few studies have investigated attention in encoder-decoder architectures <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib62\" title=\"\">2021</a>)</cite>, including in machine translation <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib105\" title=\"\">2024</a>)</cite>, but cross-attention remains largely underexplored in the speech domain and has been\nabsent from the broader <span class=\"ltx_text ltx_inline-quote ltx_outerquote\">&#8220;attention as explanation&#8221;</span> debate in NLP. Our work seeks to bridge this gap by bringing cross-attention of S2T models into this broader conversation, aiming to assess\nwhether\nit can serve as a reliable explanation&#8211;and where its limitations emerge.\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "crossattention",
                    "model",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We assess the extent to which cross-attention scores (<math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math>) explain how the model looks at\ninput features when generating a token\nby comparing them to the saliency map on the input <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math>, obtained with the state-of-the art feature-attribution method for S2T, SPES <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, to assess whether cross-attention more accurately reflects how the decoder accesses encoded representations&#8211;rather than capturing the model&#8217;s full input-output behavior&#8211;we compare <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> with the encoder-output saliency map <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m2\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>H</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}</annotation></semantics></math>.\nBy analyzing how the correlation between <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> and <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m4\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>H</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}</annotation></semantics></math> deviates from that with <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m5\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math>,\nwe can indirectly quantify the impact of context mixing in the resulting explanations.\nA visual overview of this setup is provided in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S3.F1\" title=\"Figure 1 &#8227; 3 Methodology &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "explanations",
                    "correlation",
                    "crossattention",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In S2T models, the cross-attention mechanism enables each decoder token to integrate relevant portions of the encoded speech features, thereby conditioning generation on the entire input.</p>\n\n",
                "matched_terms": [
                    "models",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Let <math alttext=\"\\mathbf{X}\\in\\mathbb{R}^{T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}\\in\\mathbb{R}^{T\\times F}</annotation></semantics></math> denote the speech input represented by mel-spectrogram features, where <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> is the number of time frames and <math alttext=\"F\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\"><semantics><mi>F</mi><annotation encoding=\"application/x-tex\">F</annotation></semantics></math> the number of frequency bins. The encoder processes <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m4\" intent=\":literal\"><semantics><mi>&#119831;</mi><annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation></semantics></math> into a sequence of hidden representations <math alttext=\"\\mathbf{H}=\\mathrm{Encoder}(\\mathbf{X})\\in\\mathbb{R}^{T^{\\prime}\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><mi>&#119815;</mi><mo>=</mo><mrow><mi>Encoder</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119831;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{H}=\\mathrm{Encoder}(\\mathbf{X})\\in\\mathbb{R}^{T^{\\prime}\\times D}</annotation></semantics></math>, where <math alttext=\"T^{\\prime}&lt;T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m6\" intent=\":literal\"><semantics><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo>&lt;</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">T^{\\prime}&lt;T</annotation></semantics></math> reflects the number of encoder time steps after subsampling with a factor of <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m7\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math><span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>As the length of the speech inputs is, in general, 10<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"footnote1.m1\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> longer that of the corresponding textual input, it is a common practice in S2T modeling to downsample the input through convolutional modules <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib88\" title=\"\">2020</a>)</cite>.</span></span></span> and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m8\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> is the hidden dimensionality. The decoder then autoregressively generates an output sequence <math alttext=\"\\mathbf{y}=(y_{0},y_{1},\\ldots,y_{I})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m9\" intent=\":literal\"><semantics><mrow><mi>&#119858;</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mn>0</mn></msub><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>y</mi><mi>I</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{y}=(y_{0},y_{1},\\ldots,y_{I})</annotation></semantics></math> of length <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m10\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math>, where each token <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m11\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math> is predicted based on the previously generated tokens <math alttext=\"y_{&lt;i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m12\" intent=\":literal\"><semantics><msub><mi>y</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">y_{&lt;i}</annotation></semantics></math> and the encoder output <math alttext=\"\\mathbf{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m13\" intent=\":literal\"><semantics><mi>&#119815;</mi><annotation encoding=\"application/x-tex\">\\mathbf{H}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "output",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At each decoder layer <math alttext=\"\\ell\\in\\{1,\\ldots,L\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\ell\\in\\{1,\\ldots,L\\}</annotation></semantics></math>, cross-attention scores are computed via dot-product attention <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib28\" title=\"\">2014</a>)</cite> between the decoder&#8217;s current hidden states <math alttext=\"\\mathbf{B}^{(\\ell)}\\in\\mathbb{R}^{I\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><msup><mi>&#119809;</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{B}^{(\\ell)}\\in\\mathbb{R}^{I\\times D}</annotation></semantics></math> and the encoder outputs <math alttext=\"\\mathbf{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m3\" intent=\":literal\"><semantics><mi>&#119815;</mi><annotation encoding=\"application/x-tex\">\\mathbf{H}</annotation></semantics></math>. Specifically, the decoder states are linearly projected to queries <math alttext=\"\\mathbf{Q}^{(\\ell)}=\\mathbf{B}^{(\\ell)}\\mathbf{W}_{Q}^{(\\ell)}\\in\\mathbb{R}^{I\\times d_{k}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m4\" intent=\":literal\"><semantics><mrow><msup><mi>&#119824;</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><mrow><msup><mi>&#119809;</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#119830;</mi><mi>Q</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Q}^{(\\ell)}=\\mathbf{B}^{(\\ell)}\\mathbf{W}_{Q}^{(\\ell)}\\in\\mathbb{R}^{I\\times d_{k}}</annotation></semantics></math>, while the encoder outputs are projected to keys <math alttext=\"\\mathbf{K}^{(\\ell)}=\\mathbf{H}\\mathbf{W}_{K}^{(\\ell)}\\in\\mathbb{R}^{T^{\\prime}\\times d_{k}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m5\" intent=\":literal\"><semantics><mrow><msup><mi>&#119818;</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><msubsup><mi>&#119815;&#119830;</mi><mi>K</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{K}^{(\\ell)}=\\mathbf{H}\\mathbf{W}_{K}^{(\\ell)}\\in\\mathbb{R}^{T^{\\prime}\\times d_{k}}</annotation></semantics></math>\nusing learned projection matrices <math alttext=\"\\mathbf{W}_{Q}^{(\\ell)},\\mathbf{W}_{K}^{(\\ell)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m6\" intent=\":literal\"><semantics><mrow><msubsup><mi>&#119830;</mi><mi>Q</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>&#119830;</mi><mi>K</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{W}_{Q}^{(\\ell)},\\mathbf{W}_{K}^{(\\ell)}</annotation></semantics></math>. The resulting cross-attention matrix <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m7\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> is:</p>\n\n",
                "matched_terms": [
                    "encoder",
                    "crossattention",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where each row <math alttext=\"\\text{{\\color[rgb]{0.75390625,0.390625,0.9609375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.75390625,0.390625,0.9609375}$\\mathbf{CA}$}}^{(\\ell)}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m8\" intent=\":literal\"><semantics><msubsup><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.75390625,0.390625,0.9609375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.75390625,0.390625,0.9609375}$\\mathbf{CA}$}}^{(\\ell)}_{i}</annotation></semantics></math> represents the attention distribution over encoder time steps for the generation of output token <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m9\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math> at layer <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m10\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>.\nTo capture diverse patterns, Transformer-based models employ multi-head attention <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib85\" title=\"\">2017</a>)</cite>. Each head <math alttext=\"h\\in\\{1,\\ldots,H\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m11\" intent=\":literal\"><semantics><mrow><mi>h</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>H</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">h\\in\\{1,\\ldots,H\\}</annotation></semantics></math> uses separate learned projections:</p>\n\n",
                "matched_terms": [
                    "models",
                    "output",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Extracting\nthe full set of scores provides a fine-grained view of how each output token in the generated hypothesis attends to the encoder&#8217;s representations across all layers and heads.\nTo derive a single layer-wise or head-wise attention distribution, we compute the mean of the attention matrices over a subset <math alttext=\"\\mathcal{S}\\subseteq\\{1,\\dots,L\\}\\times\\{1,\\dots,H\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>&#8838;</mo><mrow><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo rspace=\"0.055em\" stretchy=\"false\">}</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>H</mi><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}\\subseteq\\{1,\\dots,L\\}\\times\\{1,\\dots,H\\}</annotation></semantics></math> of layers and heads:</p>\n\n",
                "matched_terms": [
                    "layerwise",
                    "across",
                    "output",
                    "all",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By selecting different index sets <math alttext=\"\\mathcal{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><annotation encoding=\"application/x-tex\">\\mathcal{S}</annotation></semantics></math>, this formulation yields layer-wise, head-wise, or global averages. For example, setting <math alttext=\"\\mathcal{S}=\\{(\\ell,h):h=1,\\dots,H\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m3\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo>,</mo><mi>h</mi><mo rspace=\"0.278em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.278em\">:</mo><mrow><mi>h</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>H</mi></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}=\\{(\\ell,h):h=1,\\dots,H\\}</annotation></semantics></math> gives the average across heads at a given layer <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>; <math alttext=\"\\mathcal{S}=\\{(\\ell,h):\\ell=1,\\dots,L\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m5\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo>,</mo><mi>h</mi><mo rspace=\"0.278em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.278em\">:</mo><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}=\\{(\\ell,h):\\ell=1,\\dots,L\\}</annotation></semantics></math> averages across layers for head <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m6\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math>; and <math alttext=\"\\mathcal{S}=\\{1,\\dots,L\\}\\times\\{1,\\dots,H\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m7\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>=</mo><mrow><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo rspace=\"0.055em\" stretchy=\"false\">}</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>H</mi><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}=\\{1,\\dots,L\\}\\times\\{1,\\dots,H\\}</annotation></semantics></math> computes the full average.\nThis averaged attention provides a more aggregated view of the model&#8217;s attention patterns at a specific layer or attention head, summarizing how the model attends to the input speech over time.</p>\n\n",
                "matched_terms": [
                    "layerwise",
                    "across",
                    "model",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand how S2T models associate individual output tokens with specific regions of the raw speech input or of the model&#8217;s inner representations (e.g., the encoder output), we employ <span class=\"ltx_text ltx_font_italic\">feature-attribution</span> techniques that produce token-level <span class=\"ltx_text ltx_font_italic\">saliency maps</span>. These maps quantify the relevance of different portions of the input sequence in determining the model&#8217;s predictions.</p>\n\n",
                "matched_terms": [
                    "models",
                    "output",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Let again <math alttext=\"\\mathbf{X}\\in\\mathbb{R}^{T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}\\in\\mathbb{R}^{T\\times F}</annotation></semantics></math> denote a mel-spectrogram input, where <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> is the number of time frames and <math alttext=\"F\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mi>F</mi><annotation encoding=\"application/x-tex\">F</annotation></semantics></math> the number of frequency bins, and <math alttext=\"\\mathbf{y}=(y_{0},y_{1},\\ldots,y_{I})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#119858;</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mn>0</mn></msub><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>y</mi><mi>I</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{y}=(y_{0},y_{1},\\ldots,y_{I})</annotation></semantics></math> the sequence of length <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math> of the autoregressively-generated tokens predicted based on the input and the previously generated tokens <math alttext=\"y_{&lt;i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><msub><mi>y</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">y_{&lt;i}</annotation></semantics></math>.\nTo attribute the prediction of each token <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math> to specific parts of the input spectrogram, we adopt SPES&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>,\nthe state-of-the-art feature-attribution method designed for autoregressive S2T modeling. SPES assigns a saliency score to each time-frequency\nelement\nof <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><mi>&#119831;</mi><annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation></semantics></math>, producing a saliency map <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{X}\\in\\mathbb{R}^{T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m9\" intent=\":literal\"><semantics><mrow><msubsup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>i</mi><mi>X</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{X}\\in\\mathbb{R}^{T\\times F}</annotation></semantics></math> for each token <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m10\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math>, where higher values indicate greater relevance of the corresponding time-frequency regions. SPES operates by clustering spectrogram elements based on energy profiles&#8211;capturing acoustic\ncomponents such as harmonics and background noise&#8211;and estimating the influence of each cluster by perturbing it with probability <math alttext=\"p_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m11\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">p_{X}</annotation></semantics></math>, repeated <math alttext=\"N_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m12\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">N_{X}</annotation></semantics></math> times. The effect of each perturbation&#8211;i.e., masking parts of the input with 0 values&#8211;is measured by computing the Kullback-Leibler (KL) divergence <cite class=\"ltx_cite ltx_citemacro_citep\">(Kullback &amp; Leibler, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib44\" title=\"\">1951</a>)</cite> between the model&#8217;s original output distribution <math alttext=\"P(y_{i}\\mid y_{&lt;i},\\mathbf{X})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m13\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>&#8739;</mo><mrow><msub><mi>y</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><mo>,</mo><mi>&#119831;</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(y_{i}\\mid y_{&lt;i},\\mathbf{X})</annotation></semantics></math> and the distribution resulting from the perturbed input <math alttext=\"P^{(n)}(y_{i}\\mid y_{&lt;i},\\tilde{\\mathbf{X}}^{(n)})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m14\" intent=\":literal\"><semantics><mrow><msup><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>&#8739;</mo><mrow><msub><mi>y</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><mo>,</mo><msup><mover accent=\"true\"><mi>&#119831;</mi><mo>~</mo></mover><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P^{(n)}(y_{i}\\mid y_{&lt;i},\\tilde{\\mathbf{X}}^{(n)})</annotation></semantics></math> at time <math alttext=\"n\\in\\{1,\\ldots,N_{X}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m15\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>N</mi><mi>X</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">n\\in\\{1,\\ldots,N_{X}\\}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "output",
                    "values",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The divergence scores are then mapped back to the corresponding cluster positions in the spectrogram and aggregated to form the token-specific saliency map <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{X}\\in\\mathbb{R}^{T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m16\" intent=\":literal\"><semantics><mrow><msubsup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>i</mi><mi>X</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{X}\\in\\mathbb{R}^{T\\times F}</annotation></semantics></math>. Stacking all saliency maps across the output sequence <math alttext=\"\\mathbf{y}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m17\" intent=\":literal\"><semantics><mi>&#119858;</mi><annotation encoding=\"application/x-tex\">\\mathbf{y}</annotation></semantics></math> yields a 3D saliency map:</p>\n\n",
                "matched_terms": [
                    "all",
                    "across",
                    "output"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further examine the influence of the encoder&#8217;s internal representations on the prediction of each output token.\nLet again <math alttext=\"\\mathbf{H}=\\mathrm{Encoder}(\\mathbf{X})\\in\\mathbb{R}^{T^{\\prime}\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119815;</mi><mo>=</mo><mrow><mi>Encoder</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119831;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{H}=\\mathrm{Encoder}(\\mathbf{X})\\in\\mathbb{R}^{T^{\\prime}\\times D}</annotation></semantics></math> denote the sequence of encoder hidden states or <span class=\"ltx_text ltx_font_italic\">encoder output</span>, where <math alttext=\"T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><msup><mi>T</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">T^{\\prime}</annotation></semantics></math> is the subsampled time dimension and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> is the hidden dimension.\nTo assess the importance of the encoder output representations, we compute token-specific saliency maps <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{H}\\in\\mathbb{R}^{T^{\\prime}\\times 1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m4\" intent=\":literal\"><semantics><mrow><msubsup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>i</mi><mi>H</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{H}\\in\\mathbb{R}^{T^{\\prime}\\times 1}</annotation></semantics></math>, where each entry reflects the contribution of the corresponding hidden state to the generation of <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m5\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math>.\nEach encoder state <math alttext=\"\\mathbf{H}=(\\mathbf{h}_{1},\\ldots,\\mathbf{h}_{T^{\\prime}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#119815;</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119841;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119841;</mi><msup><mi>T</mi><mo>&#8242;</mo></msup></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{H}=(\\mathbf{h}_{1},\\ldots,\\mathbf{h}_{T^{\\prime}})</annotation></semantics></math> is perturbed&#8211;i.e., all its features are set to 0&#8211;independently with probability <math alttext=\"p_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m7\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">p_{H}</annotation></semantics></math>, and the process is repeated <math alttext=\"N_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m8\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">N_{H}</annotation></semantics></math> times. The KL divergence is computed for each perturbation between the original and perturbed output distributions:</p>\n\n",
                "matched_terms": [
                    "all",
                    "output",
                    "encoder",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since our focus lies in the temporal dynamics of the input <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mi>&#119831;</mi><annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation></semantics></math>, we aggregate the 3D saliency scores <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\\in\\mathbb{R}^{I\\times T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\\in\\mathbb{R}^{I\\times T\\times F}</annotation></semantics></math> across the frequency dimension and downsample the time axis to produce a compressed representation <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\\in\\mathbb{R}^{I\\times T^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>T</mi><mo>&#8242;</mo></msup></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\\in\\mathbb{R}^{I\\times T^{\\prime}}</annotation></semantics></math> compatible with the cross-attention granularity, where <math alttext=\"T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m4\" intent=\":literal\"><semantics><msup><mi>T</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">T^{\\prime}</annotation></semantics></math> corresponds to the number of encoder time steps. The aggregation is performed by taking the maximum saliency value over the frequency axis and within each corresponding time window. The resulting saliency map of each token <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{X}\\in\\mathbb{R}^{T^{\\prime}\\times 1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><msubsup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>i</mi><mi>X</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{X}\\in\\mathbb{R}^{T^{\\prime}\\times 1}</annotation></semantics></math> reflects the temporal relevance of the input spectrogram with respect to the generation of token <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m6\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math>. Complementary experiments on the choice of the aggregation function are presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A1\" title=\"Appendix A Effect of Aggregation Functions on Input Explanations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.\nBoth <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m7\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> and <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m8\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math> representations are normalized before computing the correlation scores, and the beginning and end of sentence are removed as they are not relevant for the analysis.\nThe <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m9\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> matrix is normalized frame-wise using mean-variance normalization to mitigate the impact of potential attention sinks at initial or final tokens&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib13\" title=\"\">2019</a>; Ferrando et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib19\" title=\"\">2022a</a>; Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib65\" title=\"\">2023a</a>; Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib96\" title=\"\">2024</a>)</cite> on the correlation computation.\nBoth <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m10\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math> and <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m11\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>H</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}</annotation></semantics></math> are normalized along the token dimension using the strategy proposed by <cite class=\"ltx_cite ltx_citemacro_citet\">Fucci et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>, as saliency scores can vary widely across tokens due to differences in the original output distributions used to compute the KL divergence.</p>\n\n",
                "matched_terms": [
                    "across",
                    "correlation",
                    "output",
                    "encoder",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following prior work on cross-attention matrices <cite class=\"ltx_cite ltx_citemacro_citep\">(Vig &amp; Belinkov, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib86\" title=\"\">2019</a>)</cite> and explainable AI <cite class=\"ltx_cite ltx_citemacro_citep\">(Eberle et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib18\" title=\"\">2023</a>)</cite>, we use Pearson correlation\nto quantify the relationship between cross-attention scores and saliency-based explanations.\nPearson correlation is preferred over Kendall and Spearman because saliency scores are continuous, and their magnitude&#8211;not just ranking&#8211;is crucial. Rank-based measures are overly sensitive to small fluctuations among non-important features with near-zero scores, while Pearson better captures whether features are identified as important (high score) or not (low score).\nSpecifically, given the two representations <math alttext=\"\\text{{\\color[rgb]{0.75390625,0.390625,0.9609375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.75390625,0.390625,0.9609375}$\\mathbf{CA}$}},\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}\\in\\mathbb{R}^{I\\times T^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><mo>,</mo><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>T</mi><mo>&#8242;</mo></msup></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.75390625,0.390625,0.9609375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.75390625,0.390625,0.9609375}$\\mathbf{CA}$}},\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}\\in\\mathbb{R}^{I\\times T^{\\prime}}</annotation></semantics></math>, we compute the Pearson correlation coefficient <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math> to assess the similarity of their attribution patterns across output tokens and time steps. We first flatten each matrix into a vector of size <math alttext=\"I\\cdot T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8901;</mo><msup><mi>T</mi><mo>&#8242;</mo></msup></mrow><annotation encoding=\"application/x-tex\">I\\cdot T^{\\prime}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "across",
                    "correlation",
                    "œÅrho",
                    "low",
                    "output",
                    "high",
                    "small",
                    "explanations",
                    "between",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This scalar value quantifies the linear relationship between the two saliency maps, with values closer to 1 indicating a strong positive correlation, and values near 0 indicating no correlation.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "values",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To avoid potential data contamination issues <cite class=\"ltx_cite ltx_citemacro_citep\">(Sainz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib76\" title=\"\">2023</a>)</cite>, we train from scratch a monolingual ASR model and two-sized multitask (ASR and ST) and multilingual (English and Italian) models. Details about training data and process are presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A2\" title=\"Appendix B Training Settings &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.\nBeing the only non-synthetic dataset supporting both tasks and language directions, we select EuroParl-ST <cite class=\"ltx_cite ltx_citemacro_citep\">(Iranzo-S&#225;nchez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib33\" title=\"\">2020</a>)</cite> as the test set for our analyses. The test set covers both <span class=\"ltx_text ltx_font_italic\">en</span> and <span class=\"ltx_text ltx_font_italic\">it</span> ASR, and <span class=\"ltx_text ltx_font_italic\">en-it</span> and <span class=\"ltx_text ltx_font_italic\">it-en</span> ST.\nThe <span class=\"ltx_text ltx_font_italic\">it/it-en</span> section consists of 1,686 segments, for a total of approximately 6 hours of audio, while the <span class=\"ltx_text ltx_font_italic\">en/en-it</span> section contains 1,130 segments, for a total of approximately 3 hours of audio.</p>\n\n",
                "matched_terms": [
                    "language",
                    "models",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The models analyzed in the paper are all composed of a Conformer encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib29\" title=\"\">2020</a>)</cite> and a Transformer decoder, as Conformer is the current state-of-the-art architecture for S2T processing <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib30\" title=\"\">2021</a>; Srivastava et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib81\" title=\"\">2022</a>; Li &amp; Doddipatla, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib48\" title=\"\">2023</a>)</cite>.\nThe monolingual ASR model (<span class=\"ltx_text ltx_font_typewriter\">base</span>) is composed of 12 encoder layers and 6 decoder layers. Each layer has 8 attention heads, 512 as embedding dimension, and FFNs dimension of 2,048. The vocabulary is built using a SentencePiece unigram model <cite class=\"ltx_cite ltx_citemacro_citep\">(Kudo &amp; Richardson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib42\" title=\"\">2018</a>)</cite> with size 8,000 trained on <span class=\"ltx_text ltx_font_italic\">en</span> transcripts. The resulting number of parameters is 125M.\nThe multitask and multilingual models are of two sizes, <span class=\"ltx_text ltx_font_typewriter\">small</span> and <span class=\"ltx_text ltx_font_typewriter\">large</span>, the first having 12 encoder layers and 6 decoder layers and the latter having 24 encoder layers and 12 decoder layers. In both sizes, each layer has 16 attention heads, an embedding dimension of 1,024, and an FFN dimension of 4,096. The vocabulary is built using a SentencePiece unigram model with size 16,000 trained on <span class=\"ltx_text ltx_font_italic\">en</span> and <span class=\"ltx_text ltx_font_italic\">it</span> transcripts. Two extra tokens&#8211;<span class=\"ltx_text ltx_font_typewriter\">&lt;lang:en&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;lang:it&gt;</span>&#8211;are added to indicate whether the target text is in <span class=\"ltx_text ltx_font_italic\">en</span> or <span class=\"ltx_text ltx_font_italic\">it</span>. The resulting number of parameters is 474M for the <span class=\"ltx_text ltx_font_typewriter\">small</span> model and 878M for the <span class=\"ltx_text ltx_font_typewriter\">large</span> model.\nIn all models, the Conformer encoder is preceded by two 1D convolutional layers with stride 2 and kernel size 5, resulting in a fixed subsampling factor <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> of 4. The kernel size of the Conformer convolutional module is 31 for both the point- and depth-wise convolutions. The input audio is represented by 80 Mel-filterbank features extracted every 10 ms with a window of 25 ms.</p>\n\n",
                "matched_terms": [
                    "models",
                    "base",
                    "model",
                    "large",
                    "all",
                    "target",
                    "small",
                    "encoder",
                    "asr",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Hypothesis and Cross-Attention Generation.</span>\nFor the hypothesis generation, we use beam search with a beam size of 5 and a no-repeat n-gram size of 5. The attention scores are extracted from layers or heads during the output generation. The ASR and ST quality scores of the hypotheses are presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A3\" title=\"Appendix C Quality Metrics for the Reported Models &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nThe inference is performed using a single NVIDIA A40 GPU (40GB RAM) with a batch size of 40,000 tokens and takes <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>2.5 minutes for <span class=\"ltx_text ltx_font_typewriter\">base</span>, <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>3-5.5 minutes for <span class=\"ltx_text ltx_font_typewriter\">small</span>, and <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>3-6.5 for <span class=\"ltx_text ltx_font_typewriter\">large</span>, depending on the source language.</p>\n\n",
                "matched_terms": [
                    "crossattention",
                    "base",
                    "language",
                    "output",
                    "large",
                    "source",
                    "small",
                    "asr",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Explanation Heatmaps Generation.</span>\nFollowing the best configuration obtained in SPES <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>, we adopt the Morphological Fragmental Perturbation Pyramid <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib99\" title=\"\">2021</a>)</cite> for clustering, which relies on Simple Linear Iterative Clustering or SLIC <cite class=\"ltx_cite ltx_citemacro_citep\">(Achanta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib3\" title=\"\">2012</a>)</cite>, a k-means-based algorithm that groups elements according to spectral patterns. We use the default parameters; the threshold length in seconds is\n7.50s, the\n<a class=\"ltx_ref ltx_href\" href=\"https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.slic\" title=\"\">SLIC</a>\nsigma is 0, the compactness is 0.1, and the number of patches per second for the MFPP technique is [400, 500, 600].\nFor the choice of <math alttext=\"p_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">p_{X}</annotation></semantics></math> and <math alttext=\"N_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">N_{X}</annotation></semantics></math>, we refer to the parameters used in <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>, setting <math alttext=\"p_{X}=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>X</mi></msub><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">p_{X}=0.5</annotation></semantics></math> and <math alttext=\"N_{X}=20,000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><msub><mi>N</mi><mi>X</mi></msub><mo>=</mo><mrow><mn>20</mn><mo>,</mo><mn>000</mn></mrow></mrow><annotation encoding=\"application/x-tex\">N_{X}=20,000</annotation></semantics></math>. The quality of the input explanations is presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A3\" title=\"Appendix C Quality Metrics for the Reported Models &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>. For the choice of <math alttext=\"p_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m5\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">p_{H}</annotation></semantics></math> and <math alttext=\"N_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m6\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">N_{H}</annotation></semantics></math>, we use the same number of iterations of <math alttext=\"N_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m7\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">N_{X}</annotation></semantics></math>, i.e., <math alttext=\"N_{H}=20,000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m8\" intent=\":literal\"><semantics><mrow><msub><mi>N</mi><mi>H</mi></msub><mo>=</mo><mrow><mn>20</mn><mo>,</mo><mn>000</mn></mrow></mrow><annotation encoding=\"application/x-tex\">N_{H}=20,000</annotation></semantics></math>, while the optimal occlusion probability <math alttext=\"p_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m9\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">p_{H}</annotation></semantics></math> is determined over the dev set, resulting in <math alttext=\"p_{H}=0.7\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m10\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>H</mi></msub><mo>=</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">p_{H}=0.7</annotation></semantics></math>, whose experiments are reported in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A4\" title=\"Appendix D Effect of Occlusion Probability on Encoder Output Explanations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.\nThe inference is performed using a single NVIDIA A40 GPU (40GB RAM)\nand takes <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m11\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>27 hours for <span class=\"ltx_text ltx_font_typewriter\">base</span>, <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m12\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>3-4 days for <span class=\"ltx_text ltx_font_typewriter\">small</span> and <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m13\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>6-8 days for <span class=\"ltx_text ltx_font_typewriter\">large</span>, depending on the source language.</p>\n\n",
                "matched_terms": [
                    "base",
                    "language",
                    "large",
                    "source",
                    "small",
                    "explanations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Correlation Computation.</span>\nThe Pearson <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">r</annotation></semantics></math> correlation score is computed using the <span class=\"ltx_text ltx_font_typewriter\">scipy</span> implementation<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html\" title=\"\">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html</a></span></span></span> and averaging across samples in the test set.</p>\n\n",
                "matched_terms": [
                    "across",
                    "correlation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we compare <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> with input saliency maps <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m3\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></sup>, which serve as an external reference for measuring input relevance.\nSpecifically, in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS1\" title=\"5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a>, we analyze the\n<span class=\"ltx_text ltx_font_typewriter\">base</span> model across all levels of granularity. Then, in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS2\" title=\"5.1.2 Multitask and Multilingual Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.2</span></a>, we extend the analysis to additional models (<span class=\"ltx_text ltx_font_typewriter\">small</span> and <span class=\"ltx_text ltx_font_typewriter\">large</span>), languages (<span class=\"ltx_text ltx_font_italic\">en</span> and <span class=\"ltx_text ltx_font_italic\">it</span>), and tasks (ASR and ST).</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "base",
                    "model",
                    "large",
                    "all",
                    "small",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.T1\" title=\"Table 1 &#8227; 5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reports the correlation scores for the monolingual English ASR model (<span class=\"ltx_text ltx_font_typewriter\">base</span>), considering cross-attention at the head level, layer level, and in aggregated form.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "base",
                    "model",
                    "asr",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At the individual head level, correlations with saliency maps are generally low. This suggests that attention heads, when taken in isolation, only partially capture the model&#8217;s dependency on the input and often encode noisy or inconsistent relevance signals. However, not all heads are equal: some, especially in the upper layers (layers 4-6), exhibit relatively stronger correlations. Notably, <span class=\"ltx_text ltx_font_bold\">averaging across heads consistently outperforms selecting individual heads</span>, suggesting that, despite head-level sparsity and weak individual correlations, the collective information captured across heads reflects input relevance more effectively.\nMoving from heads to layers, we find a clearer picture. Averaging attention scores across all heads within each layer boosts correlation substantially, with layer 6 standing out as the most aligned with the saliency maps. This is followed closely by layer 5 and the average across all layers, indicating that the <span class=\"ltx_text ltx_font_bold\">last layers exhibit the highest alignment with input relevance</span>. These results reinforce the idea that deeper layers encode higher-level semantic or task-relevant features, a trend previously observed in Transformer-based models <cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib13\" title=\"\">2019</a>)</cite>.\nInterestingly, while averaging across heads improves alignment, averaging across both heads and layers does not yield the overall best result, even if values are close. This indicates that not all layers contribute equally and that indiscriminate aggregation can dilute the relevance signal.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "correlation",
                    "low",
                    "overall",
                    "highest",
                    "all",
                    "indicates",
                    "layers",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, the results show that appropriately selected and aggregated cross-attention scores exhibit only a <span class=\"ltx_text ltx_font_italic\">moderate</span> to <span class=\"ltx_text ltx_font_italic\">strong</span> correlation with input saliency maps, reaching values up to 0.588. This provides an initial indication of the\nlimited explanatory power\nof cross-attention weights, which we further examine under multilingual and multitask conditions in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS2\" title=\"5.1.2 Multitask and Multilingual Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "correlation",
                    "crossattention",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the impact of multilingual and multitask training on the correlation between cross-attention scores and saliency maps, we evaluate the <span class=\"ltx_text ltx_font_typewriter\">small</span> and <span class=\"ltx_text ltx_font_typewriter\">large</span> models. Layer-wise results are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.T2\" title=\"Table 2 &#8227; 5.1.2 Multitask and Multilingual Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, while head-wise results are omitted due to the noisy behavior observed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS1\" title=\"5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "layerwise",
                    "models",
                    "correlation",
                    "large",
                    "small",
                    "between",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across all configurations, we observe that <span class=\"ltx_text ltx_font_italic\">en</span> ASR yields the highest correlation values, outperforming even the monolingual <span class=\"ltx_text ltx_font_typewriter\">base</span> model (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS1\" title=\"5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a>). This suggests that large-scale multilingual training enhances the alignment between cross-attention and saliency maps, likely due to the improved generalization capacity of the model. In contrast, <span class=\"ltx_text ltx_font_italic\">en-it</span> ST shows a drop in correlation, which is expected given the increased complexity of ST compared to ASR.\nWhen considering <span class=\"ltx_text ltx_font_italic\">it</span> as the source language, we observe a similar pattern: ASR correlations are consistently higher than ST, yet remain below their <span class=\"ltx_text ltx_font_italic\">en</span> counterparts. This discrepancy aligns with the data distribution in training, where <span class=\"ltx_text ltx_font_italic\">en</span> accounts for 84% of the data versus 16% for <span class=\"ltx_text ltx_font_italic\">it</span>, resulting in more robust representations for <span class=\"ltx_text ltx_font_italic\">en</span>.\nAt the layer level, we find consistent evidence that the last decoder layers yield stronger correlations, reaffirming the trends observed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS1\" title=\"5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a>.\nThe specific optimal layer varies with model size: layer 5 performs best in <span class=\"ltx_text ltx_font_typewriter\">small</span>, while layers 8-10 achieve the highest correlations in <span class=\"ltx_text ltx_font_typewriter\">large</span>.\nNevertheless, correlation values across the last\nlayers remain very close, suggesting that their cross-attention scores\nprovide the most robust alignment with saliency maps across both tasks and languages. This trend is further supported by downstream application results, where the final layers have shown the best token-level performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib65\" title=\"\">2023a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib66\" title=\"\">b</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib91\" title=\"\">2024a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "correlation",
                    "base",
                    "language",
                    "model",
                    "large",
                    "highest",
                    "all",
                    "source",
                    "small",
                    "asr",
                    "between",
                    "layers",
                    "crossattention",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Averaging attention scores across layers further improves the correlation with saliency maps in\nalmost\nall configurations. The only exceptions are <span class=\"ltx_text ltx_font_italic\">en</span> and <span class=\"ltx_text ltx_font_italic\">it</span> ASR in <span class=\"ltx_text ltx_font_typewriter\">small</span>, where selective-layer extraction offers a marginal improvement (0.006 for English, 0.001 for Italian).\nTherefore, similarly to what we observed in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS1\" title=\"5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a>, averaging attention across heads and layers consistently yields the best or near-best correlation with <span class=\"ltx_text ltx_font_italic\">moderate</span> to <span class=\"ltx_text ltx_font_italic\">strong</span> correlation with input saliency maps, even considering large-scale models trained in multitask and multilingual settings.\nNonetheless, this alignment accounts for only 49-63% of the total input relevance, indicating that <span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#88185B;\">cross-attention falls short of fully accounting for the S2T models&#8217; behavior</span>.\nSince this limitation may stem from the phenomenon of context mixing, in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS2\" title=\"5.2 What Is the Impact of Context Mixing? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a> we analyze the correlation between cross-attention and encoder output&#8211;representations that have already undergone transformation by the encoder&#8211;to better isolate the true explanatory power of cross-attention.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "correlation",
                    "all",
                    "small",
                    "encoder",
                    "asr",
                    "between",
                    "layers",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Even when examining encoder output representations, we observe trends consistent with those identified in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1\" title=\"5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>. Specifically, when averaged across decoder layers, cross-attention scores consistently provide the strongest or nearly optimal correlation with saliency maps, with the last decoder layers offering more representative explanations than the first ones across all models. As expected, correlation with encoder output representations consistently yields higher scores than those obtained from input representations, with absolute <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math> differences ranging from 0.03 to 0.18, quantifying the influence of context mixing effects to 6.6-16.7%.\nThe increased correlation is also visually evident in the example shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.F2\" title=\"Figure 2 &#8227; 5.2 What Is the Impact of Context Mixing? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, where <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> aligns more closely with the relevance scores from <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m4\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></sup> than with those from <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m5\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m6\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></sup>.\nHowever, despite being unaffected by context mixing, the correlation between <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m7\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> and <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m8\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m9\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></sup> remains limited&#8211;capturing only 52-75% of the relevance.\n<span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#88185B;\">This\ngap underscores the inherent limitations in relying solely on cross-attention as an explanation mechanism, reinforcing its role as an informative but <span class=\"ltx_text ltx_ulem_uline\">incomplete</span> proxy for explainability in S2T models</span>&#8211;not only for input-level saliency, but even at the encoder-output level, where cross-attention directly operates.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "correlation",
                    "ùêíùêåmathbfsmhh",
                    "œÅrho",
                    "output",
                    "all",
                    "encoder",
                    "layers",
                    "between",
                    "explanations",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Discussion.</span>\nOur results demonstrate that, although <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> scores moderately correlate with aggregated <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m2\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math> (with a correlation peaking around 0.45-0.55 in the best-performing settings), they consistently fall short of capturing the full input relevance&#8211;even when context mixing effects are factored out. To directly assess explanation quality, we compute the deletion metric (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A1\" title=\"Appendix A Effect of Aggregation Functions on Input Explanations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>) on the <span class=\"ltx_text ltx_font_typewriter\">base</span> model, finding that <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> achieves 41.2, compared to 52.9 for frequency-aggregated <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m4\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math> and 91.3 for full-resolution maps.\nThis gap underscores that <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m5\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> discards fine-grained time-frequency cues and produces weaker attributions, even under identical aggregation. As further discussed in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A6\" title=\"Appendix F Limitations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>, our analysis is bounded by the use of SPES as the attribution baseline, but the consistent underperformance across correlation and deletion confirms that <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m6\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> offers, at best, an incomplete picture of model behavior. These results also carry implications for downstream tasks. In applications such as timestamp prediction, prior work often relies on attention from a single decoder layer or head <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib91\" title=\"\">2024a</a>; Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib65\" title=\"\">2023a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib66\" title=\"\">b</a>; Zusag et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib106\" title=\"\">2024</a>)</cite>. Our analysis suggests that averaging across layers and, especially, across heads provides a closer match to saliency behavior and could improve these methods.\nBuilding on past success with attention regularization in ASR (e.g., imposing monotonicity as in <cite class=\"ltx_cite ltx_citemacro_citet\">Zhao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib104\" title=\"\">2020</a></cite>), similar training-time strategies&#8211;such as auxiliary losses that align attention with saliency&#8211;could further benefit downstream applications, enhancing both interpretability and task performance.\nIn summary, <span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"--ltx-fg-color:#88185B;\"><math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m7\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> should not be treated as a stand-alone XAI tool</span>. It provides lightweight cues that may complement attribution-based methods, but it cannot replace them. Reframing <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m8\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> as an auxiliary rather than a proxy recalibrates expectations and grounds future work on more faithful and effective approaches to explainability in S2T models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "correlation",
                    "base",
                    "model",
                    "asr",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conclusions.</span>\nWe presented the first systematic analysis of cross-attention in S2T through the lens of explainable AI, comparing it to saliency maps across tasks, languages, and model scales. Cross-attention moderately to strongly aligns with saliency&#8211;especially when averaged across heads and layers&#8211;but captures only about half of the input relevance. Even when disentangling the effect of context mixing by analyzing encoder outputs, it explains just 52-75% of saliency. This gap reveals intrinsic limits of cross-attention as an explanation mechanism: it offers informative cues but only a partial view of the factors driving S2T predictions.</p>\n\n",
                "matched_terms": [
                    "across",
                    "model",
                    "encoder",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Broader Implications.</span> Explainability in S2T systems has tangible implications for AI transparency, especially in high-stakes settings such as healthcare, legal transcription, and educational accessibility. Our findings provide insights about the usage of cross-attention as a tool for identifying how models relate output predictions to input regions, which can support auditing, debugging, and fair deployment. However, there is a risk that misinterpreted attention visualizations may be overtrusted by non-expert users, reinforcing false confidence in system behavior <cite class=\"ltx_cite ltx_citemacro_citep\">(Rudin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib75\" title=\"\">2019</a>)</cite>. Moreover, our language choices and focus on high-resource speech still reflect global imbalances in language technology access <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib36\" title=\"\">2020</a>)</cite>. Future work should extend this analysis to low-resource and underrepresented languages to promote broader inclusion.</p>\n\n",
                "matched_terms": [
                    "language",
                    "models",
                    "output",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Use of Large Language Models.</span> For the writing process, ChatGPT was employed exclusively to correct grammar in content authored by humans.</p>\n\n",
                "matched_terms": [
                    "language",
                    "models",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the reproducibility of our results, we described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S4\" title=\"4 Experimental Settings &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> all the details regarding our model training, training and evaluation data, and evaluation procedure. Moreover, we relied only on openly available data and on open source code<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/hlt-mt/FBK-fairseq/blob/master/fbk_works/XAI_FEATURE_ATTRIBUTION.md\" title=\"\">https://github.com/hlt-mt/FBK-fairseq/blob/master/fbk_works/XAI_FEATURE_ATTRIBUTION.md</a></span></span></span> for the generation of the saliency maps. Lastly, all models (described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S4\" title=\"4 Experimental Settings &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), code, attention scores, and explanation artifacts will be released under the Apache 2.0 (code) and CC-BY 4.0 (all other materials) licenses upon paper acceptance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "source",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To properly obtain input-level explanations comparable with the dimensions of cross-attention scores (i.e., making <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\\in\\mathbb{R}^{I\\times T^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m1\" intent=\":literal\"><semantics><mrow><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>T</mi><mo>&#8242;</mo></msup></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\\in\\mathbb{R}^{I\\times T^{\\prime}}</annotation></semantics></math>), we explore the effect of different aggregation strategies over the time and frequency dimensions.</p>\n\n",
                "matched_terms": [
                    "explanations",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A1.T4\" title=\"Table 4 &#8227; Appendix A Effect of Aggregation Functions on Input Explanations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> reports the deletion scores and, for completeness, the Pearson <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m1\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math> correlations between the cross attention scores <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> and the saliency maps <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m4\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></sup> for the representations aggregated following three strategies:</p>\n\n",
                "matched_terms": [
                    "œÅrho",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The aggregation functions were selected to contrast methods that either isolate the most relevant features (with maximum pooling) or represent their mean relevance (with average pooling). Similarly, the 2-step approach has been tried to first isolate relevance patterns in the frequency domain, a dimension that is not present in cross-attention representation, and then average across the time dimension to match the downsampled time resolution of the cross-attention scores.</p>\n\n",
                "matched_terms": [
                    "across",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Among the tested methods, we observe that the 2D maximum pooling aggregation (2D max) yields the best quality explanations, obtaining the highest deletion score, while the 2D average pooling (2D avg) is the worst, with the lowest deletion score. Looking at the correlations, we notice that they follow the same trend of deletion scores, with the 2D max yielding the best <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m1\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math>. In particular, 2D avg consistently has the lowest correlations compared to the 2D max, particularly in the last layers (e.g., 0.457 against 0.572 at layer 5). Regarding the 2-step pooling approach, we not only observe an improved deletion score but also better correlation scores compared to 2D avg, especially from layer 3 onward, approaching the best performance with a layer-average correlation of 0.565. Nevertheless, the explanation quality is still lower compared to 2D max (i.e., 55.18 against 57.04), which also achieves the highest correlations at nearly every layer, peaking at 0.582 in layer 6, and yielding the best overall correlation among the averaged cross-attention across layers (i.e., 0.572).</p>\n\n",
                "matched_terms": [
                    "crossattention",
                    "across",
                    "correlation",
                    "œÅrho",
                    "overall",
                    "highest",
                    "explanations",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the monolingual ASR model, we leverage the speech-to-text English data available for the IWSLT 2024 evaluation campaign (offline task),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://iwslt.org/2024/offline\" title=\"\">https://iwslt.org/2024/offline</a></span></span></span> namely: CommonVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Ardila et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib4\" title=\"\">2020</a>)</cite>, CoVoST v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib90\" title=\"\">2021b</a>)</cite>, Europarl-ST <cite class=\"ltx_cite ltx_citemacro_citep\">(Iranzo-S&#225;nchez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib33\" title=\"\">2020</a>)</cite>, LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib64\" title=\"\">2015</a>)</cite>, MuST-C v1 <cite class=\"ltx_cite ltx_citemacro_citep\">(Di&#160;Gangi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib17\" title=\"\">2019</a>)</cite>, TEDLIUM v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Hernandez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib31\" title=\"\">2018</a>)</cite>, and VoxPopuli ASR <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib89\" title=\"\">2021a</a>)</cite>. The resulting training set is about 3k hours of speech.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the multitask (ASR and ST) multilingual large-scale models, we leverage more than 150k hours of open-source speech<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>Speech, transcripts, and translations released under an open-source license such as CC-0 and CC-BY 4.0.</span></span></span> in English (<span class=\"ltx_text ltx_font_italic\">en</span>) and Italian (<span class=\"ltx_text ltx_font_italic\">it</span>), namely: CommonVoice, CoVoST v2, FLEURS <cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib14\" title=\"\">2023</a>)</cite>, MOSEL <cite class=\"ltx_cite ltx_citemacro_citep\">(Gaido et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib23\" title=\"\">2024a</a>)</cite>, MLS <cite class=\"ltx_cite ltx_citemacro_citep\">(Pratap et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib71\" title=\"\">2020</a>)</cite>, and YouTube-Commons<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hf.co/datasets/PleIAs/YouTube-Commons\" title=\"\">https://hf.co/datasets/PleIAs/YouTube-Commons</a></span></span></span> (from which 14.2k hours of <span class=\"ltx_text ltx_font_italic\">en</span> and 1.8k for <span class=\"ltx_text ltx_font_italic\">it</span> have been extracted). For datasets missing the translations, we generated them using <span class=\"ltx_text ltx_font_typewriter\">MADLAD-400 3B-MT</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Kudugunta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib43\" title=\"\">2023</a>)</cite>. This setting allows us to verify our analysis with a large-scale setting similar to the scale of a popular model such as OWSM <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib69\" title=\"\">2023</a>)</cite> and 2 times that of NVIDIA Canary <cite class=\"ltx_cite ltx_citemacro_citep\">(Puvvada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib72\" title=\"\">2024</a>)</cite> while having complete control of data used during training, ensuring that data contamination issues are avoided completely.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train all models using a combination of three losses: <em class=\"ltx_emph ltx_font_italic\">i)</em> a label-smoothed cross-entropy loss (<math alttext=\"\\mathcal{L}_{\\text{CE}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>CE</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{CE}}</annotation></semantics></math>) applied to the decoder output using the target text as the reference (transcripts for ASR and translations for ST), <em class=\"ltx_emph ltx_font_italic\">ii)</em> a CTC loss <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib27\" title=\"\">2006</a>)</cite> computed using transcripts as reference (<math alttext=\"\\mathcal{L}_{\\text{CTCsrc}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>CTCsrc</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{CTCsrc}}</annotation></semantics></math>) on the output of the 8<sup class=\"ltx_sup\">th</sup> encoder layer for <span class=\"ltx_text ltx_font_typewriter\">base</span> and <span class=\"ltx_text ltx_font_typewriter\">small</span> and the 16<sup class=\"ltx_sup\">th</sup> for <span class=\"ltx_text ltx_font_typewriter\">medium</span>, <em class=\"ltx_emph ltx_font_italic\">iii)</em> a CTC loss on the final encoder output (<math alttext=\"\\mathcal{L}_{\\text{CTCtgt}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>CTCtgt</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{CTCtgt}}</annotation></semantics></math>) applied to predict the target text <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib98\" title=\"\">2023</a>)</cite>.\nThe final loss is the weighted sum of the above-mentioned losses:</p>\n\n",
                "matched_terms": [
                    "models",
                    "base",
                    "output",
                    "all",
                    "target",
                    "small",
                    "encoder",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The monolingual ASR <span class=\"ltx_text ltx_font_typewriter\">base</span> model is trained on all 3k hours of ASR data for 200k steps using Noam as the learning rate scheduler <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib85\" title=\"\">2017</a>)</cite> with a peak of 2e-3 and 25,000 warm-up steps.</p>\n\n",
                "matched_terms": [
                    "base",
                    "all",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The multitask and multilingual models are trained using a two-stage approach, where the model is pre-trained first on ASR data only (ASR pre-training) and then trained on both ASR and ST data (ASR+ST training). For the ASR pre-training, the learning rate scheduler adopted for the <span class=\"ltx_text ltx_font_typewriter\">small</span> model is the same as the <span class=\"ltx_text ltx_font_typewriter\">base</span> model.\nFor the <span class=\"ltx_text ltx_font_typewriter\">medium</span> model, we adopted a piece-wise warm-up on the Noam scheduler to avoid divergence issues <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib70\" title=\"\">2024</a>)</cite>, with the learning rate first increasing linearly to 2e-5 for 25k steps and then to 2e-4 for an additional 25k steps, followed by the standard inverse square root function.\nFor the ASR+ST training, we sample the ASR target with probability 0.5 and use the ST target otherwise following the same settings of ASR pre-training, except for the learning rate that is set to a constant value of 1e-4 for <span class=\"ltx_text ltx_font_typewriter\">small</span> and 1e-5 for <span class=\"ltx_text ltx_font_typewriter\">medium</span>, following the same downscale of the ASR pre-taining. Both training stages lasted 1M steps, corresponding to <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>6 epochs over the training data.</p>\n\n",
                "matched_terms": [
                    "models",
                    "base",
                    "model",
                    "target",
                    "small",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the <span class=\"ltx_text ltx_font_typewriter\">base</span> model, the trainings are executed on 4 NVIDIA A100 GPUs (64GB RAM) with a mini-batch of 40,000 tokens, an update frequency of 2, and averaging the last 7 checkpoints obtained from the training.\nFor the multitask and multitlingual models, we use mini-batches of 10,000 tokens for the <span class=\"ltx_text ltx_font_typewriter\">small</span> and 4,500 for the <span class=\"ltx_text ltx_font_typewriter\">medium</span> with an update frequency of, respectively, 2 and 6 on 16 NVIDIA A100 GPUs (64GB RAM), save checkpoints every 1,000 steps and average the last 25 checkpoints to obtain the final models.</p>\n\n",
                "matched_terms": [
                    "base",
                    "models",
                    "small",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The quality of the ASR hypotheses is evaluated with the WER metric using the jiWER library<span class=\"ltx_note ltx_role_footnote\" id=\"footnote12\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_tag ltx_tag_note\">12</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://pypi.org/project/jiwer/\" title=\"\">https://pypi.org/project/jiwer/</a></span></span></span> and applying the Whisper text normalizer<span class=\"ltx_note ltx_role_footnote\" id=\"footnote13\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_tag ltx_tag_note\">13</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://pypi.org/project/whisper-normalizer/\" title=\"\">https://pypi.org/project/whisper-normalizer/</a></span></span></span>.\nThe quality of the ST hypotheses is evaluated using COMET <cite class=\"ltx_cite ltx_citemacro_citep\">(Rei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib74\" title=\"\">2020</a>)</cite> version 2.2.4, with the default model.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\">14</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hf.co/Unbabel/wmt22-comet-da\" title=\"\">https://hf.co/Unbabel/wmt22-comet-da</a></span></span></span>\nThe quality of the explanations is obtained by measuring both\n<span class=\"ltx_text ltx_font_italic\">deletion</span> and <span class=\"ltx_text ltx_font_italic\">size</span> metrics\navailable in the SPES repository, using <span class=\"ltx_text ltx_font_typewriter\">wer_max</span> as the scorer for ASR and <span class=\"ltx_text ltx_font_typewriter\">bleu</span> for ST, as described in <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "explanations",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For comparison, in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A3.T5\" title=\"Table 5 &#8227; Appendix C Quality Metrics for the Reported Models &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we also report results obtained from popular large-scale models, namely Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib73\" title=\"\">2023</a>)</cite>, OWSM v3.1 <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib70\" title=\"\">2024</a>)</cite>, and SeamlessM4T <cite class=\"ltx_cite ltx_citemacro_citep\">(Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib7\" title=\"\">2023</a>)</cite>. Looking at the transcription/translation quality performance, we observe that both the monolingual <span class=\"ltx_text ltx_font_typewriter\">base</span> model and the multitask multilingual <span class=\"ltx_text ltx_font_typewriter\">small</span> and <span class=\"ltx_text ltx_font_typewriter\">large</span> models are mostly able to achieve competitive results, even outperforming the well-known models in two cases (<span class=\"ltx_text ltx_font_italic\">en</span> ASR for <span class=\"ltx_text ltx_font_typewriter\">base</span> and <span class=\"ltx_text ltx_font_italic\">en-it</span> ST for <span class=\"ltx_text ltx_font_typewriter\">large</span>). While our models and OWSM v3.1 strive to be on par on <span class=\"ltx_text ltx_font_italic\">it</span> with models with closed training data (Whisper and Seamless), they are able to close the gap on <span class=\"ltx_text ltx_font_italic\">en</span>, most probably given a larger availability of public training data. Moreover, the highest performance of <span class=\"ltx_text ltx_font_typewriter\">base</span> on <span class=\"ltx_text ltx_font_italic\">en</span> ASR compared to the <span class=\"ltx_text ltx_font_typewriter\">small</span> and <span class=\"ltx_text ltx_font_typewriter\">large</span> can be attributed to both the specialization of the model and the presence of the EuroParl-ST training set in the training data.</p>\n\n",
                "matched_terms": [
                    "models",
                    "base",
                    "model",
                    "large",
                    "highest",
                    "small",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moving to the explanation quality, we observe that both deletion and size scores are comparable across all three models analyzed in the paper and coherent with values obtained in the original SPES paper <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite> on different benchmarks and models. Overall, the deletion scores for ASR are close to the highest possible value (i.e., 100), especially on <span class=\"ltx_text ltx_font_italic\">it</span>, where 97% is achieved. Similarly, the deletion scores for ST are close to 0, indicating that the quality of explanations is very high. The size scores are all close, ranging between 28.2 and 30.6 among models, languages, and tasks, indicating a good compactness of the explanations.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "overall",
                    "high",
                    "highest",
                    "all",
                    "asr",
                    "between",
                    "explanations",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To properly choose the occlusion probability (<math alttext=\"p_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m1\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">p_{H}</annotation></semantics></math>) for the encoder output explanations <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m3\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></sup>, we conducted experiments by varying this probability in the set of <math alttext=\"\\{0.1,0.3,0.5,0.7,0.9\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>0.1</mn><mo>,</mo><mn>0.3</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>0.7</mn><mo>,</mo><mn>0.9</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{0.1,0.3,0.5,0.7,0.9\\}</annotation></semantics></math>, similarly to what has been done for determining the input occlusion probability (<math alttext=\"p_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m5\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">p_{X}</annotation></semantics></math>) in SPES <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "output",
                    "ùêíùêåmathbfsmhh",
                    "encoder",
                    "explanations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A4.T6\" title=\"Table 6 &#8227; Appendix D Effect of Occlusion Probability on Encoder Output Explanations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we report the <span class=\"ltx_text ltx_font_italic\">deletion</span>\nmetric computed on the dev set and, for completeness, the results of the Pearson <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m1\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math> correlation with cross-attention <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math>.\nAnalogously to\nthe deletion metric computed on the input saliency maps <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>, we compute the deletion on the encoder output saliency maps by iteratively replacing\nportions of the encoder output sequence <math alttext=\"\\mathbf{H}=\\{\\mathbf{h}_{1},\\ldots,\\mathbf{h}_{I}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#119815;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>&#119841;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119841;</mi><mi>I</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{H}=\\{\\mathbf{h}_{1},\\ldots,\\mathbf{h}_{I}\\}</annotation></semantics></math> with zero vectors, and removing 5% of the most important time frames at each step based on their saliency.\nFrame importance is determined using saliency maps <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m4\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>H</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}</annotation></semantics></math> aggregated at the sentence level.\nThe output quality is evaluated using the same <span class=\"ltx_text ltx_font_typewriter\">wer_max</span> scorer from the SPES repository. Lastly, we compute the area under the curve of the WER progression to quantify the faithfulness of the explanation.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "œÅrho",
                    "output",
                    "encoder",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the results, we can notice that higher occlusion probabilities yield not only a better deletion score but also an increased correlation with <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p3.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math>. The overall best correlation is achieved when averaging across all layers, and layer 5 achieves the best layer-specific correlation values, a phenomenon that remains coherent even when varying the occlusion probability.\nInterestingly, the deletion scores and the <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p3.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math>-<math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p3.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p3.m4\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></sup> correlations always follow the same trend, with the best values achieved with <math alttext=\"p_{H}=0.7\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p3.m5\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>H</mi></msub><mo>=</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">p_{H}=0.7</annotation></semantics></math>, which we used in all experiments reported in the main paper.</p>\n\n",
                "matched_terms": [
                    "across",
                    "correlation",
                    "overall",
                    "all",
                    "layers",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Examples of different saliency maps and cross-attention representations obtained with the <span class=\"ltx_text ltx_font_typewriter\">large</span> model are presented in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "crossattention",
                    "model",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We notice similar relevance patterns in the paired samples&#8211;i.e., the samples having the same source language (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>a-f, and Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>g-l)&#8211;even if involving different tasks. We observe a reordering phenomenon from the English audio <span class=\"ltx_text ltx_inline-quote ltx_outerquote\">&#8220;<span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#0000FF;\">cheap<span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\"> <span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">money</span></span></span>&#8221;</span> and its Italian textual counterpart <span class=\"ltx_text ltx_inline-quote ltx_outerquote\">&#8220;<span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#FF0000;\">denaro<span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\"> <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">a buon mercato</span></span></span>&#8221;</span>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote15\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_tag ltx_tag_note\">15</span>Same colors reflect the same concepts.</span></span></span> which is reflected in the saliency maps (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>d and e) and also captured by cross-attention (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>f). We also observe that there are some patterns captured by the attention that are not reflected in the input. For instance, in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>f, the first words (<span class=\"ltx_text ltx_inline-quote ltx_outerquote\">&#8220;<span class=\"ltx_text ltx_font_italic\">&#200; solo</span>&#8221;</span>) attend&#8211;albeit with relatively low scores&#8211;to the audio frames between 75 and 85, while this pattern this pattern is absent in the relevance scores of both the encoder output and the input. Consistent with the findings discussed throughout the paper, this example illustrates that while attention generally follows the saliency patterns identified by feature attribution, some discrepancies persist.</p>\n\n",
                "matched_terms": [
                    "language",
                    "low",
                    "output",
                    "source",
                    "encoder",
                    "between",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work provides an in-depth analysis of cross-attention explainability in encoder-decoder S2T models. While it yields actionable insights, some limitations should be acknowledged. First, our experimental scope is restricted to ASR and ST. Although these tasks are central to S2T-based AI <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib73\" title=\"\">2023</a>; Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib7\" title=\"\">2023</a>)</cite>, we do not evaluate other downstream tasks such as spoken question answering or speech summarization, which may involve different dynamics in decoder attention. Second, our multilingual analysis is limited to English (a Germanic language) and Italian (a Romance language), due to the high computational cost of large-scale model training across a broader set of languages. Third, we focus on models trained from scratch but do not include architectures based on Speech Foundation Models (SFMs) paired with large language models (LLMs), often referred to as SpeechLLM&#8211;a recent growing area of interest in S2T modeling <cite class=\"ltx_cite ltx_citemacro_citep\">(Gaido et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib24\" title=\"\">2024b</a>)</cite>. As our analysis focuses on evaluation, our goal was to completely avoid data contamination issues <cite class=\"ltx_cite ltx_citemacro_citep\">(Sainz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib76\" title=\"\">2023</a>)</cite>, which is a problem affecting almost every SFM and SpeechLLM architectures currently available, as we have no control over their training data, and, for this reason, we decided to retrain the models from scratch.\nFourth, our analysis relies on SPES to compute reference explanations, acknowledging that, as an empirical method, it may introduce some margin of error. However, in the absence of a gold or human reference&#8211;which is unattainable in practice&#8211;we adopt SPES as a <span class=\"ltx_text ltx_font_italic\">silver</span> reference, since it represents the state of the art in explainability for speech-to-text. We further validate this choice in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A3\" title=\"Appendix C Quality Metrics for the Reported Models &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>, showing that SPES achieves very high quality explanations (deletion scores <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>90 on ASR and <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m2\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>3 on ST), making it a more faithful option than less robust alternatives from the generic XAI field such as gradient norms <cite class=\"ltx_cite ltx_citemacro_citep\">(Covert et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib15\" title=\"\">2021a</a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "language",
                    "model",
                    "large",
                    "high",
                    "asr",
                    "explanations",
                    "crossattention"
                ]
            }
        ]
    },
    "A1.T4": {
        "caption": "Table 4: Pearson œÅ\\rho correlation between layer-wise (‚Ñì\\ell) cross-attention and the explanations, and the deletion scores (ASR Del.) for the different aggregation functions of the monolingual ASR model on English (base) on the dev set. The layer average (‚Ñì\\ell-AVG) correlation is computed between the averaged cross-attention across layers (ùêÇùêÄ¬Ø(‚Ñì)\\widebar{\\text{{\\color[rgb]{0.75390625,0.390625,0.9609375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.75390625,0.390625,0.9609375}$\\mathbf{CA}$}}}^{(\\ell)}) and the input explanations ùêíùêå\\mathbf{SM}XX. Bold indicates the highest result, underline indicates the highest layer-wise correlation.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"2\" style=\"padding-left:5.9pt;padding-right:5.9pt;\"><span class=\"ltx_text ltx_font_bold\">Aggregation</span></th>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.9pt;padding-right:5.9pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.9pt;padding-right:5.9pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.9pt;padding-right:5.9pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.9pt;padding-right:5.9pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.9pt;padding-right:5.9pt;\"/>\n<th class=\"ltx_td ltx_border_r_dashed ltx_align_right ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.9pt;padding-right:5.9pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:5.9pt;padding-right:5.9pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.9pt;padding-right:5.9pt;\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.9pt;padding-right:5.9pt;\"><span class=\"ltx_text ltx_font_italic\">frequency</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:5.9pt;padding-right:5.9pt;\"><span class=\"ltx_text ltx_font_italic\">time</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:5.9pt;padding-right:5.9pt;\"><math alttext=\"\\ell=1\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m1\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=1</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:5.9pt;padding-right:5.9pt;\"><math alttext=\"\\ell=2\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m2\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=2</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:5.9pt;padding-right:5.9pt;\"><math alttext=\"\\ell=3\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m3\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=3</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:5.9pt;padding-right:5.9pt;\"><math alttext=\"\\ell=4\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m4\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=4</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:5.9pt;padding-right:5.9pt;\"><math alttext=\"\\ell=5\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m5\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=5</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_border_r_dashed ltx_align_right ltx_th ltx_th_column\" style=\"padding-left:5.9pt;padding-right:5.9pt;\">\n<math alttext=\"\\ell=6\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m6\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=6</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" style=\"padding-left:5.9pt;padding-right:5.9pt;\">\n<math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m7\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math><span class=\"ltx_text ltx_font_bold\">-AVG</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" style=\"padding-left:5.9pt;padding-right:5.9pt;\"><span class=\"ltx_text ltx_font_bold\">ASR Del.<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A1.T4.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\" style=\"padding-left:5.9pt;padding-right:5.9pt;\">2D avg</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FEFAE0;padding-left:5.9pt;padding-right:5.9pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FEFAE0;\">0.090</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#FAF8DB;padding-left:5.9pt;padding-right:5.9pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FAF8DB;\">0.142</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E7EBC2;padding-left:5.9pt;padding-right:5.9pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E7EBC2;\">0.355</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#E0E7B9;padding-left:5.9pt;padding-right:5.9pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E0E7B9;\">0.434</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#DEE6B7;padding-left:5.9pt;padding-right:5.9pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#DEE6B7;\">0.457</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" style=\"--ltx-bg-color:#DAE3B3;padding-left:5.9pt;padding-right:5.9pt;\"><span class=\"ltx_text ltx_border_r_dashed\" style=\"--ltx-bg-color:#DAE3B3;\">0.466</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#DEE6B7;padding-left:5.9pt;padding-right:5.9pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#DEE6B7;\">0.459</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.9pt;padding-right:5.9pt;\">53.03</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:5.9pt;padding-right:5.9pt;\">1D max</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:5.9pt;padding-right:5.9pt;\">1D avg</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FCF9DE;padding-left:5.9pt;padding-right:5.9pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCF9DE;\">0.115</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#F7F6D7;padding-left:5.9pt;padding-right:5.9pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F7F6D7;\">0.176</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#DFE6B8;padding-left:5.9pt;padding-right:5.9pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#DFE6B8;\">0.441</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#C0D091;padding-left:5.9pt;padding-right:5.9pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#C0D091;\">0.534</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#B6C883;padding-left:5.9pt;padding-right:5.9pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#B6C883;\">0.560</span></td>\n<td class=\"ltx_td ltx_align_right\" style=\"--ltx-bg-color:#B4C781;padding-left:5.9pt;padding-right:5.9pt;\"><span class=\"ltx_text ltx_border_r_dashed\" style=\"--ltx-bg-color:#B4C781;\">0.565</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#B4C681;padding-left:5.9pt;padding-right:5.9pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#B4C681;\">0.565</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.9pt;padding-right:5.9pt;\">55.18</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" colspan=\"2\" style=\"padding-left:5.9pt;padding-right:5.9pt;\">2D max</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"--ltx-bg-color:#FCF9DE;padding-left:5.9pt;padding-right:5.9pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCF9DE;\">0.115</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"--ltx-bg-color:#F6F5D6;padding-left:5.9pt;padding-right:5.9pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F6F5D6;\">0.180</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"--ltx-bg-color:#DFE6B8;padding-left:5.9pt;padding-right:5.9pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#DFE6B8;\">0.443</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"--ltx-bg-color:#BECE8E;padding-left:5.9pt;padding-right:5.9pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#BECE8E;\">0.540</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"--ltx-bg-color:#B1C47D;padding-left:5.9pt;padding-right:5.9pt;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#B1C47D;\">0.572</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_b\" style=\"--ltx-bg-color:#ADC178;padding-left:5.9pt;padding-right:5.9pt;\">\n<span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"--ltx-bg-color:#ADC178;\">0.582</span><span class=\"ltx_text ltx_border_r_dashed\" style=\"--ltx-bg-color:#ADC178;\"/>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" style=\"--ltx-bg-color:#B1C47D;padding-left:5.9pt;padding-right:5.9pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#B1C47D;\">0.572</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.9pt;padding-right:5.9pt;\"><span class=\"ltx_text ltx_font_bold\">57.04</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "ùêíùêåmathbfsmxx",
            "pearson",
            "‚Ñì1ell1",
            "‚Ñì2ell2",
            "underline",
            "monolingual",
            "del",
            "time",
            "base",
            "average",
            "max",
            "computed",
            "frequency",
            "between",
            "input",
            "scores",
            "layerwise",
            "across",
            "avg",
            "ùêÇùêÄ¬Ø‚Ñìwidebartextcolorrgb075390625039062509609375definecolornamedpgfstrokecolorrgb075390625039062509609375mathbfcaell",
            "english",
            "result",
            "averaged",
            "bold",
            "dev",
            "functions",
            "‚Ñì4ell4",
            "asr",
            "indicates",
            "‚Ñì6ell6",
            "‚Ñìell",
            "crossattention",
            "layers",
            "set",
            "correlation",
            "œÅrho",
            "aggregation",
            "deletion",
            "model",
            "highest",
            "layer",
            "del‚Üëuparrow",
            "explanations",
            "‚Ñì3ell3",
            "‚Ñìellavg",
            "different",
            "‚Ñì5ell5"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A1.T4\" title=\"Table 4 &#8227; Appendix A Effect of Aggregation Functions on Input Explanations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> reports the deletion scores and, for completeness, the Pearson <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m1\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math> correlations between the cross attention scores <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> and the saliency maps <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m4\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></sup> for the representations aggregated following three strategies:</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Cross-attention is a core mechanism in encoder-decoder architectures, widespread in many fields, including speech-to-text (S2T) processing. Its scores have been repurposed for various downstream applications&#8211;such as timestamp estimation and audio-text alignment&#8211;under the assumption that they reflect the dependencies between input speech representation and the generated text. While the explanatory nature of attention mechanisms has been widely debated in the broader NLP literature, this assumption remains largely unexplored within the speech domain. To address this gap, we assess the explanatory power of cross-attention in S2T models by comparing its scores to input saliency maps derived from feature attribution. Our analysis spans monolingual and multilingual, single-task and multi-task models at multiple scales, and shows that attention scores moderately to strongly align with saliency-based explanations, particularly when aggregated across heads and layers. However, it also shows that cross-attention captures only about 50% of the input relevance and, in the best case, only partially reflects how the decoder attends to the encoder&#8217;s representations&#8211;accounting for just 52-75% of the saliency. These findings uncover fundamental limitations in interpreting cross-attention as an explanatory proxy, suggesting that it offers an informative yet incomplete view of the factors driving predictions in S2T models.</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "across",
                    "layers",
                    "explanations",
                    "between",
                    "input",
                    "crossattention",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-attention <cite class=\"ltx_cite ltx_citemacro_citep\">(Bahdanau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib6\" title=\"\">2015</a>)</cite> is the core mechanism of the encoder-decoder Transformer architecture <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib85\" title=\"\">2017</a>)</cite>, a model that has become foundational across numerous AI domains <cite class=\"ltx_cite ltx_citemacro_citep\">(Galassi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib25\" title=\"\">2021</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib49\" title=\"\">2022</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib45\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib92\" title=\"\">2024b</a>; Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib51\" title=\"\">2024</a>)</cite>, including natural language processing (NLP).\nDesigned for modeling dependencies between the generated output sequence and the input representations, the cross-attention scores&#8211;derived from the attention mechanism&#8211;have been leveraged in various NLP tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib32\" title=\"\">2020</a>; Zhang &amp; Kim, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib103\" title=\"\">2023</a>)</cite>, such as\nsource-target textual alignment <cite class=\"ltx_cite ltx_citemacro_citep\">(Garg et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib26\" title=\"\">2019</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib12\" title=\"\">2020</a>)</cite>, co-reference resolution <cite class=\"ltx_cite ltx_citemacro_citep\">(Voita et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib87\" title=\"\">2018</a>)</cite>, and word sense disambiguation <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib83\" title=\"\">2018</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "model",
                    "between",
                    "input",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In speech-to-text (S2T) modeling, cross-attention scores have been widely repurposed for diverse downstream applications such as audio-text alignment <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib104\" title=\"\">2020</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib46\" title=\"\">2020</a>)</cite>, speaker identification <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib38\" title=\"\">2019</a>)</cite>, timestamp estimation <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib47\" title=\"\">2022</a>; Louradour, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib50\" title=\"\">2023</a>; Zusag et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib106\" title=\"\">2024</a>)</cite>, and guiding simultaneous automatic speech recognition (ASR) and speech translation (ST) <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib91\" title=\"\">2024a</a>; Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib65\" title=\"\">2023a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib66\" title=\"\">b</a>)</cite>.\nThese applications rely on the implicit assumption that\ncross-attention reliably indicates what the model attends to in the input signal during output generation. However, despite its widespread use, this assumption has\nnever been verified.\nA key concern is that cross-attention operates over the encoder&#8217;s output sequence&#8211;rather than directly on the raw audio&#8211;which may have been reorganized or mixed with contextual information. This phenomenon, known as context mixing <cite class=\"ltx_cite ltx_citemacro_citep\">(Mohebbi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib60\" title=\"\">2023b</a>)</cite>, can potentially obscure the alignment between cross-attention weights and the original input signal.\nSimilar concerns have been extensively debated in the NLP community, where the reliability of attention mechanisms as explanations has been both challenged and defended, leading to conflicting perspectives and empirical evidence <cite class=\"ltx_cite ltx_citemacro_citep\">(Serrano &amp; Smith, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib78\" title=\"\">2019</a>; Jain &amp; Wallace, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib35\" title=\"\">2019</a>; Wiegreffe &amp; Pinter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib93\" title=\"\">2019</a>; Bastings &amp; Filippova, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib9\" title=\"\">2020</a>)</cite>. In contrast, this question remains largely underexplored in the speech domain.\nExisting work on explainability in S2T has primarily focused on self-attention <cite class=\"ltx_cite ltx_citemacro_citep\">(Shim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib80\" title=\"\">2022</a>; Audhkhasi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib5\" title=\"\">2022</a>; A&#160;Shams et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib1\" title=\"\">2024</a>)</cite>, or on empirically measuring the effects of context mixing <cite class=\"ltx_cite ltx_citemacro_citep\">(Mohebbi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib59\" title=\"\">2023a</a>)</cite>, without directly assessing the explanatory potential of cross-attention mechanisms.</p>\n\n",
                "matched_terms": [
                    "model",
                    "asr",
                    "indicates",
                    "input",
                    "explanations",
                    "between",
                    "crossattention",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this gap, we present the first systematic\nanalysis of cross-attention as a proxy for input-output dependencies in S2T models.\nOur study serves two main objectives: <span class=\"ltx_text ltx_font_italic\">i)</span> assessing the validity of using cross-attention as a surrogate for input-output alignment,\nand <span class=\"ltx_text ltx_font_italic\">ii)</span> evaluating whether it provides insights comparable to formal explainability methods such as feature attribution&#8211;while being more lightweight and less computationally expensive to obtain <cite class=\"ltx_cite ltx_citemacro_citep\">(Samek et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib77\" title=\"\">2021</a>; Madsen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib53\" title=\"\">2022</a>)</cite>. We compare cross-attention scores with input saliency maps derived from SPES <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>, the current state-of-the-art feature-attribution method in S2T, to determine the extent to which\ncross-attention\ncaptures\nwhich input features are relevant for models&#8217; predictions.\nIn addition, we compute saliency maps on\nencoder outputs and compare them with cross-attention scores to evaluate whether cross-attention fully explains how the decoder uses encoded representations, avoiding potential discrepancies\nof context mixing.\nOur\nanalysis spans ASR and ST tasks across monolingual, multilingual, and multitask settings using\nstate-of-the-art speech processing\narchitectures <cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib29\" title=\"\">2020</a>)</cite> at multiple scales.\nWith consistent trends across different settings, we find\nthat cross-attention exhibits moderate to strong correlations with input saliency maps and aligns more closely with encoder output representations, suggesting an influence of context mixing. However, our results also indicate that the overall explanatory power of cross-attention is limited&#8211;accounting for only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>50% of input relevance and, at best, 52-75% of encoder output saliency.\nOur findings uncover fundamental limitations in interpreting cross-attention as an explanatory proxy, suggesting that it provides an informative yet incomplete view of the factors driving predictions in S2T models.\n</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "across",
                    "asr",
                    "input",
                    "different",
                    "crossattention",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Explainability in Speech-to-Text.</span>\nExplainable AI (XAI) has emerged to make model behavior more interpretable to humans, thereby supporting informed decision-making and responsible deployment <cite class=\"ltx_cite ltx_citemacro_citep\">(Barredo Arrieta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib8\" title=\"\">2020</a>)</cite>.\nWhile XAI research has seen a rapid growth in the last years across multiple modalities, including vision and language <cite class=\"ltx_cite ltx_citemacro_citep\">(Sharma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib79\" title=\"\">2024</a>)</cite>, progress in the speech domain has lagged.\nThis gap arises from the inherent complexities of speech processing, including the multidimensional nature of speech signals across time and frequency, and the variability in output sequence length <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib95\" title=\"\">2024</a>)</cite>.\nDespite these challenges, growing concerns about trustworthiness\nare driving explainability efforts in speech classification <cite class=\"ltx_cite ltx_citemacro_citep\">(Becker et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib10\" title=\"\">2024</a>; Pastor et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib68\" title=\"\">2024</a>)</cite> and S2T generation <cite class=\"ltx_cite ltx_citemacro_citep\">(Mandel, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib54\" title=\"\">2016</a>; Kavaki &amp; Mandel, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib37\" title=\"\">2020</a>; Trinh &amp; Mandel, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib84\" title=\"\">2020</a>; Markert et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib55\" title=\"\">2021</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib94\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib95\" title=\"\">2024</a>; Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>. Most of these works rely on perturbation-based methods\nthat assess how input modifications affect model predictions <cite class=\"ltx_cite ltx_citemacro_citep\">(Covert et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib16\" title=\"\">2021b</a>; Ivanovs et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib34\" title=\"\">2021</a>)</cite>.\nAmong these,\n<cite class=\"ltx_cite ltx_citemacro_citet\">Fucci et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite> recently proposed\na technique for autoregressive S2T models that identifies regions of the spectrogram that most influence predictions to generate saliency maps.\nHowever, XAI methods are generally computationally expensive&#8211;especially perturbation-based approaches applied to large models <cite class=\"ltx_cite ltx_citemacro_citep\">(Luo &amp; Specia, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib52\" title=\"\">2024</a>; Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib102\" title=\"\">2025</a>)</cite>&#8211;which motivates exploring whether cross-attention, already computed at inference time, could serve as a lightweight alternative in a landscape still lacking efficient explainability tools for speech-based models.</p>\n\n",
                "matched_terms": [
                    "time",
                    "across",
                    "model",
                    "frequency",
                    "computed",
                    "input",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Attention as Explanation.</span>\nAttention mechanisms have been widely used to probe model behavior in text-based NLP, as attention scores often align with human intuitions about relevance and salience <cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib13\" title=\"\">2019</a>; Ferrando et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib21\" title=\"\">2024</a>)</cite>.\nEarly studies proposed norm-based analyses to improve the interpretability of attention weights <cite class=\"ltx_cite ltx_citemacro_citep\">(Kobayashi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib39\" title=\"\">2020</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib40\" title=\"\">2021</a>; Mohebbi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib58\" title=\"\">2021</a>; Ferrando et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib20\" title=\"\">2022b</a>)</cite>, while others suggested aggregating attention across layers and heads to quantify input-output influence more systematically <cite class=\"ltx_cite ltx_citemacro_citep\">(Abnar &amp; Zuidema, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib2\" title=\"\">2020</a>; Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib101\" title=\"\">2021</a>; Chefer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib11\" title=\"\">2021</a>)</cite>.\nWhile some have raised concerns about whether attention reliably reflects which inputs are actually responsible for outputs <cite class=\"ltx_cite ltx_citemacro_citep\">(Jain &amp; Wallace, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib35\" title=\"\">2019</a>; Serrano &amp; Smith, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib78\" title=\"\">2019</a>; Bastings &amp; Filippova, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib9\" title=\"\">2020</a>)</cite>, others have proposed conditions under which attention can meaningfully explain model behavior <cite class=\"ltx_cite ltx_citemacro_citep\">(Wiegreffe &amp; Pinter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib93\" title=\"\">2019</a>)</cite>. More recent work highlights that attention aggregation may obscure localized, token-specific interactions <cite class=\"ltx_cite ltx_citemacro_citep\">(Modarressi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib57\" title=\"\">2023</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib100\" title=\"\">2023</a>; Oh &amp; Schuler, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib63\" title=\"\">2023</a>)</cite>, motivating hybrid approaches that combine attention with other XAI techniques, such as attribution methods <cite class=\"ltx_cite ltx_citemacro_citep\">(Modarressi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib56\" title=\"\">2022</a>)</cite>, or the use of attention as a regularization signal during interpretability-driven training <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib97\" title=\"\">2024</a>)</cite>.\nDespite the ongoing efforts, most research has focused on self-attention within encoders, with limited attention to feed-forward dynamics <cite class=\"ltx_cite ltx_citemacro_citep\">(Kobayashi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib41\" title=\"\">2024</a>)</cite> and even less to encoder-decoder models. A few studies have investigated attention in encoder-decoder architectures <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib62\" title=\"\">2021</a>)</cite>, including in machine translation <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib105\" title=\"\">2024</a>)</cite>, but cross-attention remains largely underexplored in the speech domain and has been\nabsent from the broader <span class=\"ltx_text ltx_inline-quote ltx_outerquote\">&#8220;attention as explanation&#8221;</span> debate in NLP. Our work seeks to bridge this gap by bringing cross-attention of S2T models into this broader conversation, aiming to assess\nwhether\nit can serve as a reliable explanation&#8211;and where its limitations emerge.\n</p>\n\n",
                "matched_terms": [
                    "crossattention",
                    "across",
                    "aggregation",
                    "model",
                    "layers",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We assess the extent to which cross-attention scores (<math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math>) explain how the model looks at\ninput features when generating a token\nby comparing them to the saliency map on the input <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math>, obtained with the state-of-the art feature-attribution method for S2T, SPES <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "input",
                    "model",
                    "crossattention",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, to assess whether cross-attention more accurately reflects how the decoder accesses encoded representations&#8211;rather than capturing the model&#8217;s full input-output behavior&#8211;we compare <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> with the encoder-output saliency map <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m2\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>H</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}</annotation></semantics></math>.\nBy analyzing how the correlation between <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> and <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m4\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>H</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}</annotation></semantics></math> deviates from that with <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m5\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math>,\nwe can indirectly quantify the impact of context mixing in the resulting explanations.\nA visual overview of this setup is provided in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S3.F1\" title=\"Figure 1 &#8227; 3 Methodology &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "explanations",
                    "correlation",
                    "crossattention",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In S2T models, the cross-attention mechanism enables each decoder token to integrate relevant portions of the encoded speech features, thereby conditioning generation on the entire input.</p>\n\n",
                "matched_terms": [
                    "input",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Let <math alttext=\"\\mathbf{X}\\in\\mathbb{R}^{T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}\\in\\mathbb{R}^{T\\times F}</annotation></semantics></math> denote the speech input represented by mel-spectrogram features, where <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> is the number of time frames and <math alttext=\"F\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\"><semantics><mi>F</mi><annotation encoding=\"application/x-tex\">F</annotation></semantics></math> the number of frequency bins. The encoder processes <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m4\" intent=\":literal\"><semantics><mi>&#119831;</mi><annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation></semantics></math> into a sequence of hidden representations <math alttext=\"\\mathbf{H}=\\mathrm{Encoder}(\\mathbf{X})\\in\\mathbb{R}^{T^{\\prime}\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><mi>&#119815;</mi><mo>=</mo><mrow><mi>Encoder</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119831;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{H}=\\mathrm{Encoder}(\\mathbf{X})\\in\\mathbb{R}^{T^{\\prime}\\times D}</annotation></semantics></math>, where <math alttext=\"T^{\\prime}&lt;T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m6\" intent=\":literal\"><semantics><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo>&lt;</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">T^{\\prime}&lt;T</annotation></semantics></math> reflects the number of encoder time steps after subsampling with a factor of <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m7\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math><span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>As the length of the speech inputs is, in general, 10<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"footnote1.m1\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> longer that of the corresponding textual input, it is a common practice in S2T modeling to downsample the input through convolutional modules <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib88\" title=\"\">2020</a>)</cite>.</span></span></span> and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m8\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> is the hidden dimensionality. The decoder then autoregressively generates an output sequence <math alttext=\"\\mathbf{y}=(y_{0},y_{1},\\ldots,y_{I})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m9\" intent=\":literal\"><semantics><mrow><mi>&#119858;</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mn>0</mn></msub><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>y</mi><mi>I</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{y}=(y_{0},y_{1},\\ldots,y_{I})</annotation></semantics></math> of length <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m10\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math>, where each token <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m11\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math> is predicted based on the previously generated tokens <math alttext=\"y_{&lt;i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m12\" intent=\":literal\"><semantics><msub><mi>y</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">y_{&lt;i}</annotation></semantics></math> and the encoder output <math alttext=\"\\mathbf{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m13\" intent=\":literal\"><semantics><mi>&#119815;</mi><annotation encoding=\"application/x-tex\">\\mathbf{H}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "time",
                    "frequency",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At each decoder layer <math alttext=\"\\ell\\in\\{1,\\ldots,L\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\ell\\in\\{1,\\ldots,L\\}</annotation></semantics></math>, cross-attention scores are computed via dot-product attention <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib28\" title=\"\">2014</a>)</cite> between the decoder&#8217;s current hidden states <math alttext=\"\\mathbf{B}^{(\\ell)}\\in\\mathbb{R}^{I\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><msup><mi>&#119809;</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{B}^{(\\ell)}\\in\\mathbb{R}^{I\\times D}</annotation></semantics></math> and the encoder outputs <math alttext=\"\\mathbf{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m3\" intent=\":literal\"><semantics><mi>&#119815;</mi><annotation encoding=\"application/x-tex\">\\mathbf{H}</annotation></semantics></math>. Specifically, the decoder states are linearly projected to queries <math alttext=\"\\mathbf{Q}^{(\\ell)}=\\mathbf{B}^{(\\ell)}\\mathbf{W}_{Q}^{(\\ell)}\\in\\mathbb{R}^{I\\times d_{k}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m4\" intent=\":literal\"><semantics><mrow><msup><mi>&#119824;</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><mrow><msup><mi>&#119809;</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#119830;</mi><mi>Q</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Q}^{(\\ell)}=\\mathbf{B}^{(\\ell)}\\mathbf{W}_{Q}^{(\\ell)}\\in\\mathbb{R}^{I\\times d_{k}}</annotation></semantics></math>, while the encoder outputs are projected to keys <math alttext=\"\\mathbf{K}^{(\\ell)}=\\mathbf{H}\\mathbf{W}_{K}^{(\\ell)}\\in\\mathbb{R}^{T^{\\prime}\\times d_{k}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m5\" intent=\":literal\"><semantics><mrow><msup><mi>&#119818;</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><msubsup><mi>&#119815;&#119830;</mi><mi>K</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{K}^{(\\ell)}=\\mathbf{H}\\mathbf{W}_{K}^{(\\ell)}\\in\\mathbb{R}^{T^{\\prime}\\times d_{k}}</annotation></semantics></math>\nusing learned projection matrices <math alttext=\"\\mathbf{W}_{Q}^{(\\ell)},\\mathbf{W}_{K}^{(\\ell)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m6\" intent=\":literal\"><semantics><mrow><msubsup><mi>&#119830;</mi><mi>Q</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>&#119830;</mi><mi>K</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{W}_{Q}^{(\\ell)},\\mathbf{W}_{K}^{(\\ell)}</annotation></semantics></math>. The resulting cross-attention matrix <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m7\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> is:</p>\n\n",
                "matched_terms": [
                    "layer",
                    "computed",
                    "between",
                    "crossattention",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where each row <math alttext=\"\\text{{\\color[rgb]{0.75390625,0.390625,0.9609375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.75390625,0.390625,0.9609375}$\\mathbf{CA}$}}^{(\\ell)}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m8\" intent=\":literal\"><semantics><msubsup><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.75390625,0.390625,0.9609375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.75390625,0.390625,0.9609375}$\\mathbf{CA}$}}^{(\\ell)}_{i}</annotation></semantics></math> represents the attention distribution over encoder time steps for the generation of output token <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m9\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math> at layer <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m10\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>.\nTo capture diverse patterns, Transformer-based models employ multi-head attention <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib85\" title=\"\">2017</a>)</cite>. Each head <math alttext=\"h\\in\\{1,\\ldots,H\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m11\" intent=\":literal\"><semantics><mrow><mi>h</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>H</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">h\\in\\{1,\\ldots,H\\}</annotation></semantics></math> uses separate learned projections:</p>\n\n",
                "matched_terms": [
                    "time",
                    "layer",
                    "‚Ñìell"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\mathbf{W}_{Q,h}^{(\\ell)}\\in\\mathbb{R}^{D\\times d_{k}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m12\" intent=\":literal\"><semantics><mrow><msubsup><mi>&#119830;</mi><mrow><mi>Q</mi><mo>,</mo><mi>h</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>D</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{W}_{Q,h}^{(\\ell)}\\in\\mathbb{R}^{D\\times d_{k}}</annotation></semantics></math>, and <math alttext=\"\\mathbf{W}_{K,h}^{(\\ell)}\\in\\mathbb{R}^{D\\times d_{k}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m13\" intent=\":literal\"><semantics><mrow><msubsup><mi>&#119830;</mi><mrow><mi>K</mi><mo>,</mo><mi>h</mi></mrow><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>D</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{W}_{K,h}^{(\\ell)}\\in\\mathbb{R}^{D\\times d_{k}}</annotation></semantics></math>.\nThese projections are used to compute head-specific attention scores, yielding one attention matrix per head and layer: <math alttext=\"\\{\\text{{\\color[rgb]{0.75390625,0.390625,0.9609375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.75390625,0.390625,0.9609375}$\\mathbf{CA}$}}_{h}^{(\\ell)}\\}_{\\ell=1,h=1}^{L,H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m14\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msubsup><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><mi>h</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">}</mo></mrow><mrow><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>h</mi><mo>=</mo><mn>1</mn></mrow></mrow><mrow><mi>L</mi><mo>,</mo><mi>H</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">\\{\\text{{\\color[rgb]{0.75390625,0.390625,0.9609375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.75390625,0.390625,0.9609375}$\\mathbf{CA}$}}_{h}^{(\\ell)}\\}_{\\ell=1,h=1}^{L,H}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "layer",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Extracting\nthe full set of scores provides a fine-grained view of how each output token in the generated hypothesis attends to the encoder&#8217;s representations across all layers and heads.\nTo derive a single layer-wise or head-wise attention distribution, we compute the mean of the attention matrices over a subset <math alttext=\"\\mathcal{S}\\subseteq\\{1,\\dots,L\\}\\times\\{1,\\dots,H\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>&#8838;</mo><mrow><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo rspace=\"0.055em\" stretchy=\"false\">}</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>H</mi><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}\\subseteq\\{1,\\dots,L\\}\\times\\{1,\\dots,H\\}</annotation></semantics></math> of layers and heads:</p>\n\n",
                "matched_terms": [
                    "layerwise",
                    "set",
                    "across",
                    "layers",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By selecting different index sets <math alttext=\"\\mathcal{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><annotation encoding=\"application/x-tex\">\\mathcal{S}</annotation></semantics></math>, this formulation yields layer-wise, head-wise, or global averages. For example, setting <math alttext=\"\\mathcal{S}=\\{(\\ell,h):h=1,\\dots,H\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m3\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo>,</mo><mi>h</mi><mo rspace=\"0.278em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.278em\">:</mo><mrow><mi>h</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>H</mi></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}=\\{(\\ell,h):h=1,\\dots,H\\}</annotation></semantics></math> gives the average across heads at a given layer <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>; <math alttext=\"\\mathcal{S}=\\{(\\ell,h):\\ell=1,\\dots,L\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m5\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo>,</mo><mi>h</mi><mo rspace=\"0.278em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.278em\">:</mo><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}=\\{(\\ell,h):\\ell=1,\\dots,L\\}</annotation></semantics></math> averages across layers for head <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m6\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math>; and <math alttext=\"\\mathcal{S}=\\{1,\\dots,L\\}\\times\\{1,\\dots,H\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m7\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>=</mo><mrow><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo rspace=\"0.055em\" stretchy=\"false\">}</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>H</mi><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}=\\{1,\\dots,L\\}\\times\\{1,\\dots,H\\}</annotation></semantics></math> computes the full average.\nThis averaged attention provides a more aggregated view of the model&#8217;s attention patterns at a specific layer or attention head, summarizing how the model attends to the input speech over time.</p>\n\n",
                "matched_terms": [
                    "time",
                    "layerwise",
                    "across",
                    "different",
                    "average",
                    "model",
                    "averaged",
                    "layer",
                    "input",
                    "‚Ñìell",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand how S2T models associate individual output tokens with specific regions of the raw speech input or of the model&#8217;s inner representations (e.g., the encoder output), we employ <span class=\"ltx_text ltx_font_italic\">feature-attribution</span> techniques that produce token-level <span class=\"ltx_text ltx_font_italic\">saliency maps</span>. These maps quantify the relevance of different portions of the input sequence in determining the model&#8217;s predictions.</p>\n\n",
                "matched_terms": [
                    "input",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Let again <math alttext=\"\\mathbf{X}\\in\\mathbb{R}^{T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}\\in\\mathbb{R}^{T\\times F}</annotation></semantics></math> denote a mel-spectrogram input, where <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> is the number of time frames and <math alttext=\"F\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mi>F</mi><annotation encoding=\"application/x-tex\">F</annotation></semantics></math> the number of frequency bins, and <math alttext=\"\\mathbf{y}=(y_{0},y_{1},\\ldots,y_{I})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#119858;</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mn>0</mn></msub><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>y</mi><mi>I</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{y}=(y_{0},y_{1},\\ldots,y_{I})</annotation></semantics></math> the sequence of length <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math> of the autoregressively-generated tokens predicted based on the input and the previously generated tokens <math alttext=\"y_{&lt;i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><msub><mi>y</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">y_{&lt;i}</annotation></semantics></math>.\nTo attribute the prediction of each token <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math> to specific parts of the input spectrogram, we adopt SPES&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>,\nthe state-of-the-art feature-attribution method designed for autoregressive S2T modeling. SPES assigns a saliency score to each time-frequency\nelement\nof <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><mi>&#119831;</mi><annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation></semantics></math>, producing a saliency map <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{X}\\in\\mathbb{R}^{T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m9\" intent=\":literal\"><semantics><mrow><msubsup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>i</mi><mi>X</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{X}\\in\\mathbb{R}^{T\\times F}</annotation></semantics></math> for each token <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m10\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math>, where higher values indicate greater relevance of the corresponding time-frequency regions. SPES operates by clustering spectrogram elements based on energy profiles&#8211;capturing acoustic\ncomponents such as harmonics and background noise&#8211;and estimating the influence of each cluster by perturbing it with probability <math alttext=\"p_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m11\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">p_{X}</annotation></semantics></math>, repeated <math alttext=\"N_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m12\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">N_{X}</annotation></semantics></math> times. The effect of each perturbation&#8211;i.e., masking parts of the input with 0 values&#8211;is measured by computing the Kullback-Leibler (KL) divergence <cite class=\"ltx_cite ltx_citemacro_citep\">(Kullback &amp; Leibler, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib44\" title=\"\">1951</a>)</cite> between the model&#8217;s original output distribution <math alttext=\"P(y_{i}\\mid y_{&lt;i},\\mathbf{X})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m13\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>&#8739;</mo><mrow><msub><mi>y</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><mo>,</mo><mi>&#119831;</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(y_{i}\\mid y_{&lt;i},\\mathbf{X})</annotation></semantics></math> and the distribution resulting from the perturbed input <math alttext=\"P^{(n)}(y_{i}\\mid y_{&lt;i},\\tilde{\\mathbf{X}}^{(n)})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m14\" intent=\":literal\"><semantics><mrow><msup><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>&#8739;</mo><mrow><msub><mi>y</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><mo>,</mo><msup><mover accent=\"true\"><mi>&#119831;</mi><mo>~</mo></mover><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P^{(n)}(y_{i}\\mid y_{&lt;i},\\tilde{\\mathbf{X}}^{(n)})</annotation></semantics></math> at time <math alttext=\"n\\in\\{1,\\ldots,N_{X}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m15\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>N</mi><mi>X</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">n\\in\\{1,\\ldots,N_{X}\\}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "time",
                    "frequency",
                    "input",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The divergence scores are then mapped back to the corresponding cluster positions in the spectrogram and aggregated to form the token-specific saliency map <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{X}\\in\\mathbb{R}^{T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m16\" intent=\":literal\"><semantics><mrow><msubsup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>i</mi><mi>X</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{X}\\in\\mathbb{R}^{T\\times F}</annotation></semantics></math>. Stacking all saliency maps across the output sequence <math alttext=\"\\mathbf{y}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m17\" intent=\":literal\"><semantics><mi>&#119858;</mi><annotation encoding=\"application/x-tex\">\\mathbf{y}</annotation></semantics></math> yields a 3D saliency map:</p>\n\n",
                "matched_terms": [
                    "across",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where each slice <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{X}[t,f]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m18\" intent=\":literal\"><semantics><mrow><msubsup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>i</mi><mi>X</mi></msubsup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">[</mo><mi>t</mi><mo>,</mo><mi>f</mi><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{X}[t,f]</annotation></semantics></math> quantifies the contribution of the spectrogram bin at time <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m19\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> and frequency <math alttext=\"f\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m20\" intent=\":literal\"><semantics><mi>f</mi><annotation encoding=\"application/x-tex\">f</annotation></semantics></math> to the generation of token <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m21\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "time",
                    "frequency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further examine the influence of the encoder&#8217;s internal representations on the prediction of each output token.\nLet again <math alttext=\"\\mathbf{H}=\\mathrm{Encoder}(\\mathbf{X})\\in\\mathbb{R}^{T^{\\prime}\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119815;</mi><mo>=</mo><mrow><mi>Encoder</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119831;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{H}=\\mathrm{Encoder}(\\mathbf{X})\\in\\mathbb{R}^{T^{\\prime}\\times D}</annotation></semantics></math> denote the sequence of encoder hidden states or <span class=\"ltx_text ltx_font_italic\">encoder output</span>, where <math alttext=\"T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><msup><mi>T</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">T^{\\prime}</annotation></semantics></math> is the subsampled time dimension and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> is the hidden dimension.\nTo assess the importance of the encoder output representations, we compute token-specific saliency maps <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{H}\\in\\mathbb{R}^{T^{\\prime}\\times 1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m4\" intent=\":literal\"><semantics><mrow><msubsup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>i</mi><mi>H</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{H}\\in\\mathbb{R}^{T^{\\prime}\\times 1}</annotation></semantics></math>, where each entry reflects the contribution of the corresponding hidden state to the generation of <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m5\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math>.\nEach encoder state <math alttext=\"\\mathbf{H}=(\\mathbf{h}_{1},\\ldots,\\mathbf{h}_{T^{\\prime}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#119815;</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119841;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119841;</mi><msup><mi>T</mi><mo>&#8242;</mo></msup></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{H}=(\\mathbf{h}_{1},\\ldots,\\mathbf{h}_{T^{\\prime}})</annotation></semantics></math> is perturbed&#8211;i.e., all its features are set to 0&#8211;independently with probability <math alttext=\"p_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m7\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">p_{H}</annotation></semantics></math>, and the process is repeated <math alttext=\"N_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m8\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">N_{H}</annotation></semantics></math> times. The KL divergence is computed for each perturbation between the original and perturbed output distributions:</p>\n\n",
                "matched_terms": [
                    "time",
                    "computed",
                    "set",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The divergence scores are aggregated across perturbation trials to form the final saliency map, following the same strategy of SPES for the input-level saliency maps:</p>\n\n",
                "matched_terms": [
                    "across",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since our focus lies in the temporal dynamics of the input <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mi>&#119831;</mi><annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation></semantics></math>, we aggregate the 3D saliency scores <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\\in\\mathbb{R}^{I\\times T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\\in\\mathbb{R}^{I\\times T\\times F}</annotation></semantics></math> across the frequency dimension and downsample the time axis to produce a compressed representation <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\\in\\mathbb{R}^{I\\times T^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>T</mi><mo>&#8242;</mo></msup></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\\in\\mathbb{R}^{I\\times T^{\\prime}}</annotation></semantics></math> compatible with the cross-attention granularity, where <math alttext=\"T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m4\" intent=\":literal\"><semantics><msup><mi>T</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">T^{\\prime}</annotation></semantics></math> corresponds to the number of encoder time steps. The aggregation is performed by taking the maximum saliency value over the frequency axis and within each corresponding time window. The resulting saliency map of each token <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{X}\\in\\mathbb{R}^{T^{\\prime}\\times 1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><msubsup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>i</mi><mi>X</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{X}\\in\\mathbb{R}^{T^{\\prime}\\times 1}</annotation></semantics></math> reflects the temporal relevance of the input spectrogram with respect to the generation of token <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m6\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math>. Complementary experiments on the choice of the aggregation function are presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A1\" title=\"Appendix A Effect of Aggregation Functions on Input Explanations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.\nBoth <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m7\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> and <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m8\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math> representations are normalized before computing the correlation scores, and the beginning and end of sentence are removed as they are not relevant for the analysis.\nThe <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m9\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> matrix is normalized frame-wise using mean-variance normalization to mitigate the impact of potential attention sinks at initial or final tokens&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib13\" title=\"\">2019</a>; Ferrando et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib19\" title=\"\">2022a</a>; Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib65\" title=\"\">2023a</a>; Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib96\" title=\"\">2024</a>)</cite> on the correlation computation.\nBoth <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m10\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math> and <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m11\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>H</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}</annotation></semantics></math> are normalized along the token dimension using the strategy proposed by <cite class=\"ltx_cite ltx_citemacro_citet\">Fucci et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>, as saliency scores can vary widely across tokens due to differences in the original output distributions used to compute the KL divergence.</p>\n\n",
                "matched_terms": [
                    "time",
                    "across",
                    "correlation",
                    "aggregation",
                    "frequency",
                    "input",
                    "crossattention",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following prior work on cross-attention matrices <cite class=\"ltx_cite ltx_citemacro_citep\">(Vig &amp; Belinkov, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib86\" title=\"\">2019</a>)</cite> and explainable AI <cite class=\"ltx_cite ltx_citemacro_citep\">(Eberle et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib18\" title=\"\">2023</a>)</cite>, we use Pearson correlation\nto quantify the relationship between cross-attention scores and saliency-based explanations.\nPearson correlation is preferred over Kendall and Spearman because saliency scores are continuous, and their magnitude&#8211;not just ranking&#8211;is crucial. Rank-based measures are overly sensitive to small fluctuations among non-important features with near-zero scores, while Pearson better captures whether features are identified as important (high score) or not (low score).\nSpecifically, given the two representations <math alttext=\"\\text{{\\color[rgb]{0.75390625,0.390625,0.9609375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.75390625,0.390625,0.9609375}$\\mathbf{CA}$}},\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}\\in\\mathbb{R}^{I\\times T^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><mo>,</mo><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>T</mi><mo>&#8242;</mo></msup></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.75390625,0.390625,0.9609375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.75390625,0.390625,0.9609375}$\\mathbf{CA}$}},\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}\\in\\mathbb{R}^{I\\times T^{\\prime}}</annotation></semantics></math>, we compute the Pearson correlation coefficient <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math> to assess the similarity of their attribution patterns across output tokens and time steps. We first flatten each matrix into a vector of size <math alttext=\"I\\cdot T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8901;</mo><msup><mi>T</mi><mo>&#8242;</mo></msup></mrow><annotation encoding=\"application/x-tex\">I\\cdot T^{\\prime}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "time",
                    "across",
                    "correlation",
                    "œÅrho",
                    "pearson",
                    "explanations",
                    "between",
                    "crossattention",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Then, the Pearson correlation coefficient <math alttext=\"\\rho\\in[-1,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><mi>&#961;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\rho\\in[-1,1]</annotation></semantics></math> is computed as:</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "computed",
                    "pearson"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This scalar value quantifies the linear relationship between the two saliency maps, with values closer to 1 indicating a strong positive correlation, and values near 0 indicating no correlation.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To avoid potential data contamination issues <cite class=\"ltx_cite ltx_citemacro_citep\">(Sainz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib76\" title=\"\">2023</a>)</cite>, we train from scratch a monolingual ASR model and two-sized multitask (ASR and ST) and multilingual (English and Italian) models. Details about training data and process are presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A2\" title=\"Appendix B Training Settings &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.\nBeing the only non-synthetic dataset supporting both tasks and language directions, we select EuroParl-ST <cite class=\"ltx_cite ltx_citemacro_citep\">(Iranzo-S&#225;nchez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib33\" title=\"\">2020</a>)</cite> as the test set for our analyses. The test set covers both <span class=\"ltx_text ltx_font_italic\">en</span> and <span class=\"ltx_text ltx_font_italic\">it</span> ASR, and <span class=\"ltx_text ltx_font_italic\">en-it</span> and <span class=\"ltx_text ltx_font_italic\">it-en</span> ST.\nThe <span class=\"ltx_text ltx_font_italic\">it/it-en</span> section consists of 1,686 segments, for a total of approximately 6 hours of audio, while the <span class=\"ltx_text ltx_font_italic\">en/en-it</span> section contains 1,130 segments, for a total of approximately 3 hours of audio.</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "set",
                    "english",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The models analyzed in the paper are all composed of a Conformer encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib29\" title=\"\">2020</a>)</cite> and a Transformer decoder, as Conformer is the current state-of-the-art architecture for S2T processing <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib30\" title=\"\">2021</a>; Srivastava et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib81\" title=\"\">2022</a>; Li &amp; Doddipatla, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib48\" title=\"\">2023</a>)</cite>.\nThe monolingual ASR model (<span class=\"ltx_text ltx_font_typewriter\">base</span>) is composed of 12 encoder layers and 6 decoder layers. Each layer has 8 attention heads, 512 as embedding dimension, and FFNs dimension of 2,048. The vocabulary is built using a SentencePiece unigram model <cite class=\"ltx_cite ltx_citemacro_citep\">(Kudo &amp; Richardson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib42\" title=\"\">2018</a>)</cite> with size 8,000 trained on <span class=\"ltx_text ltx_font_italic\">en</span> transcripts. The resulting number of parameters is 125M.\nThe multitask and multilingual models are of two sizes, <span class=\"ltx_text ltx_font_typewriter\">small</span> and <span class=\"ltx_text ltx_font_typewriter\">large</span>, the first having 12 encoder layers and 6 decoder layers and the latter having 24 encoder layers and 12 decoder layers. In both sizes, each layer has 16 attention heads, an embedding dimension of 1,024, and an FFN dimension of 4,096. The vocabulary is built using a SentencePiece unigram model with size 16,000 trained on <span class=\"ltx_text ltx_font_italic\">en</span> and <span class=\"ltx_text ltx_font_italic\">it</span> transcripts. Two extra tokens&#8211;<span class=\"ltx_text ltx_font_typewriter\">&lt;lang:en&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;lang:it&gt;</span>&#8211;are added to indicate whether the target text is in <span class=\"ltx_text ltx_font_italic\">en</span> or <span class=\"ltx_text ltx_font_italic\">it</span>. The resulting number of parameters is 474M for the <span class=\"ltx_text ltx_font_typewriter\">small</span> model and 878M for the <span class=\"ltx_text ltx_font_typewriter\">large</span> model.\nIn all models, the Conformer encoder is preceded by two 1D convolutional layers with stride 2 and kernel size 5, resulting in a fixed subsampling factor <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> of 4. The kernel size of the Conformer convolutional module is 31 for both the point- and depth-wise convolutions. The input audio is represented by 80 Mel-filterbank features extracted every 10 ms with a window of 25 ms.</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "base",
                    "model",
                    "layer",
                    "asr",
                    "input",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Hypothesis and Cross-Attention Generation.</span>\nFor the hypothesis generation, we use beam search with a beam size of 5 and a no-repeat n-gram size of 5. The attention scores are extracted from layers or heads during the output generation. The ASR and ST quality scores of the hypotheses are presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A3\" title=\"Appendix C Quality Metrics for the Reported Models &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nThe inference is performed using a single NVIDIA A40 GPU (40GB RAM) with a batch size of 40,000 tokens and takes <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>2.5 minutes for <span class=\"ltx_text ltx_font_typewriter\">base</span>, <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>3-5.5 minutes for <span class=\"ltx_text ltx_font_typewriter\">small</span>, and <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>3-6.5 for <span class=\"ltx_text ltx_font_typewriter\">large</span>, depending on the source language.</p>\n\n",
                "matched_terms": [
                    "base",
                    "layers",
                    "asr",
                    "crossattention",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Explanation Heatmaps Generation.</span>\nFollowing the best configuration obtained in SPES <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>, we adopt the Morphological Fragmental Perturbation Pyramid <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib99\" title=\"\">2021</a>)</cite> for clustering, which relies on Simple Linear Iterative Clustering or SLIC <cite class=\"ltx_cite ltx_citemacro_citep\">(Achanta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib3\" title=\"\">2012</a>)</cite>, a k-means-based algorithm that groups elements according to spectral patterns. We use the default parameters; the threshold length in seconds is\n7.50s, the\n<a class=\"ltx_ref ltx_href\" href=\"https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.slic\" title=\"\">SLIC</a>\nsigma is 0, the compactness is 0.1, and the number of patches per second for the MFPP technique is [400, 500, 600].\nFor the choice of <math alttext=\"p_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">p_{X}</annotation></semantics></math> and <math alttext=\"N_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">N_{X}</annotation></semantics></math>, we refer to the parameters used in <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>, setting <math alttext=\"p_{X}=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>X</mi></msub><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">p_{X}=0.5</annotation></semantics></math> and <math alttext=\"N_{X}=20,000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><msub><mi>N</mi><mi>X</mi></msub><mo>=</mo><mrow><mn>20</mn><mo>,</mo><mn>000</mn></mrow></mrow><annotation encoding=\"application/x-tex\">N_{X}=20,000</annotation></semantics></math>. The quality of the input explanations is presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A3\" title=\"Appendix C Quality Metrics for the Reported Models &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>. For the choice of <math alttext=\"p_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m5\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">p_{H}</annotation></semantics></math> and <math alttext=\"N_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m6\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">N_{H}</annotation></semantics></math>, we use the same number of iterations of <math alttext=\"N_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m7\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">N_{X}</annotation></semantics></math>, i.e., <math alttext=\"N_{H}=20,000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m8\" intent=\":literal\"><semantics><mrow><msub><mi>N</mi><mi>H</mi></msub><mo>=</mo><mrow><mn>20</mn><mo>,</mo><mn>000</mn></mrow></mrow><annotation encoding=\"application/x-tex\">N_{H}=20,000</annotation></semantics></math>, while the optimal occlusion probability <math alttext=\"p_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m9\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">p_{H}</annotation></semantics></math> is determined over the dev set, resulting in <math alttext=\"p_{H}=0.7\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m10\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>H</mi></msub><mo>=</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">p_{H}=0.7</annotation></semantics></math>, whose experiments are reported in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A4\" title=\"Appendix D Effect of Occlusion Probability on Encoder Output Explanations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.\nThe inference is performed using a single NVIDIA A40 GPU (40GB RAM)\nand takes <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m11\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>27 hours for <span class=\"ltx_text ltx_font_typewriter\">base</span>, <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m12\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>3-4 days for <span class=\"ltx_text ltx_font_typewriter\">small</span> and <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m13\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>6-8 days for <span class=\"ltx_text ltx_font_typewriter\">large</span>, depending on the source language.</p>\n\n",
                "matched_terms": [
                    "set",
                    "base",
                    "dev",
                    "explanations",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Correlation Computation.</span>\nThe Pearson <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">r</annotation></semantics></math> correlation score is computed using the <span class=\"ltx_text ltx_font_typewriter\">scipy</span> implementation<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html\" title=\"\">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html</a></span></span></span> and averaging across samples in the test set.</p>\n\n",
                "matched_terms": [
                    "set",
                    "across",
                    "correlation",
                    "pearson",
                    "computed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we compare <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> with input saliency maps <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m3\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></sup>, which serve as an external reference for measuring input relevance.\nSpecifically, in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS1\" title=\"5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a>, we analyze the\n<span class=\"ltx_text ltx_font_typewriter\">base</span> model across all levels of granularity. Then, in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS2\" title=\"5.1.2 Multitask and Multilingual Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.2</span></a>, we extend the analysis to additional models (<span class=\"ltx_text ltx_font_typewriter\">small</span> and <span class=\"ltx_text ltx_font_typewriter\">large</span>), languages (<span class=\"ltx_text ltx_font_italic\">en</span> and <span class=\"ltx_text ltx_font_italic\">it</span>), and tasks (ASR and ST).</p>\n\n",
                "matched_terms": [
                    "ùêíùêåmathbfsmxx",
                    "across",
                    "base",
                    "model",
                    "asr",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.T1\" title=\"Table 1 &#8227; 5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reports the correlation scores for the monolingual English ASR model (<span class=\"ltx_text ltx_font_typewriter\">base</span>), considering cross-attention at the head level, layer level, and in aggregated form.</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "correlation",
                    "english",
                    "base",
                    "model",
                    "layer",
                    "asr",
                    "crossattention",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At the individual head level, correlations with saliency maps are generally low. This suggests that attention heads, when taken in isolation, only partially capture the model&#8217;s dependency on the input and often encode noisy or inconsistent relevance signals. However, not all heads are equal: some, especially in the upper layers (layers 4-6), exhibit relatively stronger correlations. Notably, <span class=\"ltx_text ltx_font_bold\">averaging across heads consistently outperforms selecting individual heads</span>, suggesting that, despite head-level sparsity and weak individual correlations, the collective information captured across heads reflects input relevance more effectively.\nMoving from heads to layers, we find a clearer picture. Averaging attention scores across all heads within each layer boosts correlation substantially, with layer 6 standing out as the most aligned with the saliency maps. This is followed closely by layer 5 and the average across all layers, indicating that the <span class=\"ltx_text ltx_font_bold\">last layers exhibit the highest alignment with input relevance</span>. These results reinforce the idea that deeper layers encode higher-level semantic or task-relevant features, a trend previously observed in Transformer-based models <cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib13\" title=\"\">2019</a>)</cite>.\nInterestingly, while averaging across heads improves alignment, averaging across both heads and layers does not yield the overall best result, even if values are close. This indicates that not all layers contribute equally and that indiscriminate aggregation can dilute the relevance signal.</p>\n\n",
                "matched_terms": [
                    "across",
                    "correlation",
                    "aggregation",
                    "result",
                    "average",
                    "highest",
                    "layer",
                    "indicates",
                    "input",
                    "layers",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, the results show that appropriately selected and aggregated cross-attention scores exhibit only a <span class=\"ltx_text ltx_font_italic\">moderate</span> to <span class=\"ltx_text ltx_font_italic\">strong</span> correlation with input saliency maps, reaching values up to 0.588. This provides an initial indication of the\nlimited explanatory power\nof cross-attention weights, which we further examine under multilingual and multitask conditions in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS2\" title=\"5.1.2 Multitask and Multilingual Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "input",
                    "correlation",
                    "crossattention",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the impact of multilingual and multitask training on the correlation between cross-attention scores and saliency maps, we evaluate the <span class=\"ltx_text ltx_font_typewriter\">small</span> and <span class=\"ltx_text ltx_font_typewriter\">large</span> models. Layer-wise results are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.T2\" title=\"Table 2 &#8227; 5.1.2 Multitask and Multilingual Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, while head-wise results are omitted due to the noisy behavior observed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS1\" title=\"5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "layerwise",
                    "correlation",
                    "between",
                    "crossattention",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across all configurations, we observe that <span class=\"ltx_text ltx_font_italic\">en</span> ASR yields the highest correlation values, outperforming even the monolingual <span class=\"ltx_text ltx_font_typewriter\">base</span> model (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS1\" title=\"5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a>). This suggests that large-scale multilingual training enhances the alignment between cross-attention and saliency maps, likely due to the improved generalization capacity of the model. In contrast, <span class=\"ltx_text ltx_font_italic\">en-it</span> ST shows a drop in correlation, which is expected given the increased complexity of ST compared to ASR.\nWhen considering <span class=\"ltx_text ltx_font_italic\">it</span> as the source language, we observe a similar pattern: ASR correlations are consistently higher than ST, yet remain below their <span class=\"ltx_text ltx_font_italic\">en</span> counterparts. This discrepancy aligns with the data distribution in training, where <span class=\"ltx_text ltx_font_italic\">en</span> accounts for 84% of the data versus 16% for <span class=\"ltx_text ltx_font_italic\">it</span>, resulting in more robust representations for <span class=\"ltx_text ltx_font_italic\">en</span>.\nAt the layer level, we find consistent evidence that the last decoder layers yield stronger correlations, reaffirming the trends observed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS1\" title=\"5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a>.\nThe specific optimal layer varies with model size: layer 5 performs best in <span class=\"ltx_text ltx_font_typewriter\">small</span>, while layers 8-10 achieve the highest correlations in <span class=\"ltx_text ltx_font_typewriter\">large</span>.\nNevertheless, correlation values across the last\nlayers remain very close, suggesting that their cross-attention scores\nprovide the most robust alignment with saliency maps across both tasks and languages. This trend is further supported by downstream application results, where the final layers have shown the best token-level performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib65\" title=\"\">2023a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib66\" title=\"\">b</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib91\" title=\"\">2024a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "across",
                    "correlation",
                    "base",
                    "model",
                    "highest",
                    "layer",
                    "layers",
                    "asr",
                    "between",
                    "crossattention",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Averaging attention scores across layers further improves the correlation with saliency maps in\nalmost\nall configurations. The only exceptions are <span class=\"ltx_text ltx_font_italic\">en</span> and <span class=\"ltx_text ltx_font_italic\">it</span> ASR in <span class=\"ltx_text ltx_font_typewriter\">small</span>, where selective-layer extraction offers a marginal improvement (0.006 for English, 0.001 for Italian).\nTherefore, similarly to what we observed in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS1\" title=\"5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a>, averaging attention across heads and layers consistently yields the best or near-best correlation with <span class=\"ltx_text ltx_font_italic\">moderate</span> to <span class=\"ltx_text ltx_font_italic\">strong</span> correlation with input saliency maps, even considering large-scale models trained in multitask and multilingual settings.\nNonetheless, this alignment accounts for only 49-63% of the total input relevance, indicating that <span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#88185B;\">cross-attention falls short of fully accounting for the S2T models&#8217; behavior</span>.\nSince this limitation may stem from the phenomenon of context mixing, in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS2\" title=\"5.2 What Is the Impact of Context Mixing? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a> we analyze the correlation between cross-attention and encoder output&#8211;representations that have already undergone transformation by the encoder&#8211;to better isolate the true explanatory power of cross-attention.</p>\n\n",
                "matched_terms": [
                    "across",
                    "correlation",
                    "english",
                    "layers",
                    "asr",
                    "between",
                    "input",
                    "crossattention",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1\" title=\"5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> focused on input relevance, we now investigate whether <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math>\naligns more closely with encoder output saliency maps.\nA higher correlation with encoder output representations would support the hypothesis that discrepancies between cross-attention and input saliency arise from context mixing, due to the reorganization of information within the encoder.\nTo this end,\nwe compare <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> with encoder output saliency maps <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m4\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></sup>, which attribute relevance to the encoder hidden states for each output token (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S3.SS2\" title=\"3.2 Feature Attribution for Speech-to-Text &#8227; 3 Methodology &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>). Layer-wise results for all models are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.T3\" title=\"Table 3 &#8227; 5.2 What Is the Impact of Context Mixing? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "layerwise",
                    "correlation",
                    "between",
                    "input",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Even when examining encoder output representations, we observe trends consistent with those identified in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1\" title=\"5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>. Specifically, when averaged across decoder layers, cross-attention scores consistently provide the strongest or nearly optimal correlation with saliency maps, with the last decoder layers offering more representative explanations than the first ones across all models. As expected, correlation with encoder output representations consistently yields higher scores than those obtained from input representations, with absolute <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math> differences ranging from 0.03 to 0.18, quantifying the influence of context mixing effects to 6.6-16.7%.\nThe increased correlation is also visually evident in the example shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.F2\" title=\"Figure 2 &#8227; 5.2 What Is the Impact of Context Mixing? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, where <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> aligns more closely with the relevance scores from <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m4\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></sup> than with those from <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m5\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m6\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></sup>.\nHowever, despite being unaffected by context mixing, the correlation between <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m7\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> and <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m8\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m9\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></sup> remains limited&#8211;capturing only 52-75% of the relevance.\n<span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#88185B;\">This\ngap underscores the inherent limitations in relying solely on cross-attention as an explanation mechanism, reinforcing its role as an informative but <span class=\"ltx_text ltx_ulem_uline\">incomplete</span> proxy for explainability in S2T models</span>&#8211;not only for input-level saliency, but even at the encoder-output level, where cross-attention directly operates.</p>\n\n",
                "matched_terms": [
                    "ùêíùêåmathbfsmxx",
                    "across",
                    "correlation",
                    "œÅrho",
                    "averaged",
                    "layers",
                    "explanations",
                    "between",
                    "input",
                    "crossattention",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Discussion.</span>\nOur results demonstrate that, although <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> scores moderately correlate with aggregated <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m2\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math> (with a correlation peaking around 0.45-0.55 in the best-performing settings), they consistently fall short of capturing the full input relevance&#8211;even when context mixing effects are factored out. To directly assess explanation quality, we compute the deletion metric (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A1\" title=\"Appendix A Effect of Aggregation Functions on Input Explanations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>) on the <span class=\"ltx_text ltx_font_typewriter\">base</span> model, finding that <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> achieves 41.2, compared to 52.9 for frequency-aggregated <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m4\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math> and 91.3 for full-resolution maps.\nThis gap underscores that <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m5\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> discards fine-grained time-frequency cues and produces weaker attributions, even under identical aggregation. As further discussed in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A6\" title=\"Appendix F Limitations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>, our analysis is bounded by the use of SPES as the attribution baseline, but the consistent underperformance across correlation and deletion confirms that <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m6\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> offers, at best, an incomplete picture of model behavior. These results also carry implications for downstream tasks. In applications such as timestamp prediction, prior work often relies on attention from a single decoder layer or head <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib91\" title=\"\">2024a</a>; Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib65\" title=\"\">2023a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib66\" title=\"\">b</a>; Zusag et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib106\" title=\"\">2024</a>)</cite>. Our analysis suggests that averaging across layers and, especially, across heads provides a closer match to saliency behavior and could improve these methods.\nBuilding on past success with attention regularization in ASR (e.g., imposing monotonicity as in <cite class=\"ltx_cite ltx_citemacro_citet\">Zhao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib104\" title=\"\">2020</a></cite>), similar training-time strategies&#8211;such as auxiliary losses that align attention with saliency&#8211;could further benefit downstream applications, enhancing both interpretability and task performance.\nIn summary, <span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"--ltx-fg-color:#88185B;\"><math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m7\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> should not be treated as a stand-alone XAI tool</span>. It provides lightweight cues that may complement attribution-based methods, but it cannot replace them. Reframing <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m8\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> as an auxiliary rather than a proxy recalibrates expectations and grounds future work on more faithful and effective approaches to explainability in S2T models.</p>\n\n",
                "matched_terms": [
                    "across",
                    "correlation",
                    "aggregation",
                    "base",
                    "deletion",
                    "model",
                    "layer",
                    "asr",
                    "input",
                    "layers",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conclusions.</span>\nWe presented the first systematic analysis of cross-attention in S2T through the lens of explainable AI, comparing it to saliency maps across tasks, languages, and model scales. Cross-attention moderately to strongly aligns with saliency&#8211;especially when averaged across heads and layers&#8211;but captures only about half of the input relevance. Even when disentangling the effect of context mixing by analyzing encoder outputs, it explains just 52-75% of saliency. This gap reveals intrinsic limits of cross-attention as an explanation mechanism: it offers informative cues but only a partial view of the factors driving S2T predictions.</p>\n\n",
                "matched_terms": [
                    "across",
                    "model",
                    "averaged",
                    "input",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Broader Implications.</span> Explainability in S2T systems has tangible implications for AI transparency, especially in high-stakes settings such as healthcare, legal transcription, and educational accessibility. Our findings provide insights about the usage of cross-attention as a tool for identifying how models relate output predictions to input regions, which can support auditing, debugging, and fair deployment. However, there is a risk that misinterpreted attention visualizations may be overtrusted by non-expert users, reinforcing false confidence in system behavior <cite class=\"ltx_cite ltx_citemacro_citep\">(Rudin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib75\" title=\"\">2019</a>)</cite>. Moreover, our language choices and focus on high-resource speech still reflect global imbalances in language technology access <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib36\" title=\"\">2020</a>)</cite>. Future work should extend this analysis to low-resource and underrepresented languages to promote broader inclusion.</p>\n\n",
                "matched_terms": [
                    "input",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the reproducibility of our results, we described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S4\" title=\"4 Experimental Settings &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> all the details regarding our model training, training and evaluation data, and evaluation procedure. Moreover, we relied only on openly available data and on open source code<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/hlt-mt/FBK-fairseq/blob/master/fbk_works/XAI_FEATURE_ATTRIBUTION.md\" title=\"\">https://github.com/hlt-mt/FBK-fairseq/blob/master/fbk_works/XAI_FEATURE_ATTRIBUTION.md</a></span></span></span> for the generation of the saliency maps. Lastly, all models (described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S4\" title=\"4 Experimental Settings &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), code, attention scores, and explanation artifacts will be released under the Apache 2.0 (code) and CC-BY 4.0 (all other materials) licenses upon paper acceptance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To properly obtain input-level explanations comparable with the dimensions of cross-attention scores (i.e., making <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\\in\\mathbb{R}^{I\\times T^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m1\" intent=\":literal\"><semantics><mrow><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>T</mi><mo>&#8242;</mo></msup></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\\in\\mathbb{R}^{I\\times T^{\\prime}}</annotation></semantics></math>), we explore the effect of different aggregation strategies over the time and frequency dimensions.</p>\n\n",
                "matched_terms": [
                    "time",
                    "aggregation",
                    "frequency",
                    "explanations",
                    "different",
                    "crossattention",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To compare and select the best aggregation strategy, we adopt the <span class=\"ltx_text ltx_font_italic\">deletion</span> metric&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Nauta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib61\" title=\"\">2023</a>)</cite>, which quantifies the decline in prediction quality as the most relevant input frames&#8211;identified by the explanation&#8211;are progressively removed. Specifically, we adapt the implementation by <cite class=\"ltx_cite ltx_citemacro_citet\">Fucci et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite> for S2T tasks, replacing the top-ranked time frames in the input spectrogram <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m1\" intent=\":literal\"><semantics><mi>&#119831;</mi><annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation></semantics></math> with zero vectors in 5% increments, based on the aggregated saliency map <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m2\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math>.\nSince <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m3\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math> operates on an aggregated time dimension <math alttext=\"T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m4\" intent=\":literal\"><semantics><msup><mi>T</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">T^{\\prime}</annotation></semantics></math>, which is smaller than the original time dimension <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m5\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> of <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m6\" intent=\":literal\"><semantics><mi>&#119831;</mi><annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation></semantics></math>, we upsample <math alttext=\"T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m7\" intent=\":literal\"><semantics><msup><mi>T</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">T^{\\prime}</annotation></semantics></math> to match <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m8\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> using nearest-neighbor interpolation.\nPrediction quality is measured using the word error rate (WER), specifically the <span class=\"ltx_text ltx_font_typewriter\">wer_max</span> scorer from the SPES repository.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/hlt-mt/FBK-fairseq\" title=\"\">https://github.com/hlt-mt/FBK-fairseq</a></span></span></span> Lastly, we compute the area under the WER curve to quantify the faithfulness of each explanation method.</p>\n\n",
                "matched_terms": [
                    "time",
                    "deletion",
                    "input",
                    "aggregation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2D average pooling</span>, applied over the entire time-frequency plane to obtain its <span class=\"ltx_text ltx_font_italic\">average</span> value and computed through <span class=\"ltx_text ltx_font_typewriter\">adaptive_avg_pool2d<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">5</span></span><a class=\"ltx_ref ltx_url\" href=\"https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.adaptive_avg_pool2d.html\" title=\"\">https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.adaptive_avg_pool2d.html</a></span></span></span></span>;</p>\n\n",
                "matched_terms": [
                    "computed",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2-step pooling (1D maximum + 1D average)</span>, where max pooling is applied along the frequency axis, followed by averaging over time, and computed by applying <span class=\"ltx_text ltx_font_typewriter\">max_pool1d<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">6</span></span><a class=\"ltx_ref ltx_url\" href=\"https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.max_pool1d.html\" title=\"\">https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.max_pool1d.html</a></span></span></span></span> and <span class=\"ltx_text ltx_font_typewriter\">avg_pool1d</span>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.avg_pool1d.html\" title=\"\">https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.avg_pool1d.html</a></span></span></span> respectively;</p>\n\n",
                "matched_terms": [
                    "time",
                    "average",
                    "max",
                    "frequency",
                    "computed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The aggregation functions were selected to contrast methods that either isolate the most relevant features (with maximum pooling) or represent their mean relevance (with average pooling). Similarly, the 2-step approach has been tried to first isolate relevance patterns in the frequency domain, a dimension that is not present in cross-attention representation, and then average across the time dimension to match the downsampled time resolution of the cross-attention scores.</p>\n\n",
                "matched_terms": [
                    "time",
                    "across",
                    "aggregation",
                    "average",
                    "functions",
                    "frequency",
                    "crossattention",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Among the tested methods, we observe that the 2D maximum pooling aggregation (2D max) yields the best quality explanations, obtaining the highest deletion score, while the 2D average pooling (2D avg) is the worst, with the lowest deletion score. Looking at the correlations, we notice that they follow the same trend of deletion scores, with the 2D max yielding the best <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m1\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math>. In particular, 2D avg consistently has the lowest correlations compared to the 2D max, particularly in the last layers (e.g., 0.457 against 0.572 at layer 5). Regarding the 2-step pooling approach, we not only observe an improved deletion score but also better correlation scores compared to 2D avg, especially from layer 3 onward, approaching the best performance with a layer-average correlation of 0.565. Nevertheless, the explanation quality is still lower compared to 2D max (i.e., 55.18 against 57.04), which also achieves the highest correlations at nearly every layer, peaking at 0.582 in layer 6, and yielding the best overall correlation among the averaged cross-attention across layers (i.e., 0.572).</p>\n\n",
                "matched_terms": [
                    "across",
                    "avg",
                    "correlation",
                    "œÅrho",
                    "aggregation",
                    "average",
                    "deletion",
                    "averaged",
                    "max",
                    "highest",
                    "layer",
                    "layers",
                    "explanations",
                    "crossattention",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These results indicate that global averaging over time and frequency may obscure localized salient regions, and this is particularly impactful in the frequency dimension, where preserving saliency seems to play a crucial role.\nThis is due to the fact that key elements in the saliency maps are often well localized along the frequency axis. As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A1.F3\" title=\"Figure 3 &#8227; Appendix A Effect of Aggregation Functions on Input Explanations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, for all tokens\nsaliency consistently concentrates in specific frequency bands. These bands are typically below 2000 Hz, where many resonant frequencies for speech are found <cite class=\"ltx_cite ltx_citemacro_citep\">(Stevens, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib82\" title=\"\">2000</a>)</cite>. As a result, smoothing operations such as 2D average pooling&#8211;or, to a lesser extent, the 2-step approach&#8211;tend to blur these concentrated regions, thereby diluting the saliency. This observation motivates our choice to adopt 2D max pooling in the main experiments.</p>\n\n",
                "matched_terms": [
                    "time",
                    "result",
                    "average",
                    "max",
                    "frequency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the monolingual ASR model, we leverage the speech-to-text English data available for the IWSLT 2024 evaluation campaign (offline task),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://iwslt.org/2024/offline\" title=\"\">https://iwslt.org/2024/offline</a></span></span></span> namely: CommonVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Ardila et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib4\" title=\"\">2020</a>)</cite>, CoVoST v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib90\" title=\"\">2021b</a>)</cite>, Europarl-ST <cite class=\"ltx_cite ltx_citemacro_citep\">(Iranzo-S&#225;nchez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib33\" title=\"\">2020</a>)</cite>, LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib64\" title=\"\">2015</a>)</cite>, MuST-C v1 <cite class=\"ltx_cite ltx_citemacro_citep\">(Di&#160;Gangi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib17\" title=\"\">2019</a>)</cite>, TEDLIUM v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Hernandez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib31\" title=\"\">2018</a>)</cite>, and VoxPopuli ASR <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib89\" title=\"\">2021a</a>)</cite>. The resulting training set is about 3k hours of speech.</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "set",
                    "english",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the multitask (ASR and ST) multilingual large-scale models, we leverage more than 150k hours of open-source speech<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>Speech, transcripts, and translations released under an open-source license such as CC-0 and CC-BY 4.0.</span></span></span> in English (<span class=\"ltx_text ltx_font_italic\">en</span>) and Italian (<span class=\"ltx_text ltx_font_italic\">it</span>), namely: CommonVoice, CoVoST v2, FLEURS <cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib14\" title=\"\">2023</a>)</cite>, MOSEL <cite class=\"ltx_cite ltx_citemacro_citep\">(Gaido et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib23\" title=\"\">2024a</a>)</cite>, MLS <cite class=\"ltx_cite ltx_citemacro_citep\">(Pratap et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib71\" title=\"\">2020</a>)</cite>, and YouTube-Commons<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hf.co/datasets/PleIAs/YouTube-Commons\" title=\"\">https://hf.co/datasets/PleIAs/YouTube-Commons</a></span></span></span> (from which 14.2k hours of <span class=\"ltx_text ltx_font_italic\">en</span> and 1.8k for <span class=\"ltx_text ltx_font_italic\">it</span> have been extracted). For datasets missing the translations, we generated them using <span class=\"ltx_text ltx_font_typewriter\">MADLAD-400 3B-MT</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Kudugunta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib43\" title=\"\">2023</a>)</cite>. This setting allows us to verify our analysis with a large-scale setting similar to the scale of a popular model such as OWSM <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib69\" title=\"\">2023</a>)</cite> and 2 times that of NVIDIA Canary <cite class=\"ltx_cite ltx_citemacro_citep\">(Puvvada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib72\" title=\"\">2024</a>)</cite> while having complete control of data used during training, ensuring that data contamination issues are avoided completely.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "model",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train all models using a combination of three losses: <em class=\"ltx_emph ltx_font_italic\">i)</em> a label-smoothed cross-entropy loss (<math alttext=\"\\mathcal{L}_{\\text{CE}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>CE</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{CE}}</annotation></semantics></math>) applied to the decoder output using the target text as the reference (transcripts for ASR and translations for ST), <em class=\"ltx_emph ltx_font_italic\">ii)</em> a CTC loss <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib27\" title=\"\">2006</a>)</cite> computed using transcripts as reference (<math alttext=\"\\mathcal{L}_{\\text{CTCsrc}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>CTCsrc</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{CTCsrc}}</annotation></semantics></math>) on the output of the 8<sup class=\"ltx_sup\">th</sup> encoder layer for <span class=\"ltx_text ltx_font_typewriter\">base</span> and <span class=\"ltx_text ltx_font_typewriter\">small</span> and the 16<sup class=\"ltx_sup\">th</sup> for <span class=\"ltx_text ltx_font_typewriter\">medium</span>, <em class=\"ltx_emph ltx_font_italic\">iii)</em> a CTC loss on the final encoder output (<math alttext=\"\\mathcal{L}_{\\text{CTCtgt}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>CTCtgt</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{CTCtgt}}</annotation></semantics></math>) applied to predict the target text <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib98\" title=\"\">2023</a>)</cite>.\nThe final loss is the weighted sum of the above-mentioned losses:</p>\n\n",
                "matched_terms": [
                    "base",
                    "layer",
                    "computed",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The monolingual ASR <span class=\"ltx_text ltx_font_typewriter\">base</span> model is trained on all 3k hours of ASR data for 200k steps using Noam as the learning rate scheduler <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib85\" title=\"\">2017</a>)</cite> with a peak of 2e-3 and 25,000 warm-up steps.</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "base",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The multitask and multilingual models are trained using a two-stage approach, where the model is pre-trained first on ASR data only (ASR pre-training) and then trained on both ASR and ST data (ASR+ST training). For the ASR pre-training, the learning rate scheduler adopted for the <span class=\"ltx_text ltx_font_typewriter\">small</span> model is the same as the <span class=\"ltx_text ltx_font_typewriter\">base</span> model.\nFor the <span class=\"ltx_text ltx_font_typewriter\">medium</span> model, we adopted a piece-wise warm-up on the Noam scheduler to avoid divergence issues <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib70\" title=\"\">2024</a>)</cite>, with the learning rate first increasing linearly to 2e-5 for 25k steps and then to 2e-4 for an additional 25k steps, followed by the standard inverse square root function.\nFor the ASR+ST training, we sample the ASR target with probability 0.5 and use the ST target otherwise following the same settings of ASR pre-training, except for the learning rate that is set to a constant value of 1e-4 for <span class=\"ltx_text ltx_font_typewriter\">small</span> and 1e-5 for <span class=\"ltx_text ltx_font_typewriter\">medium</span>, following the same downscale of the ASR pre-taining. Both training stages lasted 1M steps, corresponding to <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>6 epochs over the training data.</p>\n\n",
                "matched_terms": [
                    "base",
                    "model",
                    "set",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the <span class=\"ltx_text ltx_font_typewriter\">base</span> model, the trainings are executed on 4 NVIDIA A100 GPUs (64GB RAM) with a mini-batch of 40,000 tokens, an update frequency of 2, and averaging the last 7 checkpoints obtained from the training.\nFor the multitask and multitlingual models, we use mini-batches of 10,000 tokens for the <span class=\"ltx_text ltx_font_typewriter\">small</span> and 4,500 for the <span class=\"ltx_text ltx_font_typewriter\">medium</span> with an update frequency of, respectively, 2 and 6 on 16 NVIDIA A100 GPUs (64GB RAM), save checkpoints every 1,000 steps and average the last 25 checkpoints to obtain the final models.</p>\n\n",
                "matched_terms": [
                    "base",
                    "frequency",
                    "average",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The quality of the ASR hypotheses is evaluated with the WER metric using the jiWER library<span class=\"ltx_note ltx_role_footnote\" id=\"footnote12\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_tag ltx_tag_note\">12</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://pypi.org/project/jiwer/\" title=\"\">https://pypi.org/project/jiwer/</a></span></span></span> and applying the Whisper text normalizer<span class=\"ltx_note ltx_role_footnote\" id=\"footnote13\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_tag ltx_tag_note\">13</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://pypi.org/project/whisper-normalizer/\" title=\"\">https://pypi.org/project/whisper-normalizer/</a></span></span></span>.\nThe quality of the ST hypotheses is evaluated using COMET <cite class=\"ltx_cite ltx_citemacro_citep\">(Rei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib74\" title=\"\">2020</a>)</cite> version 2.2.4, with the default model.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\">14</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hf.co/Unbabel/wmt22-comet-da\" title=\"\">https://hf.co/Unbabel/wmt22-comet-da</a></span></span></span>\nThe quality of the explanations is obtained by measuring both\n<span class=\"ltx_text ltx_font_italic\">deletion</span> and <span class=\"ltx_text ltx_font_italic\">size</span> metrics\navailable in the SPES repository, using <span class=\"ltx_text ltx_font_typewriter\">wer_max</span> as the scorer for ASR and <span class=\"ltx_text ltx_font_typewriter\">bleu</span> for ST, as described in <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "explanations",
                    "deletion",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For comparison, in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A3.T5\" title=\"Table 5 &#8227; Appendix C Quality Metrics for the Reported Models &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we also report results obtained from popular large-scale models, namely Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib73\" title=\"\">2023</a>)</cite>, OWSM v3.1 <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib70\" title=\"\">2024</a>)</cite>, and SeamlessM4T <cite class=\"ltx_cite ltx_citemacro_citep\">(Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib7\" title=\"\">2023</a>)</cite>. Looking at the transcription/translation quality performance, we observe that both the monolingual <span class=\"ltx_text ltx_font_typewriter\">base</span> model and the multitask multilingual <span class=\"ltx_text ltx_font_typewriter\">small</span> and <span class=\"ltx_text ltx_font_typewriter\">large</span> models are mostly able to achieve competitive results, even outperforming the well-known models in two cases (<span class=\"ltx_text ltx_font_italic\">en</span> ASR for <span class=\"ltx_text ltx_font_typewriter\">base</span> and <span class=\"ltx_text ltx_font_italic\">en-it</span> ST for <span class=\"ltx_text ltx_font_typewriter\">large</span>). While our models and OWSM v3.1 strive to be on par on <span class=\"ltx_text ltx_font_italic\">it</span> with models with closed training data (Whisper and Seamless), they are able to close the gap on <span class=\"ltx_text ltx_font_italic\">en</span>, most probably given a larger availability of public training data. Moreover, the highest performance of <span class=\"ltx_text ltx_font_typewriter\">base</span> on <span class=\"ltx_text ltx_font_italic\">en</span> ASR compared to the <span class=\"ltx_text ltx_font_typewriter\">small</span> and <span class=\"ltx_text ltx_font_typewriter\">large</span> can be attributed to both the specialization of the model and the presence of the EuroParl-ST training set in the training data.</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "set",
                    "base",
                    "model",
                    "highest",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moving to the explanation quality, we observe that both deletion and size scores are comparable across all three models analyzed in the paper and coherent with values obtained in the original SPES paper <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite> on different benchmarks and models. Overall, the deletion scores for ASR are close to the highest possible value (i.e., 100), especially on <span class=\"ltx_text ltx_font_italic\">it</span>, where 97% is achieved. Similarly, the deletion scores for ST are close to 0, indicating that the quality of explanations is very high. The size scores are all close, ranging between 28.2 and 30.6 among models, languages, and tasks, indicating a good compactness of the explanations.</p>\n\n",
                "matched_terms": [
                    "across",
                    "deletion",
                    "highest",
                    "asr",
                    "between",
                    "explanations",
                    "different",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To properly choose the occlusion probability (<math alttext=\"p_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m1\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">p_{H}</annotation></semantics></math>) for the encoder output explanations <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m3\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></sup>, we conducted experiments by varying this probability in the set of <math alttext=\"\\{0.1,0.3,0.5,0.7,0.9\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>0.1</mn><mo>,</mo><mn>0.3</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>0.7</mn><mo>,</mo><mn>0.9</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{0.1,0.3,0.5,0.7,0.9\\}</annotation></semantics></math>, similarly to what has been done for determining the input occlusion probability (<math alttext=\"p_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m5\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">p_{X}</annotation></semantics></math>) in SPES <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "input",
                    "set",
                    "explanations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A4.T6\" title=\"Table 6 &#8227; Appendix D Effect of Occlusion Probability on Encoder Output Explanations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we report the <span class=\"ltx_text ltx_font_italic\">deletion</span>\nmetric computed on the dev set and, for completeness, the results of the Pearson <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m1\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math> correlation with cross-attention <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math>.\nAnalogously to\nthe deletion metric computed on the input saliency maps <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>, we compute the deletion on the encoder output saliency maps by iteratively replacing\nportions of the encoder output sequence <math alttext=\"\\mathbf{H}=\\{\\mathbf{h}_{1},\\ldots,\\mathbf{h}_{I}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#119815;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>&#119841;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119841;</mi><mi>I</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{H}=\\{\\mathbf{h}_{1},\\ldots,\\mathbf{h}_{I}\\}</annotation></semantics></math> with zero vectors, and removing 5% of the most important time frames at each step based on their saliency.\nFrame importance is determined using saliency maps <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m4\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>H</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}</annotation></semantics></math> aggregated at the sentence level.\nThe output quality is evaluated using the same <span class=\"ltx_text ltx_font_typewriter\">wer_max</span> scorer from the SPES repository. Lastly, we compute the area under the curve of the WER progression to quantify the faithfulness of the explanation.</p>\n\n",
                "matched_terms": [
                    "time",
                    "set",
                    "correlation",
                    "œÅrho",
                    "deletion",
                    "pearson",
                    "dev",
                    "computed",
                    "input",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the results, we can notice that higher occlusion probabilities yield not only a better deletion score but also an increased correlation with <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p3.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math>. The overall best correlation is achieved when averaging across all layers, and layer 5 achieves the best layer-specific correlation values, a phenomenon that remains coherent even when varying the occlusion probability.\nInterestingly, the deletion scores and the <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p3.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math>-<math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p3.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p3.m4\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></sup> correlations always follow the same trend, with the best values achieved with <math alttext=\"p_{H}=0.7\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p3.m5\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>H</mi></msub><mo>=</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">p_{H}=0.7</annotation></semantics></math>, which we used in all experiments reported in the main paper.</p>\n\n",
                "matched_terms": [
                    "across",
                    "correlation",
                    "deletion",
                    "layer",
                    "layers",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Examples of different saliency maps and cross-attention representations obtained with the <span class=\"ltx_text ltx_font_typewriter\">large</span> model are presented in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "different",
                    "model",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We notice similar relevance patterns in the paired samples&#8211;i.e., the samples having the same source language (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>a-f, and Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>g-l)&#8211;even if involving different tasks. We observe a reordering phenomenon from the English audio <span class=\"ltx_text ltx_inline-quote ltx_outerquote\">&#8220;<span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#0000FF;\">cheap<span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\"> <span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">money</span></span></span>&#8221;</span> and its Italian textual counterpart <span class=\"ltx_text ltx_inline-quote ltx_outerquote\">&#8220;<span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#FF0000;\">denaro<span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\"> <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">a buon mercato</span></span></span>&#8221;</span>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote15\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_tag ltx_tag_note\">15</span>Same colors reflect the same concepts.</span></span></span> which is reflected in the saliency maps (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>d and e) and also captured by cross-attention (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>f). We also observe that there are some patterns captured by the attention that are not reflected in the input. For instance, in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>f, the first words (<span class=\"ltx_text ltx_inline-quote ltx_outerquote\">&#8220;<span class=\"ltx_text ltx_font_italic\">&#200; solo</span>&#8221;</span>) attend&#8211;albeit with relatively low scores&#8211;to the audio frames between 75 and 85, while this pattern this pattern is absent in the relevance scores of both the encoder output and the input. Consistent with the findings discussed throughout the paper, this example illustrates that while attention generally follows the saliency patterns identified by feature attribution, some discrepancies persist.</p>\n\n",
                "matched_terms": [
                    "english",
                    "between",
                    "input",
                    "different",
                    "crossattention",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work provides an in-depth analysis of cross-attention explainability in encoder-decoder S2T models. While it yields actionable insights, some limitations should be acknowledged. First, our experimental scope is restricted to ASR and ST. Although these tasks are central to S2T-based AI <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib73\" title=\"\">2023</a>; Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib7\" title=\"\">2023</a>)</cite>, we do not evaluate other downstream tasks such as spoken question answering or speech summarization, which may involve different dynamics in decoder attention. Second, our multilingual analysis is limited to English (a Germanic language) and Italian (a Romance language), due to the high computational cost of large-scale model training across a broader set of languages. Third, we focus on models trained from scratch but do not include architectures based on Speech Foundation Models (SFMs) paired with large language models (LLMs), often referred to as SpeechLLM&#8211;a recent growing area of interest in S2T modeling <cite class=\"ltx_cite ltx_citemacro_citep\">(Gaido et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib24\" title=\"\">2024b</a>)</cite>. As our analysis focuses on evaluation, our goal was to completely avoid data contamination issues <cite class=\"ltx_cite ltx_citemacro_citep\">(Sainz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib76\" title=\"\">2023</a>)</cite>, which is a problem affecting almost every SFM and SpeechLLM architectures currently available, as we have no control over their training data, and, for this reason, we decided to retrain the models from scratch.\nFourth, our analysis relies on SPES to compute reference explanations, acknowledging that, as an empirical method, it may introduce some margin of error. However, in the absence of a gold or human reference&#8211;which is unattainable in practice&#8211;we adopt SPES as a <span class=\"ltx_text ltx_font_italic\">silver</span> reference, since it represents the state of the art in explainability for speech-to-text. We further validate this choice in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A3\" title=\"Appendix C Quality Metrics for the Reported Models &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>, showing that SPES achieves very high quality explanations (deletion scores <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>90 on ASR and <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m2\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>3 on ST), making it a more faithful option than less robust alternatives from the generic XAI field such as gradient norms <cite class=\"ltx_cite ltx_citemacro_citep\">(Covert et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib15\" title=\"\">2021a</a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "set",
                    "across",
                    "english",
                    "deletion",
                    "model",
                    "asr",
                    "explanations",
                    "different",
                    "crossattention",
                    "scores"
                ]
            }
        ]
    },
    "A3.T5": {
        "caption": "Table 5: ASR and ST output quality (WER and COMET) and explanation quality (deletion and size) for all models analyzed in the paper on the EuroParl-ST test sets.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" rowspan=\"2\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"2\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_bold\">WER <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"2\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_bold\">COMET <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T5.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_border_r_dashed ltx_align_right ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">\n<span class=\"ltx_text ltx_font_bold\">ASR Del. <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T5.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"2\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_bold\">ST Del. <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T5.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"4\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_bold\">Size <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T5.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_italic\">en</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_italic\">it</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_italic\">en-it</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_italic\">it-en</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_italic\">en</span></th>\n<th class=\"ltx_td ltx_border_r_dashed ltx_align_right ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">\n<span class=\"ltx_text ltx_font_italic\">it</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_italic\">en-it</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_italic\">it-en</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_italic\">en</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_italic\">it</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_italic\">en-it</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_italic\">it-en</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">Whisper</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">10.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_bold\">9.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">0.797</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"4\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"4\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">Seamless</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">11.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_bold\">9.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">0.795</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.813</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"4\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"4\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">OWSM v3.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">11.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">17.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">0.634</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">0.559</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"4\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">-</td>\n<td class=\"ltx_td ltx_align_center\" colspan=\"4\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_typewriter\">base</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_bold\">9.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">91.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">29.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"3\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_typewriter\">small</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">11.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">22.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">0.854</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">0.754</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_bold\">92.6</span></td>\n<td class=\"ltx_td ltx_border_r_dashed ltx_align_right\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">\n<span class=\"ltx_text ltx_font_bold\">97.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_bold\">2.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">2.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_bold\">29.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_bold\">28.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">30.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">29.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_typewriter\">large</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">11.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">21.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.862</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">0.765</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">90.8</td>\n<td class=\"ltx_td ltx_border_r_dashed ltx_align_right ltx_border_b\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">\n<span class=\"ltx_text ltx_font_bold\">97.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">2.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_bold\">2.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">30.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.5pt;padding-right:5.5pt;\">30.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_bold\">29.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-left:5.5pt;padding-right:5.5pt;\"><span class=\"ltx_text ltx_font_bold\">28.7</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "wer",
            "output",
            "size",
            "enit",
            "‚Üìdownarrow",
            "explanation",
            "del",
            "base",
            "small",
            "test",
            "iten",
            "analyzed",
            "seamless",
            "sets",
            "comet",
            "asr",
            "europarlst",
            "‚Üëuparrow",
            "deletion",
            "model",
            "large",
            "all",
            "paper",
            "whisper",
            "owsm",
            "v31",
            "quality"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">For comparison, in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A3.T5\" title=\"Table 5 &#8227; Appendix C Quality Metrics for the Reported Models &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we also report results obtained from popular large-scale models, namely Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib73\" title=\"\">2023</a>)</cite>, OWSM v3.1 <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib70\" title=\"\">2024</a>)</cite>, and SeamlessM4T <cite class=\"ltx_cite ltx_citemacro_citep\">(Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib7\" title=\"\">2023</a>)</cite>. Looking at the transcription/translation quality performance, we observe that both the monolingual <span class=\"ltx_text ltx_font_typewriter\">base</span> model and the multitask multilingual <span class=\"ltx_text ltx_font_typewriter\">small</span> and <span class=\"ltx_text ltx_font_typewriter\">large</span> models are mostly able to achieve competitive results, even outperforming the well-known models in two cases (<span class=\"ltx_text ltx_font_italic\">en</span> ASR for <span class=\"ltx_text ltx_font_typewriter\">base</span> and <span class=\"ltx_text ltx_font_italic\">en-it</span> ST for <span class=\"ltx_text ltx_font_typewriter\">large</span>). While our models and OWSM v3.1 strive to be on par on <span class=\"ltx_text ltx_font_italic\">it</span> with models with closed training data (Whisper and Seamless), they are able to close the gap on <span class=\"ltx_text ltx_font_italic\">en</span>, most probably given a larger availability of public training data. Moreover, the highest performance of <span class=\"ltx_text ltx_font_typewriter\">base</span> on <span class=\"ltx_text ltx_font_italic\">en</span> ASR compared to the <span class=\"ltx_text ltx_font_typewriter\">small</span> and <span class=\"ltx_text ltx_font_typewriter\">large</span> can be attributed to both the specialization of the model and the presence of the EuroParl-ST training set in the training data.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Cross-attention <cite class=\"ltx_cite ltx_citemacro_citep\">(Bahdanau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib6\" title=\"\">2015</a>)</cite> is the core mechanism of the encoder-decoder Transformer architecture <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib85\" title=\"\">2017</a>)</cite>, a model that has become foundational across numerous AI domains <cite class=\"ltx_cite ltx_citemacro_citep\">(Galassi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib25\" title=\"\">2021</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib49\" title=\"\">2022</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib45\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib92\" title=\"\">2024b</a>; Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib51\" title=\"\">2024</a>)</cite>, including natural language processing (NLP).\nDesigned for modeling dependencies between the generated output sequence and the input representations, the cross-attention scores&#8211;derived from the attention mechanism&#8211;have been leveraged in various NLP tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib32\" title=\"\">2020</a>; Zhang &amp; Kim, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib103\" title=\"\">2023</a>)</cite>, such as\nsource-target textual alignment <cite class=\"ltx_cite ltx_citemacro_citep\">(Garg et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib26\" title=\"\">2019</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib12\" title=\"\">2020</a>)</cite>, co-reference resolution <cite class=\"ltx_cite ltx_citemacro_citep\">(Voita et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib87\" title=\"\">2018</a>)</cite>, and word sense disambiguation <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib83\" title=\"\">2018</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "output",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In speech-to-text (S2T) modeling, cross-attention scores have been widely repurposed for diverse downstream applications such as audio-text alignment <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib104\" title=\"\">2020</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib46\" title=\"\">2020</a>)</cite>, speaker identification <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib38\" title=\"\">2019</a>)</cite>, timestamp estimation <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib47\" title=\"\">2022</a>; Louradour, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib50\" title=\"\">2023</a>; Zusag et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib106\" title=\"\">2024</a>)</cite>, and guiding simultaneous automatic speech recognition (ASR) and speech translation (ST) <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib91\" title=\"\">2024a</a>; Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib65\" title=\"\">2023a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib66\" title=\"\">b</a>)</cite>.\nThese applications rely on the implicit assumption that\ncross-attention reliably indicates what the model attends to in the input signal during output generation. However, despite its widespread use, this assumption has\nnever been verified.\nA key concern is that cross-attention operates over the encoder&#8217;s output sequence&#8211;rather than directly on the raw audio&#8211;which may have been reorganized or mixed with contextual information. This phenomenon, known as context mixing <cite class=\"ltx_cite ltx_citemacro_citep\">(Mohebbi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib60\" title=\"\">2023b</a>)</cite>, can potentially obscure the alignment between cross-attention weights and the original input signal.\nSimilar concerns have been extensively debated in the NLP community, where the reliability of attention mechanisms as explanations has been both challenged and defended, leading to conflicting perspectives and empirical evidence <cite class=\"ltx_cite ltx_citemacro_citep\">(Serrano &amp; Smith, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib78\" title=\"\">2019</a>; Jain &amp; Wallace, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib35\" title=\"\">2019</a>; Wiegreffe &amp; Pinter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib93\" title=\"\">2019</a>; Bastings &amp; Filippova, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib9\" title=\"\">2020</a>)</cite>. In contrast, this question remains largely underexplored in the speech domain.\nExisting work on explainability in S2T has primarily focused on self-attention <cite class=\"ltx_cite ltx_citemacro_citep\">(Shim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib80\" title=\"\">2022</a>; Audhkhasi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib5\" title=\"\">2022</a>; A&#160;Shams et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib1\" title=\"\">2024</a>)</cite>, or on empirically measuring the effects of context mixing <cite class=\"ltx_cite ltx_citemacro_citep\">(Mohebbi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib59\" title=\"\">2023a</a>)</cite>, without directly assessing the explanatory potential of cross-attention mechanisms.</p>\n\n",
                "matched_terms": [
                    "output",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this gap, we present the first systematic\nanalysis of cross-attention as a proxy for input-output dependencies in S2T models.\nOur study serves two main objectives: <span class=\"ltx_text ltx_font_italic\">i)</span> assessing the validity of using cross-attention as a surrogate for input-output alignment,\nand <span class=\"ltx_text ltx_font_italic\">ii)</span> evaluating whether it provides insights comparable to formal explainability methods such as feature attribution&#8211;while being more lightweight and less computationally expensive to obtain <cite class=\"ltx_cite ltx_citemacro_citep\">(Samek et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib77\" title=\"\">2021</a>; Madsen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib53\" title=\"\">2022</a>)</cite>. We compare cross-attention scores with input saliency maps derived from SPES <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>, the current state-of-the-art feature-attribution method in S2T, to determine the extent to which\ncross-attention\ncaptures\nwhich input features are relevant for models&#8217; predictions.\nIn addition, we compute saliency maps on\nencoder outputs and compare them with cross-attention scores to evaluate whether cross-attention fully explains how the decoder uses encoded representations, avoiding potential discrepancies\nof context mixing.\nOur\nanalysis spans ASR and ST tasks across monolingual, multilingual, and multitask settings using\nstate-of-the-art speech processing\narchitectures <cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib29\" title=\"\">2020</a>)</cite> at multiple scales.\nWith consistent trends across different settings, we find\nthat cross-attention exhibits moderate to strong correlations with input saliency maps and aligns more closely with encoder output representations, suggesting an influence of context mixing. However, our results also indicate that the overall explanatory power of cross-attention is limited&#8211;accounting for only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>50% of input relevance and, at best, 52-75% of encoder output saliency.\nOur findings uncover fundamental limitations in interpreting cross-attention as an explanatory proxy, suggesting that it provides an informative yet incomplete view of the factors driving predictions in S2T models.\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "output",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Explainability in Speech-to-Text.</span>\nExplainable AI (XAI) has emerged to make model behavior more interpretable to humans, thereby supporting informed decision-making and responsible deployment <cite class=\"ltx_cite ltx_citemacro_citep\">(Barredo Arrieta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib8\" title=\"\">2020</a>)</cite>.\nWhile XAI research has seen a rapid growth in the last years across multiple modalities, including vision and language <cite class=\"ltx_cite ltx_citemacro_citep\">(Sharma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib79\" title=\"\">2024</a>)</cite>, progress in the speech domain has lagged.\nThis gap arises from the inherent complexities of speech processing, including the multidimensional nature of speech signals across time and frequency, and the variability in output sequence length <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib95\" title=\"\">2024</a>)</cite>.\nDespite these challenges, growing concerns about trustworthiness\nare driving explainability efforts in speech classification <cite class=\"ltx_cite ltx_citemacro_citep\">(Becker et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib10\" title=\"\">2024</a>; Pastor et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib68\" title=\"\">2024</a>)</cite> and S2T generation <cite class=\"ltx_cite ltx_citemacro_citep\">(Mandel, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib54\" title=\"\">2016</a>; Kavaki &amp; Mandel, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib37\" title=\"\">2020</a>; Trinh &amp; Mandel, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib84\" title=\"\">2020</a>; Markert et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib55\" title=\"\">2021</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib94\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib95\" title=\"\">2024</a>; Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>. Most of these works rely on perturbation-based methods\nthat assess how input modifications affect model predictions <cite class=\"ltx_cite ltx_citemacro_citep\">(Covert et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib16\" title=\"\">2021b</a>; Ivanovs et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib34\" title=\"\">2021</a>)</cite>.\nAmong these,\n<cite class=\"ltx_cite ltx_citemacro_citet\">Fucci et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite> recently proposed\na technique for autoregressive S2T models that identifies regions of the spectrogram that most influence predictions to generate saliency maps.\nHowever, XAI methods are generally computationally expensive&#8211;especially perturbation-based approaches applied to large models <cite class=\"ltx_cite ltx_citemacro_citep\">(Luo &amp; Specia, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib52\" title=\"\">2024</a>; Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib102\" title=\"\">2025</a>)</cite>&#8211;which motivates exploring whether cross-attention, already computed at inference time, could serve as a lightweight alternative in a landscape still lacking efficient explainability tools for speech-based models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "output",
                    "model",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Attention as Explanation.</span>\nAttention mechanisms have been widely used to probe model behavior in text-based NLP, as attention scores often align with human intuitions about relevance and salience <cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib13\" title=\"\">2019</a>; Ferrando et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib21\" title=\"\">2024</a>)</cite>.\nEarly studies proposed norm-based analyses to improve the interpretability of attention weights <cite class=\"ltx_cite ltx_citemacro_citep\">(Kobayashi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib39\" title=\"\">2020</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib40\" title=\"\">2021</a>; Mohebbi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib58\" title=\"\">2021</a>; Ferrando et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib20\" title=\"\">2022b</a>)</cite>, while others suggested aggregating attention across layers and heads to quantify input-output influence more systematically <cite class=\"ltx_cite ltx_citemacro_citep\">(Abnar &amp; Zuidema, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib2\" title=\"\">2020</a>; Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib101\" title=\"\">2021</a>; Chefer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib11\" title=\"\">2021</a>)</cite>.\nWhile some have raised concerns about whether attention reliably reflects which inputs are actually responsible for outputs <cite class=\"ltx_cite ltx_citemacro_citep\">(Jain &amp; Wallace, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib35\" title=\"\">2019</a>; Serrano &amp; Smith, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib78\" title=\"\">2019</a>; Bastings &amp; Filippova, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib9\" title=\"\">2020</a>)</cite>, others have proposed conditions under which attention can meaningfully explain model behavior <cite class=\"ltx_cite ltx_citemacro_citep\">(Wiegreffe &amp; Pinter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib93\" title=\"\">2019</a>)</cite>. More recent work highlights that attention aggregation may obscure localized, token-specific interactions <cite class=\"ltx_cite ltx_citemacro_citep\">(Modarressi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib57\" title=\"\">2023</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib100\" title=\"\">2023</a>; Oh &amp; Schuler, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib63\" title=\"\">2023</a>)</cite>, motivating hybrid approaches that combine attention with other XAI techniques, such as attribution methods <cite class=\"ltx_cite ltx_citemacro_citep\">(Modarressi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib56\" title=\"\">2022</a>)</cite>, or the use of attention as a regularization signal during interpretability-driven training <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib97\" title=\"\">2024</a>)</cite>.\nDespite the ongoing efforts, most research has focused on self-attention within encoders, with limited attention to feed-forward dynamics <cite class=\"ltx_cite ltx_citemacro_citep\">(Kobayashi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib41\" title=\"\">2024</a>)</cite> and even less to encoder-decoder models. A few studies have investigated attention in encoder-decoder architectures <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib62\" title=\"\">2021</a>)</cite>, including in machine translation <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib105\" title=\"\">2024</a>)</cite>, but cross-attention remains largely underexplored in the speech domain and has been\nabsent from the broader <span class=\"ltx_text ltx_inline-quote ltx_outerquote\">&#8220;attention as explanation&#8221;</span> debate in NLP. Our work seeks to bridge this gap by bringing cross-attention of S2T models into this broader conversation, aiming to assess\nwhether\nit can serve as a reliable explanation&#8211;and where its limitations emerge.\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "explanation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where each row <math alttext=\"\\text{{\\color[rgb]{0.75390625,0.390625,0.9609375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.75390625,0.390625,0.9609375}$\\mathbf{CA}$}}^{(\\ell)}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m8\" intent=\":literal\"><semantics><msubsup><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.75390625,0.390625,0.9609375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.75390625,0.390625,0.9609375}$\\mathbf{CA}$}}^{(\\ell)}_{i}</annotation></semantics></math> represents the attention distribution over encoder time steps for the generation of output token <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m9\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math> at layer <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m10\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>.\nTo capture diverse patterns, Transformer-based models employ multi-head attention <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib85\" title=\"\">2017</a>)</cite>. Each head <math alttext=\"h\\in\\{1,\\ldots,H\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m11\" intent=\":literal\"><semantics><mrow><mi>h</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>H</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">h\\in\\{1,\\ldots,H\\}</annotation></semantics></math> uses separate learned projections:</p>\n\n",
                "matched_terms": [
                    "models",
                    "output"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Extracting\nthe full set of scores provides a fine-grained view of how each output token in the generated hypothesis attends to the encoder&#8217;s representations across all layers and heads.\nTo derive a single layer-wise or head-wise attention distribution, we compute the mean of the attention matrices over a subset <math alttext=\"\\mathcal{S}\\subseteq\\{1,\\dots,L\\}\\times\\{1,\\dots,H\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>&#8838;</mo><mrow><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo rspace=\"0.055em\" stretchy=\"false\">}</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>H</mi><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}\\subseteq\\{1,\\dots,L\\}\\times\\{1,\\dots,H\\}</annotation></semantics></math> of layers and heads:</p>\n\n",
                "matched_terms": [
                    "all",
                    "output"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By selecting different index sets <math alttext=\"\\mathcal{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><annotation encoding=\"application/x-tex\">\\mathcal{S}</annotation></semantics></math>, this formulation yields layer-wise, head-wise, or global averages. For example, setting <math alttext=\"\\mathcal{S}=\\{(\\ell,h):h=1,\\dots,H\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m3\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo>,</mo><mi>h</mi><mo rspace=\"0.278em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.278em\">:</mo><mrow><mi>h</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>H</mi></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}=\\{(\\ell,h):h=1,\\dots,H\\}</annotation></semantics></math> gives the average across heads at a given layer <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>; <math alttext=\"\\mathcal{S}=\\{(\\ell,h):\\ell=1,\\dots,L\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m5\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo>,</mo><mi>h</mi><mo rspace=\"0.278em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.278em\">:</mo><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}=\\{(\\ell,h):\\ell=1,\\dots,L\\}</annotation></semantics></math> averages across layers for head <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m6\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math>; and <math alttext=\"\\mathcal{S}=\\{1,\\dots,L\\}\\times\\{1,\\dots,H\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m7\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>=</mo><mrow><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo rspace=\"0.055em\" stretchy=\"false\">}</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>H</mi><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}=\\{1,\\dots,L\\}\\times\\{1,\\dots,H\\}</annotation></semantics></math> computes the full average.\nThis averaged attention provides a more aggregated view of the model&#8217;s attention patterns at a specific layer or attention head, summarizing how the model attends to the input speech over time.</p>\n\n",
                "matched_terms": [
                    "sets",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand how S2T models associate individual output tokens with specific regions of the raw speech input or of the model&#8217;s inner representations (e.g., the encoder output), we employ <span class=\"ltx_text ltx_font_italic\">feature-attribution</span> techniques that produce token-level <span class=\"ltx_text ltx_font_italic\">saliency maps</span>. These maps quantify the relevance of different portions of the input sequence in determining the model&#8217;s predictions.</p>\n\n",
                "matched_terms": [
                    "models",
                    "output"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The divergence scores are then mapped back to the corresponding cluster positions in the spectrogram and aggregated to form the token-specific saliency map <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{X}\\in\\mathbb{R}^{T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m16\" intent=\":literal\"><semantics><mrow><msubsup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>i</mi><mi>X</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{X}\\in\\mathbb{R}^{T\\times F}</annotation></semantics></math>. Stacking all saliency maps across the output sequence <math alttext=\"\\mathbf{y}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m17\" intent=\":literal\"><semantics><mi>&#119858;</mi><annotation encoding=\"application/x-tex\">\\mathbf{y}</annotation></semantics></math> yields a 3D saliency map:</p>\n\n",
                "matched_terms": [
                    "all",
                    "output"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further examine the influence of the encoder&#8217;s internal representations on the prediction of each output token.\nLet again <math alttext=\"\\mathbf{H}=\\mathrm{Encoder}(\\mathbf{X})\\in\\mathbb{R}^{T^{\\prime}\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119815;</mi><mo>=</mo><mrow><mi>Encoder</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119831;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{H}=\\mathrm{Encoder}(\\mathbf{X})\\in\\mathbb{R}^{T^{\\prime}\\times D}</annotation></semantics></math> denote the sequence of encoder hidden states or <span class=\"ltx_text ltx_font_italic\">encoder output</span>, where <math alttext=\"T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><msup><mi>T</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">T^{\\prime}</annotation></semantics></math> is the subsampled time dimension and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> is the hidden dimension.\nTo assess the importance of the encoder output representations, we compute token-specific saliency maps <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{H}\\in\\mathbb{R}^{T^{\\prime}\\times 1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m4\" intent=\":literal\"><semantics><mrow><msubsup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>i</mi><mi>H</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{H}\\in\\mathbb{R}^{T^{\\prime}\\times 1}</annotation></semantics></math>, where each entry reflects the contribution of the corresponding hidden state to the generation of <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m5\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math>.\nEach encoder state <math alttext=\"\\mathbf{H}=(\\mathbf{h}_{1},\\ldots,\\mathbf{h}_{T^{\\prime}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#119815;</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119841;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119841;</mi><msup><mi>T</mi><mo>&#8242;</mo></msup></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{H}=(\\mathbf{h}_{1},\\ldots,\\mathbf{h}_{T^{\\prime}})</annotation></semantics></math> is perturbed&#8211;i.e., all its features are set to 0&#8211;independently with probability <math alttext=\"p_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m7\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">p_{H}</annotation></semantics></math>, and the process is repeated <math alttext=\"N_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m8\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">N_{H}</annotation></semantics></math> times. The KL divergence is computed for each perturbation between the original and perturbed output distributions:</p>\n\n",
                "matched_terms": [
                    "all",
                    "output"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following prior work on cross-attention matrices <cite class=\"ltx_cite ltx_citemacro_citep\">(Vig &amp; Belinkov, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib86\" title=\"\">2019</a>)</cite> and explainable AI <cite class=\"ltx_cite ltx_citemacro_citep\">(Eberle et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib18\" title=\"\">2023</a>)</cite>, we use Pearson correlation\nto quantify the relationship between cross-attention scores and saliency-based explanations.\nPearson correlation is preferred over Kendall and Spearman because saliency scores are continuous, and their magnitude&#8211;not just ranking&#8211;is crucial. Rank-based measures are overly sensitive to small fluctuations among non-important features with near-zero scores, while Pearson better captures whether features are identified as important (high score) or not (low score).\nSpecifically, given the two representations <math alttext=\"\\text{{\\color[rgb]{0.75390625,0.390625,0.9609375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.75390625,0.390625,0.9609375}$\\mathbf{CA}$}},\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}\\in\\mathbb{R}^{I\\times T^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><mo>,</mo><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>T</mi><mo>&#8242;</mo></msup></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.75390625,0.390625,0.9609375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.75390625,0.390625,0.9609375}$\\mathbf{CA}$}},\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}\\in\\mathbb{R}^{I\\times T^{\\prime}}</annotation></semantics></math>, we compute the Pearson correlation coefficient <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math> to assess the similarity of their attribution patterns across output tokens and time steps. We first flatten each matrix into a vector of size <math alttext=\"I\\cdot T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8901;</mo><msup><mi>T</mi><mo>&#8242;</mo></msup></mrow><annotation encoding=\"application/x-tex\">I\\cdot T^{\\prime}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "size",
                    "output",
                    "small"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To avoid potential data contamination issues <cite class=\"ltx_cite ltx_citemacro_citep\">(Sainz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib76\" title=\"\">2023</a>)</cite>, we train from scratch a monolingual ASR model and two-sized multitask (ASR and ST) and multilingual (English and Italian) models. Details about training data and process are presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A2\" title=\"Appendix B Training Settings &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.\nBeing the only non-synthetic dataset supporting both tasks and language directions, we select EuroParl-ST <cite class=\"ltx_cite ltx_citemacro_citep\">(Iranzo-S&#225;nchez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib33\" title=\"\">2020</a>)</cite> as the test set for our analyses. The test set covers both <span class=\"ltx_text ltx_font_italic\">en</span> and <span class=\"ltx_text ltx_font_italic\">it</span> ASR, and <span class=\"ltx_text ltx_font_italic\">en-it</span> and <span class=\"ltx_text ltx_font_italic\">it-en</span> ST.\nThe <span class=\"ltx_text ltx_font_italic\">it/it-en</span> section consists of 1,686 segments, for a total of approximately 6 hours of audio, while the <span class=\"ltx_text ltx_font_italic\">en/en-it</span> section contains 1,130 segments, for a total of approximately 3 hours of audio.</p>\n\n",
                "matched_terms": [
                    "models",
                    "europarlst",
                    "model",
                    "asr",
                    "test",
                    "enit",
                    "iten"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The models analyzed in the paper are all composed of a Conformer encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib29\" title=\"\">2020</a>)</cite> and a Transformer decoder, as Conformer is the current state-of-the-art architecture for S2T processing <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib30\" title=\"\">2021</a>; Srivastava et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib81\" title=\"\">2022</a>; Li &amp; Doddipatla, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib48\" title=\"\">2023</a>)</cite>.\nThe monolingual ASR model (<span class=\"ltx_text ltx_font_typewriter\">base</span>) is composed of 12 encoder layers and 6 decoder layers. Each layer has 8 attention heads, 512 as embedding dimension, and FFNs dimension of 2,048. The vocabulary is built using a SentencePiece unigram model <cite class=\"ltx_cite ltx_citemacro_citep\">(Kudo &amp; Richardson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib42\" title=\"\">2018</a>)</cite> with size 8,000 trained on <span class=\"ltx_text ltx_font_italic\">en</span> transcripts. The resulting number of parameters is 125M.\nThe multitask and multilingual models are of two sizes, <span class=\"ltx_text ltx_font_typewriter\">small</span> and <span class=\"ltx_text ltx_font_typewriter\">large</span>, the first having 12 encoder layers and 6 decoder layers and the latter having 24 encoder layers and 12 decoder layers. In both sizes, each layer has 16 attention heads, an embedding dimension of 1,024, and an FFN dimension of 4,096. The vocabulary is built using a SentencePiece unigram model with size 16,000 trained on <span class=\"ltx_text ltx_font_italic\">en</span> and <span class=\"ltx_text ltx_font_italic\">it</span> transcripts. Two extra tokens&#8211;<span class=\"ltx_text ltx_font_typewriter\">&lt;lang:en&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;lang:it&gt;</span>&#8211;are added to indicate whether the target text is in <span class=\"ltx_text ltx_font_italic\">en</span> or <span class=\"ltx_text ltx_font_italic\">it</span>. The resulting number of parameters is 474M for the <span class=\"ltx_text ltx_font_typewriter\">small</span> model and 878M for the <span class=\"ltx_text ltx_font_typewriter\">large</span> model.\nIn all models, the Conformer encoder is preceded by two 1D convolutional layers with stride 2 and kernel size 5, resulting in a fixed subsampling factor <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> of 4. The kernel size of the Conformer convolutional module is 31 for both the point- and depth-wise convolutions. The input audio is represented by 80 Mel-filterbank features extracted every 10 ms with a window of 25 ms.</p>\n\n",
                "matched_terms": [
                    "models",
                    "base",
                    "model",
                    "large",
                    "size",
                    "all",
                    "small",
                    "paper",
                    "asr",
                    "analyzed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Hypothesis and Cross-Attention Generation.</span>\nFor the hypothesis generation, we use beam search with a beam size of 5 and a no-repeat n-gram size of 5. The attention scores are extracted from layers or heads during the output generation. The ASR and ST quality scores of the hypotheses are presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A3\" title=\"Appendix C Quality Metrics for the Reported Models &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nThe inference is performed using a single NVIDIA A40 GPU (40GB RAM) with a batch size of 40,000 tokens and takes <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>2.5 minutes for <span class=\"ltx_text ltx_font_typewriter\">base</span>, <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>3-5.5 minutes for <span class=\"ltx_text ltx_font_typewriter\">small</span>, and <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>3-6.5 for <span class=\"ltx_text ltx_font_typewriter\">large</span>, depending on the source language.</p>\n\n",
                "matched_terms": [
                    "base",
                    "output",
                    "large",
                    "size",
                    "small",
                    "asr",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Explanation Heatmaps Generation.</span>\nFollowing the best configuration obtained in SPES <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>, we adopt the Morphological Fragmental Perturbation Pyramid <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib99\" title=\"\">2021</a>)</cite> for clustering, which relies on Simple Linear Iterative Clustering or SLIC <cite class=\"ltx_cite ltx_citemacro_citep\">(Achanta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib3\" title=\"\">2012</a>)</cite>, a k-means-based algorithm that groups elements according to spectral patterns. We use the default parameters; the threshold length in seconds is\n7.50s, the\n<a class=\"ltx_ref ltx_href\" href=\"https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.slic\" title=\"\">SLIC</a>\nsigma is 0, the compactness is 0.1, and the number of patches per second for the MFPP technique is [400, 500, 600].\nFor the choice of <math alttext=\"p_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">p_{X}</annotation></semantics></math> and <math alttext=\"N_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">N_{X}</annotation></semantics></math>, we refer to the parameters used in <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>, setting <math alttext=\"p_{X}=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>X</mi></msub><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">p_{X}=0.5</annotation></semantics></math> and <math alttext=\"N_{X}=20,000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><msub><mi>N</mi><mi>X</mi></msub><mo>=</mo><mrow><mn>20</mn><mo>,</mo><mn>000</mn></mrow></mrow><annotation encoding=\"application/x-tex\">N_{X}=20,000</annotation></semantics></math>. The quality of the input explanations is presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A3\" title=\"Appendix C Quality Metrics for the Reported Models &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>. For the choice of <math alttext=\"p_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m5\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">p_{H}</annotation></semantics></math> and <math alttext=\"N_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m6\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">N_{H}</annotation></semantics></math>, we use the same number of iterations of <math alttext=\"N_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m7\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">N_{X}</annotation></semantics></math>, i.e., <math alttext=\"N_{H}=20,000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m8\" intent=\":literal\"><semantics><mrow><msub><mi>N</mi><mi>H</mi></msub><mo>=</mo><mrow><mn>20</mn><mo>,</mo><mn>000</mn></mrow></mrow><annotation encoding=\"application/x-tex\">N_{H}=20,000</annotation></semantics></math>, while the optimal occlusion probability <math alttext=\"p_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m9\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">p_{H}</annotation></semantics></math> is determined over the dev set, resulting in <math alttext=\"p_{H}=0.7\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m10\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>H</mi></msub><mo>=</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">p_{H}=0.7</annotation></semantics></math>, whose experiments are reported in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A4\" title=\"Appendix D Effect of Occlusion Probability on Encoder Output Explanations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.\nThe inference is performed using a single NVIDIA A40 GPU (40GB RAM)\nand takes <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m11\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>27 hours for <span class=\"ltx_text ltx_font_typewriter\">base</span>, <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m12\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>3-4 days for <span class=\"ltx_text ltx_font_typewriter\">small</span> and <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m13\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>6-8 days for <span class=\"ltx_text ltx_font_typewriter\">large</span>, depending on the source language.</p>\n\n",
                "matched_terms": [
                    "base",
                    "large",
                    "small",
                    "explanation",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we compare <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> with input saliency maps <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m3\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></sup>, which serve as an external reference for measuring input relevance.\nSpecifically, in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS1\" title=\"5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a>, we analyze the\n<span class=\"ltx_text ltx_font_typewriter\">base</span> model across all levels of granularity. Then, in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS2\" title=\"5.1.2 Multitask and Multilingual Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.2</span></a>, we extend the analysis to additional models (<span class=\"ltx_text ltx_font_typewriter\">small</span> and <span class=\"ltx_text ltx_font_typewriter\">large</span>), languages (<span class=\"ltx_text ltx_font_italic\">en</span> and <span class=\"ltx_text ltx_font_italic\">it</span>), and tasks (ASR and ST).</p>\n\n",
                "matched_terms": [
                    "models",
                    "base",
                    "model",
                    "large",
                    "all",
                    "small",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.T1\" title=\"Table 1 &#8227; 5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reports the correlation scores for the monolingual English ASR model (<span class=\"ltx_text ltx_font_typewriter\">base</span>), considering cross-attention at the head level, layer level, and in aggregated form.</p>\n\n",
                "matched_terms": [
                    "base",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At the individual head level, correlations with saliency maps are generally low. This suggests that attention heads, when taken in isolation, only partially capture the model&#8217;s dependency on the input and often encode noisy or inconsistent relevance signals. However, not all heads are equal: some, especially in the upper layers (layers 4-6), exhibit relatively stronger correlations. Notably, <span class=\"ltx_text ltx_font_bold\">averaging across heads consistently outperforms selecting individual heads</span>, suggesting that, despite head-level sparsity and weak individual correlations, the collective information captured across heads reflects input relevance more effectively.\nMoving from heads to layers, we find a clearer picture. Averaging attention scores across all heads within each layer boosts correlation substantially, with layer 6 standing out as the most aligned with the saliency maps. This is followed closely by layer 5 and the average across all layers, indicating that the <span class=\"ltx_text ltx_font_bold\">last layers exhibit the highest alignment with input relevance</span>. These results reinforce the idea that deeper layers encode higher-level semantic or task-relevant features, a trend previously observed in Transformer-based models <cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib13\" title=\"\">2019</a>)</cite>.\nInterestingly, while averaging across heads improves alignment, averaging across both heads and layers does not yield the overall best result, even if values are close. This indicates that not all layers contribute equally and that indiscriminate aggregation can dilute the relevance signal.</p>\n\n",
                "matched_terms": [
                    "models",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the impact of multilingual and multitask training on the correlation between cross-attention scores and saliency maps, we evaluate the <span class=\"ltx_text ltx_font_typewriter\">small</span> and <span class=\"ltx_text ltx_font_typewriter\">large</span> models. Layer-wise results are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.T2\" title=\"Table 2 &#8227; 5.1.2 Multitask and Multilingual Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, while head-wise results are omitted due to the noisy behavior observed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS1\" title=\"5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "small",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across all configurations, we observe that <span class=\"ltx_text ltx_font_italic\">en</span> ASR yields the highest correlation values, outperforming even the monolingual <span class=\"ltx_text ltx_font_typewriter\">base</span> model (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS1\" title=\"5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a>). This suggests that large-scale multilingual training enhances the alignment between cross-attention and saliency maps, likely due to the improved generalization capacity of the model. In contrast, <span class=\"ltx_text ltx_font_italic\">en-it</span> ST shows a drop in correlation, which is expected given the increased complexity of ST compared to ASR.\nWhen considering <span class=\"ltx_text ltx_font_italic\">it</span> as the source language, we observe a similar pattern: ASR correlations are consistently higher than ST, yet remain below their <span class=\"ltx_text ltx_font_italic\">en</span> counterparts. This discrepancy aligns with the data distribution in training, where <span class=\"ltx_text ltx_font_italic\">en</span> accounts for 84% of the data versus 16% for <span class=\"ltx_text ltx_font_italic\">it</span>, resulting in more robust representations for <span class=\"ltx_text ltx_font_italic\">en</span>.\nAt the layer level, we find consistent evidence that the last decoder layers yield stronger correlations, reaffirming the trends observed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS1\" title=\"5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a>.\nThe specific optimal layer varies with model size: layer 5 performs best in <span class=\"ltx_text ltx_font_typewriter\">small</span>, while layers 8-10 achieve the highest correlations in <span class=\"ltx_text ltx_font_typewriter\">large</span>.\nNevertheless, correlation values across the last\nlayers remain very close, suggesting that their cross-attention scores\nprovide the most robust alignment with saliency maps across both tasks and languages. This trend is further supported by downstream application results, where the final layers have shown the best token-level performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib65\" title=\"\">2023a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib66\" title=\"\">b</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib91\" title=\"\">2024a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "base",
                    "model",
                    "large",
                    "size",
                    "all",
                    "small",
                    "asr",
                    "enit"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Averaging attention scores across layers further improves the correlation with saliency maps in\nalmost\nall configurations. The only exceptions are <span class=\"ltx_text ltx_font_italic\">en</span> and <span class=\"ltx_text ltx_font_italic\">it</span> ASR in <span class=\"ltx_text ltx_font_typewriter\">small</span>, where selective-layer extraction offers a marginal improvement (0.006 for English, 0.001 for Italian).\nTherefore, similarly to what we observed in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS1\" title=\"5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a>, averaging attention across heads and layers consistently yields the best or near-best correlation with <span class=\"ltx_text ltx_font_italic\">moderate</span> to <span class=\"ltx_text ltx_font_italic\">strong</span> correlation with input saliency maps, even considering large-scale models trained in multitask and multilingual settings.\nNonetheless, this alignment accounts for only 49-63% of the total input relevance, indicating that <span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#88185B;\">cross-attention falls short of fully accounting for the S2T models&#8217; behavior</span>.\nSince this limitation may stem from the phenomenon of context mixing, in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS2\" title=\"5.2 What Is the Impact of Context Mixing? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a> we analyze the correlation between cross-attention and encoder output&#8211;representations that have already undergone transformation by the encoder&#8211;to better isolate the true explanatory power of cross-attention.</p>\n\n",
                "matched_terms": [
                    "models",
                    "small",
                    "asr",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1\" title=\"5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> focused on input relevance, we now investigate whether <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math>\naligns more closely with encoder output saliency maps.\nA higher correlation with encoder output representations would support the hypothesis that discrepancies between cross-attention and input saliency arise from context mixing, due to the reorganization of information within the encoder.\nTo this end,\nwe compare <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> with encoder output saliency maps <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m4\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></sup>, which attribute relevance to the encoder hidden states for each output token (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S3.SS2\" title=\"3.2 Feature Attribution for Speech-to-Text &#8227; 3 Methodology &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>). Layer-wise results for all models are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.T3\" title=\"Table 3 &#8227; 5.2 What Is the Impact of Context Mixing? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "output",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Even when examining encoder output representations, we observe trends consistent with those identified in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1\" title=\"5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>. Specifically, when averaged across decoder layers, cross-attention scores consistently provide the strongest or nearly optimal correlation with saliency maps, with the last decoder layers offering more representative explanations than the first ones across all models. As expected, correlation with encoder output representations consistently yields higher scores than those obtained from input representations, with absolute <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math> differences ranging from 0.03 to 0.18, quantifying the influence of context mixing effects to 6.6-16.7%.\nThe increased correlation is also visually evident in the example shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.F2\" title=\"Figure 2 &#8227; 5.2 What Is the Impact of Context Mixing? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, where <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> aligns more closely with the relevance scores from <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m4\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></sup> than with those from <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m5\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m6\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></sup>.\nHowever, despite being unaffected by context mixing, the correlation between <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m7\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> and <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m8\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m9\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></sup> remains limited&#8211;capturing only 52-75% of the relevance.\n<span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#88185B;\">This\ngap underscores the inherent limitations in relying solely on cross-attention as an explanation mechanism, reinforcing its role as an informative but <span class=\"ltx_text ltx_ulem_uline\">incomplete</span> proxy for explainability in S2T models</span>&#8211;not only for input-level saliency, but even at the encoder-output level, where cross-attention directly operates.</p>\n\n",
                "matched_terms": [
                    "explanation",
                    "models",
                    "output",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Discussion.</span>\nOur results demonstrate that, although <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> scores moderately correlate with aggregated <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m2\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math> (with a correlation peaking around 0.45-0.55 in the best-performing settings), they consistently fall short of capturing the full input relevance&#8211;even when context mixing effects are factored out. To directly assess explanation quality, we compute the deletion metric (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A1\" title=\"Appendix A Effect of Aggregation Functions on Input Explanations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>) on the <span class=\"ltx_text ltx_font_typewriter\">base</span> model, finding that <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> achieves 41.2, compared to 52.9 for frequency-aggregated <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m4\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math> and 91.3 for full-resolution maps.\nThis gap underscores that <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m5\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> discards fine-grained time-frequency cues and produces weaker attributions, even under identical aggregation. As further discussed in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A6\" title=\"Appendix F Limitations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>, our analysis is bounded by the use of SPES as the attribution baseline, but the consistent underperformance across correlation and deletion confirms that <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m6\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> offers, at best, an incomplete picture of model behavior. These results also carry implications for downstream tasks. In applications such as timestamp prediction, prior work often relies on attention from a single decoder layer or head <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib91\" title=\"\">2024a</a>; Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib65\" title=\"\">2023a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib66\" title=\"\">b</a>; Zusag et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib106\" title=\"\">2024</a>)</cite>. Our analysis suggests that averaging across layers and, especially, across heads provides a closer match to saliency behavior and could improve these methods.\nBuilding on past success with attention regularization in ASR (e.g., imposing monotonicity as in <cite class=\"ltx_cite ltx_citemacro_citet\">Zhao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib104\" title=\"\">2020</a></cite>), similar training-time strategies&#8211;such as auxiliary losses that align attention with saliency&#8211;could further benefit downstream applications, enhancing both interpretability and task performance.\nIn summary, <span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"--ltx-fg-color:#88185B;\"><math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m7\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> should not be treated as a stand-alone XAI tool</span>. It provides lightweight cues that may complement attribution-based methods, but it cannot replace them. Reframing <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m8\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> as an auxiliary rather than a proxy recalibrates expectations and grounds future work on more faithful and effective approaches to explainability in S2T models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "base",
                    "deletion",
                    "model",
                    "asr",
                    "explanation",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conclusions.</span>\nWe presented the first systematic analysis of cross-attention in S2T through the lens of explainable AI, comparing it to saliency maps across tasks, languages, and model scales. Cross-attention moderately to strongly aligns with saliency&#8211;especially when averaged across heads and layers&#8211;but captures only about half of the input relevance. Even when disentangling the effect of context mixing by analyzing encoder outputs, it explains just 52-75% of saliency. This gap reveals intrinsic limits of cross-attention as an explanation mechanism: it offers informative cues but only a partial view of the factors driving S2T predictions.</p>\n\n",
                "matched_terms": [
                    "explanation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Broader Implications.</span> Explainability in S2T systems has tangible implications for AI transparency, especially in high-stakes settings such as healthcare, legal transcription, and educational accessibility. Our findings provide insights about the usage of cross-attention as a tool for identifying how models relate output predictions to input regions, which can support auditing, debugging, and fair deployment. However, there is a risk that misinterpreted attention visualizations may be overtrusted by non-expert users, reinforcing false confidence in system behavior <cite class=\"ltx_cite ltx_citemacro_citep\">(Rudin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib75\" title=\"\">2019</a>)</cite>. Moreover, our language choices and focus on high-resource speech still reflect global imbalances in language technology access <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib36\" title=\"\">2020</a>)</cite>. Future work should extend this analysis to low-resource and underrepresented languages to promote broader inclusion.</p>\n\n",
                "matched_terms": [
                    "models",
                    "output"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Use of Large Language Models.</span> For the writing process, ChatGPT was employed exclusively to correct grammar in content authored by humans.</p>\n\n",
                "matched_terms": [
                    "models",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure the reproducibility of our results, we described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S4\" title=\"4 Experimental Settings &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> all the details regarding our model training, training and evaluation data, and evaluation procedure. Moreover, we relied only on openly available data and on open source code<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/hlt-mt/FBK-fairseq/blob/master/fbk_works/XAI_FEATURE_ATTRIBUTION.md\" title=\"\">https://github.com/hlt-mt/FBK-fairseq/blob/master/fbk_works/XAI_FEATURE_ATTRIBUTION.md</a></span></span></span> for the generation of the saliency maps. Lastly, all models (described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S4\" title=\"4 Experimental Settings &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), code, attention scores, and explanation artifacts will be released under the Apache 2.0 (code) and CC-BY 4.0 (all other materials) licenses upon paper acceptance.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "all",
                    "paper",
                    "explanation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To compare and select the best aggregation strategy, we adopt the <span class=\"ltx_text ltx_font_italic\">deletion</span> metric&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Nauta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib61\" title=\"\">2023</a>)</cite>, which quantifies the decline in prediction quality as the most relevant input frames&#8211;identified by the explanation&#8211;are progressively removed. Specifically, we adapt the implementation by <cite class=\"ltx_cite ltx_citemacro_citet\">Fucci et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite> for S2T tasks, replacing the top-ranked time frames in the input spectrogram <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m1\" intent=\":literal\"><semantics><mi>&#119831;</mi><annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation></semantics></math> with zero vectors in 5% increments, based on the aggregated saliency map <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m2\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math>.\nSince <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m3\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math> operates on an aggregated time dimension <math alttext=\"T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m4\" intent=\":literal\"><semantics><msup><mi>T</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">T^{\\prime}</annotation></semantics></math>, which is smaller than the original time dimension <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m5\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> of <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m6\" intent=\":literal\"><semantics><mi>&#119831;</mi><annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation></semantics></math>, we upsample <math alttext=\"T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m7\" intent=\":literal\"><semantics><msup><mi>T</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">T^{\\prime}</annotation></semantics></math> to match <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m8\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> using nearest-neighbor interpolation.\nPrediction quality is measured using the word error rate (WER), specifically the <span class=\"ltx_text ltx_font_typewriter\">wer_max</span> scorer from the SPES repository.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/hlt-mt/FBK-fairseq\" title=\"\">https://github.com/hlt-mt/FBK-fairseq</a></span></span></span> Lastly, we compute the area under the WER curve to quantify the faithfulness of each explanation method.</p>\n\n",
                "matched_terms": [
                    "quality",
                    "explanation",
                    "deletion",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Among the tested methods, we observe that the 2D maximum pooling aggregation (2D max) yields the best quality explanations, obtaining the highest deletion score, while the 2D average pooling (2D avg) is the worst, with the lowest deletion score. Looking at the correlations, we notice that they follow the same trend of deletion scores, with the 2D max yielding the best <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m1\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math>. In particular, 2D avg consistently has the lowest correlations compared to the 2D max, particularly in the last layers (e.g., 0.457 against 0.572 at layer 5). Regarding the 2-step pooling approach, we not only observe an improved deletion score but also better correlation scores compared to 2D avg, especially from layer 3 onward, approaching the best performance with a layer-average correlation of 0.565. Nevertheless, the explanation quality is still lower compared to 2D max (i.e., 55.18 against 57.04), which also achieves the highest correlations at nearly every layer, peaking at 0.582 in layer 6, and yielding the best overall correlation among the averaged cross-attention across layers (i.e., 0.572).</p>\n\n",
                "matched_terms": [
                    "explanation",
                    "deletion",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the monolingual ASR model, we leverage the speech-to-text English data available for the IWSLT 2024 evaluation campaign (offline task),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://iwslt.org/2024/offline\" title=\"\">https://iwslt.org/2024/offline</a></span></span></span> namely: CommonVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Ardila et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib4\" title=\"\">2020</a>)</cite>, CoVoST v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib90\" title=\"\">2021b</a>)</cite>, Europarl-ST <cite class=\"ltx_cite ltx_citemacro_citep\">(Iranzo-S&#225;nchez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib33\" title=\"\">2020</a>)</cite>, LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib64\" title=\"\">2015</a>)</cite>, MuST-C v1 <cite class=\"ltx_cite ltx_citemacro_citep\">(Di&#160;Gangi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib17\" title=\"\">2019</a>)</cite>, TEDLIUM v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Hernandez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib31\" title=\"\">2018</a>)</cite>, and VoxPopuli ASR <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib89\" title=\"\">2021a</a>)</cite>. The resulting training set is about 3k hours of speech.</p>\n\n",
                "matched_terms": [
                    "europarlst",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the multitask (ASR and ST) multilingual large-scale models, we leverage more than 150k hours of open-source speech<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>Speech, transcripts, and translations released under an open-source license such as CC-0 and CC-BY 4.0.</span></span></span> in English (<span class=\"ltx_text ltx_font_italic\">en</span>) and Italian (<span class=\"ltx_text ltx_font_italic\">it</span>), namely: CommonVoice, CoVoST v2, FLEURS <cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib14\" title=\"\">2023</a>)</cite>, MOSEL <cite class=\"ltx_cite ltx_citemacro_citep\">(Gaido et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib23\" title=\"\">2024a</a>)</cite>, MLS <cite class=\"ltx_cite ltx_citemacro_citep\">(Pratap et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib71\" title=\"\">2020</a>)</cite>, and YouTube-Commons<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hf.co/datasets/PleIAs/YouTube-Commons\" title=\"\">https://hf.co/datasets/PleIAs/YouTube-Commons</a></span></span></span> (from which 14.2k hours of <span class=\"ltx_text ltx_font_italic\">en</span> and 1.8k for <span class=\"ltx_text ltx_font_italic\">it</span> have been extracted). For datasets missing the translations, we generated them using <span class=\"ltx_text ltx_font_typewriter\">MADLAD-400 3B-MT</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Kudugunta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib43\" title=\"\">2023</a>)</cite>. This setting allows us to verify our analysis with a large-scale setting similar to the scale of a popular model such as OWSM <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib69\" title=\"\">2023</a>)</cite> and 2 times that of NVIDIA Canary <cite class=\"ltx_cite ltx_citemacro_citep\">(Puvvada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib72\" title=\"\">2024</a>)</cite> while having complete control of data used during training, ensuring that data contamination issues are avoided completely.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "owsm",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train all models using a combination of three losses: <em class=\"ltx_emph ltx_font_italic\">i)</em> a label-smoothed cross-entropy loss (<math alttext=\"\\mathcal{L}_{\\text{CE}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>CE</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{CE}}</annotation></semantics></math>) applied to the decoder output using the target text as the reference (transcripts for ASR and translations for ST), <em class=\"ltx_emph ltx_font_italic\">ii)</em> a CTC loss <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib27\" title=\"\">2006</a>)</cite> computed using transcripts as reference (<math alttext=\"\\mathcal{L}_{\\text{CTCsrc}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>CTCsrc</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{CTCsrc}}</annotation></semantics></math>) on the output of the 8<sup class=\"ltx_sup\">th</sup> encoder layer for <span class=\"ltx_text ltx_font_typewriter\">base</span> and <span class=\"ltx_text ltx_font_typewriter\">small</span> and the 16<sup class=\"ltx_sup\">th</sup> for <span class=\"ltx_text ltx_font_typewriter\">medium</span>, <em class=\"ltx_emph ltx_font_italic\">iii)</em> a CTC loss on the final encoder output (<math alttext=\"\\mathcal{L}_{\\text{CTCtgt}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>CTCtgt</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{CTCtgt}}</annotation></semantics></math>) applied to predict the target text <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib98\" title=\"\">2023</a>)</cite>.\nThe final loss is the weighted sum of the above-mentioned losses:</p>\n\n",
                "matched_terms": [
                    "models",
                    "base",
                    "output",
                    "all",
                    "small",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The monolingual ASR <span class=\"ltx_text ltx_font_typewriter\">base</span> model is trained on all 3k hours of ASR data for 200k steps using Noam as the learning rate scheduler <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib85\" title=\"\">2017</a>)</cite> with a peak of 2e-3 and 25,000 warm-up steps.</p>\n\n",
                "matched_terms": [
                    "base",
                    "all",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The multitask and multilingual models are trained using a two-stage approach, where the model is pre-trained first on ASR data only (ASR pre-training) and then trained on both ASR and ST data (ASR+ST training). For the ASR pre-training, the learning rate scheduler adopted for the <span class=\"ltx_text ltx_font_typewriter\">small</span> model is the same as the <span class=\"ltx_text ltx_font_typewriter\">base</span> model.\nFor the <span class=\"ltx_text ltx_font_typewriter\">medium</span> model, we adopted a piece-wise warm-up on the Noam scheduler to avoid divergence issues <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib70\" title=\"\">2024</a>)</cite>, with the learning rate first increasing linearly to 2e-5 for 25k steps and then to 2e-4 for an additional 25k steps, followed by the standard inverse square root function.\nFor the ASR+ST training, we sample the ASR target with probability 0.5 and use the ST target otherwise following the same settings of ASR pre-training, except for the learning rate that is set to a constant value of 1e-4 for <span class=\"ltx_text ltx_font_typewriter\">small</span> and 1e-5 for <span class=\"ltx_text ltx_font_typewriter\">medium</span>, following the same downscale of the ASR pre-taining. Both training stages lasted 1M steps, corresponding to <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>6 epochs over the training data.</p>\n\n",
                "matched_terms": [
                    "models",
                    "base",
                    "model",
                    "small",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the <span class=\"ltx_text ltx_font_typewriter\">base</span> model, the trainings are executed on 4 NVIDIA A100 GPUs (64GB RAM) with a mini-batch of 40,000 tokens, an update frequency of 2, and averaging the last 7 checkpoints obtained from the training.\nFor the multitask and multitlingual models, we use mini-batches of 10,000 tokens for the <span class=\"ltx_text ltx_font_typewriter\">small</span> and 4,500 for the <span class=\"ltx_text ltx_font_typewriter\">medium</span> with an update frequency of, respectively, 2 and 6 on 16 NVIDIA A100 GPUs (64GB RAM), save checkpoints every 1,000 steps and average the last 25 checkpoints to obtain the final models.</p>\n\n",
                "matched_terms": [
                    "base",
                    "models",
                    "small",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The quality of the ASR hypotheses is evaluated with the WER metric using the jiWER library<span class=\"ltx_note ltx_role_footnote\" id=\"footnote12\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_tag ltx_tag_note\">12</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://pypi.org/project/jiwer/\" title=\"\">https://pypi.org/project/jiwer/</a></span></span></span> and applying the Whisper text normalizer<span class=\"ltx_note ltx_role_footnote\" id=\"footnote13\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_tag ltx_tag_note\">13</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://pypi.org/project/whisper-normalizer/\" title=\"\">https://pypi.org/project/whisper-normalizer/</a></span></span></span>.\nThe quality of the ST hypotheses is evaluated using COMET <cite class=\"ltx_cite ltx_citemacro_citep\">(Rei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib74\" title=\"\">2020</a>)</cite> version 2.2.4, with the default model.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\">14</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hf.co/Unbabel/wmt22-comet-da\" title=\"\">https://hf.co/Unbabel/wmt22-comet-da</a></span></span></span>\nThe quality of the explanations is obtained by measuring both\n<span class=\"ltx_text ltx_font_italic\">deletion</span> and <span class=\"ltx_text ltx_font_italic\">size</span> metrics\navailable in the SPES repository, using <span class=\"ltx_text ltx_font_typewriter\">wer_max</span> as the scorer for ASR and <span class=\"ltx_text ltx_font_typewriter\">bleu</span> for ST, as described in <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "deletion",
                    "size",
                    "comet",
                    "whisper",
                    "asr",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moving to the explanation quality, we observe that both deletion and size scores are comparable across all three models analyzed in the paper and coherent with values obtained in the original SPES paper <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite> on different benchmarks and models. Overall, the deletion scores for ASR are close to the highest possible value (i.e., 100), especially on <span class=\"ltx_text ltx_font_italic\">it</span>, where 97% is achieved. Similarly, the deletion scores for ST are close to 0, indicating that the quality of explanations is very high. The size scores are all close, ranging between 28.2 and 30.6 among models, languages, and tasks, indicating a good compactness of the explanations.</p>\n\n",
                "matched_terms": [
                    "models",
                    "quality",
                    "deletion",
                    "size",
                    "all",
                    "paper",
                    "asr",
                    "explanation",
                    "analyzed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A4.T6\" title=\"Table 6 &#8227; Appendix D Effect of Occlusion Probability on Encoder Output Explanations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we report the <span class=\"ltx_text ltx_font_italic\">deletion</span>\nmetric computed on the dev set and, for completeness, the results of the Pearson <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m1\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math> correlation with cross-attention <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math>.\nAnalogously to\nthe deletion metric computed on the input saliency maps <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>, we compute the deletion on the encoder output saliency maps by iteratively replacing\nportions of the encoder output sequence <math alttext=\"\\mathbf{H}=\\{\\mathbf{h}_{1},\\ldots,\\mathbf{h}_{I}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#119815;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>&#119841;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119841;</mi><mi>I</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{H}=\\{\\mathbf{h}_{1},\\ldots,\\mathbf{h}_{I}\\}</annotation></semantics></math> with zero vectors, and removing 5% of the most important time frames at each step based on their saliency.\nFrame importance is determined using saliency maps <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m4\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>H</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}</annotation></semantics></math> aggregated at the sentence level.\nThe output quality is evaluated using the same <span class=\"ltx_text ltx_font_typewriter\">wer_max</span> scorer from the SPES repository. Lastly, we compute the area under the curve of the WER progression to quantify the faithfulness of the explanation.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "output",
                    "deletion",
                    "explanation",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the results, we can notice that higher occlusion probabilities yield not only a better deletion score but also an increased correlation with <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p3.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math>. The overall best correlation is achieved when averaging across all layers, and layer 5 achieves the best layer-specific correlation values, a phenomenon that remains coherent even when varying the occlusion probability.\nInterestingly, the deletion scores and the <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p3.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math>-<math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p3.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p3.m4\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></sup> correlations always follow the same trend, with the best values achieved with <math alttext=\"p_{H}=0.7\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p3.m5\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>H</mi></msub><mo>=</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">p_{H}=0.7</annotation></semantics></math>, which we used in all experiments reported in the main paper.</p>\n\n",
                "matched_terms": [
                    "all",
                    "deletion",
                    "paper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Examples of different saliency maps and cross-attention representations obtained with the <span class=\"ltx_text ltx_font_typewriter\">large</span> model are presented in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "large"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We notice similar relevance patterns in the paired samples&#8211;i.e., the samples having the same source language (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>a-f, and Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>g-l)&#8211;even if involving different tasks. We observe a reordering phenomenon from the English audio <span class=\"ltx_text ltx_inline-quote ltx_outerquote\">&#8220;<span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#0000FF;\">cheap<span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\"> <span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">money</span></span></span>&#8221;</span> and its Italian textual counterpart <span class=\"ltx_text ltx_inline-quote ltx_outerquote\">&#8220;<span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#FF0000;\">denaro<span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\"> <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">a buon mercato</span></span></span>&#8221;</span>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote15\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_tag ltx_tag_note\">15</span>Same colors reflect the same concepts.</span></span></span> which is reflected in the saliency maps (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>d and e) and also captured by cross-attention (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>f). We also observe that there are some patterns captured by the attention that are not reflected in the input. For instance, in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>f, the first words (<span class=\"ltx_text ltx_inline-quote ltx_outerquote\">&#8220;<span class=\"ltx_text ltx_font_italic\">&#200; solo</span>&#8221;</span>) attend&#8211;albeit with relatively low scores&#8211;to the audio frames between 75 and 85, while this pattern this pattern is absent in the relevance scores of both the encoder output and the input. Consistent with the findings discussed throughout the paper, this example illustrates that while attention generally follows the saliency patterns identified by feature attribution, some discrepancies persist.</p>\n\n",
                "matched_terms": [
                    "output",
                    "paper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work provides an in-depth analysis of cross-attention explainability in encoder-decoder S2T models. While it yields actionable insights, some limitations should be acknowledged. First, our experimental scope is restricted to ASR and ST. Although these tasks are central to S2T-based AI <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib73\" title=\"\">2023</a>; Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib7\" title=\"\">2023</a>)</cite>, we do not evaluate other downstream tasks such as spoken question answering or speech summarization, which may involve different dynamics in decoder attention. Second, our multilingual analysis is limited to English (a Germanic language) and Italian (a Romance language), due to the high computational cost of large-scale model training across a broader set of languages. Third, we focus on models trained from scratch but do not include architectures based on Speech Foundation Models (SFMs) paired with large language models (LLMs), often referred to as SpeechLLM&#8211;a recent growing area of interest in S2T modeling <cite class=\"ltx_cite ltx_citemacro_citep\">(Gaido et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib24\" title=\"\">2024b</a>)</cite>. As our analysis focuses on evaluation, our goal was to completely avoid data contamination issues <cite class=\"ltx_cite ltx_citemacro_citep\">(Sainz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib76\" title=\"\">2023</a>)</cite>, which is a problem affecting almost every SFM and SpeechLLM architectures currently available, as we have no control over their training data, and, for this reason, we decided to retrain the models from scratch.\nFourth, our analysis relies on SPES to compute reference explanations, acknowledging that, as an empirical method, it may introduce some margin of error. However, in the absence of a gold or human reference&#8211;which is unattainable in practice&#8211;we adopt SPES as a <span class=\"ltx_text ltx_font_italic\">silver</span> reference, since it represents the state of the art in explainability for speech-to-text. We further validate this choice in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A3\" title=\"Appendix C Quality Metrics for the Reported Models &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>, showing that SPES achieves very high quality explanations (deletion scores <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>90 on ASR and <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m2\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>3 on ST), making it a more faithful option than less robust alternatives from the generic XAI field such as gradient norms <cite class=\"ltx_cite ltx_citemacro_citep\">(Covert et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib15\" title=\"\">2021a</a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "deletion",
                    "large",
                    "asr",
                    "quality"
                ]
            }
        ]
    },
    "A4.T6": {
        "caption": "Table 6: Pearson œÅ\\rho correlation between layer-wise (‚Ñì\\ell) cross-attention and the explanations, and deletion score (ASR Del.) by varying the occlusion probability pHp_{H} of the monolingual ASR model on English (base) on the dev set. The layer average (‚Ñì\\ell-AVG) correlation is computed between the averaged cross-attention across layers (ùêÇùêÄ¬Ø(‚Ñì)\\widebar{\\text{{\\color[rgb]{0.75390625,0.390625,0.9609375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.75390625,0.390625,0.9609375}$\\mathbf{CA}$}}}^{(\\ell)}) and the encoder output explanations ùêíùêå\\mathbf{SM}HH. For each pHp_{H}, we also report the deletion metric. Bold indicates the highest correlation, underline indicates the highest layer-wise correlation.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Occlusion Prob.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math alttext=\"\\ell=1\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m1\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=1</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math alttext=\"\\ell=2\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m2\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=2</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math alttext=\"\\ell=3\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m3\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=3</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math alttext=\"\\ell=4\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m4\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=4</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math alttext=\"\\ell=5\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m5\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=5</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math alttext=\"\\ell=6\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m6\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">\\ell=6</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m7\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math><span class=\"ltx_text ltx_font_bold\">-AVG</span>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">ASR Del. <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><math alttext=\"p_{H}=0.9\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m9\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>H</mi></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding=\"application/x-tex\">p_{H}=0.9</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#F6F5D6;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F6F5D6;\">0.170</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#F3F3D2;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F3F3D2;\">0.224</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#D6E0AD;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D6E0AD;\">0.597</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#BBCC8A;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#BBCC8A;\">0.696</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#B5C783;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#B5C783;\">0.717</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"--ltx-bg-color:#C6D498;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#C6D498;\">0.656</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#AFC37A;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#AFC37A;\">0.741</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">58.20</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><math alttext=\"p_{H}=0.7\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m10\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>H</mi></msub><mo>=</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">p_{H}=0.7</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FAF7DA;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FAF7DA;\">0.116</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#F6F5D5;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F6F5D5;\">0.184</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#D8E2B0;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D8E2B0;\">0.589</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#BACB88;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#BACB88;\">0.701</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#AFC27A;\">\n<span class=\"ltx_ERROR undefined\">\\ultab</span><span class=\"ltx_text\" style=\"--ltx-bg-color:#AFC27A;\">0.742</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#BFCE8E;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#BFCE8E;\">0.684</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#ADC178;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#ADC178;\">0.746</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">61.45</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><math alttext=\"p_{H}=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m11\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>H</mi></msub><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">p_{H}=0.5</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FCF9DD;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FCF9DD;\">0.080</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#F8F6D8;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#F8F6D8;\">0.146</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#DFE6B8;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#DFE6B8;\">0.549</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#C6D498;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#C6D498;\">0.657</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#B9CA87;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#B9CA87;\">0.706</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#C7D499;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#C7D499;\">0.654</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#BACB89;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#BACB89;\">0.700</span></td>\n<td class=\"ltx_td ltx_align_center\">57.30</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><math alttext=\"p_{H}=0.3\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m12\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>H</mi></msub><mo>=</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">p_{H}=0.3</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FEFADF;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FEFADF;\">0.053</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#FAF8DB;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FAF8DB;\">0.111</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E3E9BE;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E3E9BE;\">0.480</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#DCE4B4;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#DCE4B4;\">0.576</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#CFDBA4;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#CFDBA4;\">0.624</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#DAE3B2;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#DAE3B2;\">0.583</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#D1DCA7;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#D1DCA7;\">0.614</span></td>\n<td class=\"ltx_td ltx_align_center\">50.45</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\"><math alttext=\"p_{H}=0.1\" class=\"ltx_Math\" display=\"inline\" id=\"A4.T6.m13\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>H</mi></msub><mo>=</mo><mn>0.1</mn></mrow><annotation encoding=\"application/x-tex\">p_{H}=0.1</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"--ltx-bg-color:#FEFAE0;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FEFAE0;\">0.039</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"--ltx-bg-color:#FBF8DC;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#FBF8DC;\">0.093</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"--ltx-bg-color:#E7EBC2;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E7EBC2;\">0.418</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"--ltx-bg-color:#E1E8BB;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E1E8BB;\">0.508</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"--ltx-bg-color:#DFE7B9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#DFE7B9;\">0.544</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"--ltx-bg-color:#E0E7BA;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E0E7BA;\">0.531</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\" style=\"--ltx-bg-color:#DFE7B9;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#DFE7B9;\">0.543</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">44.34</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "ph05ph05",
            "output",
            "pearson",
            "ph07ph07",
            "‚Ñì1ell1",
            "each",
            "ph03ph03",
            "prob",
            "‚Ñì2ell2",
            "varying",
            "underline",
            "monolingual",
            "del",
            "base",
            "also",
            "average",
            "probability",
            "computed",
            "encoder",
            "between",
            "layerwise",
            "across",
            "ùêÇùêÄ¬Ø‚Ñìwidebartextcolorrgb075390625039062509609375definecolornamedpgfstrokecolorrgb075390625039062509609375mathbfcaell",
            "ùêíùêåmathbfsmhh",
            "english",
            "report",
            "ultab0742",
            "averaged",
            "bold",
            "dev",
            "‚Ñì4ell4",
            "ph01ph01",
            "asr",
            "indicates",
            "‚Ñì6ell6",
            "metric",
            "‚Ñìell",
            "crossattention",
            "layers",
            "set",
            "correlation",
            "score",
            "occlusion",
            "œÅrho",
            "‚Üëuparrow",
            "deletion",
            "model",
            "ph09ph09",
            "highest",
            "layer",
            "phph",
            "explanations",
            "‚Ñì3ell3",
            "‚Ñìellavg",
            "‚Ñì5ell5"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">In Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A4.T6\" title=\"Table 6 &#8227; Appendix D Effect of Occlusion Probability on Encoder Output Explanations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>, we report the <span class=\"ltx_text ltx_font_italic\">deletion</span>\nmetric computed on the dev set and, for completeness, the results of the Pearson <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m1\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math> correlation with cross-attention <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math>.\nAnalogously to\nthe deletion metric computed on the input saliency maps <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>, we compute the deletion on the encoder output saliency maps by iteratively replacing\nportions of the encoder output sequence <math alttext=\"\\mathbf{H}=\\{\\mathbf{h}_{1},\\ldots,\\mathbf{h}_{I}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m3\" intent=\":literal\"><semantics><mrow><mi>&#119815;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><msub><mi>&#119841;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119841;</mi><mi>I</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{H}=\\{\\mathbf{h}_{1},\\ldots,\\mathbf{h}_{I}\\}</annotation></semantics></math> with zero vectors, and removing 5% of the most important time frames at each step based on their saliency.\nFrame importance is determined using saliency maps <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p2.m4\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>H</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}</annotation></semantics></math> aggregated at the sentence level.\nThe output quality is evaluated using the same <span class=\"ltx_text ltx_font_typewriter\">wer_max</span> scorer from the SPES repository. Lastly, we compute the area under the curve of the WER progression to quantify the faithfulness of the explanation.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Cross-attention is a core mechanism in encoder-decoder architectures, widespread in many fields, including speech-to-text (S2T) processing. Its scores have been repurposed for various downstream applications&#8211;such as timestamp estimation and audio-text alignment&#8211;under the assumption that they reflect the dependencies between input speech representation and the generated text. While the explanatory nature of attention mechanisms has been widely debated in the broader NLP literature, this assumption remains largely unexplored within the speech domain. To address this gap, we assess the explanatory power of cross-attention in S2T models by comparing its scores to input saliency maps derived from feature attribution. Our analysis spans monolingual and multilingual, single-task and multi-task models at multiple scales, and shows that attention scores moderately to strongly align with saliency-based explanations, particularly when aggregated across heads and layers. However, it also shows that cross-attention captures only about 50% of the input relevance and, in the best case, only partially reflects how the decoder attends to the encoder&#8217;s representations&#8211;accounting for just 52-75% of the saliency. These findings uncover fundamental limitations in interpreting cross-attention as an explanatory proxy, suggesting that it offers an informative yet incomplete view of the factors driving predictions in S2T models.</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "across",
                    "also",
                    "layers",
                    "explanations",
                    "between",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-attention <cite class=\"ltx_cite ltx_citemacro_citep\">(Bahdanau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib6\" title=\"\">2015</a>)</cite> is the core mechanism of the encoder-decoder Transformer architecture <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib85\" title=\"\">2017</a>)</cite>, a model that has become foundational across numerous AI domains <cite class=\"ltx_cite ltx_citemacro_citep\">(Galassi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib25\" title=\"\">2021</a>; Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib49\" title=\"\">2022</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib45\" title=\"\">2023</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib92\" title=\"\">2024b</a>; Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib51\" title=\"\">2024</a>)</cite>, including natural language processing (NLP).\nDesigned for modeling dependencies between the generated output sequence and the input representations, the cross-attention scores&#8211;derived from the attention mechanism&#8211;have been leveraged in various NLP tasks <cite class=\"ltx_cite ltx_citemacro_citep\">(Hu, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib32\" title=\"\">2020</a>; Zhang &amp; Kim, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib103\" title=\"\">2023</a>)</cite>, such as\nsource-target textual alignment <cite class=\"ltx_cite ltx_citemacro_citep\">(Garg et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib26\" title=\"\">2019</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib12\" title=\"\">2020</a>)</cite>, co-reference resolution <cite class=\"ltx_cite ltx_citemacro_citep\">(Voita et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib87\" title=\"\">2018</a>)</cite>, and word sense disambiguation <cite class=\"ltx_cite ltx_citemacro_citep\">(Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib83\" title=\"\">2018</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "output",
                    "model",
                    "between",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In speech-to-text (S2T) modeling, cross-attention scores have been widely repurposed for diverse downstream applications such as audio-text alignment <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib104\" title=\"\">2020</a>; Lee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib46\" title=\"\">2020</a>)</cite>, speaker identification <cite class=\"ltx_cite ltx_citemacro_citep\">(Kim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib38\" title=\"\">2019</a>)</cite>, timestamp estimation <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib47\" title=\"\">2022</a>; Louradour, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib50\" title=\"\">2023</a>; Zusag et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib106\" title=\"\">2024</a>)</cite>, and guiding simultaneous automatic speech recognition (ASR) and speech translation (ST) <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib91\" title=\"\">2024a</a>; Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib65\" title=\"\">2023a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib66\" title=\"\">b</a>)</cite>.\nThese applications rely on the implicit assumption that\ncross-attention reliably indicates what the model attends to in the input signal during output generation. However, despite its widespread use, this assumption has\nnever been verified.\nA key concern is that cross-attention operates over the encoder&#8217;s output sequence&#8211;rather than directly on the raw audio&#8211;which may have been reorganized or mixed with contextual information. This phenomenon, known as context mixing <cite class=\"ltx_cite ltx_citemacro_citep\">(Mohebbi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib60\" title=\"\">2023b</a>)</cite>, can potentially obscure the alignment between cross-attention weights and the original input signal.\nSimilar concerns have been extensively debated in the NLP community, where the reliability of attention mechanisms as explanations has been both challenged and defended, leading to conflicting perspectives and empirical evidence <cite class=\"ltx_cite ltx_citemacro_citep\">(Serrano &amp; Smith, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib78\" title=\"\">2019</a>; Jain &amp; Wallace, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib35\" title=\"\">2019</a>; Wiegreffe &amp; Pinter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib93\" title=\"\">2019</a>; Bastings &amp; Filippova, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib9\" title=\"\">2020</a>)</cite>. In contrast, this question remains largely underexplored in the speech domain.\nExisting work on explainability in S2T has primarily focused on self-attention <cite class=\"ltx_cite ltx_citemacro_citep\">(Shim et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib80\" title=\"\">2022</a>; Audhkhasi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib5\" title=\"\">2022</a>; A&#160;Shams et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib1\" title=\"\">2024</a>)</cite>, or on empirically measuring the effects of context mixing <cite class=\"ltx_cite ltx_citemacro_citep\">(Mohebbi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib59\" title=\"\">2023a</a>)</cite>, without directly assessing the explanatory potential of cross-attention mechanisms.</p>\n\n",
                "matched_terms": [
                    "output",
                    "model",
                    "asr",
                    "indicates",
                    "explanations",
                    "between",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address this gap, we present the first systematic\nanalysis of cross-attention as a proxy for input-output dependencies in S2T models.\nOur study serves two main objectives: <span class=\"ltx_text ltx_font_italic\">i)</span> assessing the validity of using cross-attention as a surrogate for input-output alignment,\nand <span class=\"ltx_text ltx_font_italic\">ii)</span> evaluating whether it provides insights comparable to formal explainability methods such as feature attribution&#8211;while being more lightweight and less computationally expensive to obtain <cite class=\"ltx_cite ltx_citemacro_citep\">(Samek et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib77\" title=\"\">2021</a>; Madsen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib53\" title=\"\">2022</a>)</cite>. We compare cross-attention scores with input saliency maps derived from SPES <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>, the current state-of-the-art feature-attribution method in S2T, to determine the extent to which\ncross-attention\ncaptures\nwhich input features are relevant for models&#8217; predictions.\nIn addition, we compute saliency maps on\nencoder outputs and compare them with cross-attention scores to evaluate whether cross-attention fully explains how the decoder uses encoded representations, avoiding potential discrepancies\nof context mixing.\nOur\nanalysis spans ASR and ST tasks across monolingual, multilingual, and multitask settings using\nstate-of-the-art speech processing\narchitectures <cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib29\" title=\"\">2020</a>)</cite> at multiple scales.\nWith consistent trends across different settings, we find\nthat cross-attention exhibits moderate to strong correlations with input saliency maps and aligns more closely with encoder output representations, suggesting an influence of context mixing. However, our results also indicate that the overall explanatory power of cross-attention is limited&#8211;accounting for only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>50% of input relevance and, at best, 52-75% of encoder output saliency.\nOur findings uncover fundamental limitations in interpreting cross-attention as an explanatory proxy, suggesting that it provides an informative yet incomplete view of the factors driving predictions in S2T models.\n</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "across",
                    "also",
                    "output",
                    "encoder",
                    "asr",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Explainability in Speech-to-Text.</span>\nExplainable AI (XAI) has emerged to make model behavior more interpretable to humans, thereby supporting informed decision-making and responsible deployment <cite class=\"ltx_cite ltx_citemacro_citep\">(Barredo Arrieta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib8\" title=\"\">2020</a>)</cite>.\nWhile XAI research has seen a rapid growth in the last years across multiple modalities, including vision and language <cite class=\"ltx_cite ltx_citemacro_citep\">(Sharma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib79\" title=\"\">2024</a>)</cite>, progress in the speech domain has lagged.\nThis gap arises from the inherent complexities of speech processing, including the multidimensional nature of speech signals across time and frequency, and the variability in output sequence length <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib95\" title=\"\">2024</a>)</cite>.\nDespite these challenges, growing concerns about trustworthiness\nare driving explainability efforts in speech classification <cite class=\"ltx_cite ltx_citemacro_citep\">(Becker et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib10\" title=\"\">2024</a>; Pastor et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib68\" title=\"\">2024</a>)</cite> and S2T generation <cite class=\"ltx_cite ltx_citemacro_citep\">(Mandel, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib54\" title=\"\">2016</a>; Kavaki &amp; Mandel, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib37\" title=\"\">2020</a>; Trinh &amp; Mandel, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib84\" title=\"\">2020</a>; Markert et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib55\" title=\"\">2021</a>; Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib94\" title=\"\">2023</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib95\" title=\"\">2024</a>; Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>. Most of these works rely on perturbation-based methods\nthat assess how input modifications affect model predictions <cite class=\"ltx_cite ltx_citemacro_citep\">(Covert et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib16\" title=\"\">2021b</a>; Ivanovs et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib34\" title=\"\">2021</a>)</cite>.\nAmong these,\n<cite class=\"ltx_cite ltx_citemacro_citet\">Fucci et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite> recently proposed\na technique for autoregressive S2T models that identifies regions of the spectrogram that most influence predictions to generate saliency maps.\nHowever, XAI methods are generally computationally expensive&#8211;especially perturbation-based approaches applied to large models <cite class=\"ltx_cite ltx_citemacro_citep\">(Luo &amp; Specia, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib52\" title=\"\">2024</a>; Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib102\" title=\"\">2025</a>)</cite>&#8211;which motivates exploring whether cross-attention, already computed at inference time, could serve as a lightweight alternative in a landscape still lacking efficient explainability tools for speech-based models.</p>\n\n",
                "matched_terms": [
                    "across",
                    "output",
                    "model",
                    "computed",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Attention as Explanation.</span>\nAttention mechanisms have been widely used to probe model behavior in text-based NLP, as attention scores often align with human intuitions about relevance and salience <cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib13\" title=\"\">2019</a>; Ferrando et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib21\" title=\"\">2024</a>)</cite>.\nEarly studies proposed norm-based analyses to improve the interpretability of attention weights <cite class=\"ltx_cite ltx_citemacro_citep\">(Kobayashi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib39\" title=\"\">2020</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib40\" title=\"\">2021</a>; Mohebbi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib58\" title=\"\">2021</a>; Ferrando et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib20\" title=\"\">2022b</a>)</cite>, while others suggested aggregating attention across layers and heads to quantify input-output influence more systematically <cite class=\"ltx_cite ltx_citemacro_citep\">(Abnar &amp; Zuidema, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib2\" title=\"\">2020</a>; Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib101\" title=\"\">2021</a>; Chefer et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib11\" title=\"\">2021</a>)</cite>.\nWhile some have raised concerns about whether attention reliably reflects which inputs are actually responsible for outputs <cite class=\"ltx_cite ltx_citemacro_citep\">(Jain &amp; Wallace, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib35\" title=\"\">2019</a>; Serrano &amp; Smith, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib78\" title=\"\">2019</a>; Bastings &amp; Filippova, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib9\" title=\"\">2020</a>)</cite>, others have proposed conditions under which attention can meaningfully explain model behavior <cite class=\"ltx_cite ltx_citemacro_citep\">(Wiegreffe &amp; Pinter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib93\" title=\"\">2019</a>)</cite>. More recent work highlights that attention aggregation may obscure localized, token-specific interactions <cite class=\"ltx_cite ltx_citemacro_citep\">(Modarressi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib57\" title=\"\">2023</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib100\" title=\"\">2023</a>; Oh &amp; Schuler, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib63\" title=\"\">2023</a>)</cite>, motivating hybrid approaches that combine attention with other XAI techniques, such as attribution methods <cite class=\"ltx_cite ltx_citemacro_citep\">(Modarressi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib56\" title=\"\">2022</a>)</cite>, or the use of attention as a regularization signal during interpretability-driven training <cite class=\"ltx_cite ltx_citemacro_citep\">(Xie et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib97\" title=\"\">2024</a>)</cite>.\nDespite the ongoing efforts, most research has focused on self-attention within encoders, with limited attention to feed-forward dynamics <cite class=\"ltx_cite ltx_citemacro_citep\">(Kobayashi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib41\" title=\"\">2024</a>)</cite> and even less to encoder-decoder models. A few studies have investigated attention in encoder-decoder architectures <cite class=\"ltx_cite ltx_citemacro_citep\">(Nguyen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib62\" title=\"\">2021</a>)</cite>, including in machine translation <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib105\" title=\"\">2024</a>)</cite>, but cross-attention remains largely underexplored in the speech domain and has been\nabsent from the broader <span class=\"ltx_text ltx_inline-quote ltx_outerquote\">&#8220;attention as explanation&#8221;</span> debate in NLP. Our work seeks to bridge this gap by bringing cross-attention of S2T models into this broader conversation, aiming to assess\nwhether\nit can serve as a reliable explanation&#8211;and where its limitations emerge.\n</p>\n\n",
                "matched_terms": [
                    "across",
                    "model",
                    "layers",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We assess the extent to which cross-attention scores (<math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math>) explain how the model looks at\ninput features when generating a token\nby comparing them to the saliency map on the input <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math>, obtained with the state-of-the art feature-attribution method for S2T, SPES <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additionally, to assess whether cross-attention more accurately reflects how the decoder accesses encoded representations&#8211;rather than capturing the model&#8217;s full input-output behavior&#8211;we compare <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> with the encoder-output saliency map <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m2\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>H</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}</annotation></semantics></math>.\nBy analyzing how the correlation between <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> and <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m4\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>H</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}</annotation></semantics></math> deviates from that with <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p2.m5\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math>,\nwe can indirectly quantify the impact of context mixing in the resulting explanations.\nA visual overview of this setup is provided in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S3.F1\" title=\"Figure 1 &#8227; 3 Methodology &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "explanations",
                    "correlation",
                    "crossattention",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In S2T models, the cross-attention mechanism enables each decoder token to integrate relevant portions of the encoded speech features, thereby conditioning generation on the entire input.</p>\n\n",
                "matched_terms": [
                    "each",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Let <math alttext=\"\\mathbf{X}\\in\\mathbb{R}^{T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}\\in\\mathbb{R}^{T\\times F}</annotation></semantics></math> denote the speech input represented by mel-spectrogram features, where <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> is the number of time frames and <math alttext=\"F\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\"><semantics><mi>F</mi><annotation encoding=\"application/x-tex\">F</annotation></semantics></math> the number of frequency bins. The encoder processes <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m4\" intent=\":literal\"><semantics><mi>&#119831;</mi><annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation></semantics></math> into a sequence of hidden representations <math alttext=\"\\mathbf{H}=\\mathrm{Encoder}(\\mathbf{X})\\in\\mathbb{R}^{T^{\\prime}\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><mi>&#119815;</mi><mo>=</mo><mrow><mi>Encoder</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119831;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{H}=\\mathrm{Encoder}(\\mathbf{X})\\in\\mathbb{R}^{T^{\\prime}\\times D}</annotation></semantics></math>, where <math alttext=\"T^{\\prime}&lt;T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m6\" intent=\":literal\"><semantics><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo>&lt;</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">T^{\\prime}&lt;T</annotation></semantics></math> reflects the number of encoder time steps after subsampling with a factor of <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m7\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math><span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>As the length of the speech inputs is, in general, 10<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"footnote1.m1\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math> longer that of the corresponding textual input, it is a common practice in S2T modeling to downsample the input through convolutional modules <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib88\" title=\"\">2020</a>)</cite>.</span></span></span> and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m8\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> is the hidden dimensionality. The decoder then autoregressively generates an output sequence <math alttext=\"\\mathbf{y}=(y_{0},y_{1},\\ldots,y_{I})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m9\" intent=\":literal\"><semantics><mrow><mi>&#119858;</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mn>0</mn></msub><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>y</mi><mi>I</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{y}=(y_{0},y_{1},\\ldots,y_{I})</annotation></semantics></math> of length <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m10\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math>, where each token <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m11\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math> is predicted based on the previously generated tokens <math alttext=\"y_{&lt;i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m12\" intent=\":literal\"><semantics><msub><mi>y</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">y_{&lt;i}</annotation></semantics></math> and the encoder output <math alttext=\"\\mathbf{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m13\" intent=\":literal\"><semantics><mi>&#119815;</mi><annotation encoding=\"application/x-tex\">\\mathbf{H}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "output",
                    "each",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At each decoder layer <math alttext=\"\\ell\\in\\{1,\\ldots,L\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\ell\\in\\{1,\\ldots,L\\}</annotation></semantics></math>, cross-attention scores are computed via dot-product attention <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib28\" title=\"\">2014</a>)</cite> between the decoder&#8217;s current hidden states <math alttext=\"\\mathbf{B}^{(\\ell)}\\in\\mathbb{R}^{I\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><msup><mi>&#119809;</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{B}^{(\\ell)}\\in\\mathbb{R}^{I\\times D}</annotation></semantics></math> and the encoder outputs <math alttext=\"\\mathbf{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m3\" intent=\":literal\"><semantics><mi>&#119815;</mi><annotation encoding=\"application/x-tex\">\\mathbf{H}</annotation></semantics></math>. Specifically, the decoder states are linearly projected to queries <math alttext=\"\\mathbf{Q}^{(\\ell)}=\\mathbf{B}^{(\\ell)}\\mathbf{W}_{Q}^{(\\ell)}\\in\\mathbb{R}^{I\\times d_{k}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m4\" intent=\":literal\"><semantics><mrow><msup><mi>&#119824;</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><mrow><msup><mi>&#119809;</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msubsup><mi>&#119830;</mi><mi>Q</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{Q}^{(\\ell)}=\\mathbf{B}^{(\\ell)}\\mathbf{W}_{Q}^{(\\ell)}\\in\\mathbb{R}^{I\\times d_{k}}</annotation></semantics></math>, while the encoder outputs are projected to keys <math alttext=\"\\mathbf{K}^{(\\ell)}=\\mathbf{H}\\mathbf{W}_{K}^{(\\ell)}\\in\\mathbb{R}^{T^{\\prime}\\times d_{k}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m5\" intent=\":literal\"><semantics><mrow><msup><mi>&#119818;</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo>=</mo><msubsup><mi>&#119815;&#119830;</mi><mi>K</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{K}^{(\\ell)}=\\mathbf{H}\\mathbf{W}_{K}^{(\\ell)}\\in\\mathbb{R}^{T^{\\prime}\\times d_{k}}</annotation></semantics></math>\nusing learned projection matrices <math alttext=\"\\mathbf{W}_{Q}^{(\\ell)},\\mathbf{W}_{K}^{(\\ell)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m6\" intent=\":literal\"><semantics><mrow><msubsup><mi>&#119830;</mi><mi>Q</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>&#119830;</mi><mi>K</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{W}_{Q}^{(\\ell)},\\mathbf{W}_{K}^{(\\ell)}</annotation></semantics></math>. The resulting cross-attention matrix <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m7\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> is:</p>\n\n",
                "matched_terms": [
                    "layer",
                    "computed",
                    "each",
                    "encoder",
                    "between",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where each row <math alttext=\"\\text{{\\color[rgb]{0.75390625,0.390625,0.9609375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.75390625,0.390625,0.9609375}$\\mathbf{CA}$}}^{(\\ell)}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m8\" intent=\":literal\"><semantics><msubsup><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.75390625,0.390625,0.9609375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.75390625,0.390625,0.9609375}$\\mathbf{CA}$}}^{(\\ell)}_{i}</annotation></semantics></math> represents the attention distribution over encoder time steps for the generation of output token <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m9\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math> at layer <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m10\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>.\nTo capture diverse patterns, Transformer-based models employ multi-head attention <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib85\" title=\"\">2017</a>)</cite>. Each head <math alttext=\"h\\in\\{1,\\ldots,H\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m11\" intent=\":literal\"><semantics><mrow><mi>h</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>H</mi><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">h\\in\\{1,\\ldots,H\\}</annotation></semantics></math> uses separate learned projections:</p>\n\n",
                "matched_terms": [
                    "output",
                    "layer",
                    "each",
                    "encoder",
                    "‚Ñìell"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Extracting\nthe full set of scores provides a fine-grained view of how each output token in the generated hypothesis attends to the encoder&#8217;s representations across all layers and heads.\nTo derive a single layer-wise or head-wise attention distribution, we compute the mean of the attention matrices over a subset <math alttext=\"\\mathcal{S}\\subseteq\\{1,\\dots,L\\}\\times\\{1,\\dots,H\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>&#8838;</mo><mrow><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo rspace=\"0.055em\" stretchy=\"false\">}</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>H</mi><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}\\subseteq\\{1,\\dots,L\\}\\times\\{1,\\dots,H\\}</annotation></semantics></math> of layers and heads:</p>\n\n",
                "matched_terms": [
                    "layerwise",
                    "set",
                    "across",
                    "output",
                    "each",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By selecting different index sets <math alttext=\"\\mathcal{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><annotation encoding=\"application/x-tex\">\\mathcal{S}</annotation></semantics></math>, this formulation yields layer-wise, head-wise, or global averages. For example, setting <math alttext=\"\\mathcal{S}=\\{(\\ell,h):h=1,\\dots,H\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m3\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo>,</mo><mi>h</mi><mo rspace=\"0.278em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.278em\">:</mo><mrow><mi>h</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>H</mi></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}=\\{(\\ell,h):h=1,\\dots,H\\}</annotation></semantics></math> gives the average across heads at a given layer <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>; <math alttext=\"\\mathcal{S}=\\{(\\ell,h):\\ell=1,\\dots,L\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m5\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">&#8467;</mi><mo>,</mo><mi>h</mi><mo rspace=\"0.278em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.278em\">:</mo><mrow><mi mathvariant=\"normal\">&#8467;</mi><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}=\\{(\\ell,h):\\ell=1,\\dots,L\\}</annotation></semantics></math> averages across layers for head <math alttext=\"h\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m6\" intent=\":literal\"><semantics><mi>h</mi><annotation encoding=\"application/x-tex\">h</annotation></semantics></math>; and <math alttext=\"\\mathcal{S}=\\{1,\\dots,L\\}\\times\\{1,\\dots,H\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m7\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>=</mo><mrow><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>L</mi><mo rspace=\"0.055em\" stretchy=\"false\">}</mo></mrow><mo rspace=\"0.222em\">&#215;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>H</mi><mo stretchy=\"false\">}</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{S}=\\{1,\\dots,L\\}\\times\\{1,\\dots,H\\}</annotation></semantics></math> computes the full average.\nThis averaged attention provides a more aggregated view of the model&#8217;s attention patterns at a specific layer or attention head, summarizing how the model attends to the input speech over time.</p>\n\n",
                "matched_terms": [
                    "layerwise",
                    "across",
                    "average",
                    "model",
                    "averaged",
                    "layer",
                    "‚Ñìell",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better understand how S2T models associate individual output tokens with specific regions of the raw speech input or of the model&#8217;s inner representations (e.g., the encoder output), we employ <span class=\"ltx_text ltx_font_italic\">feature-attribution</span> techniques that produce token-level <span class=\"ltx_text ltx_font_italic\">saliency maps</span>. These maps quantify the relevance of different portions of the input sequence in determining the model&#8217;s predictions.</p>\n\n",
                "matched_terms": [
                    "output",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Let again <math alttext=\"\\mathbf{X}\\in\\mathbb{R}^{T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119831;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{X}\\in\\mathbb{R}^{T\\times F}</annotation></semantics></math> denote a mel-spectrogram input, where <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> is the number of time frames and <math alttext=\"F\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mi>F</mi><annotation encoding=\"application/x-tex\">F</annotation></semantics></math> the number of frequency bins, and <math alttext=\"\\mathbf{y}=(y_{0},y_{1},\\ldots,y_{I})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#119858;</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mn>0</mn></msub><mo>,</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>y</mi><mi>I</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{y}=(y_{0},y_{1},\\ldots,y_{I})</annotation></semantics></math> the sequence of length <math alttext=\"I\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mi>I</mi><annotation encoding=\"application/x-tex\">I</annotation></semantics></math> of the autoregressively-generated tokens predicted based on the input and the previously generated tokens <math alttext=\"y_{&lt;i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m6\" intent=\":literal\"><semantics><msub><mi>y</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><annotation encoding=\"application/x-tex\">y_{&lt;i}</annotation></semantics></math>.\nTo attribute the prediction of each token <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m7\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math> to specific parts of the input spectrogram, we adopt SPES&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>,\nthe state-of-the-art feature-attribution method designed for autoregressive S2T modeling. SPES assigns a saliency score to each time-frequency\nelement\nof <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m8\" intent=\":literal\"><semantics><mi>&#119831;</mi><annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation></semantics></math>, producing a saliency map <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{X}\\in\\mathbb{R}^{T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m9\" intent=\":literal\"><semantics><mrow><msubsup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>i</mi><mi>X</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{X}\\in\\mathbb{R}^{T\\times F}</annotation></semantics></math> for each token <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m10\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math>, where higher values indicate greater relevance of the corresponding time-frequency regions. SPES operates by clustering spectrogram elements based on energy profiles&#8211;capturing acoustic\ncomponents such as harmonics and background noise&#8211;and estimating the influence of each cluster by perturbing it with probability <math alttext=\"p_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m11\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">p_{X}</annotation></semantics></math>, repeated <math alttext=\"N_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m12\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">N_{X}</annotation></semantics></math> times. The effect of each perturbation&#8211;i.e., masking parts of the input with 0 values&#8211;is measured by computing the Kullback-Leibler (KL) divergence <cite class=\"ltx_cite ltx_citemacro_citep\">(Kullback &amp; Leibler, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib44\" title=\"\">1951</a>)</cite> between the model&#8217;s original output distribution <math alttext=\"P(y_{i}\\mid y_{&lt;i},\\mathbf{X})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m13\" intent=\":literal\"><semantics><mrow><mi>P</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>&#8739;</mo><mrow><msub><mi>y</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><mo>,</mo><mi>&#119831;</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P(y_{i}\\mid y_{&lt;i},\\mathbf{X})</annotation></semantics></math> and the distribution resulting from the perturbed input <math alttext=\"P^{(n)}(y_{i}\\mid y_{&lt;i},\\tilde{\\mathbf{X}}^{(n)})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m14\" intent=\":literal\"><semantics><mrow><msup><mi>P</mi><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>&#8739;</mo><mrow><msub><mi>y</mi><mrow><mi/><mo>&lt;</mo><mi>i</mi></mrow></msub><mo>,</mo><msup><mover accent=\"true\"><mi>&#119831;</mi><mo>~</mo></mover><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></msup></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">P^{(n)}(y_{i}\\mid y_{&lt;i},\\tilde{\\mathbf{X}}^{(n)})</annotation></semantics></math> at time <math alttext=\"n\\in\\{1,\\ldots,N_{X}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m15\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">{</mo><mn>1</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>N</mi><mi>X</mi></msub><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">n\\in\\{1,\\ldots,N_{X}\\}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "score",
                    "output",
                    "probability",
                    "each",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The divergence scores are then mapped back to the corresponding cluster positions in the spectrogram and aggregated to form the token-specific saliency map <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{X}\\in\\mathbb{R}^{T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m16\" intent=\":literal\"><semantics><mrow><msubsup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>i</mi><mi>X</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{X}\\in\\mathbb{R}^{T\\times F}</annotation></semantics></math>. Stacking all saliency maps across the output sequence <math alttext=\"\\mathbf{y}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px1.p1.m17\" intent=\":literal\"><semantics><mi>&#119858;</mi><annotation encoding=\"application/x-tex\">\\mathbf{y}</annotation></semantics></math> yields a 3D saliency map:</p>\n\n",
                "matched_terms": [
                    "across",
                    "output"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further examine the influence of the encoder&#8217;s internal representations on the prediction of each output token.\nLet again <math alttext=\"\\mathbf{H}=\\mathrm{Encoder}(\\mathbf{X})\\in\\mathbb{R}^{T^{\\prime}\\times D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119815;</mi><mo>=</mo><mrow><mi>Encoder</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>&#119831;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf{H}=\\mathrm{Encoder}(\\mathbf{X})\\in\\mathbb{R}^{T^{\\prime}\\times D}</annotation></semantics></math> denote the sequence of encoder hidden states or <span class=\"ltx_text ltx_font_italic\">encoder output</span>, where <math alttext=\"T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><msup><mi>T</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">T^{\\prime}</annotation></semantics></math> is the subsampled time dimension and <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math> is the hidden dimension.\nTo assess the importance of the encoder output representations, we compute token-specific saliency maps <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{H}\\in\\mathbb{R}^{T^{\\prime}\\times 1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m4\" intent=\":literal\"><semantics><mrow><msubsup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>i</mi><mi>H</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{H}\\in\\mathbb{R}^{T^{\\prime}\\times 1}</annotation></semantics></math>, where each entry reflects the contribution of the corresponding hidden state to the generation of <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m5\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math>.\nEach encoder state <math alttext=\"\\mathbf{H}=(\\mathbf{h}_{1},\\ldots,\\mathbf{h}_{T^{\\prime}})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m6\" intent=\":literal\"><semantics><mrow><mi>&#119815;</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119841;</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>&#119841;</mi><msup><mi>T</mi><mo>&#8242;</mo></msup></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{H}=(\\mathbf{h}_{1},\\ldots,\\mathbf{h}_{T^{\\prime}})</annotation></semantics></math> is perturbed&#8211;i.e., all its features are set to 0&#8211;independently with probability <math alttext=\"p_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m7\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">p_{H}</annotation></semantics></math>, and the process is repeated <math alttext=\"N_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m8\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">N_{H}</annotation></semantics></math> times. The KL divergence is computed for each perturbation between the original and perturbed output distributions:</p>\n\n",
                "matched_terms": [
                    "set",
                    "output",
                    "probability",
                    "computed",
                    "each",
                    "encoder",
                    "phph",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m9\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>H</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}</annotation></semantics></math> captures the temporal relevance of the encoder&#8217;s internal sequence representations for each output token <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS0.Px2.p1.m10\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "output",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since our focus lies in the temporal dynamics of the input <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mi>&#119831;</mi><annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation></semantics></math>, we aggregate the 3D saliency scores <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\\in\\mathbb{R}^{I\\times T\\times F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>T</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>F</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\\in\\mathbb{R}^{I\\times T\\times F}</annotation></semantics></math> across the frequency dimension and downsample the time axis to produce a compressed representation <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\\in\\mathbb{R}^{I\\times T^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>T</mi><mo>&#8242;</mo></msup></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\\in\\mathbb{R}^{I\\times T^{\\prime}}</annotation></semantics></math> compatible with the cross-attention granularity, where <math alttext=\"T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m4\" intent=\":literal\"><semantics><msup><mi>T</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">T^{\\prime}</annotation></semantics></math> corresponds to the number of encoder time steps. The aggregation is performed by taking the maximum saliency value over the frequency axis and within each corresponding time window. The resulting saliency map of each token <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{X}\\in\\mathbb{R}^{T^{\\prime}\\times 1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><msubsup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>i</mi><mi>X</mi></msubsup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><msup><mi>T</mi><mo>&#8242;</mo></msup><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}_{i}^{X}\\in\\mathbb{R}^{T^{\\prime}\\times 1}</annotation></semantics></math> reflects the temporal relevance of the input spectrogram with respect to the generation of token <math alttext=\"y_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m6\" intent=\":literal\"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">y_{i}</annotation></semantics></math>. Complementary experiments on the choice of the aggregation function are presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A1\" title=\"Appendix A Effect of Aggregation Functions on Input Explanations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>.\nBoth <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m7\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> and <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m8\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math> representations are normalized before computing the correlation scores, and the beginning and end of sentence are removed as they are not relevant for the analysis.\nThe <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m9\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> matrix is normalized frame-wise using mean-variance normalization to mitigate the impact of potential attention sinks at initial or final tokens&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib13\" title=\"\">2019</a>; Ferrando et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib19\" title=\"\">2022a</a>; Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib65\" title=\"\">2023a</a>; Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib96\" title=\"\">2024</a>)</cite> on the correlation computation.\nBoth <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m10\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math> and <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m11\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>H</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{H}</annotation></semantics></math> are normalized along the token dimension using the strategy proposed by <cite class=\"ltx_cite ltx_citemacro_citet\">Fucci et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>, as saliency scores can vary widely across tokens due to differences in the original output distributions used to compute the KL divergence.</p>\n\n",
                "matched_terms": [
                    "across",
                    "correlation",
                    "output",
                    "each",
                    "encoder",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following prior work on cross-attention matrices <cite class=\"ltx_cite ltx_citemacro_citep\">(Vig &amp; Belinkov, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib86\" title=\"\">2019</a>)</cite> and explainable AI <cite class=\"ltx_cite ltx_citemacro_citep\">(Eberle et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib18\" title=\"\">2023</a>)</cite>, we use Pearson correlation\nto quantify the relationship between cross-attention scores and saliency-based explanations.\nPearson correlation is preferred over Kendall and Spearman because saliency scores are continuous, and their magnitude&#8211;not just ranking&#8211;is crucial. Rank-based measures are overly sensitive to small fluctuations among non-important features with near-zero scores, while Pearson better captures whether features are identified as important (high score) or not (low score).\nSpecifically, given the two representations <math alttext=\"\\text{{\\color[rgb]{0.75390625,0.390625,0.9609375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.75390625,0.390625,0.9609375}$\\mathbf{CA}$}},\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}\\in\\mathbb{R}^{I\\times T^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><mo>,</mo><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi></mrow><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>T</mi><mo>&#8242;</mo></msup></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.75390625,0.390625,0.9609375}\\definecolor[named]{pgfstrokecolor}{rgb}{0.75390625,0.390625,0.9609375}$\\mathbf{CA}$}},\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}\\in\\mathbb{R}^{I\\times T^{\\prime}}</annotation></semantics></math>, we compute the Pearson correlation coefficient <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m2\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math> to assess the similarity of their attribution patterns across output tokens and time steps. We first flatten each matrix into a vector of size <math alttext=\"I\\cdot T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#8901;</mo><msup><mi>T</mi><mo>&#8242;</mo></msup></mrow><annotation encoding=\"application/x-tex\">I\\cdot T^{\\prime}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "across",
                    "correlation",
                    "score",
                    "œÅrho",
                    "output",
                    "pearson",
                    "each",
                    "explanations",
                    "between",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Then, the Pearson correlation coefficient <math alttext=\"\\rho\\in[-1,1]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><mi>&#961;</mi><mo>&#8712;</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mo>&#8722;</mo><mn>1</mn></mrow><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\rho\\in[-1,1]</annotation></semantics></math> is computed as:</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "computed",
                    "pearson"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This scalar value quantifies the linear relationship between the two saliency maps, with values closer to 1 indicating a strong positive correlation, and values near 0 indicating no correlation.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To avoid potential data contamination issues <cite class=\"ltx_cite ltx_citemacro_citep\">(Sainz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib76\" title=\"\">2023</a>)</cite>, we train from scratch a monolingual ASR model and two-sized multitask (ASR and ST) and multilingual (English and Italian) models. Details about training data and process are presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A2\" title=\"Appendix B Training Settings &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.\nBeing the only non-synthetic dataset supporting both tasks and language directions, we select EuroParl-ST <cite class=\"ltx_cite ltx_citemacro_citep\">(Iranzo-S&#225;nchez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib33\" title=\"\">2020</a>)</cite> as the test set for our analyses. The test set covers both <span class=\"ltx_text ltx_font_italic\">en</span> and <span class=\"ltx_text ltx_font_italic\">it</span> ASR, and <span class=\"ltx_text ltx_font_italic\">en-it</span> and <span class=\"ltx_text ltx_font_italic\">it-en</span> ST.\nThe <span class=\"ltx_text ltx_font_italic\">it/it-en</span> section consists of 1,686 segments, for a total of approximately 6 hours of audio, while the <span class=\"ltx_text ltx_font_italic\">en/en-it</span> section contains 1,130 segments, for a total of approximately 3 hours of audio.</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "set",
                    "english",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The models analyzed in the paper are all composed of a Conformer encoder <cite class=\"ltx_cite ltx_citemacro_citep\">(Gulati et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib29\" title=\"\">2020</a>)</cite> and a Transformer decoder, as Conformer is the current state-of-the-art architecture for S2T processing <cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib30\" title=\"\">2021</a>; Srivastava et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib81\" title=\"\">2022</a>; Li &amp; Doddipatla, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib48\" title=\"\">2023</a>)</cite>.\nThe monolingual ASR model (<span class=\"ltx_text ltx_font_typewriter\">base</span>) is composed of 12 encoder layers and 6 decoder layers. Each layer has 8 attention heads, 512 as embedding dimension, and FFNs dimension of 2,048. The vocabulary is built using a SentencePiece unigram model <cite class=\"ltx_cite ltx_citemacro_citep\">(Kudo &amp; Richardson, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib42\" title=\"\">2018</a>)</cite> with size 8,000 trained on <span class=\"ltx_text ltx_font_italic\">en</span> transcripts. The resulting number of parameters is 125M.\nThe multitask and multilingual models are of two sizes, <span class=\"ltx_text ltx_font_typewriter\">small</span> and <span class=\"ltx_text ltx_font_typewriter\">large</span>, the first having 12 encoder layers and 6 decoder layers and the latter having 24 encoder layers and 12 decoder layers. In both sizes, each layer has 16 attention heads, an embedding dimension of 1,024, and an FFN dimension of 4,096. The vocabulary is built using a SentencePiece unigram model with size 16,000 trained on <span class=\"ltx_text ltx_font_italic\">en</span> and <span class=\"ltx_text ltx_font_italic\">it</span> transcripts. Two extra tokens&#8211;<span class=\"ltx_text ltx_font_typewriter\">&lt;lang:en&gt;</span> and <span class=\"ltx_text ltx_font_typewriter\">&lt;lang:it&gt;</span>&#8211;are added to indicate whether the target text is in <span class=\"ltx_text ltx_font_italic\">en</span> or <span class=\"ltx_text ltx_font_italic\">it</span>. The resulting number of parameters is 474M for the <span class=\"ltx_text ltx_font_typewriter\">small</span> model and 878M for the <span class=\"ltx_text ltx_font_typewriter\">large</span> model.\nIn all models, the Conformer encoder is preceded by two 1D convolutional layers with stride 2 and kernel size 5, resulting in a fixed subsampling factor <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> of 4. The kernel size of the Conformer convolutional module is 31 for both the point- and depth-wise convolutions. The input audio is represented by 80 Mel-filterbank features extracted every 10 ms with a window of 25 ms.</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "base",
                    "model",
                    "layer",
                    "each",
                    "encoder",
                    "asr",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Hypothesis and Cross-Attention Generation.</span>\nFor the hypothesis generation, we use beam search with a beam size of 5 and a no-repeat n-gram size of 5. The attention scores are extracted from layers or heads during the output generation. The ASR and ST quality scores of the hypotheses are presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A3\" title=\"Appendix C Quality Metrics for the Reported Models &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.\nThe inference is performed using a single NVIDIA A40 GPU (40GB RAM) with a batch size of 40,000 tokens and takes <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>2.5 minutes for <span class=\"ltx_text ltx_font_typewriter\">base</span>, <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>3-5.5 minutes for <span class=\"ltx_text ltx_font_typewriter\">small</span>, and <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>3-6.5 for <span class=\"ltx_text ltx_font_typewriter\">large</span>, depending on the source language.</p>\n\n",
                "matched_terms": [
                    "base",
                    "output",
                    "layers",
                    "asr",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Explanation Heatmaps Generation.</span>\nFollowing the best configuration obtained in SPES <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>, we adopt the Morphological Fragmental Perturbation Pyramid <cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib99\" title=\"\">2021</a>)</cite> for clustering, which relies on Simple Linear Iterative Clustering or SLIC <cite class=\"ltx_cite ltx_citemacro_citep\">(Achanta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib3\" title=\"\">2012</a>)</cite>, a k-means-based algorithm that groups elements according to spectral patterns. We use the default parameters; the threshold length in seconds is\n7.50s, the\n<a class=\"ltx_ref ltx_href\" href=\"https://scikit-image.org/docs/dev/api/skimage.segmentation.html#skimage.segmentation.slic\" title=\"\">SLIC</a>\nsigma is 0, the compactness is 0.1, and the number of patches per second for the MFPP technique is [400, 500, 600].\nFor the choice of <math alttext=\"p_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">p_{X}</annotation></semantics></math> and <math alttext=\"N_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m2\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">N_{X}</annotation></semantics></math>, we refer to the parameters used in <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>, setting <math alttext=\"p_{X}=0.5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>X</mi></msub><mo>=</mo><mn>0.5</mn></mrow><annotation encoding=\"application/x-tex\">p_{X}=0.5</annotation></semantics></math> and <math alttext=\"N_{X}=20,000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><msub><mi>N</mi><mi>X</mi></msub><mo>=</mo><mrow><mn>20</mn><mo>,</mo><mn>000</mn></mrow></mrow><annotation encoding=\"application/x-tex\">N_{X}=20,000</annotation></semantics></math>. The quality of the input explanations is presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A3\" title=\"Appendix C Quality Metrics for the Reported Models &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>. For the choice of <math alttext=\"p_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m5\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">p_{H}</annotation></semantics></math> and <math alttext=\"N_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m6\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">N_{H}</annotation></semantics></math>, we use the same number of iterations of <math alttext=\"N_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m7\" intent=\":literal\"><semantics><msub><mi>N</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">N_{X}</annotation></semantics></math>, i.e., <math alttext=\"N_{H}=20,000\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m8\" intent=\":literal\"><semantics><mrow><msub><mi>N</mi><mi>H</mi></msub><mo>=</mo><mrow><mn>20</mn><mo>,</mo><mn>000</mn></mrow></mrow><annotation encoding=\"application/x-tex\">N_{H}=20,000</annotation></semantics></math>, while the optimal occlusion probability <math alttext=\"p_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m9\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">p_{H}</annotation></semantics></math> is determined over the dev set, resulting in <math alttext=\"p_{H}=0.7\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m10\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>H</mi></msub><mo>=</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">p_{H}=0.7</annotation></semantics></math>, whose experiments are reported in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A4\" title=\"Appendix D Effect of Occlusion Probability on Encoder Output Explanations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.\nThe inference is performed using a single NVIDIA A40 GPU (40GB RAM)\nand takes <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m11\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>27 hours for <span class=\"ltx_text ltx_font_typewriter\">base</span>, <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m12\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>3-4 days for <span class=\"ltx_text ltx_font_typewriter\">small</span> and <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m13\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>6-8 days for <span class=\"ltx_text ltx_font_typewriter\">large</span>, depending on the source language.</p>\n\n",
                "matched_terms": [
                    "set",
                    "occlusion",
                    "base",
                    "probability",
                    "ph07ph07",
                    "dev",
                    "phph",
                    "explanations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Correlation Computation.</span>\nThe Pearson <math alttext=\"r\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p3.m1\" intent=\":literal\"><semantics><mi>r</mi><annotation encoding=\"application/x-tex\">r</annotation></semantics></math> correlation score is computed using the <span class=\"ltx_text ltx_font_typewriter\">scipy</span> implementation<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html\" title=\"\">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html</a></span></span></span> and averaging across samples in the test set.</p>\n\n",
                "matched_terms": [
                    "set",
                    "across",
                    "correlation",
                    "score",
                    "pearson",
                    "computed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we compare <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> with input saliency maps <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m3\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></sup>, which serve as an external reference for measuring input relevance.\nSpecifically, in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS1\" title=\"5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a>, we analyze the\n<span class=\"ltx_text ltx_font_typewriter\">base</span> model across all levels of granularity. Then, in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS2\" title=\"5.1.2 Multitask and Multilingual Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.2</span></a>, we extend the analysis to additional models (<span class=\"ltx_text ltx_font_typewriter\">small</span> and <span class=\"ltx_text ltx_font_typewriter\">large</span>), languages (<span class=\"ltx_text ltx_font_italic\">en</span> and <span class=\"ltx_text ltx_font_italic\">it</span>), and tasks (ASR and ST).</p>\n\n",
                "matched_terms": [
                    "base",
                    "across",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.T1\" title=\"Table 1 &#8227; 5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reports the correlation scores for the monolingual English ASR model (<span class=\"ltx_text ltx_font_typewriter\">base</span>), considering cross-attention at the head level, layer level, and in aggregated form.</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "correlation",
                    "english",
                    "base",
                    "model",
                    "layer",
                    "asr",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At the individual head level, correlations with saliency maps are generally low. This suggests that attention heads, when taken in isolation, only partially capture the model&#8217;s dependency on the input and often encode noisy or inconsistent relevance signals. However, not all heads are equal: some, especially in the upper layers (layers 4-6), exhibit relatively stronger correlations. Notably, <span class=\"ltx_text ltx_font_bold\">averaging across heads consistently outperforms selecting individual heads</span>, suggesting that, despite head-level sparsity and weak individual correlations, the collective information captured across heads reflects input relevance more effectively.\nMoving from heads to layers, we find a clearer picture. Averaging attention scores across all heads within each layer boosts correlation substantially, with layer 6 standing out as the most aligned with the saliency maps. This is followed closely by layer 5 and the average across all layers, indicating that the <span class=\"ltx_text ltx_font_bold\">last layers exhibit the highest alignment with input relevance</span>. These results reinforce the idea that deeper layers encode higher-level semantic or task-relevant features, a trend previously observed in Transformer-based models <cite class=\"ltx_cite ltx_citemacro_citep\">(Clark et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib13\" title=\"\">2019</a>)</cite>.\nInterestingly, while averaging across heads improves alignment, averaging across both heads and layers does not yield the overall best result, even if values are close. This indicates that not all layers contribute equally and that indiscriminate aggregation can dilute the relevance signal.</p>\n\n",
                "matched_terms": [
                    "across",
                    "correlation",
                    "average",
                    "highest",
                    "layer",
                    "each",
                    "indicates",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, the results show that appropriately selected and aggregated cross-attention scores exhibit only a <span class=\"ltx_text ltx_font_italic\">moderate</span> to <span class=\"ltx_text ltx_font_italic\">strong</span> correlation with input saliency maps, reaching values up to 0.588. This provides an initial indication of the\nlimited explanatory power\nof cross-attention weights, which we further examine under multilingual and multitask conditions in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS2\" title=\"5.1.2 Multitask and Multilingual Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess the impact of multilingual and multitask training on the correlation between cross-attention scores and saliency maps, we evaluate the <span class=\"ltx_text ltx_font_typewriter\">small</span> and <span class=\"ltx_text ltx_font_typewriter\">large</span> models. Layer-wise results are shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.T2\" title=\"Table 2 &#8227; 5.1.2 Multitask and Multilingual Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, while head-wise results are omitted due to the noisy behavior observed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS1\" title=\"5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "layerwise",
                    "correlation",
                    "crossattention",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across all configurations, we observe that <span class=\"ltx_text ltx_font_italic\">en</span> ASR yields the highest correlation values, outperforming even the monolingual <span class=\"ltx_text ltx_font_typewriter\">base</span> model (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS1\" title=\"5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a>). This suggests that large-scale multilingual training enhances the alignment between cross-attention and saliency maps, likely due to the improved generalization capacity of the model. In contrast, <span class=\"ltx_text ltx_font_italic\">en-it</span> ST shows a drop in correlation, which is expected given the increased complexity of ST compared to ASR.\nWhen considering <span class=\"ltx_text ltx_font_italic\">it</span> as the source language, we observe a similar pattern: ASR correlations are consistently higher than ST, yet remain below their <span class=\"ltx_text ltx_font_italic\">en</span> counterparts. This discrepancy aligns with the data distribution in training, where <span class=\"ltx_text ltx_font_italic\">en</span> accounts for 84% of the data versus 16% for <span class=\"ltx_text ltx_font_italic\">it</span>, resulting in more robust representations for <span class=\"ltx_text ltx_font_italic\">en</span>.\nAt the layer level, we find consistent evidence that the last decoder layers yield stronger correlations, reaffirming the trends observed in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS1\" title=\"5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a>.\nThe specific optimal layer varies with model size: layer 5 performs best in <span class=\"ltx_text ltx_font_typewriter\">small</span>, while layers 8-10 achieve the highest correlations in <span class=\"ltx_text ltx_font_typewriter\">large</span>.\nNevertheless, correlation values across the last\nlayers remain very close, suggesting that their cross-attention scores\nprovide the most robust alignment with saliency maps across both tasks and languages. This trend is further supported by downstream application results, where the final layers have shown the best token-level performance <cite class=\"ltx_cite ltx_citemacro_citep\">(Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib65\" title=\"\">2023a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib66\" title=\"\">b</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib91\" title=\"\">2024a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "across",
                    "correlation",
                    "base",
                    "model",
                    "highest",
                    "layer",
                    "layers",
                    "asr",
                    "between",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Averaging attention scores across layers further improves the correlation with saliency maps in\nalmost\nall configurations. The only exceptions are <span class=\"ltx_text ltx_font_italic\">en</span> and <span class=\"ltx_text ltx_font_italic\">it</span> ASR in <span class=\"ltx_text ltx_font_typewriter\">small</span>, where selective-layer extraction offers a marginal improvement (0.006 for English, 0.001 for Italian).\nTherefore, similarly to what we observed in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1.SSS1\" title=\"5.1.1 Head-wise and Layer-wise Correlations &#8227; 5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1.1</span></a>, averaging attention across heads and layers consistently yields the best or near-best correlation with <span class=\"ltx_text ltx_font_italic\">moderate</span> to <span class=\"ltx_text ltx_font_italic\">strong</span> correlation with input saliency maps, even considering large-scale models trained in multitask and multilingual settings.\nNonetheless, this alignment accounts for only 49-63% of the total input relevance, indicating that <span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#88185B;\">cross-attention falls short of fully accounting for the S2T models&#8217; behavior</span>.\nSince this limitation may stem from the phenomenon of context mixing, in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS2\" title=\"5.2 What Is the Impact of Context Mixing? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.2</span></a> we analyze the correlation between cross-attention and encoder output&#8211;representations that have already undergone transformation by the encoder&#8211;to better isolate the true explanatory power of cross-attention.</p>\n\n",
                "matched_terms": [
                    "across",
                    "correlation",
                    "english",
                    "encoder",
                    "asr",
                    "between",
                    "layers",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1\" title=\"5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a> focused on input relevance, we now investigate whether <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math>\naligns more closely with encoder output saliency maps.\nA higher correlation with encoder output representations would support the hypothesis that discrepancies between cross-attention and input saliency arise from context mixing, due to the reorganization of information within the encoder.\nTo this end,\nwe compare <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> with encoder output saliency maps <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m4\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></sup>, which attribute relevance to the encoder hidden states for each output token (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S3.SS2\" title=\"3.2 Feature Attribution for Speech-to-Text &#8227; 3 Methodology &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>). Layer-wise results for all models are presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.T3\" title=\"Table 3 &#8227; 5.2 What Is the Impact of Context Mixing? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "layerwise",
                    "correlation",
                    "ùêíùêåmathbfsmhh",
                    "output",
                    "each",
                    "encoder",
                    "between",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Even when examining encoder output representations, we observe trends consistent with those identified in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.SS1\" title=\"5.1 Does Cross-Attention Reflect Input-Output Dependencies? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>. Specifically, when averaged across decoder layers, cross-attention scores consistently provide the strongest or nearly optimal correlation with saliency maps, with the last decoder layers offering more representative explanations than the first ones across all models. As expected, correlation with encoder output representations consistently yields higher scores than those obtained from input representations, with absolute <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math> differences ranging from 0.03 to 0.18, quantifying the influence of context mixing effects to 6.6-16.7%.\nThe increased correlation is also visually evident in the example shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#S5.F2\" title=\"Figure 2 &#8227; 5.2 What Is the Impact of Context Mixing? &#8227; 5 Results &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, where <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> aligns more closely with the relevance scores from <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m4\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></sup> than with those from <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m5\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m6\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></sup>.\nHowever, despite being unaffected by context mixing, the correlation between <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m7\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> and <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m8\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m9\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></sup> remains limited&#8211;capturing only 52-75% of the relevance.\n<span class=\"ltx_text ltx_font_bold\" style=\"--ltx-fg-color:#88185B;\">This\ngap underscores the inherent limitations in relying solely on cross-attention as an explanation mechanism, reinforcing its role as an informative but <span class=\"ltx_text ltx_ulem_uline\">incomplete</span> proxy for explainability in S2T models</span>&#8211;not only for input-level saliency, but even at the encoder-output level, where cross-attention directly operates.</p>\n\n",
                "matched_terms": [
                    "across",
                    "correlation",
                    "ùêíùêåmathbfsmhh",
                    "œÅrho",
                    "also",
                    "output",
                    "averaged",
                    "encoder",
                    "layers",
                    "between",
                    "explanations",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Discussion.</span>\nOur results demonstrate that, although <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> scores moderately correlate with aggregated <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m2\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math> (with a correlation peaking around 0.45-0.55 in the best-performing settings), they consistently fall short of capturing the full input relevance&#8211;even when context mixing effects are factored out. To directly assess explanation quality, we compute the deletion metric (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A1\" title=\"Appendix A Effect of Aggregation Functions on Input Explanations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>) on the <span class=\"ltx_text ltx_font_typewriter\">base</span> model, finding that <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> achieves 41.2, compared to 52.9 for frequency-aggregated <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m4\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math> and 91.3 for full-resolution maps.\nThis gap underscores that <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m5\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> discards fine-grained time-frequency cues and produces weaker attributions, even under identical aggregation. As further discussed in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A6\" title=\"Appendix F Limitations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>, our analysis is bounded by the use of SPES as the attribution baseline, but the consistent underperformance across correlation and deletion confirms that <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m6\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> offers, at best, an incomplete picture of model behavior. These results also carry implications for downstream tasks. In applications such as timestamp prediction, prior work often relies on attention from a single decoder layer or head <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib91\" title=\"\">2024a</a>; Papi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib65\" title=\"\">2023a</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib66\" title=\"\">b</a>; Zusag et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib106\" title=\"\">2024</a>)</cite>. Our analysis suggests that averaging across layers and, especially, across heads provides a closer match to saliency behavior and could improve these methods.\nBuilding on past success with attention regularization in ASR (e.g., imposing monotonicity as in <cite class=\"ltx_cite ltx_citemacro_citet\">Zhao et&#160;al. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib104\" title=\"\">2020</a></cite>), similar training-time strategies&#8211;such as auxiliary losses that align attention with saliency&#8211;could further benefit downstream applications, enhancing both interpretability and task performance.\nIn summary, <span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\" style=\"--ltx-fg-color:#88185B;\"><math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m7\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> should not be treated as a stand-alone XAI tool</span>. It provides lightweight cues that may complement attribution-based methods, but it cannot replace them. Reframing <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.p1.m8\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> as an auxiliary rather than a proxy recalibrates expectations and grounds future work on more faithful and effective approaches to explainability in S2T models.</p>\n\n",
                "matched_terms": [
                    "across",
                    "correlation",
                    "base",
                    "also",
                    "deletion",
                    "model",
                    "layer",
                    "asr",
                    "metric",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conclusions.</span>\nWe presented the first systematic analysis of cross-attention in S2T through the lens of explainable AI, comparing it to saliency maps across tasks, languages, and model scales. Cross-attention moderately to strongly aligns with saliency&#8211;especially when averaged across heads and layers&#8211;but captures only about half of the input relevance. Even when disentangling the effect of context mixing by analyzing encoder outputs, it explains just 52-75% of saliency. This gap reveals intrinsic limits of cross-attention as an explanation mechanism: it offers informative cues but only a partial view of the factors driving S2T predictions.</p>\n\n",
                "matched_terms": [
                    "across",
                    "model",
                    "averaged",
                    "encoder",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Broader Implications.</span> Explainability in S2T systems has tangible implications for AI transparency, especially in high-stakes settings such as healthcare, legal transcription, and educational accessibility. Our findings provide insights about the usage of cross-attention as a tool for identifying how models relate output predictions to input regions, which can support auditing, debugging, and fair deployment. However, there is a risk that misinterpreted attention visualizations may be overtrusted by non-expert users, reinforcing false confidence in system behavior <cite class=\"ltx_cite ltx_citemacro_citep\">(Rudin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib75\" title=\"\">2019</a>)</cite>. Moreover, our language choices and focus on high-resource speech still reflect global imbalances in language technology access <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib36\" title=\"\">2020</a>)</cite>. Future work should extend this analysis to low-resource and underrepresented languages to promote broader inclusion.</p>\n\n",
                "matched_terms": [
                    "output",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To properly obtain input-level explanations comparable with the dimensions of cross-attention scores (i.e., making <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\\in\\mathbb{R}^{I\\times T^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p1.m1\" intent=\":literal\"><semantics><mrow><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mi>I</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mi>T</mi><mo>&#8242;</mo></msup></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\\in\\mathbb{R}^{I\\times T^{\\prime}}</annotation></semantics></math>), we explore the effect of different aggregation strategies over the time and frequency dimensions.</p>\n\n",
                "matched_terms": [
                    "explanations",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To compare and select the best aggregation strategy, we adopt the <span class=\"ltx_text ltx_font_italic\">deletion</span> metric&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Nauta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib61\" title=\"\">2023</a>)</cite>, which quantifies the decline in prediction quality as the most relevant input frames&#8211;identified by the explanation&#8211;are progressively removed. Specifically, we adapt the implementation by <cite class=\"ltx_cite ltx_citemacro_citet\">Fucci et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite> for S2T tasks, replacing the top-ranked time frames in the input spectrogram <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m1\" intent=\":literal\"><semantics><mi>&#119831;</mi><annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation></semantics></math> with zero vectors in 5% increments, based on the aggregated saliency map <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m2\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math>.\nSince <math alttext=\"\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m3\" intent=\":literal\"><semantics><msup><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><mi>X</mi></msup><annotation encoding=\"application/x-tex\">\\text{{\\color[rgb]{0.0078125,0.65234375,0.96875}\\definecolor[named]{pgfstrokecolor}{rgb}{0.0078125,0.65234375,0.96875}$\\mathbf{SM}$}}^{X}</annotation></semantics></math> operates on an aggregated time dimension <math alttext=\"T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m4\" intent=\":literal\"><semantics><msup><mi>T</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">T^{\\prime}</annotation></semantics></math>, which is smaller than the original time dimension <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m5\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> of <math alttext=\"\\mathbf{X}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m6\" intent=\":literal\"><semantics><mi>&#119831;</mi><annotation encoding=\"application/x-tex\">\\mathbf{X}</annotation></semantics></math>, we upsample <math alttext=\"T^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m7\" intent=\":literal\"><semantics><msup><mi>T</mi><mo>&#8242;</mo></msup><annotation encoding=\"application/x-tex\">T^{\\prime}</annotation></semantics></math> to match <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p2.m8\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> using nearest-neighbor interpolation.\nPrediction quality is measured using the word error rate (WER), specifically the <span class=\"ltx_text ltx_font_typewriter\">wer_max</span> scorer from the SPES repository.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/hlt-mt/FBK-fairseq\" title=\"\">https://github.com/hlt-mt/FBK-fairseq</a></span></span></span> Lastly, we compute the area under the WER curve to quantify the faithfulness of each explanation method.</p>\n\n",
                "matched_terms": [
                    "deletion",
                    "metric",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A1.T4\" title=\"Table 4 &#8227; Appendix A Effect of Aggregation Functions on Input Explanations &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> reports the deletion scores and, for completeness, the Pearson <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m1\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math> correlations between the cross attention scores <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math> and the saliency maps <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p3.m4\" intent=\":literal\"><semantics><mi>X</mi><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></sup> for the representations aggregated following three strategies:</p>\n\n",
                "matched_terms": [
                    "deletion",
                    "œÅrho",
                    "pearson",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2D average pooling</span>, applied over the entire time-frequency plane to obtain its <span class=\"ltx_text ltx_font_italic\">average</span> value and computed through <span class=\"ltx_text ltx_font_typewriter\">adaptive_avg_pool2d<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">5</span></span><a class=\"ltx_ref ltx_url\" href=\"https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.adaptive_avg_pool2d.html\" title=\"\">https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.adaptive_avg_pool2d.html</a></span></span></span></span>;</p>\n\n",
                "matched_terms": [
                    "computed",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2-step pooling (1D maximum + 1D average)</span>, where max pooling is applied along the frequency axis, followed by averaging over time, and computed by applying <span class=\"ltx_text ltx_font_typewriter\">max_pool1d<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">6</span></span><a class=\"ltx_ref ltx_url\" href=\"https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.max_pool1d.html\" title=\"\">https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.max_pool1d.html</a></span></span></span></span> and <span class=\"ltx_text ltx_font_typewriter\">avg_pool1d</span>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.avg_pool1d.html\" title=\"\">https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.avg_pool1d.html</a></span></span></span> respectively;</p>\n\n",
                "matched_terms": [
                    "computed",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The aggregation functions were selected to contrast methods that either isolate the most relevant features (with maximum pooling) or represent their mean relevance (with average pooling). Similarly, the 2-step approach has been tried to first isolate relevance patterns in the frequency domain, a dimension that is not present in cross-attention representation, and then average across the time dimension to match the downsampled time resolution of the cross-attention scores.</p>\n\n",
                "matched_terms": [
                    "across",
                    "average",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Among the tested methods, we observe that the 2D maximum pooling aggregation (2D max) yields the best quality explanations, obtaining the highest deletion score, while the 2D average pooling (2D avg) is the worst, with the lowest deletion score. Looking at the correlations, we notice that they follow the same trend of deletion scores, with the 2D max yielding the best <math alttext=\"\\rho\" class=\"ltx_Math\" display=\"inline\" id=\"A1.p4.m1\" intent=\":literal\"><semantics><mi>&#961;</mi><annotation encoding=\"application/x-tex\">\\rho</annotation></semantics></math>. In particular, 2D avg consistently has the lowest correlations compared to the 2D max, particularly in the last layers (e.g., 0.457 against 0.572 at layer 5). Regarding the 2-step pooling approach, we not only observe an improved deletion score but also better correlation scores compared to 2D avg, especially from layer 3 onward, approaching the best performance with a layer-average correlation of 0.565. Nevertheless, the explanation quality is still lower compared to 2D max (i.e., 55.18 against 57.04), which also achieves the highest correlations at nearly every layer, peaking at 0.582 in layer 6, and yielding the best overall correlation among the averaged cross-attention across layers (i.e., 0.572).</p>\n\n",
                "matched_terms": [
                    "across",
                    "correlation",
                    "score",
                    "œÅrho",
                    "also",
                    "average",
                    "deletion",
                    "averaged",
                    "highest",
                    "layer",
                    "layers",
                    "explanations",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the monolingual ASR model, we leverage the speech-to-text English data available for the IWSLT 2024 evaluation campaign (offline task),<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://iwslt.org/2024/offline\" title=\"\">https://iwslt.org/2024/offline</a></span></span></span> namely: CommonVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Ardila et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib4\" title=\"\">2020</a>)</cite>, CoVoST v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib90\" title=\"\">2021b</a>)</cite>, Europarl-ST <cite class=\"ltx_cite ltx_citemacro_citep\">(Iranzo-S&#225;nchez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib33\" title=\"\">2020</a>)</cite>, LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib64\" title=\"\">2015</a>)</cite>, MuST-C v1 <cite class=\"ltx_cite ltx_citemacro_citep\">(Di&#160;Gangi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib17\" title=\"\">2019</a>)</cite>, TEDLIUM v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Hernandez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib31\" title=\"\">2018</a>)</cite>, and VoxPopuli ASR <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib89\" title=\"\">2021a</a>)</cite>. The resulting training set is about 3k hours of speech.</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "set",
                    "english",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the multitask (ASR and ST) multilingual large-scale models, we leverage more than 150k hours of open-source speech<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>Speech, transcripts, and translations released under an open-source license such as CC-0 and CC-BY 4.0.</span></span></span> in English (<span class=\"ltx_text ltx_font_italic\">en</span>) and Italian (<span class=\"ltx_text ltx_font_italic\">it</span>), namely: CommonVoice, CoVoST v2, FLEURS <cite class=\"ltx_cite ltx_citemacro_citep\">(Conneau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib14\" title=\"\">2023</a>)</cite>, MOSEL <cite class=\"ltx_cite ltx_citemacro_citep\">(Gaido et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib23\" title=\"\">2024a</a>)</cite>, MLS <cite class=\"ltx_cite ltx_citemacro_citep\">(Pratap et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib71\" title=\"\">2020</a>)</cite>, and YouTube-Commons<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hf.co/datasets/PleIAs/YouTube-Commons\" title=\"\">https://hf.co/datasets/PleIAs/YouTube-Commons</a></span></span></span> (from which 14.2k hours of <span class=\"ltx_text ltx_font_italic\">en</span> and 1.8k for <span class=\"ltx_text ltx_font_italic\">it</span> have been extracted). For datasets missing the translations, we generated them using <span class=\"ltx_text ltx_font_typewriter\">MADLAD-400 3B-MT</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Kudugunta et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib43\" title=\"\">2023</a>)</cite>. This setting allows us to verify our analysis with a large-scale setting similar to the scale of a popular model such as OWSM <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib69\" title=\"\">2023</a>)</cite> and 2 times that of NVIDIA Canary <cite class=\"ltx_cite ltx_citemacro_citep\">(Puvvada et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib72\" title=\"\">2024</a>)</cite> while having complete control of data used during training, ensuring that data contamination issues are avoided completely.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "model",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train all models using a combination of three losses: <em class=\"ltx_emph ltx_font_italic\">i)</em> a label-smoothed cross-entropy loss (<math alttext=\"\\mathcal{L}_{\\text{CE}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>CE</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{CE}}</annotation></semantics></math>) applied to the decoder output using the target text as the reference (transcripts for ASR and translations for ST), <em class=\"ltx_emph ltx_font_italic\">ii)</em> a CTC loss <cite class=\"ltx_cite ltx_citemacro_citep\">(Graves et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib27\" title=\"\">2006</a>)</cite> computed using transcripts as reference (<math alttext=\"\\mathcal{L}_{\\text{CTCsrc}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>CTCsrc</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{CTCsrc}}</annotation></semantics></math>) on the output of the 8<sup class=\"ltx_sup\">th</sup> encoder layer for <span class=\"ltx_text ltx_font_typewriter\">base</span> and <span class=\"ltx_text ltx_font_typewriter\">small</span> and the 16<sup class=\"ltx_sup\">th</sup> for <span class=\"ltx_text ltx_font_typewriter\">medium</span>, <em class=\"ltx_emph ltx_font_italic\">iii)</em> a CTC loss on the final encoder output (<math alttext=\"\\mathcal{L}_{\\text{CTCtgt}}\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p1.m3\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>CTCtgt</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{CTCtgt}}</annotation></semantics></math>) applied to predict the target text <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib98\" title=\"\">2023</a>)</cite>.\nThe final loss is the weighted sum of the above-mentioned losses:</p>\n\n",
                "matched_terms": [
                    "base",
                    "output",
                    "layer",
                    "computed",
                    "encoder",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The monolingual ASR <span class=\"ltx_text ltx_font_typewriter\">base</span> model is trained on all 3k hours of ASR data for 200k steps using Noam as the learning rate scheduler <cite class=\"ltx_cite ltx_citemacro_citep\">(Vaswani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib85\" title=\"\">2017</a>)</cite> with a peak of 2e-3 and 25,000 warm-up steps.</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "base",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The multitask and multilingual models are trained using a two-stage approach, where the model is pre-trained first on ASR data only (ASR pre-training) and then trained on both ASR and ST data (ASR+ST training). For the ASR pre-training, the learning rate scheduler adopted for the <span class=\"ltx_text ltx_font_typewriter\">small</span> model is the same as the <span class=\"ltx_text ltx_font_typewriter\">base</span> model.\nFor the <span class=\"ltx_text ltx_font_typewriter\">medium</span> model, we adopted a piece-wise warm-up on the Noam scheduler to avoid divergence issues <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib70\" title=\"\">2024</a>)</cite>, with the learning rate first increasing linearly to 2e-5 for 25k steps and then to 2e-4 for an additional 25k steps, followed by the standard inverse square root function.\nFor the ASR+ST training, we sample the ASR target with probability 0.5 and use the ST target otherwise following the same settings of ASR pre-training, except for the learning rate that is set to a constant value of 1e-4 for <span class=\"ltx_text ltx_font_typewriter\">small</span> and 1e-5 for <span class=\"ltx_text ltx_font_typewriter\">medium</span>, following the same downscale of the ASR pre-taining. Both training stages lasted 1M steps, corresponding to <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A2.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>6 epochs over the training data.</p>\n\n",
                "matched_terms": [
                    "set",
                    "base",
                    "model",
                    "probability",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the <span class=\"ltx_text ltx_font_typewriter\">base</span> model, the trainings are executed on 4 NVIDIA A100 GPUs (64GB RAM) with a mini-batch of 40,000 tokens, an update frequency of 2, and averaging the last 7 checkpoints obtained from the training.\nFor the multitask and multitlingual models, we use mini-batches of 10,000 tokens for the <span class=\"ltx_text ltx_font_typewriter\">small</span> and 4,500 for the <span class=\"ltx_text ltx_font_typewriter\">medium</span> with an update frequency of, respectively, 2 and 6 on 16 NVIDIA A100 GPUs (64GB RAM), save checkpoints every 1,000 steps and average the last 25 checkpoints to obtain the final models.</p>\n\n",
                "matched_terms": [
                    "base",
                    "average",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The quality of the ASR hypotheses is evaluated with the WER metric using the jiWER library<span class=\"ltx_note ltx_role_footnote\" id=\"footnote12\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_tag ltx_tag_note\">12</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://pypi.org/project/jiwer/\" title=\"\">https://pypi.org/project/jiwer/</a></span></span></span> and applying the Whisper text normalizer<span class=\"ltx_note ltx_role_footnote\" id=\"footnote13\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_tag ltx_tag_note\">13</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://pypi.org/project/whisper-normalizer/\" title=\"\">https://pypi.org/project/whisper-normalizer/</a></span></span></span>.\nThe quality of the ST hypotheses is evaluated using COMET <cite class=\"ltx_cite ltx_citemacro_citep\">(Rei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib74\" title=\"\">2020</a>)</cite> version 2.2.4, with the default model.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\">14</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hf.co/Unbabel/wmt22-comet-da\" title=\"\">https://hf.co/Unbabel/wmt22-comet-da</a></span></span></span>\nThe quality of the explanations is obtained by measuring both\n<span class=\"ltx_text ltx_font_italic\">deletion</span> and <span class=\"ltx_text ltx_font_italic\">size</span> metrics\navailable in the SPES repository, using <span class=\"ltx_text ltx_font_typewriter\">wer_max</span> as the scorer for ASR and <span class=\"ltx_text ltx_font_typewriter\">bleu</span> for ST, as described in <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "explanations",
                    "metric",
                    "deletion",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For comparison, in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A3.T5\" title=\"Table 5 &#8227; Appendix C Quality Metrics for the Reported Models &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we also report results obtained from popular large-scale models, namely Whisper <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib73\" title=\"\">2023</a>)</cite>, OWSM v3.1 <cite class=\"ltx_cite ltx_citemacro_citep\">(Peng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib70\" title=\"\">2024</a>)</cite>, and SeamlessM4T <cite class=\"ltx_cite ltx_citemacro_citep\">(Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib7\" title=\"\">2023</a>)</cite>. Looking at the transcription/translation quality performance, we observe that both the monolingual <span class=\"ltx_text ltx_font_typewriter\">base</span> model and the multitask multilingual <span class=\"ltx_text ltx_font_typewriter\">small</span> and <span class=\"ltx_text ltx_font_typewriter\">large</span> models are mostly able to achieve competitive results, even outperforming the well-known models in two cases (<span class=\"ltx_text ltx_font_italic\">en</span> ASR for <span class=\"ltx_text ltx_font_typewriter\">base</span> and <span class=\"ltx_text ltx_font_italic\">en-it</span> ST for <span class=\"ltx_text ltx_font_typewriter\">large</span>). While our models and OWSM v3.1 strive to be on par on <span class=\"ltx_text ltx_font_italic\">it</span> with models with closed training data (Whisper and Seamless), they are able to close the gap on <span class=\"ltx_text ltx_font_italic\">en</span>, most probably given a larger availability of public training data. Moreover, the highest performance of <span class=\"ltx_text ltx_font_typewriter\">base</span> on <span class=\"ltx_text ltx_font_italic\">en</span> ASR compared to the <span class=\"ltx_text ltx_font_typewriter\">small</span> and <span class=\"ltx_text ltx_font_typewriter\">large</span> can be attributed to both the specialization of the model and the presence of the EuroParl-ST training set in the training data.</p>\n\n",
                "matched_terms": [
                    "monolingual",
                    "set",
                    "base",
                    "also",
                    "report",
                    "model",
                    "highest",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Moving to the explanation quality, we observe that both deletion and size scores are comparable across all three models analyzed in the paper and coherent with values obtained in the original SPES paper <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite> on different benchmarks and models. Overall, the deletion scores for ASR are close to the highest possible value (i.e., 100), especially on <span class=\"ltx_text ltx_font_italic\">it</span>, where 97% is achieved. Similarly, the deletion scores for ST are close to 0, indicating that the quality of explanations is very high. The size scores are all close, ranging between 28.2 and 30.6 among models, languages, and tasks, indicating a good compactness of the explanations.</p>\n\n",
                "matched_terms": [
                    "across",
                    "deletion",
                    "highest",
                    "asr",
                    "between",
                    "explanations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To properly choose the occlusion probability (<math alttext=\"p_{H}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m1\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>H</mi></msub><annotation encoding=\"application/x-tex\">p_{H}</annotation></semantics></math>) for the encoder output explanations <math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m3\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></sup>, we conducted experiments by varying this probability in the set of <math alttext=\"\\{0.1,0.3,0.5,0.7,0.9\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><mn>0.1</mn><mo>,</mo><mn>0.3</mn><mo>,</mo><mn>0.5</mn><mo>,</mo><mn>0.7</mn><mo>,</mo><mn>0.9</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{0.1,0.3,0.5,0.7,0.9\\}</annotation></semantics></math>, similarly to what has been done for determining the input occlusion probability (<math alttext=\"p_{X}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p1.m5\" intent=\":literal\"><semantics><msub><mi>p</mi><mi>X</mi></msub><annotation encoding=\"application/x-tex\">p_{X}</annotation></semantics></math>) in SPES <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib22\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "occlusion",
                    "ùêíùêåmathbfsmhh",
                    "output",
                    "probability",
                    "phph",
                    "encoder",
                    "explanations",
                    "varying"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the results, we can notice that higher occlusion probabilities yield not only a better deletion score but also an increased correlation with <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p3.m1\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math>. The overall best correlation is achieved when averaging across all layers, and layer 5 achieves the best layer-specific correlation values, a phenomenon that remains coherent even when varying the occlusion probability.\nInterestingly, the deletion scores and the <math alttext=\"\\mathbf{CA}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p3.m2\" intent=\":literal\"><semantics><mi mathcolor=\"#C064F5\" style=\"--ltx-fg-color:#C064F5;\">&#119810;&#119808;</mi><annotation encoding=\"application/x-tex\">\\mathbf{CA}</annotation></semantics></math>-<math alttext=\"\\mathbf{SM}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p3.m3\" intent=\":literal\"><semantics><mi mathcolor=\"#02A6F7\" style=\"--ltx-fg-color:#02A6F7;\">&#119826;&#119820;</mi><annotation encoding=\"application/x-tex\">\\mathbf{SM}</annotation></semantics></math><sup class=\"ltx_sup\"><math alttext=\"H\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p3.m4\" intent=\":literal\"><semantics><mi>H</mi><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></sup> correlations always follow the same trend, with the best values achieved with <math alttext=\"p_{H}=0.7\" class=\"ltx_Math\" display=\"inline\" id=\"A4.p3.m5\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>H</mi></msub><mo>=</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">p_{H}=0.7</annotation></semantics></math>, which we used in all experiments reported in the main paper.</p>\n\n",
                "matched_terms": [
                    "across",
                    "score",
                    "occlusion",
                    "correlation",
                    "also",
                    "deletion",
                    "probability",
                    "ph07ph07",
                    "layer",
                    "varying",
                    "layers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Examples of different saliency maps and cross-attention representations obtained with the <span class=\"ltx_text ltx_font_typewriter\">large</span> model are presented in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We notice similar relevance patterns in the paired samples&#8211;i.e., the samples having the same source language (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>a-f, and Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>g-l)&#8211;even if involving different tasks. We observe a reordering phenomenon from the English audio <span class=\"ltx_text ltx_inline-quote ltx_outerquote\">&#8220;<span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#0000FF;\">cheap<span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\"> <span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">money</span></span></span>&#8221;</span> and its Italian textual counterpart <span class=\"ltx_text ltx_inline-quote ltx_outerquote\">&#8220;<span class=\"ltx_text ltx_font_italic\" style=\"--ltx-fg-color:#FF0000;\">denaro<span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\"> <span class=\"ltx_text\" style=\"--ltx-fg-color:#0000FF;\">a buon mercato</span></span></span>&#8221;</span>,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote15\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">15</sup><span class=\"ltx_tag ltx_tag_note\">15</span>Same colors reflect the same concepts.</span></span></span> which is reflected in the saliency maps (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>d and e) and also captured by cross-attention (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>f). We also observe that there are some patterns captured by the attention that are not reflected in the input. For instance, in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A5.F4\" title=\"Figure 4 &#8227; Appendix E Examples &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>f, the first words (<span class=\"ltx_text ltx_inline-quote ltx_outerquote\">&#8220;<span class=\"ltx_text ltx_font_italic\">&#200; solo</span>&#8221;</span>) attend&#8211;albeit with relatively low scores&#8211;to the audio frames between 75 and 85, while this pattern this pattern is absent in the relevance scores of both the encoder output and the input. Consistent with the findings discussed throughout the paper, this example illustrates that while attention generally follows the saliency patterns identified by feature attribution, some discrepancies persist.</p>\n\n",
                "matched_terms": [
                    "english",
                    "also",
                    "output",
                    "encoder",
                    "between",
                    "crossattention"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work provides an in-depth analysis of cross-attention explainability in encoder-decoder S2T models. While it yields actionable insights, some limitations should be acknowledged. First, our experimental scope is restricted to ASR and ST. Although these tasks are central to S2T-based AI <cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib73\" title=\"\">2023</a>; Barrault et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib7\" title=\"\">2023</a>)</cite>, we do not evaluate other downstream tasks such as spoken question answering or speech summarization, which may involve different dynamics in decoder attention. Second, our multilingual analysis is limited to English (a Germanic language) and Italian (a Romance language), due to the high computational cost of large-scale model training across a broader set of languages. Third, we focus on models trained from scratch but do not include architectures based on Speech Foundation Models (SFMs) paired with large language models (LLMs), often referred to as SpeechLLM&#8211;a recent growing area of interest in S2T modeling <cite class=\"ltx_cite ltx_citemacro_citep\">(Gaido et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib24\" title=\"\">2024b</a>)</cite>. As our analysis focuses on evaluation, our goal was to completely avoid data contamination issues <cite class=\"ltx_cite ltx_citemacro_citep\">(Sainz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib76\" title=\"\">2023</a>)</cite>, which is a problem affecting almost every SFM and SpeechLLM architectures currently available, as we have no control over their training data, and, for this reason, we decided to retrain the models from scratch.\nFourth, our analysis relies on SPES to compute reference explanations, acknowledging that, as an empirical method, it may introduce some margin of error. However, in the absence of a gold or human reference&#8211;which is unattainable in practice&#8211;we adopt SPES as a <span class=\"ltx_text ltx_font_italic\">silver</span> reference, since it represents the state of the art in explainability for speech-to-text. We further validate this choice in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#A3\" title=\"Appendix C Quality Metrics for the Reported Models &#8227; Cross-Attention is Half Explanation in Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>, showing that SPES achieves very high quality explanations (deletion scores <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m1\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>90 on ASR and <math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p1.m2\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math>3 on ST), making it a more faithful option than less robust alternatives from the generic XAI field such as gradient norms <cite class=\"ltx_cite ltx_citemacro_citep\">(Covert et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18010v1#bib.bib15\" title=\"\">2021a</a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "set",
                    "across",
                    "english",
                    "deletion",
                    "model",
                    "asr",
                    "explanations",
                    "crossattention"
                ]
            }
        ]
    }
}