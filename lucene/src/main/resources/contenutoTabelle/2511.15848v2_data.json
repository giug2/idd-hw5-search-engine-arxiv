{
    "S5.T1": {
        "caption": "Table 1: Performance comparison (in %) on speech-to-text benchmarks across Big Bench Audio, Spoken MQA, MMSU, MMAU, Wild Speech, and Average Score.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">Avg.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Big Bench Audio</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Spoken MQA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">MMSU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">MMAU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Wild Speech</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Step-Audio 2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">68.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">59.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">88.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">64.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">78.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">51.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Gemini 2.5 Pro</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">81.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">96.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">94.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">79.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">77.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">60.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Gemini 3 Pro</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">85.1</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">92.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">95.3</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">82.9</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">78.9</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">76.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Step-Audio-R1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">83.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">98.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">95.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">75.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">77.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">70.6</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "mmau",
            "big",
            "spoken",
            "stepaudio",
            "comparison",
            "pro",
            "bench",
            "mqa",
            "audio",
            "average",
            "wild",
            "mmsu",
            "performance",
            "across",
            "avg",
            "stepaudior1",
            "speechtotext",
            "speech",
            "score",
            "gemini",
            "model",
            "benchmarks"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#S5.T1\" title=\"Table 1 &#8227; 5.1 Evaluation on Speech-to-Text Benchmarks &#8227; 5 Evaluation &#8227; Step-Audio-R1 Technical Report\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, Step-Audio-R1 achieves an average score of 83.6%, significantly outperforming Gemini 2.5 Pro while being slightly lower than Gemini 3 Pro. This competitive performance confirms that our MGRD approach effectively enhances deep audio comprehension.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\">Recent advances in reasoning models have demonstrated remarkable success in text and vision domains through extended chain-of-thought deliberation. However, a perplexing phenomenon persists in audio language models: they consistently perform better with minimal or no reasoning, raising a fundamental question&#8212;<span class=\"ltx_text ltx_font_bold\">can audio intelligence truly benefit from deliberate thinking?</span> We introduce Step-Audio-R1, the first audio reasoning model that successfully unlocks reasoning capabilities in the audio domain. Through our proposed Modality-Grounded Reasoning Distillation (MGRD) framework, Step-Audio-R1 learns to generate audio-relevant reasoning chains that genuinely ground themselves in acoustic features rather than hallucinating disconnected deliberations. Our model exhibits strong audio reasoning capabilities, surpassing Gemini 2.5 Pro and achieving performance comparable to the state-of-the-art Gemini 3 Pro across comprehensive audio understanding and reasoning benchmarks spanning speech, environmental sounds, and music. These results demonstrate that reasoning is a transferable capability across modalities when appropriately anchored, transforming extended deliberation from a liability into a powerful asset for audio intelligence. By establishing the first successful audio reasoning model, Step-Audio-R1 opens new pathways toward building truly multimodal reasoning systems that think deeply across all sensory modalities.\n\n<span class=\"ltx_figure\" id=\"S0.F1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"215\" id=\"S0.F1.g1\" src=\"x1.png\" width=\"561\"/>\n<span class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 1: </span>Benchmark performance of Step-Audio-R1</span>\n</span></span>\n</p>\n\n",
                "matched_terms": [
                    "across",
                    "gemini",
                    "stepaudior1",
                    "audio",
                    "model",
                    "speech",
                    "benchmarks",
                    "pro",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Chain-of-thought reasoning has transformed modern artificial intelligence, enabling language models to solve complex mathematical problems <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib3\" title=\"\">3</a>]</cite>, generate executable code <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib4\" title=\"\">4</a>]</cite>, and engage in sophisticated logical deduction through extended deliberation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib5\" title=\"\">5</a>]</cite>. Vision-language models have similarly adopted this paradigm, leveraging deliberate reasoning to interpret spatial relationships <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib2\" title=\"\">2</a>]</cite>, analyze visual scenes, and answer intricate questions about images <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib7\" title=\"\">7</a>]</cite>. Underlying these successes is a fundamental principle known as test-time compute scaling: allocating more computational resources during inference&#8212;through longer chains of thought, iterative refinement, or search&#8212;predictably improves model performance <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib10\" title=\"\">10</a>]</cite>. This scaling law has become so robust that it now guides the design and deployment of AI systems across modalities.</p>\n\n",
                "matched_terms": [
                    "across",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The audio domain, however, presents a stark exception to this principle. Existing audio language models consistently demonstrate superior performance with minimal or no reasoning <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib12\" title=\"\">12</a>]</cite>. Empirical observations across benchmarks reveal that direct responses outperform elaborate chain-of-thought explanations, with performance systematically degrading as reasoning length increases <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib14\" title=\"\">14</a>]</cite>. This inverted scaling behavior persists across architectures, training methodologies, and model scales <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib13\" title=\"\">13</a>]</cite>, suggesting a fundamental incompatibility between test-time compute scaling and auditory intelligence. This raises a critical question:</p>\n\n",
                "matched_terms": [
                    "across",
                    "audio",
                    "model",
                    "benchmarks",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent efforts have attempted to address this anomaly through reinforcement learning approaches that employ language model judges to verify consistency between reasoning chains and final answers <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib16\" title=\"\">16</a>]</cite>. While these methods improve alignment, they treat the symptom rather than the root cause&#8212;enforcing consistency without understanding <span class=\"ltx_text ltx_font_italic\">why</span> reasoning fails in audio. Through systematic case studies, we uncover a striking pattern: existing audio language models engage in <span class=\"ltx_text ltx_font_italic\">textual surrogate reasoning</span> rather than acoustic reasoning. When prompted to deliberate, models systematically reason from the perspective of transcripts or textual captions instead of acoustic properties&#8212;for instance, attributing musical melancholy to \"lyrics mentioning sadness\" rather than \"minor key progressions and descending melodic contours\". This leads to a critical hypothesis: <span class=\"ltx_text ltx_font_bold\">the performance degradation stems not from reasoning itself, but from reasoning about the wrong modality</span>. We trace this to a fundamental design choice: most audio language models initialize their reasoning capabilities through supervised fine-tuning on COT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib17\" title=\"\">17</a>]</cite> data derived from text-based models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib12\" title=\"\">12</a>]</cite>. Consequently, these models inherit linguistic grounding mechanisms, creating a modality mismatch that undermines performance as reasoning chains lengthen.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate this hypothesis and unlock reasoning capabilities in audio, we propose <span class=\"ltx_text ltx_font_italic\">Modality-Grounded Reasoning Distillation</span> (MGRD), an iterative training framework that progressively shifts reasoning from textual abstractions to acoustic properties. Starting from text-based reasoning initialization, MGRD employs iterative cycles of self-distillation and refinement on audio tasks, systematically curating reasoning chains that genuinely ground in acoustic analysis. Through these iterations, we obtain Step-Audio-R1, the first audio reasoning model that successfully benefits from test-time compute scaling, outperforming Gemini 2.5 Pro <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib14\" title=\"\">14</a>]</cite> and demonstrating capabilities competitive with the latest Gemini 3 Pro <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib18\" title=\"\">18</a>]</cite> across comprehensive audio benchmarks. These results confirm that reasoning is a transferable capability across modalities when appropriately anchored, transforming extended deliberation from a liability into a powerful asset for audio intelligence.</p>\n\n",
                "matched_terms": [
                    "across",
                    "gemini",
                    "stepaudior1",
                    "audio",
                    "model",
                    "benchmarks",
                    "pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Drawing from the architecture of our previous Step-Audio 2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib13\" title=\"\">13</a>]</cite>, Step-Audio-R1 is designed for audio-based reasoning tasks. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#S2.F2\" title=\"Figure 2 &#8227; 2 Model Overview &#8227; Step-Audio-R1 Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the model consists of an audio encoder, an audio adaptor, and an LLM decoder.</p>\n\n",
                "matched_terms": [
                    "stepaudior1",
                    "audio",
                    "stepaudio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the audio encoder, we utilize the Qwen2 audio encoder <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib19\" title=\"\">19</a>]</cite>, which is pretrained on various speech and audio understanding tasks. The audio encoder has an output frame rate of 25 Hz and is frozen during the entire training process. An audio adaptor with a downsampling rate of 2, identical to the one in Step-Audio 2, is employed to connect the audio encoder to the LLM, thereby reducing the output frame rate to 12.5 Hz.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "stepaudio",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The LLM decoder, based on Qwen2.5 32B <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib20\" title=\"\">20</a>]</cite>, directly takes the latent audio features from the audio adaptor as input to generate a purely textual output. The model is structured to first generate the reasoning content, followed by the final reply.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Step-Audio-R1 is pretrained using the same data and methodology as Step-Audio 2. Following this, the model undergoes a Post-Training phase, with specific details provided in Section 4.</p>\n\n",
                "matched_terms": [
                    "stepaudior1",
                    "stepaudio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Data:</span> This includes Automatic Speech Recognition (ASR), Paralinguistic Understanding, and standard Audio Question Text Answer (AQTA) dialogues.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio CoT Data:</span> We incorporate AQTA Chain-of-Thought (CoT) data, which is generated via self-distillation from our own model after its audio reasoning capabilities were elicited. This CoT data constitutes 10% of our total audio dataset.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Supervised Chain-of-Thought Initialization.</span> Given a base audio-language model <math alttext=\"\\pi_{\\theta_{0}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><msub><mi>&#952;</mi><mn>0</mn></msub></msub><annotation encoding=\"application/x-tex\">\\pi_{\\theta_{0}}</annotation></semantics></math>, we perform supervised fine-tuning on chain-of-thought demonstrations from both task-oriented and conversational domains, along with audio data to preserve multimodal capabilities. The training objective unifies three data sources:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This iterative process is motivated by the emergent audio Chain-of-Thought (CoT) capability observed after the cold-start phase. Our goal is to maintain and enhance this ability. We first construct a new set of perception-grounded questions based on our existing audio data. Then, at each iteration <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, we use the model from the previous iteration (<math alttext=\"\\pi_{\\theta_{t-1}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><msub><mi>&#952;</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub></msub><annotation encoding=\"application/x-tex\">\\pi_{\\theta_{t-1}}</annotation></semantics></math>) to perform self-distillation, generating new reasoning chains for this data.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Self-Distillation with Acoustic Reasoning.</span> At each iteration <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, we begin by curating audio data that strongly emphasizes perceptual analysis. Given an audio dataset <math alttext=\"\\mathcal{D}_{\\text{audio}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext>audio</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{D}_{\\text{audio}}</annotation></semantics></math>, we select examples <math alttext=\"(x_{\\text{audio}},q)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mtext>audio</mtext></msub><mo>,</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{\\text{audio}},q)</annotation></semantics></math> where answering question <math alttext=\"q\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m4\" intent=\":literal\"><semantics><mi>q</mi><annotation encoding=\"application/x-tex\">q</annotation></semantics></math> requires direct acoustic feature analysis rather than high-level semantic understanding. This selection prioritizes tasks demanding attention to timbral qualities, temporal patterns, pitch contours, rhythmic structures, and other low-level auditory properties, ensuring the model cannot rely on textual surrogates. For each selected audio-question pair <math alttext=\"(x_{\\text{audio}},q)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mtext>audio</mtext></msub><mo>,</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{\\text{audio}},q)</annotation></semantics></math>, we prompt the current model <math alttext=\"\\pi_{\\theta_{t}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m6\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><msub><mi>&#952;</mi><mi>t</mi></msub></msub><annotation encoding=\"application/x-tex\">\\pi_{\\theta_{t}}</annotation></semantics></math> to generate reasoning chains that explicitly reference acoustic features:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Reinforcement Learning.</span> We further refine the model through reinforcement learning on both audio and text tasks with carefully designed reward structures.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final model <math alttext=\"\\pi_{\\theta_{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p16.m1\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><msub><mi>&#952;</mi><mi>T</mi></msub></msub><annotation encoding=\"application/x-tex\">\\pi_{\\theta_{T}}</annotation></semantics></math> achieves the desired capability: generating extended reasoning chains that genuinely attend to audio properties, thereby unlocking test-time compute scaling benefits in the audio domain.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RL Data Curation and Filtering Details.</span> To construct the dataset for the RL phase, we extract text QA and audio data spanning diverse tasks and topics. We then filter these questions to identify a high-quality, challenging subset. Using the model from the <math alttext=\"t-1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">t-1</annotation></semantics></math> iteration, we sample <math alttext=\"k=8\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">k=8</annotation></semantics></math> responses for each question (<math alttext=\"pass@8\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">pass@8</annotation></semantics></math>). A question is selected for the RL dataset if the number of correct passes falls within the range of <math alttext=\"[3,6]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>3</mn><mo>,</mo><mn>6</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[3,6]</annotation></semantics></math>. This filtering mechanism ensures we select for problems that are relatively difficult, filtering out both overly simple questions (where <math alttext=\"pass@8&gt;6\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>8</mn></mrow><mo>&gt;</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">pass@8&gt;6</annotation></semantics></math>) and potentially harmful or nonsensical questions (where <math alttext=\"pass@8&lt;3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m6\" intent=\":literal\"><semantics><mrow><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>8</mn></mrow><mo>&lt;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">pass@8&lt;3</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Having established that audio intelligence can indeed benefit from deliberate reasoning, we now present a comprehensive empirical evaluation of Step-Audio-R1. Our assessment rigorously examines its capabilities across a spectrum of complex audio tasks, structured into two key benchmarks: the Evaluation on Speech-to-Text Benchmarks, which measures understanding and reasoning from acoustic signals,\nand the Evaluation on Speech-to-Speech Benchmarks, which assesses the model&#8217;s ability to perform generative and interactive reasoning in real-time spoken dialogue scenarios within the auditory domain.</p>\n\n",
                "matched_terms": [
                    "across",
                    "stepaudior1",
                    "audio",
                    "speechtotext",
                    "spoken",
                    "benchmarks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section evaluates the speech understanding and reasoning capabilities of Step-Audio-R1 against several state-of-the-art baselines: the powerful large-language model Gemini 2.5 Pro, the newly released Gemini 3 Pro, our own previous-generation model Step-Audio 2, and the base Step-Audio-R1 model. The assessment is conducted across a comprehensive suite of benchmarks designed to probe advanced audio intelligence. These include MMSU <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib23\" title=\"\">23</a>]</cite> and MMAU <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib24\" title=\"\">24</a>]</cite> for expert-level audio understanding and reasoning, Big Bench Audio<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/datasets/ArtificialAnalysis/big_bench_audio\" title=\"\">https://huggingface.co/datasets/ArtificialAnalysis/big_bench_audio</a></span></span></span> for complex multi-step logical reasoning from audio, Spoken MQA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib25\" title=\"\">25</a>]</cite> for mathematical reasoning with verbally expressed problems, and Wild Speech <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib26\" title=\"\">26</a>]</cite> for evaluating conversational speech.</p>\n\n",
                "matched_terms": [
                    "mmau",
                    "across",
                    "bench",
                    "mqa",
                    "stepaudior1",
                    "audio",
                    "big",
                    "gemini",
                    "model",
                    "wild",
                    "speech",
                    "mmsu",
                    "spoken",
                    "stepaudio",
                    "benchmarks",
                    "pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate the model&#8217;s performance on the Big Bench Audio speech-to-speech benchmark.\nThis benchmark consists of two major dimensions, namely the speech reasoning performance score, which assesses the model&#8217;s ability to perform reasoning over spoken content, and the latency metric, which measures response speed as an indicator of dialogue fluency.\nFollowing the design of the listen-while-thinking<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib27\" title=\"\">27</a>]</cite> and think-while-speaking<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib28\" title=\"\">28</a>]</cite> architecture, we adapt Step-Audio-R1 into Step-Audio-R1 Realtime, attaining high-quality reasoning together with rapid responsiveness.\nAccording to Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#S5.T2\" title=\"Table 2 &#8227; 5.2 Evaluation on Speech-to-Speech Benchmarks &#8227; 5 Evaluation &#8227; Step-Audio-R1 Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, Step-Audio-R1 Realtime reaches a speech reasoning performance score of 96.1%, outperforming exemplary closed-source systems including GPT Realtime 0825 and Gemini 2.5 Flash Native Audio Dialog.\nBesides, Step-Audio-R1 Realtime achieves a first-packet latency of 0.92 s, maintaining sub-second responsiveness that represents a highly competitive interaction performance among contemporary audio language models.\nThese results demonstrate that Step-Audio-R1 Realtime integrates real-time responsiveness with advanced reasoning capabilities, highlighting its potential for building efficient, intelligent, and interactive large audio language models.</p>\n\n",
                "matched_terms": [
                    "score",
                    "bench",
                    "gemini",
                    "stepaudior1",
                    "audio",
                    "big",
                    "speech",
                    "spoken",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate the necessity of our composite reward design for audio tasks, we conduct an ablation study comparing training with and without the format reward component. The results reveal crucial insights into how reward structure shapes model behavior in audio reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Format Reward Drives Stable Convergence.</span> Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#S6.F4.sf1\" title=\"Figure 4(a) &#8227; Figure 4 &#8227; 6.1 Extended Reasoning Benefits Audio: Evidence from Format Reward Ablation &#8227; 6 Empirical Study and Analysis &#8227; Step-Audio-R1 Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4(a)</span></a> presents the evolution of mean reward on audio tasks across training iterations. Both configurations eventually converge to similar reward levels (approximately 0.75-0.80), but their trajectories differ significantly. The model <span class=\"ltx_text ltx_font_italic\">with</span> think format reward (cyan line) achieves stable performance earlier, reaching the 0.70 threshold around iteration 35-40, while the model <span class=\"ltx_text ltx_font_italic\">without</span> format reward (blue line) requires nearly 25 iterations to reach comparable performance. More critically, the format-rewarded model maintains more stable training dynamics in later iterations (30-60), whereas the baseline exhibits higher variance and occasional performance drops. This stability advantage translates to meaningful performance gains: on the MMAU benchmark, incorporating the format reward improves accuracy from 76.5 to 77.7.</p>\n\n",
                "matched_terms": [
                    "mmau",
                    "across",
                    "audio",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Format Rewards Prevent Reasoning Collapse.</span> Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#S6.F4.sf2\" title=\"Figure 4(b) &#8227; Figure 4 &#8227; 6.1 Extended Reasoning Benefits Audio: Evidence from Format Reward Ablation &#8227; 6 Empirical Study and Analysis &#8227; Step-Audio-R1 Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4(b)</span></a> reveals a striking phenomenon that explains the performance difference. Without format rewards, the model exhibits systematic collapse of reasoning length: starting from approximately 3000 tokens in early iterations, it progressively decays to below 1500 tokens by iteration 60, with a particularly sharp decline after iteration 30. In contrast, the model with format rewards maintains substantially longer and more stable reasoning chains throughout training, consistently generating 2300-2800 tokens even in later iterations. This 50-80% increase in reasoning length is not merely superficial verbosity&#8212;the accompanying performance improvements on MMAU confirm that these extended deliberations contain meaningful acoustic analysis.</p>\n\n",
                "matched_terms": [
                    "mmau",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Extended Reasoning Improves Audio Understanding.</span> Most importantly, these training dynamics yield a fundamental shift in model capabilities: Step-Audio-R1 with extended reasoning chains consistently outperforms variants with minimal or no deliberation. This validates the central thesis of our work&#8212;that audio intelligence <span class=\"ltx_text ltx_font_italic\">can</span> benefit from extended deliberation when reasoning is properly grounded in acoustic properties. The performance gap between models with full reasoning (MMAU: 77.7) versus abbreviated or absent reasoning demonstrates that test-time compute scaling, once considered incompatible with audio tasks, now provides measurable advantages. This breakthrough confirms that the historical performance degradation with reasoning in audio models stemmed not from fundamental incompatibility, but from inadequate grounding mechanisms&#8212;a problem our MGRD framework successfully addresses.</p>\n\n",
                "matched_terms": [
                    "mmau",
                    "stepaudior1",
                    "audio",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Comparing Selection Criteria.</span> We evaluate two distinct data selection approaches for the RL phase: (1) <span class=\"ltx_text ltx_font_italic\">Consistently-failed problems</span>: questions where the SFT model from iteration <math alttext=\"t-1\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">t-1</annotation></semantics></math> fails all <math alttext=\"k=8\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">k=8</annotation></semantics></math> sampled attempts (<math alttext=\"pass@8=0\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>8</mn></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">pass@8=0</annotation></semantics></math>); (2) <span class=\"ltx_text ltx_font_italic\">Moderately difficult problems</span>: questions where correct passes fall within <math alttext=\"[3,6]\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>3</mn><mo>,</mo><mn>6</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[3,6]</annotation></semantics></math> out of 8 attempts, as described in our MGRD framework. Additionally, we experiment with (3) <span class=\"ltx_text ltx_font_italic\">Unfiltered scaling</span>: expanding the audio RL dataset to 200K examples without difficulty-based selection.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This performance gap stems from fundamental differences in learning signals. Problems where the SFT model consistently fails often indicate inherent ambiguity or insufficient information in the audio modality&#8212;for instance, inferring a car&#8217;s brand from engine sounds alone, a task trivial in vision but nearly impossible from audio. Without correct reasoning exemplars in sampled trajectories, the model explores blindly, unable to distinguish between genuine acoustic limitations and solvable challenges. Moderately difficult problems, conversely, provide a crucial mix: some responses demonstrate correct acoustic reasoning paths while others reveal common failure modes. This combination enables effective policy gradient updates&#8212;the model learns both successful reasoning strategies and mistakes to avoid, while naturally filtering out acoustically ambiguous questions.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scale Without Strategy Provides No Benefit.</span> Most surprisingly, we experiment with scaling the audio RL dataset to 200K examples&#8212;over 10&#215; our curated subset&#8212;and observe no performance improvement. This null result carries important implications: in audio reasoning tasks, data quality substantially outweighs quantity. Indiscriminate scaling introduces noise from acoustically ambiguous or inherently unsolvable problems, diluting the learning signal from genuinely informative examples. The effectiveness of challenging-but-solvable problems suggests that successful audio reasoning requires careful curriculum design rather than brute-force data scaling.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A critical challenge emerges when training Audio LLMs on massive textual data: models tend to develop incorrect self-cognition <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib13\" title=\"\">13</a>]</cite>. Due to the dominance of text-only patterns in the training corpora, these models frequently claim inability to process audio inputs by stating &#8220;I cannot hear sounds&#8221; or &#8220;I am a text model.&#8221; This misalignment between actual capabilities and self-perception severely undermines user experience and model utility. We address this systematic bias through a multi-stage correction pipeline combining iterative self-distillation with preference optimization.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Iterative Self-Distillation with Cognition Filtering.</span> Our correction process begins with targeted data curation. We construct a dataset of audio perception queries specifically designed to elicit self-cognition responses&#8212;questions about sound identification, audio quality assessment, and acoustic property analysis. During the self-distillation SFT iterations, we employ an LLM judge to filter responses exhibiting incorrect self-cognition. The judge evaluates whether the model appropriately acknowledges its audio processing capabilities or incorrectly identifies as text-only. Only responses with correct self-cognition pass to the next training round, progressively reinforcing accurate self-perception while eliminating erroneous beliefs.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Preference Optimization for Final Correction.</span> Following the filtered self-distillation phase, we apply DPO <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib29\" title=\"\">29</a>]</cite> for precise calibration. We construct 8,000 preference pairs through self-distillation: positive examples comprise responses where the model correctly acknowledges and utilizes its audio capabilities, while negative examples contain responses claiming text-only limitations. This contrastive learning directly targets the remaining self-cognition errors, teaching the model to consistently choose responses aligned with its true multimodal nature. Despite the relatively modest dataset size, this targeted approach proves remarkably effective due to the specificity of the correction task.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Progressive Error Reduction.</span> Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#S6.T3\" title=\"Table 3 &#8227; 6.3 Self-Cognition Correction Through Iterative Refinement &#8227; 6 Empirical Study and Analysis &#8227; Step-Audio-R1 Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> demonstrates the effectiveness of our multi-stage approach. The base model exhibits noticeable self-cognition errors (6.76%), reflecting the bias from text training data. Through iterative self-distillation, we successfully reduce the error rate to 2.63% by filtering misaligned responses and reinforcing correct self-perception. However, the most dramatic improvement comes from the final DPO alignment with 8,000 preference pairs: error rates plummet to near-zero (0.02%), effectively eliminating self-cognition misalignment. This final stage proves crucial&#8212;while self-distillation significantly improves cognition, only explicit preference optimization achieves complete correction. The efficiency of this approach highlights the power of targeted preference learning for addressing specific behavioral biases. For a detailed qualitative comparison of model responses before and after Modality-Grounded Reasoning Distillation, please refer to Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#A1.SS2\" title=\"A.2 Case Studies in Self-Recognition &#8227; Appendix A Appendix &#8227; Step-Audio-R1 Technical Report\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The success of this approach reveals an important insight: self-cognition errors are not fundamental model limitations but learned biases from training data distribution. Through systematic correction combining iterative refinement with targeted preference optimization, we demonstrate that models can maintain accurate self-perception even when pretrained on predominantly text data. This correction is essential for Step-Audio-R1&#8217;s deployment&#8212;users expect the model to confidently engage with audio inputs rather than apologetically claim incapability.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we address the challenging problem where audio language models historically fail to benefit from long reasoning processes, often performing worse as the reasoning length increases. We identify the primary cause of this failure as &#8220;textual surrogate reasoning&#8221;&#8212;a persistent tendency for models to reason based on text descriptions, such as transcripts or captions, rather than focusing on actual acoustic properties. We introduce Step-Audio-R1, the first model to successfully unlock and benefit from deliberate thinking in the audio domain. Our core contribution is Modality-Grounded Reasoning Distillation (MGRD), an iterative framework that progressively shifts the model&#8217;s reasoning basis from text-based patterns to genuine acoustic analysis. Comprehensive evaluations confirm that Step-Audio-R1 outperforms strong baselines, including Gemini 2.5 Pro, and achieves performance comparable to the state-of-the-art Gemini 3 Pro across a wide range of complex audio understanding and reasoning benchmarks. These results provide clear evidence that reasoning is a capability that works across modalities; when properly connected to the correct input, extended reasoning transforms from a weakness into a powerful asset for audio intelligence, opening new paths for building truly multimodal systems.</p>\n\n",
                "matched_terms": [
                    "across",
                    "gemini",
                    "stepaudior1",
                    "audio",
                    "model",
                    "benchmarks",
                    "pro",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we demonstrate the model&#8217;s advanced capabilities in audio reasoning across diverse scenarios. As shown in the following examples, the model can effectively capture and reason about paralinguistic features (e.g., emotion, vocal characteristics, prosody) and identify complex environmental sound scenes. These cases highlight the model&#8217;s versatility in processing rich acoustic information beyond simple speech recognition.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio",
                    "across",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section illustrates the effectiveness of Modality-Grounded Reasoning Distillation (introduced in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#S4.SS2\" title=\"4.2 Modality-Grounded Reasoning Distillation &#8227; 4 Post-Training Recipes &#8227; Step-Audio-R1 Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>) in establishing correct self-recognition. We compare the model&#8217;s responses before and after this training stage. The &#8220;Before&#8221; case reveals a common issue where the model, influenced by its text-only backbone initialization, incorrectly claims inability to process audio. The &#8220;After&#8221; case demonstrates how the distillation process corrects this, enabling the model to acknowledge and utilize its audio modality for in-depth reasoning.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            }
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Performance comparison of representative models on the Big Bench Audio speech-to-speech benchmark. The benchmark comprises two evaluation metrics: the Speech Reasoning Performance Score (%), measuring the models reasoning ability over spoken content, and the first-packet Latency (seconds) metric, quantifying response speed as an indicator of dialogue fluency.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Speech Reasoning Performance Score (%)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Latency (seconds)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">GPT-4o mini Realtime</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.81</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">GPT Realtime 0825</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">83</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.98</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Gemini 2.5 Flash Live</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">74</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.64</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Gemini 2.5 Flash Native Audio Dialog</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">92</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">0.63</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Step-Audio-R1 Realtime</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">96.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">0.92</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "representative",
            "models",
            "comprises",
            "models",
            "content",
            "seconds",
            "big",
            "dialogue",
            "evaluation",
            "indicator",
            "spoken",
            "comparison",
            "two",
            "bench",
            "audio",
            "dialog",
            "speed",
            "performance",
            "over",
            "stepaudior1",
            "metrics",
            "response",
            "live",
            "fluency",
            "ability",
            "speechtospeech",
            "realtime",
            "reasoning",
            "speech",
            "metric",
            "gpt",
            "benchmark",
            "score",
            "gpt4o",
            "gemini",
            "quantifying",
            "firstpacket",
            "model",
            "flash",
            "latency",
            "native",
            "measuring",
            "mini"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">In this section, we evaluate the model&#8217;s performance on the Big Bench Audio speech-to-speech benchmark.\nThis benchmark consists of two major dimensions, namely the speech reasoning performance score, which assesses the model&#8217;s ability to perform reasoning over spoken content, and the latency metric, which measures response speed as an indicator of dialogue fluency.\nFollowing the design of the listen-while-thinking<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib27\" title=\"\">27</a>]</cite> and think-while-speaking<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib28\" title=\"\">28</a>]</cite> architecture, we adapt Step-Audio-R1 into Step-Audio-R1 Realtime, attaining high-quality reasoning together with rapid responsiveness.\nAccording to Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#S5.T2\" title=\"Table 2 &#8227; 5.2 Evaluation on Speech-to-Speech Benchmarks &#8227; 5 Evaluation &#8227; Step-Audio-R1 Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, Step-Audio-R1 Realtime reaches a speech reasoning performance score of 96.1%, outperforming exemplary closed-source systems including GPT Realtime 0825 and Gemini 2.5 Flash Native Audio Dialog.\nBesides, Step-Audio-R1 Realtime achieves a first-packet latency of 0.92 s, maintaining sub-second responsiveness that represents a highly competitive interaction performance among contemporary audio language models.\nThese results demonstrate that Step-Audio-R1 Realtime integrates real-time responsiveness with advanced reasoning capabilities, highlighting its potential for building efficient, intelligent, and interactive large audio language models.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\">Recent advances in reasoning models have demonstrated remarkable success in text and vision domains through extended chain-of-thought deliberation. However, a perplexing phenomenon persists in audio language models: they consistently perform better with minimal or no reasoning, raising a fundamental question&#8212;<span class=\"ltx_text ltx_font_bold\">can audio intelligence truly benefit from deliberate thinking?</span> We introduce Step-Audio-R1, the first audio reasoning model that successfully unlocks reasoning capabilities in the audio domain. Through our proposed Modality-Grounded Reasoning Distillation (MGRD) framework, Step-Audio-R1 learns to generate audio-relevant reasoning chains that genuinely ground themselves in acoustic features rather than hallucinating disconnected deliberations. Our model exhibits strong audio reasoning capabilities, surpassing Gemini 2.5 Pro and achieving performance comparable to the state-of-the-art Gemini 3 Pro across comprehensive audio understanding and reasoning benchmarks spanning speech, environmental sounds, and music. These results demonstrate that reasoning is a transferable capability across modalities when appropriately anchored, transforming extended deliberation from a liability into a powerful asset for audio intelligence. By establishing the first successful audio reasoning model, Step-Audio-R1 opens new pathways toward building truly multimodal reasoning systems that think deeply across all sensory modalities.\n\n<span class=\"ltx_figure\" id=\"S0.F1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"215\" id=\"S0.F1.g1\" src=\"x1.png\" width=\"561\"/>\n<span class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 1: </span>Benchmark performance of Step-Audio-R1</span>\n</span></span>\n</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "models",
                    "gemini",
                    "stepaudior1",
                    "audio",
                    "model",
                    "speech",
                    "reasoning",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Chain-of-thought reasoning has transformed modern artificial intelligence, enabling language models to solve complex mathematical problems <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib3\" title=\"\">3</a>]</cite>, generate executable code <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib4\" title=\"\">4</a>]</cite>, and engage in sophisticated logical deduction through extended deliberation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib5\" title=\"\">5</a>]</cite>. Vision-language models have similarly adopted this paradigm, leveraging deliberate reasoning to interpret spatial relationships <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib2\" title=\"\">2</a>]</cite>, analyze visual scenes, and answer intricate questions about images <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib7\" title=\"\">7</a>]</cite>. Underlying these successes is a fundamental principle known as test-time compute scaling: allocating more computational resources during inference&#8212;through longer chains of thought, iterative refinement, or search&#8212;predictably improves model performance <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib10\" title=\"\">10</a>]</cite>. This scaling law has become so robust that it now guides the design and deployment of AI systems across modalities.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "models",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The audio domain, however, presents a stark exception to this principle. Existing audio language models consistently demonstrate superior performance with minimal or no reasoning <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib12\" title=\"\">12</a>]</cite>. Empirical observations across benchmarks reveal that direct responses outperform elaborate chain-of-thought explanations, with performance systematically degrading as reasoning length increases <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib14\" title=\"\">14</a>]</cite>. This inverted scaling behavior persists across architectures, training methodologies, and model scales <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib13\" title=\"\">13</a>]</cite>, suggesting a fundamental incompatibility between test-time compute scaling and auditory intelligence. This raises a critical question:</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio",
                    "model",
                    "reasoning",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent efforts have attempted to address this anomaly through reinforcement learning approaches that employ language model judges to verify consistency between reasoning chains and final answers <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib16\" title=\"\">16</a>]</cite>. While these methods improve alignment, they treat the symptom rather than the root cause&#8212;enforcing consistency without understanding <span class=\"ltx_text ltx_font_italic\">why</span> reasoning fails in audio. Through systematic case studies, we uncover a striking pattern: existing audio language models engage in <span class=\"ltx_text ltx_font_italic\">textual surrogate reasoning</span> rather than acoustic reasoning. When prompted to deliberate, models systematically reason from the perspective of transcripts or textual captions instead of acoustic properties&#8212;for instance, attributing musical melancholy to \"lyrics mentioning sadness\" rather than \"minor key progressions and descending melodic contours\". This leads to a critical hypothesis: <span class=\"ltx_text ltx_font_bold\">the performance degradation stems not from reasoning itself, but from reasoning about the wrong modality</span>. We trace this to a fundamental design choice: most audio language models initialize their reasoning capabilities through supervised fine-tuning on COT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib17\" title=\"\">17</a>]</cite> data derived from text-based models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib12\" title=\"\">12</a>]</cite>. Consequently, these models inherit linguistic grounding mechanisms, creating a modality mismatch that undermines performance as reasoning chains lengthen.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio",
                    "model",
                    "reasoning",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate this hypothesis and unlock reasoning capabilities in audio, we propose <span class=\"ltx_text ltx_font_italic\">Modality-Grounded Reasoning Distillation</span> (MGRD), an iterative training framework that progressively shifts reasoning from textual abstractions to acoustic properties. Starting from text-based reasoning initialization, MGRD employs iterative cycles of self-distillation and refinement on audio tasks, systematically curating reasoning chains that genuinely ground in acoustic analysis. Through these iterations, we obtain Step-Audio-R1, the first audio reasoning model that successfully benefits from test-time compute scaling, outperforming Gemini 2.5 Pro <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib14\" title=\"\">14</a>]</cite> and demonstrating capabilities competitive with the latest Gemini 3 Pro <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib18\" title=\"\">18</a>]</cite> across comprehensive audio benchmarks. These results confirm that reasoning is a transferable capability across modalities when appropriately anchored, transforming extended deliberation from a liability into a powerful asset for audio intelligence.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "stepaudior1",
                    "audio",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Drawing from the architecture of our previous Step-Audio 2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib13\" title=\"\">13</a>]</cite>, Step-Audio-R1 is designed for audio-based reasoning tasks. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#S2.F2\" title=\"Figure 2 &#8227; 2 Model Overview &#8227; Step-Audio-R1 Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the model consists of an audio encoder, an audio adaptor, and an LLM decoder.</p>\n\n",
                "matched_terms": [
                    "stepaudior1",
                    "audio",
                    "reasoning",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the audio encoder, we utilize the Qwen2 audio encoder <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib19\" title=\"\">19</a>]</cite>, which is pretrained on various speech and audio understanding tasks. The audio encoder has an output frame rate of 25 Hz and is frozen during the entire training process. An audio adaptor with a downsampling rate of 2, identical to the one in Step-Audio 2, is employed to connect the audio encoder to the LLM, thereby reducing the output frame rate to 12.5 Hz.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The LLM decoder, based on Qwen2.5 32B <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib20\" title=\"\">20</a>]</cite>, directly takes the latent audio features from the audio adaptor as input to generate a purely textual output. The model is structured to first generate the reasoning content, followed by the final reply.</p>\n\n",
                "matched_terms": [
                    "content",
                    "audio",
                    "reasoning",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A key innovation in this process is the Modality-Grounded Reasoning Distillation (MGRD) method. Initially, the model&#8217;s reasoning process may operate on a purely semantic level. MGRD iteratively refines these thoughts, progressively strengthening their connection to the underlying audio features until they evolve into \"native audio think.\" This distillation process ensures that the model&#8217;s reasoning is not merely about the transcribed text, but is deeply grounded in the acoustic nuances of the audio itself, leading to a more holistic and accurate final response.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio",
                    "response",
                    "native",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Step-Audio-R1 is pretrained using the same data and methodology as Step-Audio 2. Following this, the model undergoes a Post-Training phase, with specific details provided in Section 4.</p>\n\n",
                "matched_terms": [
                    "stepaudior1",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our cold-start phase is designed to jointly elicit audio reasoning capabilities through a combination of Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verified Reward (RLVR). This phase utilizes a total dataset of 5 million samples. This token budget is comprised of 1B tokens from text-only data, with the remaining 4B tokens derived from our audio-side data.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Data:</span> This includes Automatic Speech Recognition (ASR), Paralinguistic Understanding, and standard Audio Question Text Answer (AQTA) dialogues.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio CoT Data:</span> We incorporate AQTA Chain-of-Thought (CoT) data, which is generated via self-distillation from our own model after its audio reasoning capabilities were elicited. This CoT data constitutes 10% of our total audio dataset.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A critical aspect of our data strategy is the standardized reasoning format. To train the model to recognize the reasoning structure, we prepend all samples lacking native CoT with an empty <span class=\"ltx_text ltx_font_typewriter\">&lt;think&gt;</span> tag. The format is standardized as:\n<span class=\"ltx_text ltx_font_typewriter\">&lt;think&gt;\\n\\n&lt;/think&gt;\\n{response}</span></p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "model",
                    "native"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We establish fundamental reasoning capabilities through a two-stage training process that builds robust reasoning primitives while maintaining basic audio understanding.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Supervised Chain-of-Thought Initialization.</span> Given a base audio-language model <math alttext=\"\\pi_{\\theta_{0}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><msub><mi>&#952;</mi><mn>0</mn></msub></msub><annotation encoding=\"application/x-tex\">\\pi_{\\theta_{0}}</annotation></semantics></math>, we perform supervised fine-tuning on chain-of-thought demonstrations from both task-oriented and conversational domains, along with audio data to preserve multimodal capabilities. The training objective unifies three data sources:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"(q,r,a)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(q,r,a)</annotation></semantics></math> denotes task questions with reasoning chains and answers, <math alttext=\"(c,r,s)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(c,r,s)</annotation></semantics></math> represents conversational contexts with deliberation and responses, and <math alttext=\"(x_{\\text{audio}},q,a)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mtext>audio</mtext></msub><mo>,</mo><mi>q</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{\\text{audio}},q,a)</annotation></semantics></math> indicates audio questions with direct answers. For audio data, we use empty reasoning markers (i.e., &lt;think&gt;\\n\\n&lt;/think&gt;\\n) to maintain the structural format without actual deliberation content. This tri-modal training instills diverse reasoning patterns in text domains&#8212;spanning analytical problem-solving, code generation, logical inference, and contextual dialogue&#8212;while preserving the model&#8217;s audio understanding capabilities for subsequent acoustic reasoning distillation.</p>\n\n",
                "matched_terms": [
                    "content",
                    "audio",
                    "models",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Reinforcement Learning with Verified Rewards.</span> Building upon the supervised foundation, we refine reasoning quality on task-oriented data through Reinforcement Learning with Verified Rewards (RLVR) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib3\" title=\"\">3</a>]</cite>. For mathematical problems, coding challenges, and logical puzzles, the model samples reasoning trajectories and receives binary verification rewards:</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With textual reasoning foundation established, we now address the core challenge: transforming reasoning capabilities from textual abstractions to acoustic grounding. We propose an iterative self-distillation framework that progressively refines the model&#8217;s reasoning to genuinely attend to audio properties.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "audio",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This iterative process is motivated by the emergent audio Chain-of-Thought (CoT) capability observed after the cold-start phase. Our goal is to maintain and enhance this ability. We first construct a new set of perception-grounded questions based on our existing audio data. Then, at each iteration <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, we use the model from the previous iteration (<math alttext=\"\\pi_{\\theta_{t-1}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><msub><mi>&#952;</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub></msub><annotation encoding=\"application/x-tex\">\\pi_{\\theta_{t-1}}</annotation></semantics></math>) to perform self-distillation, generating new reasoning chains for this data.</p>\n\n",
                "matched_terms": [
                    "ability",
                    "audio",
                    "reasoning",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Self-Distillation with Acoustic Reasoning.</span> At each iteration <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, we begin by curating audio data that strongly emphasizes perceptual analysis. Given an audio dataset <math alttext=\"\\mathcal{D}_{\\text{audio}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext>audio</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{D}_{\\text{audio}}</annotation></semantics></math>, we select examples <math alttext=\"(x_{\\text{audio}},q)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mtext>audio</mtext></msub><mo>,</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{\\text{audio}},q)</annotation></semantics></math> where answering question <math alttext=\"q\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m4\" intent=\":literal\"><semantics><mi>q</mi><annotation encoding=\"application/x-tex\">q</annotation></semantics></math> requires direct acoustic feature analysis rather than high-level semantic understanding. This selection prioritizes tasks demanding attention to timbral qualities, temporal patterns, pitch contours, rhythmic structures, and other low-level auditory properties, ensuring the model cannot rely on textual surrogates. For each selected audio-question pair <math alttext=\"(x_{\\text{audio}},q)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mtext>audio</mtext></msub><mo>,</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{\\text{audio}},q)</annotation></semantics></math>, we prompt the current model <math alttext=\"\\pi_{\\theta_{t}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m6\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><msub><mi>&#952;</mi><mi>t</mi></msub></msub><annotation encoding=\"application/x-tex\">\\pi_{\\theta_{t}}</annotation></semantics></math> to generate reasoning chains that explicitly reference acoustic features:</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Reinforcement Learning.</span> We further refine the model through reinforcement learning on both audio and text tasks with carefully designed reward structures.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This design prioritizes answer correctness (0.8 weight) while incentivizing reasoning generation (0.2 weight), preventing the model from reverting to direct responses. The combined optimization objective is:</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Iterative Refinement.</span> We repeat this cycle for <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p15.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> iterations, with each iteration <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p15.m2\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> producing model <math alttext=\"\\pi_{\\theta_{t+1}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p15.m3\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><msub><mi>&#952;</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></msub><annotation encoding=\"application/x-tex\">\\pi_{\\theta_{t+1}}</annotation></semantics></math> that generates progressively more acoustically-grounded reasoning. As iterations advance, the model&#8217;s reasoning chains shift from textual surrogates&#8212;such as inferring emotion from \"lyrics mentioning sadness\"&#8212;to genuine acoustic analysis&#8212;such as \"minor key progressions and descending melodic contours.\" This iterative distillation progressively transforms the model&#8217;s reasoning substrate from linguistic to acoustic grounding.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final model <math alttext=\"\\pi_{\\theta_{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p16.m1\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><msub><mi>&#952;</mi><mi>T</mi></msub></msub><annotation encoding=\"application/x-tex\">\\pi_{\\theta_{T}}</annotation></semantics></math> achieves the desired capability: generating extended reasoning chains that genuinely attend to audio properties, thereby unlocking test-time compute scaling benefits in the audio domain.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RL Data Curation and Filtering Details.</span> To construct the dataset for the RL phase, we extract text QA and audio data spanning diverse tasks and topics. We then filter these questions to identify a high-quality, challenging subset. Using the model from the <math alttext=\"t-1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">t-1</annotation></semantics></math> iteration, we sample <math alttext=\"k=8\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">k=8</annotation></semantics></math> responses for each question (<math alttext=\"pass@8\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">pass@8</annotation></semantics></math>). A question is selected for the RL dataset if the number of correct passes falls within the range of <math alttext=\"[3,6]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>3</mn><mo>,</mo><mn>6</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[3,6]</annotation></semantics></math>. This filtering mechanism ensures we select for problems that are relatively difficult, filtering out both overly simple questions (where <math alttext=\"pass@8&gt;6\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>8</mn></mrow><mo>&gt;</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">pass@8&gt;6</annotation></semantics></math>) and potentially harmful or nonsensical questions (where <math alttext=\"pass@8&lt;3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m6\" intent=\":literal\"><semantics><mrow><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>8</mn></mrow><mo>&lt;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">pass@8&lt;3</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RL Implementation Details</span> We employ an on-policy Proximal Policy Optimization framework <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib22\" title=\"\">22</a>]</cite> with binary verification rewards: responses receive a reward of 1.0 when matching verified solutions and 0.0 otherwise. Critically, we remove reference model KL penalties by setting the penalty coefficient to zero, allowing the model to freely explore reasoning strategies without being constrained by its initialization distribution. During training, we sample 16 candidate responses per prompt, assigning rewards exclusively at the final token position to encourage complete reasoning trajectories. We configure PPO with a clipping parameter of 0.2 and set both the discount factor and GAE lambda to 1.0, training on sequences up to 10,240 tokens to accommodate extended deliberation.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Having established that audio intelligence can indeed benefit from deliberate reasoning, we now present a comprehensive empirical evaluation of Step-Audio-R1. Our assessment rigorously examines its capabilities across a spectrum of complex audio tasks, structured into two key benchmarks: the Evaluation on Speech-to-Text Benchmarks, which measures understanding and reasoning from acoustic signals,\nand the Evaluation on Speech-to-Speech Benchmarks, which assesses the model&#8217;s ability to perform generative and interactive reasoning in real-time spoken dialogue scenarios within the auditory domain.</p>\n\n",
                "matched_terms": [
                    "models",
                    "stepaudior1",
                    "audio",
                    "reasoning",
                    "dialogue",
                    "ability",
                    "evaluation",
                    "speechtospeech",
                    "realtime",
                    "spoken",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section evaluates the speech understanding and reasoning capabilities of Step-Audio-R1 against several state-of-the-art baselines: the powerful large-language model Gemini 2.5 Pro, the newly released Gemini 3 Pro, our own previous-generation model Step-Audio 2, and the base Step-Audio-R1 model. The assessment is conducted across a comprehensive suite of benchmarks designed to probe advanced audio intelligence. These include MMSU <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib23\" title=\"\">23</a>]</cite> and MMAU <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib24\" title=\"\">24</a>]</cite> for expert-level audio understanding and reasoning, Big Bench Audio<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/datasets/ArtificialAnalysis/big_bench_audio\" title=\"\">https://huggingface.co/datasets/ArtificialAnalysis/big_bench_audio</a></span></span></span> for complex multi-step logical reasoning from audio, Spoken MQA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib25\" title=\"\">25</a>]</cite> for mathematical reasoning with verbally expressed problems, and Wild Speech <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib26\" title=\"\">26</a>]</cite> for evaluating conversational speech.</p>\n\n",
                "matched_terms": [
                    "bench",
                    "gemini",
                    "stepaudior1",
                    "audio",
                    "big",
                    "reasoning",
                    "model",
                    "speech",
                    "spoken"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#S5.T1\" title=\"Table 1 &#8227; 5.1 Evaluation on Speech-to-Text Benchmarks &#8227; 5 Evaluation &#8227; Step-Audio-R1 Technical Report\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, Step-Audio-R1 achieves an average score of 83.6%, significantly outperforming Gemini 2.5 Pro while being slightly lower than Gemini 3 Pro. This competitive performance confirms that our MGRD approach effectively enhances deep audio comprehension.</p>\n\n",
                "matched_terms": [
                    "score",
                    "gemini",
                    "stepaudior1",
                    "audio",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate the necessity of our composite reward design for audio tasks, we conduct an ablation study comparing training with and without the format reward component. The results reveal crucial insights into how reward structure shapes model behavior in audio reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Format Reward Drives Stable Convergence.</span> Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#S6.F4.sf1\" title=\"Figure 4(a) &#8227; Figure 4 &#8227; 6.1 Extended Reasoning Benefits Audio: Evidence from Format Reward Ablation &#8227; 6 Empirical Study and Analysis &#8227; Step-Audio-R1 Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4(a)</span></a> presents the evolution of mean reward on audio tasks across training iterations. Both configurations eventually converge to similar reward levels (approximately 0.75-0.80), but their trajectories differ significantly. The model <span class=\"ltx_text ltx_font_italic\">with</span> think format reward (cyan line) achieves stable performance earlier, reaching the 0.70 threshold around iteration 35-40, while the model <span class=\"ltx_text ltx_font_italic\">without</span> format reward (blue line) requires nearly 25 iterations to reach comparable performance. More critically, the format-rewarded model maintains more stable training dynamics in later iterations (30-60), whereas the baseline exhibits higher variance and occasional performance drops. This stability advantage translates to meaningful performance gains: on the MMAU benchmark, incorporating the format reward improves accuracy from 76.5 to 77.7.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "audio",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Format Rewards Prevent Reasoning Collapse.</span> Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#S6.F4.sf2\" title=\"Figure 4(b) &#8227; Figure 4 &#8227; 6.1 Extended Reasoning Benefits Audio: Evidence from Format Reward Ablation &#8227; 6 Empirical Study and Analysis &#8227; Step-Audio-R1 Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4(b)</span></a> reveals a striking phenomenon that explains the performance difference. Without format rewards, the model exhibits systematic collapse of reasoning length: starting from approximately 3000 tokens in early iterations, it progressively decays to below 1500 tokens by iteration 60, with a particularly sharp decline after iteration 30. In contrast, the model with format rewards maintains substantially longer and more stable reasoning chains throughout training, consistently generating 2300-2800 tokens even in later iterations. This 50-80% increase in reasoning length is not merely superficial verbosity&#8212;the accompanying performance improvements on MMAU confirm that these extended deliberations contain meaningful acoustic analysis.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The collapse pattern reveals a critical failure mode: without explicit incentives for reasoning generation, reinforcement learning naturally gravitates toward the most token-efficient strategy&#8212;direct answers without deliberation. This optimization pressure directly contradicts our goal of developing genuine reasoning capabilities. The think format reward component acts as a crucial regularizer, ensuring the model maintains extended thought chains even when pure accuracy-based rewards might prefer brevity.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Extended Reasoning Improves Audio Understanding.</span> Most importantly, these training dynamics yield a fundamental shift in model capabilities: Step-Audio-R1 with extended reasoning chains consistently outperforms variants with minimal or no deliberation. This validates the central thesis of our work&#8212;that audio intelligence <span class=\"ltx_text ltx_font_italic\">can</span> benefit from extended deliberation when reasoning is properly grounded in acoustic properties. The performance gap between models with full reasoning (MMAU: 77.7) versus abbreviated or absent reasoning demonstrates that test-time compute scaling, once considered incompatible with audio tasks, now provides measurable advantages. This breakthrough confirms that the historical performance degradation with reasoning in audio models stemmed not from fundamental incompatibility, but from inadequate grounding mechanisms&#8212;a problem our MGRD framework successfully addresses.</p>\n\n",
                "matched_terms": [
                    "models",
                    "stepaudior1",
                    "audio",
                    "model",
                    "reasoning",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We discover that careful data curation proves more critical than dataset volume for audio reasoning tasks. Through comparing three data selection strategies, we reveal what constitutes effective training data for MGRD.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Comparing Selection Criteria.</span> We evaluate two distinct data selection approaches for the RL phase: (1) <span class=\"ltx_text ltx_font_italic\">Consistently-failed problems</span>: questions where the SFT model from iteration <math alttext=\"t-1\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">t-1</annotation></semantics></math> fails all <math alttext=\"k=8\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">k=8</annotation></semantics></math> sampled attempts (<math alttext=\"pass@8=0\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>8</mn></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">pass@8=0</annotation></semantics></math>); (2) <span class=\"ltx_text ltx_font_italic\">Moderately difficult problems</span>: questions where correct passes fall within <math alttext=\"[3,6]\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>3</mn><mo>,</mo><mn>6</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[3,6]</annotation></semantics></math> out of 8 attempts, as described in our MGRD framework. Additionally, we experiment with (3) <span class=\"ltx_text ltx_font_italic\">Unfiltered scaling</span>: expanding the audio RL dataset to 200K examples without difficulty-based selection.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Learning from Partial Success Outperforms Learning from Failure.</span> Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#S6.F5.sf1\" title=\"Figure 5(a) &#8227; Figure 5 &#8227; 6.2 Strategic Data Selection: Quality Over Quantity in Audio RL &#8227; 6 Empirical Study and Analysis &#8227; Step-Audio-R1 Technical Report\"><span class=\"ltx_text ltx_ref_tag\">5(a)</span></a> reveals striking differences in training dynamics. Models trained on moderately difficult problems (cyan line, \"correct passed problem\") achieve substantially higher and more stable rewards, converging to approximately 0.75-0.80 by iteration 20 and maintaining this performance throughout training. In sharp contrast, models trained exclusively on consistently-failed problems (blue line, \"failed problem\") exhibit significantly lower rewards (0.45-0.70) with higher variance and unstable convergence, eventually collapsing after iteration 50.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This performance gap stems from fundamental differences in learning signals. Problems where the SFT model consistently fails often indicate inherent ambiguity or insufficient information in the audio modality&#8212;for instance, inferring a car&#8217;s brand from engine sounds alone, a task trivial in vision but nearly impossible from audio. Without correct reasoning exemplars in sampled trajectories, the model explores blindly, unable to distinguish between genuine acoustic limitations and solvable challenges. Moderately difficult problems, conversely, provide a crucial mix: some responses demonstrate correct acoustic reasoning paths while others reveal common failure modes. This combination enables effective policy gradient updates&#8212;the model learns both successful reasoning strategies and mistakes to avoid, while naturally filtering out acoustically ambiguous questions.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "audio",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Reasoning Complexity Reflects Learning Quality.</span> Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#S6.F5.sf2\" title=\"Figure 5(b) &#8227; Figure 5 &#8227; 6.2 Strategic Data Selection: Quality Over Quantity in Audio RL &#8227; 6 Empirical Study and Analysis &#8227; Step-Audio-R1 Technical Report\"><span class=\"ltx_text ltx_ref_tag\">5(b)</span></a> corroborates this finding through reasoning length analysis. Initially, both strategies generate similar reasoning lengths (approximately 3000-3500 tokens in early iterations), as they start from the same SFT checkpoint. However, their trajectories diverge significantly after iteration 20. Models trained on moderately difficult problems (cyan line) maintain substantial reasoning chains, stabilizing at 2300-2800 tokens throughout later training, demonstrating sustained deliberation. Models trained on consistently-failed problems (blue line), however, show progressive decline: reasoning length gradually decreases from iteration 20 onward, eventually settling around 1800-2000 tokens by iteration 60&#8212;a 30-40% reduction from the moderately difficult setting.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This divergence reveals how data quality shapes reasoning behavior over time. While both models initially maintain extended thinking inherited from SFT, continued training on consistently-failed problems gradually erodes this capability. The absence of successful reasoning exemplars provides no positive reinforcement for extended deliberation, causing the model to progressively abandon lengthy reasoning chains. In contrast, moderately difficult problems&#8212;which contain both successful and failed attempts&#8212;sustain the model&#8217;s reasoning complexity by rewarding extended acoustic analysis that leads to correct answers.</p>\n\n",
                "matched_terms": [
                    "over",
                    "models",
                    "models",
                    "model",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scale Without Strategy Provides No Benefit.</span> Most surprisingly, we experiment with scaling the audio RL dataset to 200K examples&#8212;over 10&#215; our curated subset&#8212;and observe no performance improvement. This null result carries important implications: in audio reasoning tasks, data quality substantially outweighs quantity. Indiscriminate scaling introduces noise from acoustically ambiguous or inherently unsolvable problems, diluting the learning signal from genuinely informative examples. The effectiveness of challenging-but-solvable problems suggests that successful audio reasoning requires careful curriculum design rather than brute-force data scaling.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "audio",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A critical challenge emerges when training Audio LLMs on massive textual data: models tend to develop incorrect self-cognition <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib13\" title=\"\">13</a>]</cite>. Due to the dominance of text-only patterns in the training corpora, these models frequently claim inability to process audio inputs by stating &#8220;I cannot hear sounds&#8221; or &#8220;I am a text model.&#8221; This misalignment between actual capabilities and self-perception severely undermines user experience and model utility. We address this systematic bias through a multi-stage correction pipeline combining iterative self-distillation with preference optimization.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Iterative Self-Distillation with Cognition Filtering.</span> Our correction process begins with targeted data curation. We construct a dataset of audio perception queries specifically designed to elicit self-cognition responses&#8212;questions about sound identification, audio quality assessment, and acoustic property analysis. During the self-distillation SFT iterations, we employ an LLM judge to filter responses exhibiting incorrect self-cognition. The judge evaluates whether the model appropriately acknowledges its audio processing capabilities or incorrectly identifies as text-only. Only responses with correct self-cognition pass to the next training round, progressively reinforcing accurate self-perception while eliminating erroneous beliefs.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Preference Optimization for Final Correction.</span> Following the filtered self-distillation phase, we apply DPO <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib29\" title=\"\">29</a>]</cite> for precise calibration. We construct 8,000 preference pairs through self-distillation: positive examples comprise responses where the model correctly acknowledges and utilizes its audio capabilities, while negative examples contain responses claiming text-only limitations. This contrastive learning directly targets the remaining self-cognition errors, teaching the model to consistently choose responses aligned with its true multimodal nature. Despite the relatively modest dataset size, this targeted approach proves remarkably effective due to the specificity of the correction task.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Progressive Error Reduction.</span> Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#S6.T3\" title=\"Table 3 &#8227; 6.3 Self-Cognition Correction Through Iterative Refinement &#8227; 6 Empirical Study and Analysis &#8227; Step-Audio-R1 Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> demonstrates the effectiveness of our multi-stage approach. The base model exhibits noticeable self-cognition errors (6.76%), reflecting the bias from text training data. Through iterative self-distillation, we successfully reduce the error rate to 2.63% by filtering misaligned responses and reinforcing correct self-perception. However, the most dramatic improvement comes from the final DPO alignment with 8,000 preference pairs: error rates plummet to near-zero (0.02%), effectively eliminating self-cognition misalignment. This final stage proves crucial&#8212;while self-distillation significantly improves cognition, only explicit preference optimization achieves complete correction. The efficiency of this approach highlights the power of targeted preference learning for addressing specific behavioral biases. For a detailed qualitative comparison of model responses before and after Modality-Grounded Reasoning Distillation, please refer to Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#A1.SS2\" title=\"A.2 Case Studies in Self-Recognition &#8227; Appendix A Appendix &#8227; Step-Audio-R1 Technical Report\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "model",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The success of this approach reveals an important insight: self-cognition errors are not fundamental model limitations but learned biases from training data distribution. Through systematic correction combining iterative refinement with targeted preference optimization, we demonstrate that models can maintain accurate self-perception even when pretrained on predominantly text data. This correction is essential for Step-Audio-R1&#8217;s deployment&#8212;users expect the model to confidently engage with audio inputs rather than apologetically claim incapability.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we address the challenging problem where audio language models historically fail to benefit from long reasoning processes, often performing worse as the reasoning length increases. We identify the primary cause of this failure as &#8220;textual surrogate reasoning&#8221;&#8212;a persistent tendency for models to reason based on text descriptions, such as transcripts or captions, rather than focusing on actual acoustic properties. We introduce Step-Audio-R1, the first model to successfully unlock and benefit from deliberate thinking in the audio domain. Our core contribution is Modality-Grounded Reasoning Distillation (MGRD), an iterative framework that progressively shifts the model&#8217;s reasoning basis from text-based patterns to genuine acoustic analysis. Comprehensive evaluations confirm that Step-Audio-R1 outperforms strong baselines, including Gemini 2.5 Pro, and achieves performance comparable to the state-of-the-art Gemini 3 Pro across a wide range of complex audio understanding and reasoning benchmarks. These results provide clear evidence that reasoning is a capability that works across modalities; when properly connected to the correct input, extended reasoning transforms from a weakness into a powerful asset for audio intelligence, opening new paths for building truly multimodal systems.</p>\n\n",
                "matched_terms": [
                    "models",
                    "models",
                    "gemini",
                    "stepaudior1",
                    "audio",
                    "model",
                    "reasoning",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we demonstrate the model&#8217;s advanced capabilities in audio reasoning across diverse scenarios. As shown in the following examples, the model can effectively capture and reason about paralinguistic features (e.g., emotion, vocal characteristics, prosody) and identify complex environmental sound scenes. These cases highlight the model&#8217;s versatility in processing rich acoustic information beyond simple speech recognition.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio",
                    "model",
                    "speech",
                    "reasoning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section illustrates the effectiveness of Modality-Grounded Reasoning Distillation (introduced in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#S4.SS2\" title=\"4.2 Modality-Grounded Reasoning Distillation &#8227; 4 Post-Training Recipes &#8227; Step-Audio-R1 Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>) in establishing correct self-recognition. We compare the model&#8217;s responses before and after this training stage. The &#8220;Before&#8221; case reveals a common issue where the model, influenced by its text-only backbone initialization, incorrectly claims inability to process audio. The &#8220;After&#8221; case demonstrates how the distillation process corrects this, enabling the model to acknowledge and utilize its audio modality for in-depth reasoning.</p>\n\n",
                "matched_terms": [
                    "reasoning",
                    "audio",
                    "models",
                    "model"
                ]
            }
        ]
    },
    "S6.T3": {
        "caption": "Table 3: Self-cognition error rates across training stages on our constructed test set of 5,000 diverse audio perception samples. Error rate measures the percentage of responses where the model incorrectly claims inability to process audio.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Training Stage</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Self-Cognition Error Rate</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Base model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.76%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Iterative Self-Distillation</td>\n<td class=\"ltx_td ltx_align_center\">2.63%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Iterative Self-Distillation + DPO</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.02%</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "selfcognition",
            "claims",
            "our",
            "error",
            "measures",
            "rate",
            "audio",
            "stage",
            "base",
            "selfdistillation",
            "incorrectly",
            "percentage",
            "training",
            "test",
            "rates",
            "inability",
            "across",
            "responses",
            "samples",
            "constructed",
            "where",
            "set",
            "process",
            "model",
            "perception",
            "stages",
            "iterative",
            "dpo",
            "diverse"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Progressive Error Reduction.</span> Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#S6.T3\" title=\"Table 3 &#8227; 6.3 Self-Cognition Correction Through Iterative Refinement &#8227; 6 Empirical Study and Analysis &#8227; Step-Audio-R1 Technical Report\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> demonstrates the effectiveness of our multi-stage approach. The base model exhibits noticeable self-cognition errors (6.76%), reflecting the bias from text training data. Through iterative self-distillation, we successfully reduce the error rate to 2.63% by filtering misaligned responses and reinforcing correct self-perception. However, the most dramatic improvement comes from the final DPO alignment with 8,000 preference pairs: error rates plummet to near-zero (0.02%), effectively eliminating self-cognition misalignment. This final stage proves crucial&#8212;while self-distillation significantly improves cognition, only explicit preference optimization achieves complete correction. The efficiency of this approach highlights the power of targeted preference learning for addressing specific behavioral biases. For a detailed qualitative comparison of model responses before and after Modality-Grounded Reasoning Distillation, please refer to Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#A1.SS2\" title=\"A.2 Case Studies in Self-Recognition &#8227; Appendix A Appendix &#8227; Step-Audio-R1 Technical Report\"><span class=\"ltx_text ltx_ref_tag\">A.2</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\">Recent advances in reasoning models have demonstrated remarkable success in text and vision domains through extended chain-of-thought deliberation. However, a perplexing phenomenon persists in audio language models: they consistently perform better with minimal or no reasoning, raising a fundamental question&#8212;<span class=\"ltx_text ltx_font_bold\">can audio intelligence truly benefit from deliberate thinking?</span> We introduce Step-Audio-R1, the first audio reasoning model that successfully unlocks reasoning capabilities in the audio domain. Through our proposed Modality-Grounded Reasoning Distillation (MGRD) framework, Step-Audio-R1 learns to generate audio-relevant reasoning chains that genuinely ground themselves in acoustic features rather than hallucinating disconnected deliberations. Our model exhibits strong audio reasoning capabilities, surpassing Gemini 2.5 Pro and achieving performance comparable to the state-of-the-art Gemini 3 Pro across comprehensive audio understanding and reasoning benchmarks spanning speech, environmental sounds, and music. These results demonstrate that reasoning is a transferable capability across modalities when appropriately anchored, transforming extended deliberation from a liability into a powerful asset for audio intelligence. By establishing the first successful audio reasoning model, Step-Audio-R1 opens new pathways toward building truly multimodal reasoning systems that think deeply across all sensory modalities.\n\n<span class=\"ltx_figure\" id=\"S0.F1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"215\" id=\"S0.F1.g1\" src=\"x1.png\" width=\"561\"/>\n<span class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 1: </span>Benchmark performance of Step-Audio-R1</span>\n</span></span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Chain-of-thought reasoning has transformed modern artificial intelligence, enabling language models to solve complex mathematical problems <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib3\" title=\"\">3</a>]</cite>, generate executable code <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib4\" title=\"\">4</a>]</cite>, and engage in sophisticated logical deduction through extended deliberation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib5\" title=\"\">5</a>]</cite>. Vision-language models have similarly adopted this paradigm, leveraging deliberate reasoning to interpret spatial relationships <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib2\" title=\"\">2</a>]</cite>, analyze visual scenes, and answer intricate questions about images <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib7\" title=\"\">7</a>]</cite>. Underlying these successes is a fundamental principle known as test-time compute scaling: allocating more computational resources during inference&#8212;through longer chains of thought, iterative refinement, or search&#8212;predictably improves model performance <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib10\" title=\"\">10</a>]</cite>. This scaling law has become so robust that it now guides the design and deployment of AI systems across modalities.</p>\n\n",
                "matched_terms": [
                    "across",
                    "iterative",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The audio domain, however, presents a stark exception to this principle. Existing audio language models consistently demonstrate superior performance with minimal or no reasoning <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib12\" title=\"\">12</a>]</cite>. Empirical observations across benchmarks reveal that direct responses outperform elaborate chain-of-thought explanations, with performance systematically degrading as reasoning length increases <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib14\" title=\"\">14</a>]</cite>. This inverted scaling behavior persists across architectures, training methodologies, and model scales <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib13\" title=\"\">13</a>]</cite>, suggesting a fundamental incompatibility between test-time compute scaling and auditory intelligence. This raises a critical question:</p>\n\n",
                "matched_terms": [
                    "across",
                    "responses",
                    "audio",
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent efforts have attempted to address this anomaly through reinforcement learning approaches that employ language model judges to verify consistency between reasoning chains and final answers <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib16\" title=\"\">16</a>]</cite>. While these methods improve alignment, they treat the symptom rather than the root cause&#8212;enforcing consistency without understanding <span class=\"ltx_text ltx_font_italic\">why</span> reasoning fails in audio. Through systematic case studies, we uncover a striking pattern: existing audio language models engage in <span class=\"ltx_text ltx_font_italic\">textual surrogate reasoning</span> rather than acoustic reasoning. When prompted to deliberate, models systematically reason from the perspective of transcripts or textual captions instead of acoustic properties&#8212;for instance, attributing musical melancholy to \"lyrics mentioning sadness\" rather than \"minor key progressions and descending melodic contours\". This leads to a critical hypothesis: <span class=\"ltx_text ltx_font_bold\">the performance degradation stems not from reasoning itself, but from reasoning about the wrong modality</span>. We trace this to a fundamental design choice: most audio language models initialize their reasoning capabilities through supervised fine-tuning on COT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib17\" title=\"\">17</a>]</cite> data derived from text-based models <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib12\" title=\"\">12</a>]</cite>. Consequently, these models inherit linguistic grounding mechanisms, creating a modality mismatch that undermines performance as reasoning chains lengthen.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate this hypothesis and unlock reasoning capabilities in audio, we propose <span class=\"ltx_text ltx_font_italic\">Modality-Grounded Reasoning Distillation</span> (MGRD), an iterative training framework that progressively shifts reasoning from textual abstractions to acoustic properties. Starting from text-based reasoning initialization, MGRD employs iterative cycles of self-distillation and refinement on audio tasks, systematically curating reasoning chains that genuinely ground in acoustic analysis. Through these iterations, we obtain Step-Audio-R1, the first audio reasoning model that successfully benefits from test-time compute scaling, outperforming Gemini 2.5 Pro <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib14\" title=\"\">14</a>]</cite> and demonstrating capabilities competitive with the latest Gemini 3 Pro <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib18\" title=\"\">18</a>]</cite> across comprehensive audio benchmarks. These results confirm that reasoning is a transferable capability across modalities when appropriately anchored, transforming extended deliberation from a liability into a powerful asset for audio intelligence.</p>\n\n",
                "matched_terms": [
                    "across",
                    "selfdistillation",
                    "audio",
                    "model",
                    "iterative",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Drawing from the architecture of our previous Step-Audio 2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib13\" title=\"\">13</a>]</cite>, Step-Audio-R1 is designed for audio-based reasoning tasks. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#S2.F2\" title=\"Figure 2 &#8227; 2 Model Overview &#8227; Step-Audio-R1 Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the model consists of an audio encoder, an audio adaptor, and an LLM decoder.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the audio encoder, we utilize the Qwen2 audio encoder <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib19\" title=\"\">19</a>]</cite>, which is pretrained on various speech and audio understanding tasks. The audio encoder has an output frame rate of 25 Hz and is frozen during the entire training process. An audio adaptor with a downsampling rate of 2, identical to the one in Step-Audio 2, is employed to connect the audio encoder to the LLM, thereby reducing the output frame rate to 12.5 Hz.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "audio",
                    "training",
                    "process"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The LLM decoder, based on Qwen2.5 32B <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib20\" title=\"\">20</a>]</cite>, directly takes the latent audio features from the audio adaptor as input to generate a purely textual output. The model is structured to first generate the reasoning content, followed by the final reply.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A key innovation in this process is the Modality-Grounded Reasoning Distillation (MGRD) method. Initially, the model&#8217;s reasoning process may operate on a purely semantic level. MGRD iteratively refines these thoughts, progressively strengthening their connection to the underlying audio features until they evolve into \"native audio think.\" This distillation process ensures that the model&#8217;s reasoning is not merely about the transcribed text, but is deeply grounded in the acoustic nuances of the audio itself, leading to a more holistic and accurate final response.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "process"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our cold-start phase is designed to jointly elicit audio reasoning capabilities through a combination of Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verified Reward (RLVR). This phase utilizes a total dataset of 5 million samples. This token budget is comprised of 1B tokens from text-only data, with the remaining 4B tokens derived from our audio-side data.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "samples",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio CoT Data:</span> We incorporate AQTA Chain-of-Thought (CoT) data, which is generated via self-distillation from our own model after its audio reasoning capabilities were elicited. This CoT data constitutes 10% of our total audio dataset.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "selfdistillation",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A critical aspect of our data strategy is the standardized reasoning format. To train the model to recognize the reasoning structure, we prepend all samples lacking native CoT with an empty <span class=\"ltx_text ltx_font_typewriter\">&lt;think&gt;</span> tag. The format is standardized as:\n<span class=\"ltx_text ltx_font_typewriter\">&lt;think&gt;\\n\\n&lt;/think&gt;\\n{response}</span></p>\n\n",
                "matched_terms": [
                    "samples",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the subsequent Reinforcement Learning (RL) phase, we curated a smaller, high-quality dataset of 5,000 samples. This dataset is composed of 2,000 high-quality text-only samples (focusing on math and code) and 3,000 augmented speech-based QA samples. The augmentation methods used to process this data are described in detail in Section 4.2.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "process"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We establish fundamental reasoning capabilities through a two-stage training process that builds robust reasoning primitives while maintaining basic audio understanding.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "training",
                    "process"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Supervised Chain-of-Thought Initialization.</span> Given a base audio-language model <math alttext=\"\\pi_{\\theta_{0}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><msub><mi>&#952;</mi><mn>0</mn></msub></msub><annotation encoding=\"application/x-tex\">\\pi_{\\theta_{0}}</annotation></semantics></math>, we perform supervised fine-tuning on chain-of-thought demonstrations from both task-oriented and conversational domains, along with audio data to preserve multimodal capabilities. The training objective unifies three data sources:</p>\n\n",
                "matched_terms": [
                    "base",
                    "audio",
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"(q,r,a)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>q</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(q,r,a)</annotation></semantics></math> denotes task questions with reasoning chains and answers, <math alttext=\"(c,r,s)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(c,r,s)</annotation></semantics></math> represents conversational contexts with deliberation and responses, and <math alttext=\"(x_{\\text{audio}},q,a)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mtext>audio</mtext></msub><mo>,</mo><mi>q</mi><mo>,</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{\\text{audio}},q,a)</annotation></semantics></math> indicates audio questions with direct answers. For audio data, we use empty reasoning markers (i.e., &lt;think&gt;\\n\\n&lt;/think&gt;\\n) to maintain the structural format without actual deliberation content. This tri-modal training instills diverse reasoning patterns in text domains&#8212;spanning analytical problem-solving, code generation, logical inference, and contextual dialogue&#8212;while preserving the model&#8217;s audio understanding capabilities for subsequent acoustic reasoning distillation.</p>\n\n",
                "matched_terms": [
                    "responses",
                    "audio",
                    "training",
                    "diverse",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Reinforcement Learning with Verified Rewards.</span> Building upon the supervised foundation, we refine reasoning quality on task-oriented data through Reinforcement Learning with Verified Rewards (RLVR) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib21\" title=\"\">21</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib3\" title=\"\">3</a>]</cite>. For mathematical problems, coding challenges, and logical puzzles, the model samples reasoning trajectories and receives binary verification rewards:</p>\n\n",
                "matched_terms": [
                    "samples",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">With textual reasoning foundation established, we now address the core challenge: transforming reasoning capabilities from textual abstractions to acoustic grounding. We propose an iterative self-distillation framework that progressively refines the model&#8217;s reasoning to genuinely attend to audio properties.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "iterative",
                    "selfdistillation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This iterative process is motivated by the emergent audio Chain-of-Thought (CoT) capability observed after the cold-start phase. Our goal is to maintain and enhance this ability. We first construct a new set of perception-grounded questions based on our existing audio data. Then, at each iteration <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, we use the model from the previous iteration (<math alttext=\"\\pi_{\\theta_{t-1}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><msub><mi>&#952;</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub></msub><annotation encoding=\"application/x-tex\">\\pi_{\\theta_{t-1}}</annotation></semantics></math>) to perform self-distillation, generating new reasoning chains for this data.</p>\n\n",
                "matched_terms": [
                    "set",
                    "process",
                    "selfdistillation",
                    "audio",
                    "model",
                    "iterative",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Self-Distillation with Acoustic Reasoning.</span> At each iteration <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, we begin by curating audio data that strongly emphasizes perceptual analysis. Given an audio dataset <math alttext=\"\\mathcal{D}_{\\text{audio}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mtext>audio</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{D}_{\\text{audio}}</annotation></semantics></math>, we select examples <math alttext=\"(x_{\\text{audio}},q)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mtext>audio</mtext></msub><mo>,</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{\\text{audio}},q)</annotation></semantics></math> where answering question <math alttext=\"q\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m4\" intent=\":literal\"><semantics><mi>q</mi><annotation encoding=\"application/x-tex\">q</annotation></semantics></math> requires direct acoustic feature analysis rather than high-level semantic understanding. This selection prioritizes tasks demanding attention to timbral qualities, temporal patterns, pitch contours, rhythmic structures, and other low-level auditory properties, ensuring the model cannot rely on textual surrogates. For each selected audio-question pair <math alttext=\"(x_{\\text{audio}},q)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mtext>audio</mtext></msub><mo>,</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{\\text{audio}},q)</annotation></semantics></math>, we prompt the current model <math alttext=\"\\pi_{\\theta_{t}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p3.m6\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><msub><mi>&#952;</mi><mi>t</mi></msub></msub><annotation encoding=\"application/x-tex\">\\pi_{\\theta_{t}}</annotation></semantics></math> to generate reasoning chains that explicitly reference acoustic features:</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "where",
                    "selfdistillation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal Reinforcement Learning.</span> We further refine the model through reinforcement learning on both audio and text tasks with carefully designed reward structures.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This design prioritizes answer correctness (0.8 weight) while incentivizing reasoning generation (0.2 weight), preventing the model from reverting to direct responses. The combined optimization objective is:</p>\n\n",
                "matched_terms": [
                    "model",
                    "responses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Iterative Refinement.</span> We repeat this cycle for <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p15.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> iterations, with each iteration <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p15.m2\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> producing model <math alttext=\"\\pi_{\\theta_{t+1}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p15.m3\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><msub><mi>&#952;</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></msub><annotation encoding=\"application/x-tex\">\\pi_{\\theta_{t+1}}</annotation></semantics></math> that generates progressively more acoustically-grounded reasoning. As iterations advance, the model&#8217;s reasoning chains shift from textual surrogates&#8212;such as inferring emotion from \"lyrics mentioning sadness\"&#8212;to genuine acoustic analysis&#8212;such as \"minor key progressions and descending melodic contours.\" This iterative distillation progressively transforms the model&#8217;s reasoning substrate from linguistic to acoustic grounding.</p>\n\n",
                "matched_terms": [
                    "iterative",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final model <math alttext=\"\\pi_{\\theta_{T}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p16.m1\" intent=\":literal\"><semantics><msub><mi>&#960;</mi><msub><mi>&#952;</mi><mi>T</mi></msub></msub><annotation encoding=\"application/x-tex\">\\pi_{\\theta_{T}}</annotation></semantics></math> achieves the desired capability: generating extended reasoning chains that genuinely attend to audio properties, thereby unlocking test-time compute scaling benefits in the audio domain.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RL Data Curation and Filtering Details.</span> To construct the dataset for the RL phase, we extract text QA and audio data spanning diverse tasks and topics. We then filter these questions to identify a high-quality, challenging subset. Using the model from the <math alttext=\"t-1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">t-1</annotation></semantics></math> iteration, we sample <math alttext=\"k=8\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">k=8</annotation></semantics></math> responses for each question (<math alttext=\"pass@8\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">pass@8</annotation></semantics></math>). A question is selected for the RL dataset if the number of correct passes falls within the range of <math alttext=\"[3,6]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>3</mn><mo>,</mo><mn>6</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[3,6]</annotation></semantics></math>. This filtering mechanism ensures we select for problems that are relatively difficult, filtering out both overly simple questions (where <math alttext=\"pass@8&gt;6\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>8</mn></mrow><mo>&gt;</mo><mn>6</mn></mrow><annotation encoding=\"application/x-tex\">pass@8&gt;6</annotation></semantics></math>) and potentially harmful or nonsensical questions (where <math alttext=\"pass@8&lt;3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m6\" intent=\":literal\"><semantics><mrow><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>8</mn></mrow><mo>&lt;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">pass@8&lt;3</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "responses",
                    "audio",
                    "model",
                    "diverse",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RL Implementation Details</span> We employ an on-policy Proximal Policy Optimization framework <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib22\" title=\"\">22</a>]</cite> with binary verification rewards: responses receive a reward of 1.0 when matching verified solutions and 0.0 otherwise. Critically, we remove reference model KL penalties by setting the penalty coefficient to zero, allowing the model to freely explore reasoning strategies without being constrained by its initialization distribution. During training, we sample 16 candidate responses per prompt, assigning rewards exclusively at the final token position to encourage complete reasoning trajectories. We configure PPO with a clipping parameter of 0.2 and set both the discount factor and GAE lambda to 1.0, training on sequences up to 10,240 tokens to accommodate extended deliberation.</p>\n\n",
                "matched_terms": [
                    "set",
                    "model",
                    "training",
                    "responses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Having established that audio intelligence can indeed benefit from deliberate reasoning, we now present a comprehensive empirical evaluation of Step-Audio-R1. Our assessment rigorously examines its capabilities across a spectrum of complex audio tasks, structured into two key benchmarks: the Evaluation on Speech-to-Text Benchmarks, which measures understanding and reasoning from acoustic signals,\nand the Evaluation on Speech-to-Speech Benchmarks, which assesses the model&#8217;s ability to perform generative and interactive reasoning in real-time spoken dialogue scenarios within the auditory domain.</p>\n\n",
                "matched_terms": [
                    "measures",
                    "across",
                    "audio",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section evaluates the speech understanding and reasoning capabilities of Step-Audio-R1 against several state-of-the-art baselines: the powerful large-language model Gemini 2.5 Pro, the newly released Gemini 3 Pro, our own previous-generation model Step-Audio 2, and the base Step-Audio-R1 model. The assessment is conducted across a comprehensive suite of benchmarks designed to probe advanced audio intelligence. These include MMSU <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib23\" title=\"\">23</a>]</cite> and MMAU <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib24\" title=\"\">24</a>]</cite> for expert-level audio understanding and reasoning, Big Bench Audio<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/datasets/ArtificialAnalysis/big_bench_audio\" title=\"\">https://huggingface.co/datasets/ArtificialAnalysis/big_bench_audio</a></span></span></span> for complex multi-step logical reasoning from audio, Spoken MQA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib25\" title=\"\">25</a>]</cite> for mathematical reasoning with verbally expressed problems, and Wild Speech <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib26\" title=\"\">26</a>]</cite> for evaluating conversational speech.</p>\n\n",
                "matched_terms": [
                    "across",
                    "base",
                    "audio",
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#S5.T1\" title=\"Table 1 &#8227; 5.1 Evaluation on Speech-to-Text Benchmarks &#8227; 5 Evaluation &#8227; Step-Audio-R1 Technical Report\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, Step-Audio-R1 achieves an average score of 83.6%, significantly outperforming Gemini 2.5 Pro while being slightly lower than Gemini 3 Pro. This competitive performance confirms that our MGRD approach effectively enhances deep audio comprehension.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we evaluate the model&#8217;s performance on the Big Bench Audio speech-to-speech benchmark.\nThis benchmark consists of two major dimensions, namely the speech reasoning performance score, which assesses the model&#8217;s ability to perform reasoning over spoken content, and the latency metric, which measures response speed as an indicator of dialogue fluency.\nFollowing the design of the listen-while-thinking<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib27\" title=\"\">27</a>]</cite> and think-while-speaking<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib28\" title=\"\">28</a>]</cite> architecture, we adapt Step-Audio-R1 into Step-Audio-R1 Realtime, attaining high-quality reasoning together with rapid responsiveness.\nAccording to Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#S5.T2\" title=\"Table 2 &#8227; 5.2 Evaluation on Speech-to-Speech Benchmarks &#8227; 5 Evaluation &#8227; Step-Audio-R1 Technical Report\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, Step-Audio-R1 Realtime reaches a speech reasoning performance score of 96.1%, outperforming exemplary closed-source systems including GPT Realtime 0825 and Gemini 2.5 Flash Native Audio Dialog.\nBesides, Step-Audio-R1 Realtime achieves a first-packet latency of 0.92 s, maintaining sub-second responsiveness that represents a highly competitive interaction performance among contemporary audio language models.\nThese results demonstrate that Step-Audio-R1 Realtime integrates real-time responsiveness with advanced reasoning capabilities, highlighting its potential for building efficient, intelligent, and interactive large audio language models.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "measures"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate the necessity of our composite reward design for audio tasks, we conduct an ablation study comparing training with and without the format reward component. The results reveal crucial insights into how reward structure shapes model behavior in audio reasoning tasks.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Format Reward Drives Stable Convergence.</span> Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#S6.F4.sf1\" title=\"Figure 4(a) &#8227; Figure 4 &#8227; 6.1 Extended Reasoning Benefits Audio: Evidence from Format Reward Ablation &#8227; 6 Empirical Study and Analysis &#8227; Step-Audio-R1 Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4(a)</span></a> presents the evolution of mean reward on audio tasks across training iterations. Both configurations eventually converge to similar reward levels (approximately 0.75-0.80), but their trajectories differ significantly. The model <span class=\"ltx_text ltx_font_italic\">with</span> think format reward (cyan line) achieves stable performance earlier, reaching the 0.70 threshold around iteration 35-40, while the model <span class=\"ltx_text ltx_font_italic\">without</span> format reward (blue line) requires nearly 25 iterations to reach comparable performance. More critically, the format-rewarded model maintains more stable training dynamics in later iterations (30-60), whereas the baseline exhibits higher variance and occasional performance drops. This stability advantage translates to meaningful performance gains: on the MMAU benchmark, incorporating the format reward improves accuracy from 76.5 to 77.7.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "across",
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Format Rewards Prevent Reasoning Collapse.</span> Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#S6.F4.sf2\" title=\"Figure 4(b) &#8227; Figure 4 &#8227; 6.1 Extended Reasoning Benefits Audio: Evidence from Format Reward Ablation &#8227; 6 Empirical Study and Analysis &#8227; Step-Audio-R1 Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4(b)</span></a> reveals a striking phenomenon that explains the performance difference. Without format rewards, the model exhibits systematic collapse of reasoning length: starting from approximately 3000 tokens in early iterations, it progressively decays to below 1500 tokens by iteration 60, with a particularly sharp decline after iteration 30. In contrast, the model with format rewards maintains substantially longer and more stable reasoning chains throughout training, consistently generating 2300-2800 tokens even in later iterations. This 50-80% increase in reasoning length is not merely superficial verbosity&#8212;the accompanying performance improvements on MMAU confirm that these extended deliberations contain meaningful acoustic analysis.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The collapse pattern reveals a critical failure mode: without explicit incentives for reasoning generation, reinforcement learning naturally gravitates toward the most token-efficient strategy&#8212;direct answers without deliberation. This optimization pressure directly contradicts our goal of developing genuine reasoning capabilities. The think format reward component acts as a crucial regularizer, ensuring the model maintains extended thought chains even when pure accuracy-based rewards might prefer brevity.</p>\n\n",
                "matched_terms": [
                    "model",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Extended Reasoning Improves Audio Understanding.</span> Most importantly, these training dynamics yield a fundamental shift in model capabilities: Step-Audio-R1 with extended reasoning chains consistently outperforms variants with minimal or no deliberation. This validates the central thesis of our work&#8212;that audio intelligence <span class=\"ltx_text ltx_font_italic\">can</span> benefit from extended deliberation when reasoning is properly grounded in acoustic properties. The performance gap between models with full reasoning (MMAU: 77.7) versus abbreviated or absent reasoning demonstrates that test-time compute scaling, once considered incompatible with audio tasks, now provides measurable advantages. This breakthrough confirms that the historical performance degradation with reasoning in audio models stemmed not from fundamental incompatibility, but from inadequate grounding mechanisms&#8212;a problem our MGRD framework successfully addresses.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "training",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We discover that careful data curation proves more critical than dataset volume for audio reasoning tasks. Through comparing three data selection strategies, we reveal what constitutes effective training data for MGRD.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Comparing Selection Criteria.</span> We evaluate two distinct data selection approaches for the RL phase: (1) <span class=\"ltx_text ltx_font_italic\">Consistently-failed problems</span>: questions where the SFT model from iteration <math alttext=\"t-1\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">t-1</annotation></semantics></math> fails all <math alttext=\"k=8\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding=\"application/x-tex\">k=8</annotation></semantics></math> sampled attempts (<math alttext=\"pass@8=0\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m3\" intent=\":literal\"><semantics><mrow><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathvariant=\"normal\">@</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mn>8</mn></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">pass@8=0</annotation></semantics></math>); (2) <span class=\"ltx_text ltx_font_italic\">Moderately difficult problems</span>: questions where correct passes fall within <math alttext=\"[3,6]\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>3</mn><mo>,</mo><mn>6</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[3,6]</annotation></semantics></math> out of 8 attempts, as described in our MGRD framework. Additionally, we experiment with (3) <span class=\"ltx_text ltx_font_italic\">Unfiltered scaling</span>: expanding the audio RL dataset to 200K examples without difficulty-based selection.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "where",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This performance gap stems from fundamental differences in learning signals. Problems where the SFT model consistently fails often indicate inherent ambiguity or insufficient information in the audio modality&#8212;for instance, inferring a car&#8217;s brand from engine sounds alone, a task trivial in vision but nearly impossible from audio. Without correct reasoning exemplars in sampled trajectories, the model explores blindly, unable to distinguish between genuine acoustic limitations and solvable challenges. Moderately difficult problems, conversely, provide a crucial mix: some responses demonstrate correct acoustic reasoning paths while others reveal common failure modes. This combination enables effective policy gradient updates&#8212;the model learns both successful reasoning strategies and mistakes to avoid, while naturally filtering out acoustically ambiguous questions.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "where",
                    "responses"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This divergence reveals how data quality shapes reasoning behavior over time. While both models initially maintain extended thinking inherited from SFT, continued training on consistently-failed problems gradually erodes this capability. The absence of successful reasoning exemplars provides no positive reinforcement for extended deliberation, causing the model to progressively abandon lengthy reasoning chains. In contrast, moderately difficult problems&#8212;which contain both successful and failed attempts&#8212;sustain the model&#8217;s reasoning complexity by rewarding extended acoustic analysis that leads to correct answers.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scale Without Strategy Provides No Benefit.</span> Most surprisingly, we experiment with scaling the audio RL dataset to 200K examples&#8212;over 10&#215; our curated subset&#8212;and observe no performance improvement. This null result carries important implications: in audio reasoning tasks, data quality substantially outweighs quantity. Indiscriminate scaling introduces noise from acoustically ambiguous or inherently unsolvable problems, diluting the learning signal from genuinely informative examples. The effectiveness of challenging-but-solvable problems suggests that successful audio reasoning requires careful curriculum design rather than brute-force data scaling.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A critical challenge emerges when training Audio LLMs on massive textual data: models tend to develop incorrect self-cognition <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib13\" title=\"\">13</a>]</cite>. Due to the dominance of text-only patterns in the training corpora, these models frequently claim inability to process audio inputs by stating &#8220;I cannot hear sounds&#8221; or &#8220;I am a text model.&#8221; This misalignment between actual capabilities and self-perception severely undermines user experience and model utility. We address this systematic bias through a multi-stage correction pipeline combining iterative self-distillation with preference optimization.</p>\n\n",
                "matched_terms": [
                    "inability",
                    "selfdistillation",
                    "process",
                    "audio",
                    "model",
                    "iterative",
                    "training",
                    "selfcognition"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Iterative Self-Distillation with Cognition Filtering.</span> Our correction process begins with targeted data curation. We construct a dataset of audio perception queries specifically designed to elicit self-cognition responses&#8212;questions about sound identification, audio quality assessment, and acoustic property analysis. During the self-distillation SFT iterations, we employ an LLM judge to filter responses exhibiting incorrect self-cognition. The judge evaluates whether the model appropriately acknowledges its audio processing capabilities or incorrectly identifies as text-only. Only responses with correct self-cognition pass to the next training round, progressively reinforcing accurate self-perception while eliminating erroneous beliefs.</p>\n\n",
                "matched_terms": [
                    "selfdistillation",
                    "responses",
                    "audio",
                    "process",
                    "model",
                    "perception",
                    "incorrectly",
                    "iterative",
                    "training",
                    "selfcognition",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Preference Optimization for Final Correction.</span> Following the filtered self-distillation phase, we apply DPO <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#bib.bib29\" title=\"\">29</a>]</cite> for precise calibration. We construct 8,000 preference pairs through self-distillation: positive examples comprise responses where the model correctly acknowledges and utilizes its audio capabilities, while negative examples contain responses claiming text-only limitations. This contrastive learning directly targets the remaining self-cognition errors, teaching the model to consistently choose responses aligned with its true multimodal nature. Despite the relatively modest dataset size, this targeted approach proves remarkably effective due to the specificity of the correction task.</p>\n\n",
                "matched_terms": [
                    "responses",
                    "selfdistillation",
                    "audio",
                    "model",
                    "dpo",
                    "selfcognition",
                    "where"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The success of this approach reveals an important insight: self-cognition errors are not fundamental model limitations but learned biases from training data distribution. Through systematic correction combining iterative refinement with targeted preference optimization, we demonstrate that models can maintain accurate self-perception even when pretrained on predominantly text data. This correction is essential for Step-Audio-R1&#8217;s deployment&#8212;users expect the model to confidently engage with audio inputs rather than apologetically claim incapability.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "iterative",
                    "training",
                    "selfcognition"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we address the challenging problem where audio language models historically fail to benefit from long reasoning processes, often performing worse as the reasoning length increases. We identify the primary cause of this failure as &#8220;textual surrogate reasoning&#8221;&#8212;a persistent tendency for models to reason based on text descriptions, such as transcripts or captions, rather than focusing on actual acoustic properties. We introduce Step-Audio-R1, the first model to successfully unlock and benefit from deliberate thinking in the audio domain. Our core contribution is Modality-Grounded Reasoning Distillation (MGRD), an iterative framework that progressively shifts the model&#8217;s reasoning basis from text-based patterns to genuine acoustic analysis. Comprehensive evaluations confirm that Step-Audio-R1 outperforms strong baselines, including Gemini 2.5 Pro, and achieves performance comparable to the state-of-the-art Gemini 3 Pro across a wide range of complex audio understanding and reasoning benchmarks. These results provide clear evidence that reasoning is a capability that works across modalities; when properly connected to the correct input, extended reasoning transforms from a weakness into a powerful asset for audio intelligence, opening new paths for building truly multimodal systems.</p>\n\n",
                "matched_terms": [
                    "across",
                    "audio",
                    "model",
                    "iterative",
                    "where",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we demonstrate the model&#8217;s advanced capabilities in audio reasoning across diverse scenarios. As shown in the following examples, the model can effectively capture and reason about paralinguistic features (e.g., emotion, vocal characteristics, prosody) and identify complex environmental sound scenes. These cases highlight the model&#8217;s versatility in processing rich acoustic information beyond simple speech recognition.</p>\n\n",
                "matched_terms": [
                    "diverse",
                    "audio",
                    "across",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section illustrates the effectiveness of Modality-Grounded Reasoning Distillation (introduced in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.15848v2#S4.SS2\" title=\"4.2 Modality-Grounded Reasoning Distillation &#8227; 4 Post-Training Recipes &#8227; Step-Audio-R1 Technical Report\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>) in establishing correct self-recognition. We compare the model&#8217;s responses before and after this training stage. The &#8220;Before&#8221; case reveals a common issue where the model, influenced by its text-only backbone initialization, incorrectly claims inability to process audio. The &#8220;After&#8221; case demonstrates how the distillation process corrects this, enabling the model to acknowledge and utilize its audio modality for in-depth reasoning.</p>\n\n",
                "matched_terms": [
                    "inability",
                    "where",
                    "responses",
                    "process",
                    "audio",
                    "stage",
                    "model",
                    "incorrectly",
                    "training",
                    "claims"
                ]
            }
        ]
    }
}