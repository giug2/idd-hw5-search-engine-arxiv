{
    "A2.T1": {
        "source_file": "The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models",
        "caption": "Table 1: Coverage (Cov.) and Accuracy (Acc.) for the the models considered across the three language pairs.",
        "body": "en-it\nen-fr\nen-es\n\n\n\nCov.\nAcc.\nCov.\nAcc.\nCov.\nAcc.\n\n\n\nF\nM\nF\nM\nF\nM\nF\nM\nF\nM\nF\nM\n\n\nMult. Transformer\n54.61\n53.73\n77.09\n94.40\n53.30\n54.39\n77.35\n91.85\n66.33\n65.55\n80.60\n91.41\n\n\nMono. Conformer\n49.63\n46.99\n46.38\n75.76\n56.84\n56.10\n49.80\n72.50\n69.13\n65.07\n39.21\n76.74\n\n\nLarge-scale Conformer\n-\n-\n-\n-\n-\n-\n-\n-\n76.28\n71.53\n53.27\n69.52",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_rr ltx_border_tt\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\">en-it</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\">en-fr</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\">en-es</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_rr\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\">Cov.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\">Acc.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\">Cov.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\">Acc.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\">Cov.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\">Acc.</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_rr\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\">F</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\">M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\">F</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\">M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\">F</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\">M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\">F</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\">M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\">F</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\">M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\">F</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\">M</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\">Mult. Transformer</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">54.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">53.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">77.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">94.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">53.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">54.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">77.35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">91.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">66.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">65.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">80.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">91.41</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_rr\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\">Mono. Conformer</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">49.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">46.99</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">46.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">75.76</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">56.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">56.10</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">49.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">72.50</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">69.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">65.07</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">39.21</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">76.74</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_rr\" style=\"padding-left:4.5pt;padding-right:4.5pt;\"><span class=\"ltx_text ltx_font_bold\">Large-scale Conformer</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">76.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">71.53</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">53.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">69.52</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "language",
            "largescale",
            "across",
            "three",
            "pairs",
            "coverage",
            "models",
            "accuracy",
            "transformer",
            "enfr",
            "acc",
            "enes",
            "mult",
            "conformer",
            "mono",
            "considered",
            "cov",
            "enit"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The ST Transformer encoder-decoder model by <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib62\" title=\"\">2020a</a>)</cite> is a multilingual model trained on the 8 language directions of MuST-C <cite class=\"ltx_cite ltx_citemacro_citep\">(Cattoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib10\" title=\"\">2021</a>)</cite> with 72M of parameters, distributed under the MIT License. The model takes as input 80 Mel-filterbank audio features extracted every 10 milliseconds, employing a sample window of 25. The input features are then preprocessed with two 1D convolutional layers with stride 2, reducing input length by a factor of 4, before being fed to the Transformer encoder. We choose this model for our experiments both for its permissive license and its strong gender translation accuracy (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A2.T1\" title=\"Table 1 &#8227; Appendix B Model Details &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).\nFor inference, we used a beam size of 5 and a no-repeat-ngram-size of 5.\nThe maximum source position was set to 7,000.</p>\n\n",
            "<p class=\"ltx_p\">In this section, we report the results obtained for the three language pairs en-es/fr/it using the monolingual Conformer encoder-Transformer decoder models by <cite class=\"ltx_cite ltx_citemacro_citet\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib40\" title=\"\">2024</a>)</cite>, released under the Apache 2.0 License. Such models feature a higher translation quality than the Transformer-based model we reported on above, but has a lower gender accuracy, as reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A2.T1\" title=\"Table 1 &#8227; Appendix B Model Details &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">The gender-flipping behavior shows interesting variations from the Transformer results. As seen in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.F9\" title=\"Figure 9 &#8227; E.2 Results on Conformer Models &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, while the relative scorer still triggers gender switches, the effect is more balanced between feminine and masculine predictions - both achieving flip rates between 30-60% when 10% of features are deleted. This differs from the Transformer&#8217;s strong asymmetry between genders. This more\nbalanced gender-flipping\neffect aligns with the Conformer models&#8217; general gender translation patterns (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A2.T1\" title=\"Table 1 &#8227; Appendix B Model Details &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Indeed, these models show substantially lower gender accuracy, particularly for feminine terms referring to the speaker, where performance approaches chance level. The reduced impact of feature deletion on gender prediction suggests these models\nmay rely not only on the speech input but also on other factors, such as previously generated tokens or\npotential biases embedded in the decoder&#8217;s &#8220;internal language model&#8221;\n<cite class=\"ltx_cite ltx_citemacro_cite\">Fucci et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib18\" title=\"\">2023</a>)</cite>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We evaluate our method through a case study in speech translation (ST), focusing on the translation of gender-neutral terms referring to the speaker to languages requiring grammatical gender choices. For instance, translating &#8220;I am curious&#8221; to Italian typically involves choosing between an adjective with masculine inflection or one with feminine inflection. This setting is well suited for evaluating contrastive explanations in S2T as it provides natural pairs to contrast (masculine/feminine forms), while offering an opportunity to study whether models use acoustic cues like the speaker&#8217;s pitch to disambiguate gender <cite class=\"ltx_cite ltx_citemacro_cite\">Bentivogli et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib8\" title=\"\">2020</a>)</cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We acknowledge that using vocal features like pitch for gender prediction and framing gender as a binary construct raise important ethical concerns, which we discuss in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S7\" title=\"7 Ethics Statement &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "models",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our method produces markedly different saliency maps from non-contrastive approaches (see Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.d), and our experiments confirm that these differences reflect its ability to isolate gender-specific features, while non-contrastive explanations highlight regions affecting general word prediction.\nOur contributions are:\n<span class=\"ltx_text ltx_font_italic\">i)</span> <span class=\"ltx_text ltx_font_bold\">the first method for contrastive explanations in S2T</span> (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S2\" title=\"2 Contrastive Explanations for S2T &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), enabling precise identification of the input features a model relies on to translate gender-ambiguous terms referring to the speaker; <span class=\"ltx_text ltx_font_italic\">ii)</span> <span class=\"ltx_text ltx_font_bold\">a methodology to evaluate their faithfulness</span> &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S3\" title=\"3 Evaluation &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>); <span class=\"ltx_text ltx_font_italic\">iii)</span> <span class=\"ltx_text ltx_font_bold\">empirical validation</span> on three language pairs (en<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>fr/it/es) demonstrating that our method successfully isolates gender-relevant features (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S4\" title=\"4 Use Case &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Our code is available at https://github.com/hlt-mt/FBK-fairseq under the Apache License 2.0.</span></span></span></p>\n\n",
                "matched_terms": [
                    "language",
                    "three",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, unlike in classification tasks with fixed output classes, generative models can produce any text when perturbed, including synonyms or unrelated outputs. For our application, this complicates detecting when the predicted gender changes. Therefore, we separate our evaluation into two complementary metrics: <span class=\"ltx_text ltx_font_bold\">coverage</span>, which tracks the percentage of cases where the model generates either term of interest, and <span class=\"ltx_text ltx_font_bold\">flip rate</span>, which measures how often the model switches from target to foil within these covered cases. While the flip rate indicates whether we identified the right features for the target-foil contrast, coverage ensures these features are specific to the contrast rather than affecting general text generation.</p>\n\n",
                "matched_terms": [
                    "coverage",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We test our contrastive feature attribution method on speaker&#8217;s gender assignment in S2T translation.\nAs a <span class=\"ltx_text ltx_font_bold\">benchmark</span>, we use MuST-SHE <cite class=\"ltx_cite ltx_citemacro_cite\">Bentivogli et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib8\" title=\"\">2020</a>)</cite>, which contains English-to-Spanish/French/Italian ST pairs annotated for terms that lack explicit gender marking in the source but require gender assignment in the target language.\nWe focus on terms referring to the speaker,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>For details on example selection, see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A3\" title=\"Appendix C Dataset Description and Processing &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</span></span></span> for which MuST-SHE provides both correct gender translations (based on gold speaker gender labels) and their alternative incorrect versions<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Ethical implications of this study are discussed in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S7\" title=\"7 Ethics Statement &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</span></span></span> (e.g., &#8216;curiosa<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">F</span></sup>&#8217; vs. &#8216;curioso<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">M</span></sup>&#8217;) which we use to construct the target/foil pairs.\nHere, we compare our relative scoring function with the difference scorer on the en<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>fr split, using the multilingual S2T Transformer encoder-decoder model by <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib62\" title=\"\">2020a</a>)</cite>, which has demonstrated strong performance in gender translation accuracy.\nExperiments with different architectures and language pairs, showing the same trends, are reported in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5\" title=\"Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>, while the word-level aggregation methods are compared in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6\" title=\"Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "transformer",
                    "accuracy",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S4.F2\" title=\"Figure 2 &#8227; 4 Use Case &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reports how coverage evolves as we progressively delete the features identified as most important.\nThe coverage of the difference scorer drops below 20% after just 2% deletion, while the relative scorer maintains higher coverage: more than 30% even after 20% deletion.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>We focus on the first 20% of feature deletion, as beyond this point the input becomes too degraded for meaningful model output. Full results are reported in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A4\" title=\"Appendix D Full Deletion Range Analysis &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</span></span></span>\nThe dramatic coverage loss of the difference scorer occurs because its explanations fail to be truly contrastive; they do not target gender-relevant features.\nThis is demonstrated by their similarity to non-contrastive explanations:\nsaliency maps obtained with the contrastive difference scorer <math alttext=\"S_{CD}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><msub><mi>S</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow></msub><annotation encoding=\"application/x-tex\">S_{CD}</annotation></semantics></math> on en-fr data exhibit a Pearson correlation coefficient of 0.93 with those obtained with the base scorer <math alttext=\"S_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>B</mi></msub><annotation encoding=\"application/x-tex\">S_{B}</annotation></semantics></math>, while relative scorer (<math alttext=\"S_{CR}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msub><mi>S</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>R</mi></mrow></msub><annotation encoding=\"application/x-tex\">S_{CR}</annotation></semantics></math>) heatmaps show only 0.33 correlation (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A7\" title=\"Appendix G Correlation Between Contrastive and Non-Contrastive Explanations &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">G</span></a> for other languages and models).\nWe conclude that the difference scorer provides generic explanations, while <span class=\"ltx_text ltx_font_bold\">the features highlighted by the relative scorer are more precisely linked to gender assignment.</span></p>\n\n",
                "matched_terms": [
                    "coverage",
                    "enfr",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We now demonstrate that the features identified by the relative scorer are responsible for shifting the model&#8217;s predictions in the expected direction along the gender axis.\nFig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S4.F3\" title=\"Figure 3 &#8227; Coverage. &#8227; 4 Use Case &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows that the relative scorer effectively isolates features driving feminine gender predictions: occluding the top 5% most relevant features causes the model to switch to masculine translations for over 70% of the terms that remain in coverage. This high flip rate, combined with the maintained coverage discussed above, demonstrates that <span class=\"ltx_text ltx_font_bold\">our method precisely discriminates the exact input regions driving translation toward one gender instead of the other</span>: the model generates the same terms (coverage) but changes their gender (flip rate) when these features are removed.\nFor masculine predictions, in contrast, the flip rate is much lower, plateauing at <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>30%.\nHowever, this may not indicate a limitation of the method, but rather reflect an underlying property of the ST model: if masculine serves as the default prediction rather than one actively triggered by specific input cues, then no occluded features could shift the prediction toward the feminine class.\nThe XAI literature on gender bias supports this hypothesis: <cite class=\"ltx_cite ltx_citemacro_citet\">Jumelet et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib26\" title=\"\">2019</a>)</cite> observe that models use masculine as a default, generating feminine forms only when strong feminine signals are present. This masculine default bias has also been observed in ST systems&#8217; internal language models <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib18\" title=\"\">2023</a>)</cite> and likely stems from training data gender imbalance <cite class=\"ltx_cite ltx_citemacro_cite\">Tatman (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib56\" title=\"\">2017</a>)</cite>&#8212;MuST-C contains twice as many male as female speakers <cite class=\"ltx_cite ltx_citemacro_cite\">Cattoni et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib10\" title=\"\">2021</a>)</cite>.\nWe defer to future work a thorough analysis of the acoustic features the model exploits for gender translation and their interaction with other model components and potential sources of bias.</p>\n\n",
                "matched_terms": [
                    "coverage",
                    "language",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While this work demonstrates the effectiveness of the proposed contrastive explanation methodology for analyzing gender translation, the method&#8217;s applicability beyond gender analysis remains to be validated across other phenomena. However, we do not foresee any reason for which its applicability could be limited.\nPossible areas of investigation include:\ni) understanding homophone disambiguation (e.g., &#8220;plain&#8221; vs. &#8220;plane&#8221;) based on audio features <cite class=\"ltx_cite ltx_citemacro_cite\">Mohebbi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib37\" title=\"\">2023</a>); Yu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib68\" title=\"\">2024</a>)</cite>, ii) analyzing how models resolve coreference resolution (e.g., identifying the features that make a model associate an ambiguous pronoun with one referent over another) <cite class=\"ltx_cite ltx_citemacro_cite\">Amoia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib3\" title=\"\">2012</a>); Roesiger et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib46\" title=\"\">2017</a>)</cite>, iii) investigating factuality issues by contrasting correct translations against model errors, and iv) exploring politeness register selection (e.g., why models choose formal &#8220;vous/Sie&#8221; over informal &#8220;tu/du&#8221;) <cite class=\"ltx_cite ltx_citemacro_cite\">Sennrich et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib52\" title=\"\">2016</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed\n<span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S4.SS0.SSS0.Px2\" title=\"Flip rate. &#8227; 4 Use Case &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>,</span>\nthe flip rate for feminine and masculine forms is different, and never reaches 100%. For the Conformer models in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5\" title=\"Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>, the flip rate is closer among genders.\nWe propose potential reasons for these variations, including differences in models&#8217; initial gender translation capabilities and possible masculine default bias in ST models&#8217; internal language models. But these remain conjectures that require empirical verification. Such investigation of how different ST models translate gender falls outside the scope of this work, which focuses on developing and evaluating contrastive explanations rather than reaching conclusions about gender translation mechanisms in ST models.</p>\n\n",
                "matched_terms": [
                    "language",
                    "models",
                    "conformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analysis uses MuST-SHE <cite class=\"ltx_cite ltx_citemacro_cite\">Bentivogli et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib8\" title=\"\">2020</a>)</cite>, a speech translation dataset compiled from TED talks that includes annotations for words which are gender-neutral in the source language but have gender marking in the reference translation to the target language. The dataset is licensed under CC BY NC ND 4.0 International, and our usage aligns with its intended purpose of studying gender term translation in speech translation systems. Comprehensive information about the dataset&#8217;s domain coverage, language scope, linguistic phenomena, and represented demographic groups is available in the dataset&#8217;s data statement.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>https://mt.fbk.eu/data-statement-for-must-she/</span></span></span> Given its origin in educational talks, the corpus is free of offensive content. It contains identifiable information in the form of speaker names, which are publicly available with their consent as part of their TED talks. The dataset includes speaker&#8217;s gender information, which was annotated based on speakers&#8217; self-identification as documented in their public profiles at the time of dataset creation.</p>\n\n",
                "matched_terms": [
                    "coverage",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We run our experiments on English-to-Spanish/French/Italian translations, specifically examining gender terms that refer to the speaker (category 1 in MuST-SHE), as these instances may benefit from acoustic cues for gender disambiguation. The dataset provides pairs of correct and incorrect gender translations for each term (e.g., &#8216;curiosa&#8217;/&#8216;curioso&#8217;), which we use in our contrastive analysis. Using the annotations from <cite class=\"ltx_cite ltx_citemacro_citet\">Savoldi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib51\" title=\"\">2022</a>)</cite>, we exclude gender articles from our analysis due to their high frequency in both genders across sentences, which makes it challenging to reliably identify specific instances referring to the speaker and could introduce noise into the evaluation.</p>\n\n",
                "matched_terms": [
                    "across",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, to make sure our contrastive explanation method works for different models and language pairs, we extend our study to the English-Italian and English-Spanish sections of the data (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.SS1\" title=\"E.1 Results on Additional Language Pairs &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">E.1</span></a>) and to two other types of models. In &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.SS2\" title=\"E.2 Results on Conformer Models &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">E.2</span></a>, we analyze the monolingual Conformer encoder-Transformer decoder models by <cite class=\"ltx_cite ltx_citemacro_citet\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib40\" title=\"\">2024</a>)</cite>. Similarly to the Transformer model used in the main body of the paper, this model is trained on MuST-C. In &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.SS3\" title=\"E.3 Results on Large-scale Model &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">E.3</span></a>, to analyze a larger-scale system, we study a Conformer encoder-Transformer decoder model trained on the English-Spanish data of the constrained track of the last IWSLT campaign <cite class=\"ltx_cite ltx_citemacro_citep\">(Ahmad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib2\" title=\"\">2024</a>)</cite>.\nWhile our perturbation-based method for contrastive explanations is inherently model-agnostic and applicable to any architecture, we empirically validate that the explanations generated across these diverse model configurations continue to satisfy our quantitative faithfulness metrics.</p>\n\n",
                "matched_terms": [
                    "language",
                    "across",
                    "pairs",
                    "models",
                    "transformer",
                    "conformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the multilingual Transformer model, we extend our analysis to English-Italian and English-Spanish translations to verify whether the patterns observed for English-French generalize across language pairs. As shown in\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.F6\" title=\"Figure 6 &#8227; E.1 Results on Additional Language Pairs &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>,\nthe relative scorer maintains substantially higher coverage than the difference scorer across\nthe two languages, confirming the findings reported in the main paper for English-French.\nWhen deleting 10% of the features, coverage remains above 20% for Italian and Spanish, matching the robustness observed for French translations.\n</p>\n\n",
                "matched_terms": [
                    "language",
                    "across",
                    "pairs",
                    "coverage",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The gender-flipping behavior is also consistent across languages, as shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.F7\" title=\"Figure 7 &#8227; E.1 Results on Additional Language Pairs &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. For feminine predictions,\ndeleting just 5% of the most relevant features identified by our method causes the model to switch to masculine translations in over 60% of the covered cases\nfor both Italian and Spanish, as observed for French in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S4\" title=\"4 Use Case &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nThe asymmetry between feminine and masculine predictions persists as well, with masculine-to-feminine flip rates not going much higher than 40% across all language pairs. This consistent pattern across three Romance languages with similar grammatical gender systems suggests that our earlier hypothesis about a masculine default could plausibly explain the model&#8217;s behavior in these three cases.</p>\n\n",
                "matched_terms": [
                    "language",
                    "across",
                    "three",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The consistency across three target languages shows the robustness of our method and suggests that the model may be using similar strategies to make gender choices in all languages considered.</p>\n\n",
                "matched_terms": [
                    "considered",
                    "across",
                    "three"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The coverage patterns observed for the Transformer model extend to the monolingual Conformer models. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.F8\" title=\"Figure 8 &#8227; E.2 Results on Conformer Models &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, the relative scorer maintains consistently higher coverage than the difference scorer, although with lower absolute values compared to the Transformer model. At 10% deletion, coverage with the relative scorer remains above 30% for all language pairs, indicating that our method still effectively isolates gender-relevant features despite the architectural differences.</p>\n\n",
                "matched_terms": [
                    "language",
                    "pairs",
                    "coverage",
                    "models",
                    "transformer",
                    "conformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further confirm the robustness of our findings, we investigate whether they also hold for larger-scale models trained on different data from only MuST-C. To this aim, we train a model composed of a 12-layer Conformer encoder and 6-layer Transformer decoder. Besides MuST-C, the training data includes EuroParl-ST <cite class=\"ltx_cite ltx_citemacro_citep\">(Iranzo-S&#225;nchez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib23\" title=\"\">2020</a>)</cite>, CoVoST v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib63\" title=\"\">2020b</a>)</cite>, and the ASR datasets CommonVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Ardila et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib5\" title=\"\">2020</a>)</cite>, LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib39\" title=\"\">2015</a>)</cite>, TEDLIUM v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Hernandez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib21\" title=\"\">2018</a>)</cite>, and VoxPopuli <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib61\" title=\"\">2021</a>)</cite>, whose transcripts we automatically translate into Spanish using the NeMo\nMT models.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>Publicly available at: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/machine_translation/machine_translation.html\" title=\"\">https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/machine_translation/machine_translation.html</a>.</span></span></span> The model is trained with a composite loss function that comprises a label-smoothed cross entropy on the decoder output, with the translation as target, and two auxiliary CTC losses, respectively, on the encoder output, with the translation as target and, on the 8th encoder layer, with the transcript as target <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib66\" title=\"\">2023</a>)</cite>. The training is performed using the Noam scheduler with 2e-3 as peak learning rate and is stopped after 200,000 updates. Utterance-level Cepstral Mean and Variance Normalization (CMVN) and SpecAugment <cite class=\"ltx_cite ltx_citemacro_cite\">Park et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib41\" title=\"\">2019</a>)</cite> are applied during training and segments longer than 30 seconds are filtered out (fairseq-ST default) to avoid excessive VRAM requirements. The model is the average of the last 7 checkpoints obtained from the training and has 133M parameters. The training is run on 4 NVIDIA Ampere GPU A100 (64GB VRAM) with mini-batches of 40,000 input elements and 2 as update frequency.</p>\n\n",
                "matched_terms": [
                    "conformer",
                    "models",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This large-scale model exhibits similar patterns to the smaller Conformer models. As shown in Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.F10\" title=\"Figure 10 &#8227; E.3 Results on Large-scale Model &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.F11\" title=\"Figure 11 &#8227; E.3 Results on Large-scale Model &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, the relative scorer maintains higher coverage than the difference scorer. The flip rate displays a reduced asymmetry between feminine and masculine predictions, with the first reaching around 50% after 10% feature deletion, and the second, a little over 40%. This behavior is aligned with that of the monolignual small Conformer models.</p>\n\n",
                "matched_terms": [
                    "coverage",
                    "conformer",
                    "models",
                    "largescale"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents an empirical comparison, for our XAI application, of different methods for computing word-level probabilities from the sequences of subword token probabilities output by ST models. Previous work has employed various approaches to address this issue. Some studies have avoided the problem entirely by limiting their analysis to targets and foils tokenized as single tokens <cite class=\"ltx_cite ltx_citemacro_cite\">Yin and Neubig (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib67\" title=\"\">2022</a>)</cite>. Others have worked at the subword level without aggregation <cite class=\"ltx_cite ltx_citemacro_cite\">Ferrando et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib15\" title=\"\">2022</a>)</cite>, or employed the chain rule to multiply individual token probabilities <cite class=\"ltx_cite ltx_citemacro_cite\">Sarti et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib50\" title=\"\">2023</a>)</cite>. Here, we leverage our evaluation metrics for explanation faithfulness&#8212;coverage and flip rate&#8212;to compare three methods and determine if any of them yields more reliable contrastive explanations when integrated into our methodology.</p>\n\n",
                "matched_terms": [
                    "models",
                    "three"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the performance of each method, we recorded coverage and flip rate measurements at progressive deletion steps, as shown in Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.F12\" title=\"Figure 12 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.F13\" title=\"Figure 13 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. To quantitatively assess differences between methods, we conducted paired t-tests comparing coverage and flip rate values. Each pair of methods was evaluated using the sequence of measurements at corresponding deletion percentages as dependent samples, allowing us to determine whether observed differences were statistically significant. Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.T2.fig1\" title=\"Table 2 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.T3.fig1\" title=\"Table 3 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> present the resulting p-values for each comparison across language pairs.</p>\n\n",
                "matched_terms": [
                    "coverage",
                    "language",
                    "across",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These comparisons reveal that all three methods perform similarly in their ability to identify gender-relevant features. While coverage differences are often statistically significant (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.T2.fig1\" title=\"Table 2 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), the absolute differences are relatively small as can be seen in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.F12\" title=\"Figure 12 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>, with all methods maintaining sufficient coverage levels to ensure reliable flip rate calculations. The flip rate analysis&#8212;our primary metric for evaluating how precisely methods isolate features responsible for gender prediction&#8212;shows no statistically significant differences (<math alttext=\"p&gt;0.05\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p6.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&gt;</mo><mn>0.05</mn></mrow><annotation encoding=\"application/x-tex\">p&gt;0.05</annotation></semantics></math>) between Word Boundary and Chain Rule methods across all language pairs (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.T3.fig1\" title=\"Table 3 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). As we see in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.F13\" title=\"Figure 13 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, when over 10% of the highlighted features are occluded, all methods achieve comparable flip rates between 50-60% for most language pairs. The only notable exception is the Length Normalization method, which performs significantly worse for French in terms of flip rate (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.T3.fig1\" title=\"Table 3 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p6.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>).\nWhile the reasons for this language-specific underperformance warrant further investigation, the observation supports our decision to exclude the Length Normalization method from our main experiments.\n</p>\n\n",
                "matched_terms": [
                    "language",
                    "across",
                    "three",
                    "pairs",
                    "coverage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The patterns regarding similarity between contrastive and non-contrastive explanations observed for the Transformer model on English-French translations extend consistently across all models and language pairs.\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.T4\" title=\"Table 4 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the Pearson correlation coefficients between non-contrastive explanations and contrastive explanations generated using the difference and relative scorers, respectively. The difference scorer produces explanations that strongly correlate with non-contrastive ones (correlation coefficients between 0.86 and 0.94), while the relative scorer generates distinctly different explanations (correlation coefficients between 0.29 and 0.43). These results reinforce our findings from Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S4\" title=\"4 Use Case &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> regarding the relative scorer&#8217;s superior ability to generate truly contrastive explanations.</p>\n\n",
                "matched_terms": [
                    "language",
                    "across",
                    "pairs",
                    "models",
                    "transformer"
                ]
            }
        ]
    },
    "A6.T2.fig1": {
        "source_file": "The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models",
        "caption": "Table 2: Statistical significance (p-values) of pairwise method comparisons for coverage performance across language pairs.",
        "body": "Method Comparison\nen-fr\nen-es\nen-it\n\n\n\n\nWord Boundary vs. Length Norm\n\n<< 0.001***\n0.218\n\n<< 0.001***\n\n\nWord Boundary vs. Chain Rule\n0.376\n\n<< 0.001***\n\n<< 0.001***\n\n\nLength Norm vs. Chain Rule\n\n<< 0.001***\n\n<< 0.01**\n\n<< 0.001***",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Method Comparison</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">en-fr</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">en-es</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">en-it</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Word Boundary vs. Length Norm</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">\n<math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A6.T2.m1\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math> 0.001***</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">0.218</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A6.T2.m2\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math> 0.001***</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Word Boundary vs. Chain Rule</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">0.376</th>\n<td class=\"ltx_td ltx_align_left\">\n<math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A6.T2.m3\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math> 0.001***</td>\n<td class=\"ltx_td ltx_align_left\">\n<math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A6.T2.m4\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math> 0.001***</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\">Length Norm vs. Chain Rule</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\">\n<math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A6.T2.m5\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math> 0.001***</th>\n<td class=\"ltx_td ltx_align_left ltx_border_b\">\n<math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A6.T2.m6\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math> 0.01**</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\">\n<math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A6.T2.m7\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math> 0.001***</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "norm",
            "rule",
            "statistical",
            "enit",
            "pairs",
            "coverage",
            "method",
            "performance",
            "comparison",
            "comparisons",
            "enfr",
            "length",
            "pairwise",
            "pvalues",
            "language",
            "word",
            "across",
            "significance",
            "enes",
            "boundary",
            "chain"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To evaluate the performance of each method, we recorded coverage and flip rate measurements at progressive deletion steps, as shown in Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.F12\" title=\"Figure 12 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.F13\" title=\"Figure 13 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. To quantitatively assess differences between methods, we conducted paired t-tests comparing coverage and flip rate values. Each pair of methods was evaluated using the sequence of measurements at corresponding deletion percentages as dependent samples, allowing us to determine whether observed differences were statistically significant. Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.T2.fig1\" title=\"Table 2 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.T3.fig1\" title=\"Table 3 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> present the resulting p-values for each comparison across language pairs.</p>\n\n",
            "<p class=\"ltx_p\">These comparisons reveal that all three methods perform similarly in their ability to identify gender-relevant features. While coverage differences are often statistically significant (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.T2.fig1\" title=\"Table 2 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), the absolute differences are relatively small as can be seen in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.F12\" title=\"Figure 12 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>, with all methods maintaining sufficient coverage levels to ensure reliable flip rate calculations. The flip rate analysis&#8212;our primary metric for evaluating how precisely methods isolate features responsible for gender prediction&#8212;shows no statistically significant differences (<math alttext=\"p&gt;0.05\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p6.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&gt;</mo><mn>0.05</mn></mrow><annotation encoding=\"application/x-tex\">p&gt;0.05</annotation></semantics></math>) between Word Boundary and Chain Rule methods across all language pairs (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.T3.fig1\" title=\"Table 3 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). As we see in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.F13\" title=\"Figure 13 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, when over 10% of the highlighted features are occluded, all methods achieve comparable flip rates between 50-60% for most language pairs. The only notable exception is the Length Normalization method, which performs significantly worse for French in terms of flip rate (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.T3.fig1\" title=\"Table 3 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p6.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>).\nWhile the reasons for this language-specific underperformance warrant further investigation, the observation supports our decision to exclude the Length Normalization method from our main experiments.\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Nevertheless, contrastive explanations have yet to be applied to speech-to-text (S2T) models. As S2T adoption grows <cite class=\"ltx_cite ltx_citemacro_cite\">Latif et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib29\" title=\"\">2023</a>); Barrault et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib7\" title=\"\">2025</a>)</cite>, extending XAI advances like contrastive explanations to speech becomes crucial.\nSome pioneering works have tackled the explanation of S2T models&#8217; decisions, braving key challenges including the multidimensional nature of speech signals, which span time and frequency, and the variable length of output sequences <cite class=\"ltx_cite ltx_citemacro_cite\">Wu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib65\" title=\"\">2024</a>)</cite>. The main approach used in the literature is to measure how perturbing the input audio signal affects the output <cite class=\"ltx_cite ltx_citemacro_cite\">Mandel (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib31\" title=\"\">2016</a>); Kavaki and Mandel (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib27\" title=\"\">2020</a>); Trinh and Mandel (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib58\" title=\"\">2020</a>); Markert et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib32\" title=\"\">2021</a>); Mohebbi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib37\" title=\"\">2023</a>); Wu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib64\" title=\"\">2023</a>); Fucci et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib19\" title=\"\">2024</a>); Wu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib65\" title=\"\">2024</a>)</cite>.\nThe resulting explanations take the form of saliency maps over a spectrogram representation of the audio input (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.a). These maps highlight the input regions that most strongly influence the model&#8217;s predictions (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.b). However, such explanations are holistic: they identify features relevant for all aspects of word generation (e.g. why the model produces &#8216;<span class=\"ltx_text ltx_framed ltx_framed_underline\">curioso</span>&#8217;), without focusing on specific contrastive aspects (why &#8216;curios<span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">o</span>&#8217;, in the masculine, instead of &#8216;curios<span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">a</span>&#8217;).\n<span class=\"ltx_text ltx_font_bold\">How, then, can we obtain contrastive explanations of the use of speech features by S2T models?</span></p>\n\n",
                "matched_terms": [
                    "word",
                    "length"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To meet this challenge, we build upon a prior non-contrastive feature attribution method for S2T, SPES <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib19\" title=\"\">2024</a>)</cite>. Adapting it to produce contrastive explanations requires two main interventions: <span class=\"ltx_text ltx_font_italic\">i)</span> aggregating token-level probabilities for word-level analysis, since we aim to explain why one word was generated instead of another, while models generate subword tokens rather than complete words, and <span class=\"ltx_text ltx_font_italic\">ii)</span> designing a scoring function that quantifies relative probability changes between a <span class=\"ltx_text ltx_font_italic\">target</span> word and an alternative (the <span class=\"ltx_text ltx_font_italic\">foil</span>). We investigate multiple approaches for both challenges, finding that standard solutions from text-based NLP are inadequate for our scenario, and propose improvements upon them.</p>\n\n",
                "matched_terms": [
                    "word",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our method through a case study in speech translation (ST), focusing on the translation of gender-neutral terms referring to the speaker to languages requiring grammatical gender choices. For instance, translating &#8220;I am curious&#8221; to Italian typically involves choosing between an adjective with masculine inflection or one with feminine inflection. This setting is well suited for evaluating contrastive explanations in S2T as it provides natural pairs to contrast (masculine/feminine forms), while offering an opportunity to study whether models use acoustic cues like the speaker&#8217;s pitch to disambiguate gender <cite class=\"ltx_cite ltx_citemacro_cite\">Bentivogli et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib8\" title=\"\">2020</a>)</cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We acknowledge that using vocal features like pitch for gender prediction and framing gender as a binary construct raise important ethical concerns, which we discuss in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S7\" title=\"7 Ethics Statement &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "method",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our method produces markedly different saliency maps from non-contrastive approaches (see Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.d), and our experiments confirm that these differences reflect its ability to isolate gender-specific features, while non-contrastive explanations highlight regions affecting general word prediction.\nOur contributions are:\n<span class=\"ltx_text ltx_font_italic\">i)</span> <span class=\"ltx_text ltx_font_bold\">the first method for contrastive explanations in S2T</span> (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S2\" title=\"2 Contrastive Explanations for S2T &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), enabling precise identification of the input features a model relies on to translate gender-ambiguous terms referring to the speaker; <span class=\"ltx_text ltx_font_italic\">ii)</span> <span class=\"ltx_text ltx_font_bold\">a methodology to evaluate their faithfulness</span> &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S3\" title=\"3 Evaluation &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>); <span class=\"ltx_text ltx_font_italic\">iii)</span> <span class=\"ltx_text ltx_font_bold\">empirical validation</span> on three language pairs (en<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>fr/it/es) demonstrating that our method successfully isolates gender-relevant features (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S4\" title=\"4 Use Case &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Our code is available at https://github.com/hlt-mt/FBK-fairseq under the Apache License 2.0.</span></span></span></p>\n\n",
                "matched_terms": [
                    "language",
                    "method",
                    "word",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our Solution.</span> To overcome the limitations of previous approaches and obtain accurate word-level probabilities, we adopt the methodology of <cite class=\"ltx_cite ltx_citemacro_citet\">Pimentel and Meister (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib44\" title=\"\">2024</a>)</cite>.\nUnlike other simplistic methods, this approach accounts for the fact that a sequence of subwords should be followed by a beginning of word or punctuation token to form a word rather than a prefix.\nThis distinction is crucial to avoid overestimating the likelihood of subword sequences that could be prefixes (e.g., Italian: <span class=\"ltx_text ltx_font_typewriter\">_professor e<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_serif ltx_font_italic\">M</span></sup></span> vs. <span class=\"ltx_text ltx_font_typewriter\">_professor e ssa<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_serif ltx_font_italic\">F</span></sup></span>).\nTo the best of our knowledge, we are the first to apply this principled method to feature attribution, ensuring accurate word-level explanations.</p>\n\n",
                "matched_terms": [
                    "word",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We test our contrastive feature attribution method on speaker&#8217;s gender assignment in S2T translation.\nAs a <span class=\"ltx_text ltx_font_bold\">benchmark</span>, we use MuST-SHE <cite class=\"ltx_cite ltx_citemacro_cite\">Bentivogli et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib8\" title=\"\">2020</a>)</cite>, which contains English-to-Spanish/French/Italian ST pairs annotated for terms that lack explicit gender marking in the source but require gender assignment in the target language.\nWe focus on terms referring to the speaker,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>For details on example selection, see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A3\" title=\"Appendix C Dataset Description and Processing &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</span></span></span> for which MuST-SHE provides both correct gender translations (based on gold speaker gender labels) and their alternative incorrect versions<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Ethical implications of this study are discussed in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S7\" title=\"7 Ethics Statement &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</span></span></span> (e.g., &#8216;curiosa<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">F</span></sup>&#8217; vs. &#8216;curioso<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">M</span></sup>&#8217;) which we use to construct the target/foil pairs.\nHere, we compare our relative scoring function with the difference scorer on the en<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>fr split, using the multilingual S2T Transformer encoder-decoder model by <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib62\" title=\"\">2020a</a>)</cite>, which has demonstrated strong performance in gender translation accuracy.\nExperiments with different architectures and language pairs, showing the same trends, are reported in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5\" title=\"Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>, while the word-level aggregation methods are compared in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6\" title=\"Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "method",
                    "language",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S4.F2\" title=\"Figure 2 &#8227; 4 Use Case &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reports how coverage evolves as we progressively delete the features identified as most important.\nThe coverage of the difference scorer drops below 20% after just 2% deletion, while the relative scorer maintains higher coverage: more than 30% even after 20% deletion.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>We focus on the first 20% of feature deletion, as beyond this point the input becomes too degraded for meaningful model output. Full results are reported in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A4\" title=\"Appendix D Full Deletion Range Analysis &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</span></span></span>\nThe dramatic coverage loss of the difference scorer occurs because its explanations fail to be truly contrastive; they do not target gender-relevant features.\nThis is demonstrated by their similarity to non-contrastive explanations:\nsaliency maps obtained with the contrastive difference scorer <math alttext=\"S_{CD}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><msub><mi>S</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow></msub><annotation encoding=\"application/x-tex\">S_{CD}</annotation></semantics></math> on en-fr data exhibit a Pearson correlation coefficient of 0.93 with those obtained with the base scorer <math alttext=\"S_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>B</mi></msub><annotation encoding=\"application/x-tex\">S_{B}</annotation></semantics></math>, while relative scorer (<math alttext=\"S_{CR}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msub><mi>S</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>R</mi></mrow></msub><annotation encoding=\"application/x-tex\">S_{CR}</annotation></semantics></math>) heatmaps show only 0.33 correlation (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A7\" title=\"Appendix G Correlation Between Contrastive and Non-Contrastive Explanations &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">G</span></a> for other languages and models).\nWe conclude that the difference scorer provides generic explanations, while <span class=\"ltx_text ltx_font_bold\">the features highlighted by the relative scorer are more precisely linked to gender assignment.</span></p>\n\n",
                "matched_terms": [
                    "coverage",
                    "enfr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We now demonstrate that the features identified by the relative scorer are responsible for shifting the model&#8217;s predictions in the expected direction along the gender axis.\nFig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S4.F3\" title=\"Figure 3 &#8227; Coverage. &#8227; 4 Use Case &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows that the relative scorer effectively isolates features driving feminine gender predictions: occluding the top 5% most relevant features causes the model to switch to masculine translations for over 70% of the terms that remain in coverage. This high flip rate, combined with the maintained coverage discussed above, demonstrates that <span class=\"ltx_text ltx_font_bold\">our method precisely discriminates the exact input regions driving translation toward one gender instead of the other</span>: the model generates the same terms (coverage) but changes their gender (flip rate) when these features are removed.\nFor masculine predictions, in contrast, the flip rate is much lower, plateauing at <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>30%.\nHowever, this may not indicate a limitation of the method, but rather reflect an underlying property of the ST model: if masculine serves as the default prediction rather than one actively triggered by specific input cues, then no occluded features could shift the prediction toward the feminine class.\nThe XAI literature on gender bias supports this hypothesis: <cite class=\"ltx_cite ltx_citemacro_citet\">Jumelet et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib26\" title=\"\">2019</a>)</cite> observe that models use masculine as a default, generating feminine forms only when strong feminine signals are present. This masculine default bias has also been observed in ST systems&#8217; internal language models <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib18\" title=\"\">2023</a>)</cite> and likely stems from training data gender imbalance <cite class=\"ltx_cite ltx_citemacro_cite\">Tatman (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib56\" title=\"\">2017</a>)</cite>&#8212;MuST-C contains twice as many male as female speakers <cite class=\"ltx_cite ltx_citemacro_cite\">Cattoni et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib10\" title=\"\">2021</a>)</cite>.\nWe defer to future work a thorough analysis of the acoustic features the model exploits for gender translation and their interaction with other model components and potential sources of bias.</p>\n\n",
                "matched_terms": [
                    "coverage",
                    "language",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper has introduced the first method to obtain contrastive explanations for S2T models, which identify input features that lead the model to generate one output word instead of another. Our solution builds on perturbation-based feature attribution over spectrograms. The key element is our scorer, which ensures truly contrastive explanations, unlike the difference scorer widely used in NLP. Our case study on the input features driving the translation of speakers&#8217; gender in ST demonstrated the effectiveness of our approach in isolating the input features driving the model&#8217;s choice between feminine and masculine grammatical forms. Our methodology will enable future research not only to investigate which phonetic cues ST models use for gender disambiguation, but could also be applied to other phenomena and S2T tasks.</p>\n\n",
                "matched_terms": [
                    "word",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The ST Transformer encoder-decoder model by <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib62\" title=\"\">2020a</a>)</cite> is a multilingual model trained on the 8 language directions of MuST-C <cite class=\"ltx_cite ltx_citemacro_citep\">(Cattoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib10\" title=\"\">2021</a>)</cite> with 72M of parameters, distributed under the MIT License. The model takes as input 80 Mel-filterbank audio features extracted every 10 milliseconds, employing a sample window of 25. The input features are then preprocessed with two 1D convolutional layers with stride 2, reducing input length by a factor of 4, before being fed to the Transformer encoder. We choose this model for our experiments both for its permissive license and its strong gender translation accuracy (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A2.T1\" title=\"Table 1 &#8227; Appendix B Model Details &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).\nFor inference, we used a beam size of 5 and a no-repeat-ngram-size of 5.\nThe maximum source position was set to 7,000.</p>\n\n",
                "matched_terms": [
                    "language",
                    "length"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analysis uses MuST-SHE <cite class=\"ltx_cite ltx_citemacro_cite\">Bentivogli et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib8\" title=\"\">2020</a>)</cite>, a speech translation dataset compiled from TED talks that includes annotations for words which are gender-neutral in the source language but have gender marking in the reference translation to the target language. The dataset is licensed under CC BY NC ND 4.0 International, and our usage aligns with its intended purpose of studying gender term translation in speech translation systems. Comprehensive information about the dataset&#8217;s domain coverage, language scope, linguistic phenomena, and represented demographic groups is available in the dataset&#8217;s data statement.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>https://mt.fbk.eu/data-statement-for-must-she/</span></span></span> Given its origin in educational talks, the corpus is free of offensive content. It contains identifiable information in the form of speaker names, which are publicly available with their consent as part of their TED talks. The dataset includes speaker&#8217;s gender information, which was annotated based on speakers&#8217; self-identification as documented in their public profiles at the time of dataset creation.</p>\n\n",
                "matched_terms": [
                    "coverage",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We run our experiments on English-to-Spanish/French/Italian translations, specifically examining gender terms that refer to the speaker (category 1 in MuST-SHE), as these instances may benefit from acoustic cues for gender disambiguation. The dataset provides pairs of correct and incorrect gender translations for each term (e.g., &#8216;curiosa&#8217;/&#8216;curioso&#8217;), which we use in our contrastive analysis. Using the annotations from <cite class=\"ltx_cite ltx_citemacro_citet\">Savoldi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib51\" title=\"\">2022</a>)</cite>, we exclude gender articles from our analysis due to their high frequency in both genders across sentences, which makes it challenging to reliably identify specific instances referring to the speaker and could introduce noise into the evaluation.</p>\n\n",
                "matched_terms": [
                    "across",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, to make sure our contrastive explanation method works for different models and language pairs, we extend our study to the English-Italian and English-Spanish sections of the data (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.SS1\" title=\"E.1 Results on Additional Language Pairs &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">E.1</span></a>) and to two other types of models. In &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.SS2\" title=\"E.2 Results on Conformer Models &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">E.2</span></a>, we analyze the monolingual Conformer encoder-Transformer decoder models by <cite class=\"ltx_cite ltx_citemacro_citet\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib40\" title=\"\">2024</a>)</cite>. Similarly to the Transformer model used in the main body of the paper, this model is trained on MuST-C. In &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.SS3\" title=\"E.3 Results on Large-scale Model &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">E.3</span></a>, to analyze a larger-scale system, we study a Conformer encoder-Transformer decoder model trained on the English-Spanish data of the constrained track of the last IWSLT campaign <cite class=\"ltx_cite ltx_citemacro_citep\">(Ahmad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib2\" title=\"\">2024</a>)</cite>.\nWhile our perturbation-based method for contrastive explanations is inherently model-agnostic and applicable to any architecture, we empirically validate that the explanations generated across these diverse model configurations continue to satisfy our quantitative faithfulness metrics.</p>\n\n",
                "matched_terms": [
                    "language",
                    "method",
                    "across",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the multilingual Transformer model, we extend our analysis to English-Italian and English-Spanish translations to verify whether the patterns observed for English-French generalize across language pairs. As shown in\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.F6\" title=\"Figure 6 &#8227; E.1 Results on Additional Language Pairs &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>,\nthe relative scorer maintains substantially higher coverage than the difference scorer across\nthe two languages, confirming the findings reported in the main paper for English-French.\nWhen deleting 10% of the features, coverage remains above 20% for Italian and Spanish, matching the robustness observed for French translations.\n</p>\n\n",
                "matched_terms": [
                    "coverage",
                    "language",
                    "across",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The gender-flipping behavior is also consistent across languages, as shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.F7\" title=\"Figure 7 &#8227; E.1 Results on Additional Language Pairs &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. For feminine predictions,\ndeleting just 5% of the most relevant features identified by our method causes the model to switch to masculine translations in over 60% of the covered cases\nfor both Italian and Spanish, as observed for French in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S4\" title=\"4 Use Case &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nThe asymmetry between feminine and masculine predictions persists as well, with masculine-to-feminine flip rates not going much higher than 40% across all language pairs. This consistent pattern across three Romance languages with similar grammatical gender systems suggests that our earlier hypothesis about a masculine default could plausibly explain the model&#8217;s behavior in these three cases.</p>\n\n",
                "matched_terms": [
                    "language",
                    "method",
                    "across",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The consistency across three target languages shows the robustness of our method and suggests that the model may be using similar strategies to make gender choices in all languages considered.</p>\n\n",
                "matched_terms": [
                    "across",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we report the results obtained for the three language pairs en-es/fr/it using the monolingual Conformer encoder-Transformer decoder models by <cite class=\"ltx_cite ltx_citemacro_citet\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib40\" title=\"\">2024</a>)</cite>, released under the Apache 2.0 License. Such models feature a higher translation quality than the Transformer-based model we reported on above, but has a lower gender accuracy, as reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A2.T1\" title=\"Table 1 &#8227; Appendix B Model Details &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The coverage patterns observed for the Transformer model extend to the monolingual Conformer models. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.F8\" title=\"Figure 8 &#8227; E.2 Results on Conformer Models &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, the relative scorer maintains consistently higher coverage than the difference scorer, although with lower absolute values compared to the Transformer model. At 10% deletion, coverage with the relative scorer remains above 30% for all language pairs, indicating that our method still effectively isolates gender-relevant features despite the architectural differences.</p>\n\n",
                "matched_terms": [
                    "coverage",
                    "language",
                    "method",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The gender-flipping behavior shows interesting variations from the Transformer results. As seen in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.F9\" title=\"Figure 9 &#8227; E.2 Results on Conformer Models &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, while the relative scorer still triggers gender switches, the effect is more balanced between feminine and masculine predictions - both achieving flip rates between 30-60% when 10% of features are deleted. This differs from the Transformer&#8217;s strong asymmetry between genders. This more\nbalanced gender-flipping\neffect aligns with the Conformer models&#8217; general gender translation patterns (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A2.T1\" title=\"Table 1 &#8227; Appendix B Model Details &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Indeed, these models show substantially lower gender accuracy, particularly for feminine terms referring to the speaker, where performance approaches chance level. The reduced impact of feature deletion on gender prediction suggests these models\nmay rely not only on the speech input but also on other factors, such as previously generated tokens or\npotential biases embedded in the decoder&#8217;s &#8220;internal language model&#8221;\n<cite class=\"ltx_cite ltx_citemacro_cite\">Fucci et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib18\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents an empirical comparison, for our XAI application, of different methods for computing word-level probabilities from the sequences of subword token probabilities output by ST models. Previous work has employed various approaches to address this issue. Some studies have avoided the problem entirely by limiting their analysis to targets and foils tokenized as single tokens <cite class=\"ltx_cite ltx_citemacro_cite\">Yin and Neubig (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib67\" title=\"\">2022</a>)</cite>. Others have worked at the subword level without aggregation <cite class=\"ltx_cite ltx_citemacro_cite\">Ferrando et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib15\" title=\"\">2022</a>)</cite>, or employed the chain rule to multiply individual token probabilities <cite class=\"ltx_cite ltx_citemacro_cite\">Sarti et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib50\" title=\"\">2023</a>)</cite>. Here, we leverage our evaluation metrics for explanation faithfulness&#8212;coverage and flip rate&#8212;to compare three methods and determine if any of them yields more reliable contrastive explanations when integrated into our methodology.</p>\n\n",
                "matched_terms": [
                    "rule",
                    "comparison",
                    "chain"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">Chain Rule</span> method simply multiplies the probabilities of all subword tokens that compose a word, applying the standard chain rule of probability:</p>\n\n",
                "matched_terms": [
                    "rule",
                    "word",
                    "method",
                    "chain"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This approach is commonly used for aggregating subword probabilities, and serves as the default method in <cite class=\"ltx_cite ltx_citemacro_cite\">Sarti et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib50\" title=\"\">2023</a>)</cite>. While straightforward, this method does not account for the varying number of tokens across different words, potentially biasing toward shorter token sequences.</p>\n\n",
                "matched_terms": [
                    "method",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">Length Normalization</span> method addresses this limitation by taking the n<sup class=\"ltx_sup\">th</sup> root of the product of token probabilities, where n is the number of tokens:</p>\n\n",
                "matched_terms": [
                    "method",
                    "length"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the log domain, this corresponds to computing the average log probability over the sequence of subwords, effectively normalizing by the sequence length. This normalization, which is typically used in beam search, attempts to create a fairer comparison between words of different lengths <cite class=\"ltx_cite ltx_citemacro_cite\">Meister et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib34\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "comparison",
                    "length"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">Word Boundary</span> method, proposed by <cite class=\"ltx_cite ltx_citemacro_citet\">Pimentel and Meister (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib44\" title=\"\">2024</a>)</cite>, considers not only the probability of the subword sequence itself but also the probability that this sequence forms a complete word rather than being part of a longer word. This is calculated using:</p>\n\n",
                "matched_terms": [
                    "boundary",
                    "word",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"S^{bow}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p4.m1\" intent=\":literal\"><semantics><msubsup><mi>S</mi><mi>i</mi><mrow><mi>b</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>w</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">S^{bow}_{i}</annotation></semantics></math> is the set of beginning-of-word tokens at step <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p4.m2\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>. This approach ensures that the subwords <math alttext=\"w_{0,...,n}\\in w\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p4.m3\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mrow><mn>0</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>n</mi></mrow></msub><mo>&#8712;</mo><mi>w</mi></mrow><annotation encoding=\"application/x-tex\">w_{0,...,n}\\in w</annotation></semantics></math> are followed by a new word boundary, preventing probability overestimation for words that are prefixes of longer terms.</p>\n\n",
                "matched_terms": [
                    "word",
                    "boundary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the Chain Rule and Word Boundary methods yield comparable performance, we looked at individual examples in which the ranking of the target and foil is different with the two methods to deepen our comparison. Namely, we found a few examples in which the Chain Rule method assigns a higher probability to the foil than to the target, even though the beam search selects the target word in the end.\nFor instance, in the sentence &#8220;In one, I was the classic Asian <span class=\"ltx_text ltx_framed ltx_framed_underline\">student</span>, relentless in the demands that I made on myself&#8221;, translated into Italian as &#8220;In uno, ero la classica <span class=\"ltx_text ltx_framed ltx_framed_underline\">studentessa</span> asiatica, incantata nelle richieste di me stessa&#8221;, the feminine form <span class=\"ltx_text ltx_font_italic\">studentessa</span>&#8212;tokenized as <span class=\"ltx_text ltx_font_typewriter\">_studente ssa</span>&#8212;has a joint probability of 0.899, while the masculine form <span class=\"ltx_text ltx_font_typewriter\">_studente</span> scores 0.908.\nHowever, when applying the Word Boundary method, the unchosen masculine form receives a much lower probability (0.004), reflecting a more accurate estimate. This method&#8217;s principled treatment of complete words versus prefixes makes it especially suited for analyzing gendered terms, where precise probability computation is critical. However, the number of cases in which this happens is fairly limited (less than 10), which explains why there are no significant differences between the scores of the two methods when aggregating over the whole dataset. Nonetheless, this example demonstrates that in the few cases in which they differ, the Word Boundary method provides the most correct probability estimation.</p>\n\n",
                "matched_terms": [
                    "word",
                    "method",
                    "performance",
                    "comparison",
                    "rule",
                    "boundary",
                    "chain"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The patterns regarding similarity between contrastive and non-contrastive explanations observed for the Transformer model on English-French translations extend consistently across all models and language pairs.\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.T4\" title=\"Table 4 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the Pearson correlation coefficients between non-contrastive explanations and contrastive explanations generated using the difference and relative scorers, respectively. The difference scorer produces explanations that strongly correlate with non-contrastive ones (correlation coefficients between 0.86 and 0.94), while the relative scorer generates distinctly different explanations (correlation coefficients between 0.29 and 0.43). These results reinforce our findings from Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S4\" title=\"4 Use Case &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> regarding the relative scorer&#8217;s superior ability to generate truly contrastive explanations.</p>\n\n",
                "matched_terms": [
                    "language",
                    "across",
                    "pairs"
                ]
            }
        ]
    },
    "A6.T3.fig1": {
        "source_file": "The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models",
        "caption": "Table 3: Statistical significance (p-values) of pairwise method comparisons for flip rate performance across language pairs.",
        "body": "Method Comparison\nen-fr\nen-es\nen-it\n\n\n\n\nWord Boundary vs. Length Norm\n\n<< 0.001***\n0.196\n0.129\n\n\nWord Boundary vs. Chain Rule\n0.605\n0.998\n0.657\n\n\nLength Norm vs. Chain Rule\n\n<< 0.001***\n0.110\n0.074",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Method Comparison</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">en-fr</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">en-es</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">en-it</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Word Boundary vs. Length Norm</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A6.T3.m1\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math> 0.001***</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">0.196</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">0.129</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Word Boundary vs. Chain Rule</td>\n<td class=\"ltx_td ltx_align_left\">0.605</td>\n<td class=\"ltx_td ltx_align_left\">0.998</td>\n<td class=\"ltx_td ltx_align_left\">0.657</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\">Length Norm vs. Chain Rule</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\">\n<math alttext=\"&lt;\" class=\"ltx_Math\" display=\"inline\" id=\"A6.T3.m2\" intent=\":literal\"><semantics><mo>&lt;</mo><annotation encoding=\"application/x-tex\">&lt;</annotation></semantics></math> 0.001***</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\">0.110</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\">0.074</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "norm",
            "rule",
            "statistical",
            "enit",
            "pairs",
            "rate",
            "method",
            "performance",
            "comparison",
            "comparisons",
            "enfr",
            "length",
            "pairwise",
            "pvalues",
            "language",
            "word",
            "across",
            "significance",
            "flip",
            "enes",
            "boundary",
            "chain"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To evaluate the performance of each method, we recorded coverage and flip rate measurements at progressive deletion steps, as shown in Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.F12\" title=\"Figure 12 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.F13\" title=\"Figure 13 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. To quantitatively assess differences between methods, we conducted paired t-tests comparing coverage and flip rate values. Each pair of methods was evaluated using the sequence of measurements at corresponding deletion percentages as dependent samples, allowing us to determine whether observed differences were statistically significant. Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.T2.fig1\" title=\"Table 2 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.T3.fig1\" title=\"Table 3 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> present the resulting p-values for each comparison across language pairs.</p>\n\n",
            "<p class=\"ltx_p\">These comparisons reveal that all three methods perform similarly in their ability to identify gender-relevant features. While coverage differences are often statistically significant (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.T2.fig1\" title=\"Table 2 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), the absolute differences are relatively small as can be seen in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.F12\" title=\"Figure 12 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>, with all methods maintaining sufficient coverage levels to ensure reliable flip rate calculations. The flip rate analysis&#8212;our primary metric for evaluating how precisely methods isolate features responsible for gender prediction&#8212;shows no statistically significant differences (<math alttext=\"p&gt;0.05\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p6.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&gt;</mo><mn>0.05</mn></mrow><annotation encoding=\"application/x-tex\">p&gt;0.05</annotation></semantics></math>) between Word Boundary and Chain Rule methods across all language pairs (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.T3.fig1\" title=\"Table 3 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). As we see in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.F13\" title=\"Figure 13 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, when over 10% of the highlighted features are occluded, all methods achieve comparable flip rates between 50-60% for most language pairs. The only notable exception is the Length Normalization method, which performs significantly worse for French in terms of flip rate (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.T3.fig1\" title=\"Table 3 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p6.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>).\nWhile the reasons for this language-specific underperformance warrant further investigation, the observation supports our decision to exclude the Length Normalization method from our main experiments.\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Nevertheless, contrastive explanations have yet to be applied to speech-to-text (S2T) models. As S2T adoption grows <cite class=\"ltx_cite ltx_citemacro_cite\">Latif et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib29\" title=\"\">2023</a>); Barrault et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib7\" title=\"\">2025</a>)</cite>, extending XAI advances like contrastive explanations to speech becomes crucial.\nSome pioneering works have tackled the explanation of S2T models&#8217; decisions, braving key challenges including the multidimensional nature of speech signals, which span time and frequency, and the variable length of output sequences <cite class=\"ltx_cite ltx_citemacro_cite\">Wu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib65\" title=\"\">2024</a>)</cite>. The main approach used in the literature is to measure how perturbing the input audio signal affects the output <cite class=\"ltx_cite ltx_citemacro_cite\">Mandel (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib31\" title=\"\">2016</a>); Kavaki and Mandel (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib27\" title=\"\">2020</a>); Trinh and Mandel (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib58\" title=\"\">2020</a>); Markert et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib32\" title=\"\">2021</a>); Mohebbi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib37\" title=\"\">2023</a>); Wu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib64\" title=\"\">2023</a>); Fucci et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib19\" title=\"\">2024</a>); Wu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib65\" title=\"\">2024</a>)</cite>.\nThe resulting explanations take the form of saliency maps over a spectrogram representation of the audio input (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.a). These maps highlight the input regions that most strongly influence the model&#8217;s predictions (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.b). However, such explanations are holistic: they identify features relevant for all aspects of word generation (e.g. why the model produces &#8216;<span class=\"ltx_text ltx_framed ltx_framed_underline\">curioso</span>&#8217;), without focusing on specific contrastive aspects (why &#8216;curios<span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">o</span>&#8217;, in the masculine, instead of &#8216;curios<span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">a</span>&#8217;).\n<span class=\"ltx_text ltx_font_bold\">How, then, can we obtain contrastive explanations of the use of speech features by S2T models?</span></p>\n\n",
                "matched_terms": [
                    "word",
                    "length"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To meet this challenge, we build upon a prior non-contrastive feature attribution method for S2T, SPES <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib19\" title=\"\">2024</a>)</cite>. Adapting it to produce contrastive explanations requires two main interventions: <span class=\"ltx_text ltx_font_italic\">i)</span> aggregating token-level probabilities for word-level analysis, since we aim to explain why one word was generated instead of another, while models generate subword tokens rather than complete words, and <span class=\"ltx_text ltx_font_italic\">ii)</span> designing a scoring function that quantifies relative probability changes between a <span class=\"ltx_text ltx_font_italic\">target</span> word and an alternative (the <span class=\"ltx_text ltx_font_italic\">foil</span>). We investigate multiple approaches for both challenges, finding that standard solutions from text-based NLP are inadequate for our scenario, and propose improvements upon them.</p>\n\n",
                "matched_terms": [
                    "word",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our method through a case study in speech translation (ST), focusing on the translation of gender-neutral terms referring to the speaker to languages requiring grammatical gender choices. For instance, translating &#8220;I am curious&#8221; to Italian typically involves choosing between an adjective with masculine inflection or one with feminine inflection. This setting is well suited for evaluating contrastive explanations in S2T as it provides natural pairs to contrast (masculine/feminine forms), while offering an opportunity to study whether models use acoustic cues like the speaker&#8217;s pitch to disambiguate gender <cite class=\"ltx_cite ltx_citemacro_cite\">Bentivogli et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib8\" title=\"\">2020</a>)</cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We acknowledge that using vocal features like pitch for gender prediction and framing gender as a binary construct raise important ethical concerns, which we discuss in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S7\" title=\"7 Ethics Statement &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "method",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our method produces markedly different saliency maps from non-contrastive approaches (see Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.d), and our experiments confirm that these differences reflect its ability to isolate gender-specific features, while non-contrastive explanations highlight regions affecting general word prediction.\nOur contributions are:\n<span class=\"ltx_text ltx_font_italic\">i)</span> <span class=\"ltx_text ltx_font_bold\">the first method for contrastive explanations in S2T</span> (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S2\" title=\"2 Contrastive Explanations for S2T &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), enabling precise identification of the input features a model relies on to translate gender-ambiguous terms referring to the speaker; <span class=\"ltx_text ltx_font_italic\">ii)</span> <span class=\"ltx_text ltx_font_bold\">a methodology to evaluate their faithfulness</span> &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S3\" title=\"3 Evaluation &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>); <span class=\"ltx_text ltx_font_italic\">iii)</span> <span class=\"ltx_text ltx_font_bold\">empirical validation</span> on three language pairs (en<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>fr/it/es) demonstrating that our method successfully isolates gender-relevant features (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S4\" title=\"4 Use Case &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Our code is available at https://github.com/hlt-mt/FBK-fairseq under the Apache License 2.0.</span></span></span></p>\n\n",
                "matched_terms": [
                    "language",
                    "method",
                    "word",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our Solution.</span> To overcome the limitations of previous approaches and obtain accurate word-level probabilities, we adopt the methodology of <cite class=\"ltx_cite ltx_citemacro_citet\">Pimentel and Meister (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib44\" title=\"\">2024</a>)</cite>.\nUnlike other simplistic methods, this approach accounts for the fact that a sequence of subwords should be followed by a beginning of word or punctuation token to form a word rather than a prefix.\nThis distinction is crucial to avoid overestimating the likelihood of subword sequences that could be prefixes (e.g., Italian: <span class=\"ltx_text ltx_font_typewriter\">_professor e<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_serif ltx_font_italic\">M</span></sup></span> vs. <span class=\"ltx_text ltx_font_typewriter\">_professor e ssa<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_serif ltx_font_italic\">F</span></sup></span>).\nTo the best of our knowledge, we are the first to apply this principled method to feature attribution, ensuring accurate word-level explanations.</p>\n\n",
                "matched_terms": [
                    "word",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adapt the flip rate metric proposed by <cite class=\"ltx_cite ltx_citemacro_citet\">Chemmengath et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib11\" title=\"\">2022</a>)</cite> for evaluating counterfactuals in text classification. In their work, the flip rate measures how often a counterfactual input causes the model to predict the foil instead of the target. Since our explanations are saliency maps rather than input variants, we combine this concept with the <span class=\"ltx_text ltx_font_bold\">deletion metric</span>&#8212;a common approach for evaluating feature attribution methods <cite class=\"ltx_cite ltx_citemacro_cite\">Samek et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib47\" title=\"\">2016</a>); Arras et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib6\" title=\"\">2017</a>); Tomsett et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib57\" title=\"\">2020</a>); Samek et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib48\" title=\"\">2021</a>); Fucci et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib19\" title=\"\">2024</a>); Gevaert et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib20\" title=\"\">2024</a>)</cite>. Starting with the most salient regions identified by our method, we progressively remove input features and measure how quickly this causes the model to switch from predicting the target to the foil. A faithful explanation should identify features that require minimal deletion to flip the model&#8217;s prediction.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "method",
                    "flip"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, unlike in classification tasks with fixed output classes, generative models can produce any text when perturbed, including synonyms or unrelated outputs. For our application, this complicates detecting when the predicted gender changes. Therefore, we separate our evaluation into two complementary metrics: <span class=\"ltx_text ltx_font_bold\">coverage</span>, which tracks the percentage of cases where the model generates either term of interest, and <span class=\"ltx_text ltx_font_bold\">flip rate</span>, which measures how often the model switches from target to foil within these covered cases. While the flip rate indicates whether we identified the right features for the target-foil contrast, coverage ensures these features are specific to the contrast rather than affecting general text generation.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "flip"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We test our contrastive feature attribution method on speaker&#8217;s gender assignment in S2T translation.\nAs a <span class=\"ltx_text ltx_font_bold\">benchmark</span>, we use MuST-SHE <cite class=\"ltx_cite ltx_citemacro_cite\">Bentivogli et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib8\" title=\"\">2020</a>)</cite>, which contains English-to-Spanish/French/Italian ST pairs annotated for terms that lack explicit gender marking in the source but require gender assignment in the target language.\nWe focus on terms referring to the speaker,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>For details on example selection, see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A3\" title=\"Appendix C Dataset Description and Processing &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</span></span></span> for which MuST-SHE provides both correct gender translations (based on gold speaker gender labels) and their alternative incorrect versions<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Ethical implications of this study are discussed in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S7\" title=\"7 Ethics Statement &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</span></span></span> (e.g., &#8216;curiosa<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">F</span></sup>&#8217; vs. &#8216;curioso<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">M</span></sup>&#8217;) which we use to construct the target/foil pairs.\nHere, we compare our relative scoring function with the difference scorer on the en<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>fr split, using the multilingual S2T Transformer encoder-decoder model by <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib62\" title=\"\">2020a</a>)</cite>, which has demonstrated strong performance in gender translation accuracy.\nExperiments with different architectures and language pairs, showing the same trends, are reported in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5\" title=\"Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>, while the word-level aggregation methods are compared in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6\" title=\"Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "method",
                    "language",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We now demonstrate that the features identified by the relative scorer are responsible for shifting the model&#8217;s predictions in the expected direction along the gender axis.\nFig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S4.F3\" title=\"Figure 3 &#8227; Coverage. &#8227; 4 Use Case &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows that the relative scorer effectively isolates features driving feminine gender predictions: occluding the top 5% most relevant features causes the model to switch to masculine translations for over 70% of the terms that remain in coverage. This high flip rate, combined with the maintained coverage discussed above, demonstrates that <span class=\"ltx_text ltx_font_bold\">our method precisely discriminates the exact input regions driving translation toward one gender instead of the other</span>: the model generates the same terms (coverage) but changes their gender (flip rate) when these features are removed.\nFor masculine predictions, in contrast, the flip rate is much lower, plateauing at <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>30%.\nHowever, this may not indicate a limitation of the method, but rather reflect an underlying property of the ST model: if masculine serves as the default prediction rather than one actively triggered by specific input cues, then no occluded features could shift the prediction toward the feminine class.\nThe XAI literature on gender bias supports this hypothesis: <cite class=\"ltx_cite ltx_citemacro_citet\">Jumelet et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib26\" title=\"\">2019</a>)</cite> observe that models use masculine as a default, generating feminine forms only when strong feminine signals are present. This masculine default bias has also been observed in ST systems&#8217; internal language models <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib18\" title=\"\">2023</a>)</cite> and likely stems from training data gender imbalance <cite class=\"ltx_cite ltx_citemacro_cite\">Tatman (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib56\" title=\"\">2017</a>)</cite>&#8212;MuST-C contains twice as many male as female speakers <cite class=\"ltx_cite ltx_citemacro_cite\">Cattoni et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib10\" title=\"\">2021</a>)</cite>.\nWe defer to future work a thorough analysis of the acoustic features the model exploits for gender translation and their interaction with other model components and potential sources of bias.</p>\n\n",
                "matched_terms": [
                    "language",
                    "rate",
                    "method",
                    "flip"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper has introduced the first method to obtain contrastive explanations for S2T models, which identify input features that lead the model to generate one output word instead of another. Our solution builds on perturbation-based feature attribution over spectrograms. The key element is our scorer, which ensures truly contrastive explanations, unlike the difference scorer widely used in NLP. Our case study on the input features driving the translation of speakers&#8217; gender in ST demonstrated the effectiveness of our approach in isolating the input features driving the model&#8217;s choice between feminine and masculine grammatical forms. Our methodology will enable future research not only to investigate which phonetic cues ST models use for gender disambiguation, but could also be applied to other phenomena and S2T tasks.</p>\n\n",
                "matched_terms": [
                    "word",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed\n<span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S4.SS0.SSS0.Px2\" title=\"Flip rate. &#8227; 4 Use Case &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>,</span>\nthe flip rate for feminine and masculine forms is different, and never reaches 100%. For the Conformer models in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5\" title=\"Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>, the flip rate is closer among genders.\nWe propose potential reasons for these variations, including differences in models&#8217; initial gender translation capabilities and possible masculine default bias in ST models&#8217; internal language models. But these remain conjectures that require empirical verification. Such investigation of how different ST models translate gender falls outside the scope of this work, which focuses on developing and evaluating contrastive explanations rather than reaching conclusions about gender translation mechanisms in ST models.</p>\n\n",
                "matched_terms": [
                    "language",
                    "rate",
                    "flip"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The ST Transformer encoder-decoder model by <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib62\" title=\"\">2020a</a>)</cite> is a multilingual model trained on the 8 language directions of MuST-C <cite class=\"ltx_cite ltx_citemacro_citep\">(Cattoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib10\" title=\"\">2021</a>)</cite> with 72M of parameters, distributed under the MIT License. The model takes as input 80 Mel-filterbank audio features extracted every 10 milliseconds, employing a sample window of 25. The input features are then preprocessed with two 1D convolutional layers with stride 2, reducing input length by a factor of 4, before being fed to the Transformer encoder. We choose this model for our experiments both for its permissive license and its strong gender translation accuracy (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A2.T1\" title=\"Table 1 &#8227; Appendix B Model Details &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).\nFor inference, we used a beam size of 5 and a no-repeat-ngram-size of 5.\nThe maximum source position was set to 7,000.</p>\n\n",
                "matched_terms": [
                    "language",
                    "length"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We run our experiments on English-to-Spanish/French/Italian translations, specifically examining gender terms that refer to the speaker (category 1 in MuST-SHE), as these instances may benefit from acoustic cues for gender disambiguation. The dataset provides pairs of correct and incorrect gender translations for each term (e.g., &#8216;curiosa&#8217;/&#8216;curioso&#8217;), which we use in our contrastive analysis. Using the annotations from <cite class=\"ltx_cite ltx_citemacro_citet\">Savoldi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib51\" title=\"\">2022</a>)</cite>, we exclude gender articles from our analysis due to their high frequency in both genders across sentences, which makes it challenging to reliably identify specific instances referring to the speaker and could introduce noise into the evaluation.</p>\n\n",
                "matched_terms": [
                    "across",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The choice to focus on the first 20% of feature deletion in our main analysis stems from the fact that beyond this point the input becomes too degraded for model outputs to be meaningful. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A4.F4\" title=\"Figure 4 &#8227; Appendix D Full Deletion Range Analysis &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, coverage drops substantially with increased deletion, making the flip rate measurements unreliable, as evidenced by the erratic behavior of the curve in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A4.F5\" title=\"Figure 5 &#8227; Appendix D Full Deletion Range Analysis &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> at higher deletion percentages.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "flip"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, to make sure our contrastive explanation method works for different models and language pairs, we extend our study to the English-Italian and English-Spanish sections of the data (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.SS1\" title=\"E.1 Results on Additional Language Pairs &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">E.1</span></a>) and to two other types of models. In &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.SS2\" title=\"E.2 Results on Conformer Models &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">E.2</span></a>, we analyze the monolingual Conformer encoder-Transformer decoder models by <cite class=\"ltx_cite ltx_citemacro_citet\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib40\" title=\"\">2024</a>)</cite>. Similarly to the Transformer model used in the main body of the paper, this model is trained on MuST-C. In &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.SS3\" title=\"E.3 Results on Large-scale Model &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">E.3</span></a>, to analyze a larger-scale system, we study a Conformer encoder-Transformer decoder model trained on the English-Spanish data of the constrained track of the last IWSLT campaign <cite class=\"ltx_cite ltx_citemacro_citep\">(Ahmad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib2\" title=\"\">2024</a>)</cite>.\nWhile our perturbation-based method for contrastive explanations is inherently model-agnostic and applicable to any architecture, we empirically validate that the explanations generated across these diverse model configurations continue to satisfy our quantitative faithfulness metrics.</p>\n\n",
                "matched_terms": [
                    "language",
                    "method",
                    "across",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the multilingual Transformer model, we extend our analysis to English-Italian and English-Spanish translations to verify whether the patterns observed for English-French generalize across language pairs. As shown in\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.F6\" title=\"Figure 6 &#8227; E.1 Results on Additional Language Pairs &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>,\nthe relative scorer maintains substantially higher coverage than the difference scorer across\nthe two languages, confirming the findings reported in the main paper for English-French.\nWhen deleting 10% of the features, coverage remains above 20% for Italian and Spanish, matching the robustness observed for French translations.\n</p>\n\n",
                "matched_terms": [
                    "language",
                    "across",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The gender-flipping behavior is also consistent across languages, as shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.F7\" title=\"Figure 7 &#8227; E.1 Results on Additional Language Pairs &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. For feminine predictions,\ndeleting just 5% of the most relevant features identified by our method causes the model to switch to masculine translations in over 60% of the covered cases\nfor both Italian and Spanish, as observed for French in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S4\" title=\"4 Use Case &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nThe asymmetry between feminine and masculine predictions persists as well, with masculine-to-feminine flip rates not going much higher than 40% across all language pairs. This consistent pattern across three Romance languages with similar grammatical gender systems suggests that our earlier hypothesis about a masculine default could plausibly explain the model&#8217;s behavior in these three cases.</p>\n\n",
                "matched_terms": [
                    "language",
                    "method",
                    "across",
                    "pairs",
                    "flip"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The consistency across three target languages shows the robustness of our method and suggests that the model may be using similar strategies to make gender choices in all languages considered.</p>\n\n",
                "matched_terms": [
                    "across",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we report the results obtained for the three language pairs en-es/fr/it using the monolingual Conformer encoder-Transformer decoder models by <cite class=\"ltx_cite ltx_citemacro_citet\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib40\" title=\"\">2024</a>)</cite>, released under the Apache 2.0 License. Such models feature a higher translation quality than the Transformer-based model we reported on above, but has a lower gender accuracy, as reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A2.T1\" title=\"Table 1 &#8227; Appendix B Model Details &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The coverage patterns observed for the Transformer model extend to the monolingual Conformer models. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.F8\" title=\"Figure 8 &#8227; E.2 Results on Conformer Models &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, the relative scorer maintains consistently higher coverage than the difference scorer, although with lower absolute values compared to the Transformer model. At 10% deletion, coverage with the relative scorer remains above 30% for all language pairs, indicating that our method still effectively isolates gender-relevant features despite the architectural differences.</p>\n\n",
                "matched_terms": [
                    "language",
                    "method",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The gender-flipping behavior shows interesting variations from the Transformer results. As seen in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.F9\" title=\"Figure 9 &#8227; E.2 Results on Conformer Models &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, while the relative scorer still triggers gender switches, the effect is more balanced between feminine and masculine predictions - both achieving flip rates between 30-60% when 10% of features are deleted. This differs from the Transformer&#8217;s strong asymmetry between genders. This more\nbalanced gender-flipping\neffect aligns with the Conformer models&#8217; general gender translation patterns (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A2.T1\" title=\"Table 1 &#8227; Appendix B Model Details &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Indeed, these models show substantially lower gender accuracy, particularly for feminine terms referring to the speaker, where performance approaches chance level. The reduced impact of feature deletion on gender prediction suggests these models\nmay rely not only on the speech input but also on other factors, such as previously generated tokens or\npotential biases embedded in the decoder&#8217;s &#8220;internal language model&#8221;\n<cite class=\"ltx_cite ltx_citemacro_cite\">Fucci et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib18\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "flip",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This large-scale model exhibits similar patterns to the smaller Conformer models. As shown in Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.F10\" title=\"Figure 10 &#8227; E.3 Results on Large-scale Model &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.F11\" title=\"Figure 11 &#8227; E.3 Results on Large-scale Model &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, the relative scorer maintains higher coverage than the difference scorer. The flip rate displays a reduced asymmetry between feminine and masculine predictions, with the first reaching around 50% after 10% feature deletion, and the second, a little over 40%. This behavior is aligned with that of the monolignual small Conformer models.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "flip"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents an empirical comparison, for our XAI application, of different methods for computing word-level probabilities from the sequences of subword token probabilities output by ST models. Previous work has employed various approaches to address this issue. Some studies have avoided the problem entirely by limiting their analysis to targets and foils tokenized as single tokens <cite class=\"ltx_cite ltx_citemacro_cite\">Yin and Neubig (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib67\" title=\"\">2022</a>)</cite>. Others have worked at the subword level without aggregation <cite class=\"ltx_cite ltx_citemacro_cite\">Ferrando et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib15\" title=\"\">2022</a>)</cite>, or employed the chain rule to multiply individual token probabilities <cite class=\"ltx_cite ltx_citemacro_cite\">Sarti et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib50\" title=\"\">2023</a>)</cite>. Here, we leverage our evaluation metrics for explanation faithfulness&#8212;coverage and flip rate&#8212;to compare three methods and determine if any of them yields more reliable contrastive explanations when integrated into our methodology.</p>\n\n",
                "matched_terms": [
                    "rule",
                    "flip",
                    "comparison",
                    "chain"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">Chain Rule</span> method simply multiplies the probabilities of all subword tokens that compose a word, applying the standard chain rule of probability:</p>\n\n",
                "matched_terms": [
                    "rule",
                    "word",
                    "method",
                    "chain"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This approach is commonly used for aggregating subword probabilities, and serves as the default method in <cite class=\"ltx_cite ltx_citemacro_cite\">Sarti et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib50\" title=\"\">2023</a>)</cite>. While straightforward, this method does not account for the varying number of tokens across different words, potentially biasing toward shorter token sequences.</p>\n\n",
                "matched_terms": [
                    "method",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">Length Normalization</span> method addresses this limitation by taking the n<sup class=\"ltx_sup\">th</sup> root of the product of token probabilities, where n is the number of tokens:</p>\n\n",
                "matched_terms": [
                    "method",
                    "length"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the log domain, this corresponds to computing the average log probability over the sequence of subwords, effectively normalizing by the sequence length. This normalization, which is typically used in beam search, attempts to create a fairer comparison between words of different lengths <cite class=\"ltx_cite ltx_citemacro_cite\">Meister et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib34\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "comparison",
                    "length"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The <span class=\"ltx_text ltx_font_bold\">Word Boundary</span> method, proposed by <cite class=\"ltx_cite ltx_citemacro_citet\">Pimentel and Meister (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib44\" title=\"\">2024</a>)</cite>, considers not only the probability of the subword sequence itself but also the probability that this sequence forms a complete word rather than being part of a longer word. This is calculated using:</p>\n\n",
                "matched_terms": [
                    "boundary",
                    "word",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"S^{bow}_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p4.m1\" intent=\":literal\"><semantics><msubsup><mi>S</mi><mi>i</mi><mrow><mi>b</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>w</mi></mrow></msubsup><annotation encoding=\"application/x-tex\">S^{bow}_{i}</annotation></semantics></math> is the set of beginning-of-word tokens at step <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p4.m2\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>. This approach ensures that the subwords <math alttext=\"w_{0,...,n}\\in w\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p4.m3\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mrow><mn>0</mn><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><mi>n</mi></mrow></msub><mo>&#8712;</mo><mi>w</mi></mrow><annotation encoding=\"application/x-tex\">w_{0,...,n}\\in w</annotation></semantics></math> are followed by a new word boundary, preventing probability overestimation for words that are prefixes of longer terms.</p>\n\n",
                "matched_terms": [
                    "word",
                    "boundary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the Chain Rule and Word Boundary methods yield comparable performance, we looked at individual examples in which the ranking of the target and foil is different with the two methods to deepen our comparison. Namely, we found a few examples in which the Chain Rule method assigns a higher probability to the foil than to the target, even though the beam search selects the target word in the end.\nFor instance, in the sentence &#8220;In one, I was the classic Asian <span class=\"ltx_text ltx_framed ltx_framed_underline\">student</span>, relentless in the demands that I made on myself&#8221;, translated into Italian as &#8220;In uno, ero la classica <span class=\"ltx_text ltx_framed ltx_framed_underline\">studentessa</span> asiatica, incantata nelle richieste di me stessa&#8221;, the feminine form <span class=\"ltx_text ltx_font_italic\">studentessa</span>&#8212;tokenized as <span class=\"ltx_text ltx_font_typewriter\">_studente ssa</span>&#8212;has a joint probability of 0.899, while the masculine form <span class=\"ltx_text ltx_font_typewriter\">_studente</span> scores 0.908.\nHowever, when applying the Word Boundary method, the unchosen masculine form receives a much lower probability (0.004), reflecting a more accurate estimate. This method&#8217;s principled treatment of complete words versus prefixes makes it especially suited for analyzing gendered terms, where precise probability computation is critical. However, the number of cases in which this happens is fairly limited (less than 10), which explains why there are no significant differences between the scores of the two methods when aggregating over the whole dataset. Nonetheless, this example demonstrates that in the few cases in which they differ, the Word Boundary method provides the most correct probability estimation.</p>\n\n",
                "matched_terms": [
                    "word",
                    "method",
                    "performance",
                    "comparison",
                    "rule",
                    "boundary",
                    "chain"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The patterns regarding similarity between contrastive and non-contrastive explanations observed for the Transformer model on English-French translations extend consistently across all models and language pairs.\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.T4\" title=\"Table 4 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the Pearson correlation coefficients between non-contrastive explanations and contrastive explanations generated using the difference and relative scorers, respectively. The difference scorer produces explanations that strongly correlate with non-contrastive ones (correlation coefficients between 0.86 and 0.94), while the relative scorer generates distinctly different explanations (correlation coefficients between 0.29 and 0.43). These results reinforce our findings from Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S4\" title=\"4 Use Case &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> regarding the relative scorer&#8217;s superior ability to generate truly contrastive explanations.</p>\n\n",
                "matched_terms": [
                    "language",
                    "across",
                    "pairs"
                ]
            }
        ]
    },
    "A6.T4": {
        "source_file": "The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models",
        "caption": "Table 4: Pearson correlation between non-contrastive explanations and contrastive explanations obtained with the difference and relative scorer for different models and language pairs.",
        "body": "Mult. Transformer\nMono. Conformer\nLarge-scale Conformer\n\n\n\n\n\nen-it\nen-fr\nen-es\nen-it\nen-fr\nen-es\n\n\nDifference\n0.94\n0.93\n0.92\n0.88\n0.86\n0.89\n\n\nRelative\n0.36\n0.33\n0.39\n0.29\n0.32\n0.43",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Mult. Transformer</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Mono. Conformer</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Large-scale Conformer</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">en-it</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">en-fr</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">en-es</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">en-it</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">en-fr</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">en-es</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Difference</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.94</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.93</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">0.92</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.88</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">0.86</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">0.89</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">Relative</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.43</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "difference",
            "largescale",
            "correlation",
            "obtained",
            "transformer",
            "enit",
            "pearson",
            "pairs",
            "noncontrastive",
            "different",
            "conformer",
            "mono",
            "scorer",
            "relative",
            "enfr",
            "mult",
            "explanations",
            "language",
            "models",
            "enes",
            "contrastive",
            "between"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The patterns regarding similarity between contrastive and non-contrastive explanations observed for the Transformer model on English-French translations extend consistently across all models and language pairs.\nTable <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.T4\" title=\"Table 4 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the Pearson correlation coefficients between non-contrastive explanations and contrastive explanations generated using the difference and relative scorers, respectively. The difference scorer produces explanations that strongly correlate with non-contrastive ones (correlation coefficients between 0.86 and 0.94), while the relative scorer generates distinctly different explanations (correlation coefficients between 0.29 and 0.43). These results reinforce our findings from Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S4\" title=\"4 Use Case &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> regarding the relative scorer&#8217;s superior ability to generate truly contrastive explanations.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Contrastive explanations, which indicate why an AI system produced one output (the target) instead of another (the foil), are widely regarded in explainable AI as more informative and interpretable than standard explanations. However, obtaining such explanations for speech-to-text (S2T) generative models remains an open challenge. Drawing from feature attribution techniques, we propose the first method to obtain contrastive explanations in S2T by analyzing how parts of the input spectrogram influence the choice between alternative outputs. Through a case study on gender assignment in speech translation, we show that our method accurately identifies the audio features that drive the selection of one gender over another.\nBy extending the scope of contrastive explanations to S2T, our work provides a foundation for better understanding S2T models.</p>\n\n",
                "matched_terms": [
                    "between",
                    "contrastive",
                    "models",
                    "explanations"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">The Unheard Alternative: \n<br class=\"ltx_break\"/>Contrastive Explanations for Speech-to-Text Models</span>\n</p>\n\n",
                "matched_terms": [
                    "contrastive",
                    "models",
                    "explanations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The rise of deep neural networks has increased the demand for explainable AI (XAI) methods to understand model behavior <cite class=\"ltx_cite ltx_citemacro_cite\">R&#228;uker et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib45\" title=\"\">2023</a>); Ferrando et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib17\" title=\"\">2024</a>)</cite>. Within XAI, contrastive explanations&#8212;which aim to answer the question &#8216;<span class=\"ltx_text ltx_font_italic\">Why did P happen rather than Q?</span>&#8217; instead of simply &#8216;<span class=\"ltx_text ltx_font_italic\">Why did P happen?</span>&#8217; <cite class=\"ltx_cite ltx_citemacro_cite\">Lipton (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib30\" title=\"\">1990</a>)</cite>&#8212;have emerged as a promising approach with increasing adoption across various XAI applications <cite class=\"ltx_cite ltx_citemacro_cite\">Stepin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib55\" title=\"\">2021</a>)</cite>. Their\nadvantage stems from mirroring human reasoning\n<cite class=\"ltx_cite ltx_citemacro_cite\">Byrne (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib9\" title=\"\">2002</a>); Miller (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib36\" title=\"\">2019</a>)</cite> and providing more targeted insights than traditional explanations <cite class=\"ltx_cite ltx_citemacro_cite\">Lipton (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib30\" title=\"\">1990</a>); Jacovi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib25\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "contrastive",
                    "explanations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Nevertheless, contrastive explanations have yet to be applied to speech-to-text (S2T) models. As S2T adoption grows <cite class=\"ltx_cite ltx_citemacro_cite\">Latif et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib29\" title=\"\">2023</a>); Barrault et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib7\" title=\"\">2025</a>)</cite>, extending XAI advances like contrastive explanations to speech becomes crucial.\nSome pioneering works have tackled the explanation of S2T models&#8217; decisions, braving key challenges including the multidimensional nature of speech signals, which span time and frequency, and the variable length of output sequences <cite class=\"ltx_cite ltx_citemacro_cite\">Wu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib65\" title=\"\">2024</a>)</cite>. The main approach used in the literature is to measure how perturbing the input audio signal affects the output <cite class=\"ltx_cite ltx_citemacro_cite\">Mandel (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib31\" title=\"\">2016</a>); Kavaki and Mandel (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib27\" title=\"\">2020</a>); Trinh and Mandel (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib58\" title=\"\">2020</a>); Markert et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib32\" title=\"\">2021</a>); Mohebbi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib37\" title=\"\">2023</a>); Wu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib64\" title=\"\">2023</a>); Fucci et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib19\" title=\"\">2024</a>); Wu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib65\" title=\"\">2024</a>)</cite>.\nThe resulting explanations take the form of saliency maps over a spectrogram representation of the audio input (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.a). These maps highlight the input regions that most strongly influence the model&#8217;s predictions (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.b). However, such explanations are holistic: they identify features relevant for all aspects of word generation (e.g. why the model produces &#8216;<span class=\"ltx_text ltx_framed ltx_framed_underline\">curioso</span>&#8217;), without focusing on specific contrastive aspects (why &#8216;curios<span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">o</span>&#8217;, in the masculine, instead of &#8216;curios<span class=\"ltx_text ltx_font_bold ltx_framed ltx_framed_underline\">a</span>&#8217;).\n<span class=\"ltx_text ltx_font_bold\">How, then, can we obtain contrastive explanations of the use of speech features by S2T models?</span></p>\n\n",
                "matched_terms": [
                    "contrastive",
                    "models",
                    "explanations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To meet this challenge, we build upon a prior non-contrastive feature attribution method for S2T, SPES <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib19\" title=\"\">2024</a>)</cite>. Adapting it to produce contrastive explanations requires two main interventions: <span class=\"ltx_text ltx_font_italic\">i)</span> aggregating token-level probabilities for word-level analysis, since we aim to explain why one word was generated instead of another, while models generate subword tokens rather than complete words, and <span class=\"ltx_text ltx_font_italic\">ii)</span> designing a scoring function that quantifies relative probability changes between a <span class=\"ltx_text ltx_font_italic\">target</span> word and an alternative (the <span class=\"ltx_text ltx_font_italic\">foil</span>). We investigate multiple approaches for both challenges, finding that standard solutions from text-based NLP are inadequate for our scenario, and propose improvements upon them.</p>\n\n",
                "matched_terms": [
                    "relative",
                    "models",
                    "noncontrastive",
                    "contrastive",
                    "between",
                    "explanations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our method through a case study in speech translation (ST), focusing on the translation of gender-neutral terms referring to the speaker to languages requiring grammatical gender choices. For instance, translating &#8220;I am curious&#8221; to Italian typically involves choosing between an adjective with masculine inflection or one with feminine inflection. This setting is well suited for evaluating contrastive explanations in S2T as it provides natural pairs to contrast (masculine/feminine forms), while offering an opportunity to study whether models use acoustic cues like the speaker&#8217;s pitch to disambiguate gender <cite class=\"ltx_cite ltx_citemacro_cite\">Bentivogli et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib8\" title=\"\">2020</a>)</cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We acknowledge that using vocal features like pitch for gender prediction and framing gender as a binary construct raise important ethical concerns, which we discuss in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S7\" title=\"7 Ethics Statement &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "pairs",
                    "models",
                    "contrastive",
                    "between",
                    "explanations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our method produces markedly different saliency maps from non-contrastive approaches (see Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.d), and our experiments confirm that these differences reflect its ability to isolate gender-specific features, while non-contrastive explanations highlight regions affecting general word prediction.\nOur contributions are:\n<span class=\"ltx_text ltx_font_italic\">i)</span> <span class=\"ltx_text ltx_font_bold\">the first method for contrastive explanations in S2T</span> (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S2\" title=\"2 Contrastive Explanations for S2T &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), enabling precise identification of the input features a model relies on to translate gender-ambiguous terms referring to the speaker; <span class=\"ltx_text ltx_font_italic\">ii)</span> <span class=\"ltx_text ltx_font_bold\">a methodology to evaluate their faithfulness</span> &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S3\" title=\"3 Evaluation &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>); <span class=\"ltx_text ltx_font_italic\">iii)</span> <span class=\"ltx_text ltx_font_bold\">empirical validation</span> on three language pairs (en<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p5.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>fr/it/es) demonstrating that our method successfully isolates gender-relevant features (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S4\" title=\"4 Use Case &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Our code is available at https://github.com/hlt-mt/FBK-fairseq under the Apache License 2.0.</span></span></span></p>\n\n",
                "matched_terms": [
                    "language",
                    "pairs",
                    "noncontrastive",
                    "different",
                    "contrastive",
                    "explanations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Obtaining contrastive explanations, that identify why one word was chosen instead of another, requires two main changes to SPES:\n<span class=\"ltx_text ltx_font_italic\">i)</span> aggregating token-level probabilities for a word-level analysis (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S2.SS1\" title=\"2.1 Word-Level Probabilities &#8227; 2 Contrastive Explanations for S2T &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">2.1</span></a>) and <span class=\"ltx_text ltx_font_italic\">ii)</span> designing a scoring function to quantify relative changes in probability between target <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> and foil <math alttext=\"f\" class=\"ltx_Math\" display=\"inline\" id=\"S2.p2.m2\" intent=\":literal\"><semantics><mi>f</mi><annotation encoding=\"application/x-tex\">f</annotation></semantics></math> (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S2.SS2\" title=\"2.2 Scoring Functions &#8227; 2 Contrastive Explanations for S2T &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>).</p>\n\n",
                "matched_terms": [
                    "between",
                    "contrastive",
                    "explanations",
                    "relative"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In perturbation-based feature attribution, the base scoring function <math alttext=\"S_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>B</mi></msub><annotation encoding=\"application/x-tex\">S_{B}</annotation></semantics></math> is generally defined as the difference between the original probability <math alttext=\"p(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p(t)</annotation></semantics></math> and the probability <math alttext=\"\\tilde{p}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mover accent=\"true\"><mi>p</mi><mo>~</mo></mover><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\tilde{p}(t)</annotation></semantics></math> under the perturbed input <cite class=\"ltx_cite ltx_citemacro_citep\">(Ancona et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib4\" title=\"\">2018</a>; Seo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib53\" title=\"\">2019</a>)</cite>:</p>\n\n",
                "matched_terms": [
                    "between",
                    "difference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Instead, to explain why a model produced an output rather than another, contrastive feature attribution works in NLP <cite class=\"ltx_cite ltx_citemacro_cite\">Eberle et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib14\" title=\"\">2023</a>); Ferrando et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib16\" title=\"\">2023</a>); Sarti et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib50\" title=\"\">2023</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib49\" title=\"\">2024</a>); Krishna et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib28\" title=\"\">2024</a>)</cite> typically use the contrastive <span class=\"ltx_text ltx_font_italic\">difference scorer</span> by <cite class=\"ltx_cite ltx_citemacro_citet\">Yin and Neubig (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib67\" title=\"\">2022</a>)</cite>:</p>\n\n",
                "matched_terms": [
                    "contrastive",
                    "difference",
                    "scorer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><math alttext=\"S_{CD}(t,f)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>S</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>f</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">S_{CD}(t,f)</annotation></semantics></math> assigns high scores to features whose perturbation simultaneously decreases <math alttext=\"p(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p(t)</annotation></semantics></math> and increases <math alttext=\"p(f)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>f</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p(f)</annotation></semantics></math>.\nHowever, if the model strongly favors the target over the foil (<math alttext=\"p(t)\\gg p(f)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\"><semantics><mrow><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8811;</mo><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>f</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">p(t)\\gg p(f)</annotation></semantics></math>), the difference score <math alttext=\"S_{CD}(t,f)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m8\" intent=\":literal\"><semantics><mrow><msub><mi>S</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><mi>f</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">S_{CD}(t,f)</annotation></semantics></math> becomes nearly identical to the base score <math alttext=\"S_{B}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m9\" intent=\":literal\"><semantics><mrow><msub><mi>S</mi><mi>B</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">S_{B}(t)</annotation></semantics></math>.\nAs shown in Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.c, this produces explanations indistinguishable from non-contrastive ones, highlighting features involved in every aspect of the prediction of <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m10\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> rather than focusing on the contrastive aspect under study&#8212;a limitation also noted by <cite class=\"ltx_cite ltx_citemacro_citet\">Eberle et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib14\" title=\"\">2023</a>)</cite> in NLP contexts.</p>\n\n",
                "matched_terms": [
                    "explanations",
                    "contrastive",
                    "difference",
                    "noncontrastive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our Solution.</span> To address this limitation, we repurpose the <span class=\"ltx_text ltx_font_italic\">relative scorer</span> of <cite class=\"ltx_cite ltx_citemacro_citet\">Jacovi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib25\" title=\"\">2021</a>)</cite>, originally introduced to evaluate counterfactuals:</p>\n\n",
                "matched_terms": [
                    "scorer",
                    "relative"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This score normalizes the contribution of each term by their sum, which ensures that both terms remain influential in the final score even when their probabilities differ by orders of magnitude.\nThe resulting saliency maps (Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.d) differ significantly from non-contrastive ones, indicating its precision in isolating gender-specific features.\nThe theoretical advantages of <math alttext=\"S_{CR}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m1\" intent=\":literal\"><semantics><msub><mi>S</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>R</mi></mrow></msub><annotation encoding=\"application/x-tex\">S_{CR}</annotation></semantics></math>&#8217;s probability ratio normalization extend beyond our S2T application, since generative models commonly produce cases where <math alttext=\"p(t)\\gg p(f)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8811;</mo><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>f</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">p(t)\\gg p(f)</annotation></semantics></math>.\nVerifying whether <math alttext=\"S_{CR}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m3\" intent=\":literal\"><semantics><msub><mi>S</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>R</mi></mrow></msub><annotation encoding=\"application/x-tex\">S_{CR}</annotation></semantics></math> is also preferable to <math alttext=\"S_{CD}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m4\" intent=\":literal\"><semantics><msub><mi>S</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow></msub><annotation encoding=\"application/x-tex\">S_{CD}</annotation></semantics></math> for contrastive explanations of other generation tasks falls outside the scope of this work, but represents a promising direction for future research within the broader XAI community.</p>\n\n",
                "matched_terms": [
                    "explanations",
                    "models",
                    "contrastive",
                    "noncontrastive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While existing work on contrastive explanations often evaluates against linguistically plausible human-annotated explanations <cite class=\"ltx_cite ltx_citemacro_cite\">Yin and Neubig (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib67\" title=\"\">2022</a>); Eberle et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib14\" title=\"\">2023</a>); Ferrando et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib16\" title=\"\">2023</a>)</cite>, we focus on faithfulness&#8212;how accurately explanations reflect model behavior, regardless of human interpretability <cite class=\"ltx_cite ltx_citemacro_cite\">Jacovi and Goldberg (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib24\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "contrastive",
                    "explanations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We test our contrastive feature attribution method on speaker&#8217;s gender assignment in S2T translation.\nAs a <span class=\"ltx_text ltx_font_bold\">benchmark</span>, we use MuST-SHE <cite class=\"ltx_cite ltx_citemacro_cite\">Bentivogli et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib8\" title=\"\">2020</a>)</cite>, which contains English-to-Spanish/French/Italian ST pairs annotated for terms that lack explicit gender marking in the source but require gender assignment in the target language.\nWe focus on terms referring to the speaker,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>For details on example selection, see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A3\" title=\"Appendix C Dataset Description and Processing &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</span></span></span> for which MuST-SHE provides both correct gender translations (based on gold speaker gender labels) and their alternative incorrect versions<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>Ethical implications of this study are discussed in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S7\" title=\"7 Ethics Statement &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</span></span></span> (e.g., &#8216;curiosa<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">F</span></sup>&#8217; vs. &#8216;curioso<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">M</span></sup>&#8217;) which we use to construct the target/foil pairs.\nHere, we compare our relative scoring function with the difference scorer on the en<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>fr split, using the multilingual S2T Transformer encoder-decoder model by <cite class=\"ltx_cite ltx_citemacro_citet\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib62\" title=\"\">2020a</a>)</cite>, which has demonstrated strong performance in gender translation accuracy.\nExperiments with different architectures and language pairs, showing the same trends, are reported in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5\" title=\"Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>, while the word-level aggregation methods are compared in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6\" title=\"Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">F</span></a>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "difference",
                    "pairs",
                    "relative",
                    "transformer",
                    "different",
                    "contrastive",
                    "scorer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S4.F2\" title=\"Figure 2 &#8227; 4 Use Case &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reports how coverage evolves as we progressively delete the features identified as most important.\nThe coverage of the difference scorer drops below 20% after just 2% deletion, while the relative scorer maintains higher coverage: more than 30% even after 20% deletion.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>We focus on the first 20% of feature deletion, as beyond this point the input becomes too degraded for meaningful model output. Full results are reported in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A4\" title=\"Appendix D Full Deletion Range Analysis &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</span></span></span>\nThe dramatic coverage loss of the difference scorer occurs because its explanations fail to be truly contrastive; they do not target gender-relevant features.\nThis is demonstrated by their similarity to non-contrastive explanations:\nsaliency maps obtained with the contrastive difference scorer <math alttext=\"S_{CD}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><msub><mi>S</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>D</mi></mrow></msub><annotation encoding=\"application/x-tex\">S_{CD}</annotation></semantics></math> on en-fr data exhibit a Pearson correlation coefficient of 0.93 with those obtained with the base scorer <math alttext=\"S_{B}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><msub><mi>S</mi><mi>B</mi></msub><annotation encoding=\"application/x-tex\">S_{B}</annotation></semantics></math>, while relative scorer (<math alttext=\"S_{CR}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msub><mi>S</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>R</mi></mrow></msub><annotation encoding=\"application/x-tex\">S_{CR}</annotation></semantics></math>) heatmaps show only 0.33 correlation (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A7\" title=\"Appendix G Correlation Between Contrastive and Non-Contrastive Explanations &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">G</span></a> for other languages and models).\nWe conclude that the difference scorer provides generic explanations, while <span class=\"ltx_text ltx_font_bold\">the features highlighted by the relative scorer are more precisely linked to gender assignment.</span></p>\n\n",
                "matched_terms": [
                    "pearson",
                    "difference",
                    "correlation",
                    "relative",
                    "obtained",
                    "models",
                    "noncontrastive",
                    "enfr",
                    "contrastive",
                    "explanations",
                    "scorer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We now demonstrate that the features identified by the relative scorer are responsible for shifting the model&#8217;s predictions in the expected direction along the gender axis.\nFig. <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S4.F3\" title=\"Figure 3 &#8227; Coverage. &#8227; 4 Use Case &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows that the relative scorer effectively isolates features driving feminine gender predictions: occluding the top 5% most relevant features causes the model to switch to masculine translations for over 70% of the terms that remain in coverage. This high flip rate, combined with the maintained coverage discussed above, demonstrates that <span class=\"ltx_text ltx_font_bold\">our method precisely discriminates the exact input regions driving translation toward one gender instead of the other</span>: the model generates the same terms (coverage) but changes their gender (flip rate) when these features are removed.\nFor masculine predictions, in contrast, the flip rate is much lower, plateauing at <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS0.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>30%.\nHowever, this may not indicate a limitation of the method, but rather reflect an underlying property of the ST model: if masculine serves as the default prediction rather than one actively triggered by specific input cues, then no occluded features could shift the prediction toward the feminine class.\nThe XAI literature on gender bias supports this hypothesis: <cite class=\"ltx_cite ltx_citemacro_citet\">Jumelet et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib26\" title=\"\">2019</a>)</cite> observe that models use masculine as a default, generating feminine forms only when strong feminine signals are present. This masculine default bias has also been observed in ST systems&#8217; internal language models <cite class=\"ltx_cite ltx_citemacro_citep\">(Fucci et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib18\" title=\"\">2023</a>)</cite> and likely stems from training data gender imbalance <cite class=\"ltx_cite ltx_citemacro_cite\">Tatman (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib56\" title=\"\">2017</a>)</cite>&#8212;MuST-C contains twice as many male as female speakers <cite class=\"ltx_cite ltx_citemacro_cite\">Cattoni et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib10\" title=\"\">2021</a>)</cite>.\nWe defer to future work a thorough analysis of the acoustic features the model exploits for gender translation and their interaction with other model components and potential sources of bias.</p>\n\n",
                "matched_terms": [
                    "language",
                    "relative",
                    "models",
                    "scorer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper has introduced the first method to obtain contrastive explanations for S2T models, which identify input features that lead the model to generate one output word instead of another. Our solution builds on perturbation-based feature attribution over spectrograms. The key element is our scorer, which ensures truly contrastive explanations, unlike the difference scorer widely used in NLP. Our case study on the input features driving the translation of speakers&#8217; gender in ST demonstrated the effectiveness of our approach in isolating the input features driving the model&#8217;s choice between feminine and masculine grammatical forms. Our methodology will enable future research not only to investigate which phonetic cues ST models use for gender disambiguation, but could also be applied to other phenomena and S2T tasks.</p>\n\n",
                "matched_terms": [
                    "difference",
                    "models",
                    "contrastive",
                    "between",
                    "explanations",
                    "scorer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While this work demonstrates the effectiveness of the proposed contrastive explanation methodology for analyzing gender translation, the method&#8217;s applicability beyond gender analysis remains to be validated across other phenomena. However, we do not foresee any reason for which its applicability could be limited.\nPossible areas of investigation include:\ni) understanding homophone disambiguation (e.g., &#8220;plain&#8221; vs. &#8220;plane&#8221;) based on audio features <cite class=\"ltx_cite ltx_citemacro_cite\">Mohebbi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib37\" title=\"\">2023</a>); Yu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib68\" title=\"\">2024</a>)</cite>, ii) analyzing how models resolve coreference resolution (e.g., identifying the features that make a model associate an ambiguous pronoun with one referent over another) <cite class=\"ltx_cite ltx_citemacro_cite\">Amoia et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib3\" title=\"\">2012</a>); Roesiger et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib46\" title=\"\">2017</a>)</cite>, iii) investigating factuality issues by contrasting correct translations against model errors, and iv) exploring politeness register selection (e.g., why models choose formal &#8220;vous/Sie&#8221; over informal &#8220;tu/du&#8221;) <cite class=\"ltx_cite ltx_citemacro_cite\">Sennrich et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib52\" title=\"\">2016</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "contrastive",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As discussed\n<span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S4.SS0.SSS0.Px2\" title=\"Flip rate. &#8227; 4 Use Case &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>,</span>\nthe flip rate for feminine and masculine forms is different, and never reaches 100%. For the Conformer models in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5\" title=\"Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>, the flip rate is closer among genders.\nWe propose potential reasons for these variations, including differences in models&#8217; initial gender translation capabilities and possible masculine default bias in ST models&#8217; internal language models. But these remain conjectures that require empirical verification. Such investigation of how different ST models translate gender falls outside the scope of this work, which focuses on developing and evaluating contrastive explanations rather than reaching conclusions about gender translation mechanisms in ST models.</p>\n\n",
                "matched_terms": [
                    "language",
                    "models",
                    "different",
                    "conformer",
                    "contrastive",
                    "explanations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, our contrastive study is constrained by <span class=\"ltx_text ltx_font_bold\">the binary (masculine/feminine) gender</span> annotations in our dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Bentivogli et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib8\" title=\"\">2020</a>)</cite>. While this binary framework reflects the grammatical conventions of our target languages, from a social perspective, it fails to acknowledge individuals whose gender identity exists outside the male/female binary <cite class=\"ltx_cite ltx_citemacro_cite\">Zimman (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib69\" title=\"\">2020</a>)</cite>. Although we analyze grammatical gender, we acknowledge its interaction with gender identity and recognize that enforcing binary translations can erase non-binary identities. While emerging linguistic innovations like neologisms, neopronouns, and new morphological forms offer alternatives <cite class=\"ltx_cite ltx_citemacro_cite\">Piergentili et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib43\" title=\"\">2023</a>)</cite>, to the best of our knowledge, ST datasets incorporating these forms are not yet available. Once such data becomes available, our methodology could be used to analyze how systems choose between binary terms and neutral alternatives. Furthermore, extending this analysis to languages with diverse gender systems could provide valuable insights into how ST models handle varying degrees of grammatical gender complexity <cite class=\"ltx_cite ltx_citemacro_cite\">Corbett (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib13\" title=\"\">1991</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "between",
                    "contrastive",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The ST Transformer encoder-decoder model by <cite class=\"ltx_cite ltx_citemacro_cite\">Wang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib62\" title=\"\">2020a</a>)</cite> is a multilingual model trained on the 8 language directions of MuST-C <cite class=\"ltx_cite ltx_citemacro_citep\">(Cattoni et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib10\" title=\"\">2021</a>)</cite> with 72M of parameters, distributed under the MIT License. The model takes as input 80 Mel-filterbank audio features extracted every 10 milliseconds, employing a sample window of 25. The input features are then preprocessed with two 1D convolutional layers with stride 2, reducing input length by a factor of 4, before being fed to the Transformer encoder. We choose this model for our experiments both for its permissive license and its strong gender translation accuracy (see Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A2.T1\" title=\"Table 1 &#8227; Appendix B Model Details &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).\nFor inference, we used a beam size of 5 and a no-repeat-ngram-size of 5.\nThe maximum source position was set to 7,000.</p>\n\n",
                "matched_terms": [
                    "language",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We run our experiments on English-to-Spanish/French/Italian translations, specifically examining gender terms that refer to the speaker (category 1 in MuST-SHE), as these instances may benefit from acoustic cues for gender disambiguation. The dataset provides pairs of correct and incorrect gender translations for each term (e.g., &#8216;curiosa&#8217;/&#8216;curioso&#8217;), which we use in our contrastive analysis. Using the annotations from <cite class=\"ltx_cite ltx_citemacro_citet\">Savoldi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib51\" title=\"\">2022</a>)</cite>, we exclude gender articles from our analysis due to their high frequency in both genders across sentences, which makes it challenging to reliably identify specific instances referring to the speaker and could introduce noise into the evaluation.</p>\n\n",
                "matched_terms": [
                    "contrastive",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our analysis encompasses only those gender terms for which the model generates one of the forms annotated in MuST-SHE (either correct or incorrect version), as these are the only instances where we have access to the contrastive alternative. For the Transformer model on English-to-French translation, for which we present experimental results in the main paper, this filtering process yields a final set of 975 terms for analysis.</p>\n\n",
                "matched_terms": [
                    "contrastive",
                    "transformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, to make sure our contrastive explanation method works for different models and language pairs, we extend our study to the English-Italian and English-Spanish sections of the data (&#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.SS1\" title=\"E.1 Results on Additional Language Pairs &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">E.1</span></a>) and to two other types of models. In &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.SS2\" title=\"E.2 Results on Conformer Models &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">E.2</span></a>, we analyze the monolingual Conformer encoder-Transformer decoder models by <cite class=\"ltx_cite ltx_citemacro_citet\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib40\" title=\"\">2024</a>)</cite>. Similarly to the Transformer model used in the main body of the paper, this model is trained on MuST-C. In &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.SS3\" title=\"E.3 Results on Large-scale Model &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">E.3</span></a>, to analyze a larger-scale system, we study a Conformer encoder-Transformer decoder model trained on the English-Spanish data of the constrained track of the last IWSLT campaign <cite class=\"ltx_cite ltx_citemacro_citep\">(Ahmad et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib2\" title=\"\">2024</a>)</cite>.\nWhile our perturbation-based method for contrastive explanations is inherently model-agnostic and applicable to any architecture, we empirically validate that the explanations generated across these diverse model configurations continue to satisfy our quantitative faithfulness metrics.</p>\n\n",
                "matched_terms": [
                    "language",
                    "pairs",
                    "models",
                    "transformer",
                    "different",
                    "conformer",
                    "contrastive",
                    "explanations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the multilingual Transformer model, we extend our analysis to English-Italian and English-Spanish translations to verify whether the patterns observed for English-French generalize across language pairs. As shown in\nFigure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.F6\" title=\"Figure 6 &#8227; E.1 Results on Additional Language Pairs &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>,\nthe relative scorer maintains substantially higher coverage than the difference scorer across\nthe two languages, confirming the findings reported in the main paper for English-French.\nWhen deleting 10% of the features, coverage remains above 20% for Italian and Spanish, matching the robustness observed for French translations.\n</p>\n\n",
                "matched_terms": [
                    "language",
                    "difference",
                    "pairs",
                    "relative",
                    "transformer",
                    "scorer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The gender-flipping behavior is also consistent across languages, as shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.F7\" title=\"Figure 7 &#8227; E.1 Results on Additional Language Pairs &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>. For feminine predictions,\ndeleting just 5% of the most relevant features identified by our method causes the model to switch to masculine translations in over 60% of the covered cases\nfor both Italian and Spanish, as observed for French in &#167;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#S4\" title=\"4 Use Case &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nThe asymmetry between feminine and masculine predictions persists as well, with masculine-to-feminine flip rates not going much higher than 40% across all language pairs. This consistent pattern across three Romance languages with similar grammatical gender systems suggests that our earlier hypothesis about a masculine default could plausibly explain the model&#8217;s behavior in these three cases.</p>\n\n",
                "matched_terms": [
                    "between",
                    "language",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we report the results obtained for the three language pairs en-es/fr/it using the monolingual Conformer encoder-Transformer decoder models by <cite class=\"ltx_cite ltx_citemacro_citet\">Papi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib40\" title=\"\">2024</a>)</cite>, released under the Apache 2.0 License. Such models feature a higher translation quality than the Transformer-based model we reported on above, but has a lower gender accuracy, as reported in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A2.T1\" title=\"Table 1 &#8227; Appendix B Model Details &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "pairs",
                    "obtained",
                    "models",
                    "conformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The coverage patterns observed for the Transformer model extend to the monolingual Conformer models. As shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.F8\" title=\"Figure 8 &#8227; E.2 Results on Conformer Models &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, the relative scorer maintains consistently higher coverage than the difference scorer, although with lower absolute values compared to the Transformer model. At 10% deletion, coverage with the relative scorer remains above 30% for all language pairs, indicating that our method still effectively isolates gender-relevant features despite the architectural differences.</p>\n\n",
                "matched_terms": [
                    "language",
                    "difference",
                    "pairs",
                    "relative",
                    "models",
                    "transformer",
                    "conformer",
                    "scorer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The gender-flipping behavior shows interesting variations from the Transformer results. As seen in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.F9\" title=\"Figure 9 &#8227; E.2 Results on Conformer Models &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, while the relative scorer still triggers gender switches, the effect is more balanced between feminine and masculine predictions - both achieving flip rates between 30-60% when 10% of features are deleted. This differs from the Transformer&#8217;s strong asymmetry between genders. This more\nbalanced gender-flipping\neffect aligns with the Conformer models&#8217; general gender translation patterns (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A2.T1\" title=\"Table 1 &#8227; Appendix B Model Details &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Indeed, these models show substantially lower gender accuracy, particularly for feminine terms referring to the speaker, where performance approaches chance level. The reduced impact of feature deletion on gender prediction suggests these models\nmay rely not only on the speech input but also on other factors, such as previously generated tokens or\npotential biases embedded in the decoder&#8217;s &#8220;internal language model&#8221;\n<cite class=\"ltx_cite ltx_citemacro_cite\">Fucci et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib18\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "language",
                    "relative",
                    "models",
                    "transformer",
                    "conformer",
                    "between",
                    "scorer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further confirm the robustness of our findings, we investigate whether they also hold for larger-scale models trained on different data from only MuST-C. To this aim, we train a model composed of a 12-layer Conformer encoder and 6-layer Transformer decoder. Besides MuST-C, the training data includes EuroParl-ST <cite class=\"ltx_cite ltx_citemacro_citep\">(Iranzo-S&#225;nchez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib23\" title=\"\">2020</a>)</cite>, CoVoST v2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib63\" title=\"\">2020b</a>)</cite>, and the ASR datasets CommonVoice <cite class=\"ltx_cite ltx_citemacro_citep\">(Ardila et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib5\" title=\"\">2020</a>)</cite>, LibriSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib39\" title=\"\">2015</a>)</cite>, TEDLIUM v3 <cite class=\"ltx_cite ltx_citemacro_citep\">(Hernandez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib21\" title=\"\">2018</a>)</cite>, and VoxPopuli <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib61\" title=\"\">2021</a>)</cite>, whose transcripts we automatically translate into Spanish using the NeMo\nMT models.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>Publicly available at: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/machine_translation/machine_translation.html\" title=\"\">https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/machine_translation/machine_translation.html</a>.</span></span></span> The model is trained with a composite loss function that comprises a label-smoothed cross entropy on the decoder output, with the translation as target, and two auxiliary CTC losses, respectively, on the encoder output, with the translation as target and, on the 8th encoder layer, with the transcript as target <cite class=\"ltx_cite ltx_citemacro_citep\">(Yan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib66\" title=\"\">2023</a>)</cite>. The training is performed using the Noam scheduler with 2e-3 as peak learning rate and is stopped after 200,000 updates. Utterance-level Cepstral Mean and Variance Normalization (CMVN) and SpecAugment <cite class=\"ltx_cite ltx_citemacro_cite\">Park et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib41\" title=\"\">2019</a>)</cite> are applied during training and segments longer than 30 seconds are filtered out (fairseq-ST default) to avoid excessive VRAM requirements. The model is the average of the last 7 checkpoints obtained from the training and has 133M parameters. The training is run on 4 NVIDIA Ampere GPU A100 (64GB VRAM) with mini-batches of 40,000 input elements and 2 as update frequency.</p>\n\n",
                "matched_terms": [
                    "obtained",
                    "models",
                    "transformer",
                    "different",
                    "conformer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This large-scale model exhibits similar patterns to the smaller Conformer models. As shown in Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.F10\" title=\"Figure 10 &#8227; E.3 Results on Large-scale Model &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A5.F11\" title=\"Figure 11 &#8227; E.3 Results on Large-scale Model &#8227; Appendix E Extended Results Across Models and Languages &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, the relative scorer maintains higher coverage than the difference scorer. The flip rate displays a reduced asymmetry between feminine and masculine predictions, with the first reaching around 50% after 10% feature deletion, and the second, a little over 40%. This behavior is aligned with that of the monolignual small Conformer models.</p>\n\n",
                "matched_terms": [
                    "difference",
                    "largescale",
                    "relative",
                    "models",
                    "conformer",
                    "between",
                    "scorer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents an empirical comparison, for our XAI application, of different methods for computing word-level probabilities from the sequences of subword token probabilities output by ST models. Previous work has employed various approaches to address this issue. Some studies have avoided the problem entirely by limiting their analysis to targets and foils tokenized as single tokens <cite class=\"ltx_cite ltx_citemacro_cite\">Yin and Neubig (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib67\" title=\"\">2022</a>)</cite>. Others have worked at the subword level without aggregation <cite class=\"ltx_cite ltx_citemacro_cite\">Ferrando et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib15\" title=\"\">2022</a>)</cite>, or employed the chain rule to multiply individual token probabilities <cite class=\"ltx_cite ltx_citemacro_cite\">Sarti et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib50\" title=\"\">2023</a>)</cite>. Here, we leverage our evaluation metrics for explanation faithfulness&#8212;coverage and flip rate&#8212;to compare three methods and determine if any of them yields more reliable contrastive explanations when integrated into our methodology.</p>\n\n",
                "matched_terms": [
                    "contrastive",
                    "models",
                    "explanations",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the log domain, this corresponds to computing the average log probability over the sequence of subwords, effectively normalizing by the sequence length. This normalization, which is typically used in beam search, attempts to create a fairer comparison between words of different lengths <cite class=\"ltx_cite ltx_citemacro_cite\">Meister et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#bib.bib34\" title=\"\">2020</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "between",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate the performance of each method, we recorded coverage and flip rate measurements at progressive deletion steps, as shown in Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.F12\" title=\"Figure 12 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.F13\" title=\"Figure 13 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>. To quantitatively assess differences between methods, we conducted paired t-tests comparing coverage and flip rate values. Each pair of methods was evaluated using the sequence of measurements at corresponding deletion percentages as dependent samples, allowing us to determine whether observed differences were statistically significant. Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.T2.fig1\" title=\"Table 2 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.T3.fig1\" title=\"Table 3 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> present the resulting p-values for each comparison across language pairs.</p>\n\n",
                "matched_terms": [
                    "between",
                    "language",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These comparisons reveal that all three methods perform similarly in their ability to identify gender-relevant features. While coverage differences are often statistically significant (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.T2.fig1\" title=\"Table 2 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), the absolute differences are relatively small as can be seen in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.F12\" title=\"Figure 12 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>, with all methods maintaining sufficient coverage levels to ensure reliable flip rate calculations. The flip rate analysis&#8212;our primary metric for evaluating how precisely methods isolate features responsible for gender prediction&#8212;shows no statistically significant differences (<math alttext=\"p&gt;0.05\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p6.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&gt;</mo><mn>0.05</mn></mrow><annotation encoding=\"application/x-tex\">p&gt;0.05</annotation></semantics></math>) between Word Boundary and Chain Rule methods across all language pairs (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.T3.fig1\" title=\"Table 3 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). As we see in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.F13\" title=\"Figure 13 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, when over 10% of the highlighted features are occluded, all methods achieve comparable flip rates between 50-60% for most language pairs. The only notable exception is the Length Normalization method, which performs significantly worse for French in terms of flip rate (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.26543v1#A6.T3.fig1\" title=\"Table 3 &#8227; Appendix F Comparison of Subword-to-Word Probability Aggregation Techniques &#8227; The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"A6.p6.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>).\nWhile the reasons for this language-specific underperformance warrant further investigation, the observation supports our decision to exclude the Length Normalization method from our main experiments.\n</p>\n\n",
                "matched_terms": [
                    "between",
                    "language",
                    "pairs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the Chain Rule and Word Boundary methods yield comparable performance, we looked at individual examples in which the ranking of the target and foil is different with the two methods to deepen our comparison. Namely, we found a few examples in which the Chain Rule method assigns a higher probability to the foil than to the target, even though the beam search selects the target word in the end.\nFor instance, in the sentence &#8220;In one, I was the classic Asian <span class=\"ltx_text ltx_framed ltx_framed_underline\">student</span>, relentless in the demands that I made on myself&#8221;, translated into Italian as &#8220;In uno, ero la classica <span class=\"ltx_text ltx_framed ltx_framed_underline\">studentessa</span> asiatica, incantata nelle richieste di me stessa&#8221;, the feminine form <span class=\"ltx_text ltx_font_italic\">studentessa</span>&#8212;tokenized as <span class=\"ltx_text ltx_font_typewriter\">_studente ssa</span>&#8212;has a joint probability of 0.899, while the masculine form <span class=\"ltx_text ltx_font_typewriter\">_studente</span> scores 0.908.\nHowever, when applying the Word Boundary method, the unchosen masculine form receives a much lower probability (0.004), reflecting a more accurate estimate. This method&#8217;s principled treatment of complete words versus prefixes makes it especially suited for analyzing gendered terms, where precise probability computation is critical. However, the number of cases in which this happens is fairly limited (less than 10), which explains why there are no significant differences between the scores of the two methods when aggregating over the whole dataset. Nonetheless, this example demonstrates that in the few cases in which they differ, the Word Boundary method provides the most correct probability estimation.</p>\n\n",
                "matched_terms": [
                    "between",
                    "different"
                ]
            }
        ]
    }
}