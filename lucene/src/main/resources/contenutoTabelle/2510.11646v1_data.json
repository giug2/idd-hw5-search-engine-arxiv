{
    "S3.T1": {
        "source_file": "BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis",
        "caption": "Table 1: Comparison Experiments on LibriTTS Development and Test Set. SMOS and QMOS scores are reported with 95% confidence intervals. The best results are shown in bold, and the second-best are underlined.",
        "body": "Model\nToken Rate (↓)\\left(\\downarrow\\right)\nWER (↓)\\left(\\downarrow\\right)\nSMOS (↑)\\left(\\uparrow\\right)\nQMOS (↑)\\left(\\uparrow\\right)\nUTMOS (↑)\\left(\\uparrow\\right)\n\n\nLibriTTS Development Set\n\n\nGT\n/\n2.3%\n\n4.41 ±\\pm 0.11\n\n\n4.41 ±\\pm 0.13\n\n4.258\n\n\n\nUniAudio [26]\n\n50Hz\n11.4%\n\n3.81 ±\\pm 0.12\n\n\n3.92 ±\\pm 0.09\n\n3.676\n\n\n\nGPT-Talker [17]\n\n50Hz\n5.9%\n\n3.78 ±\\pm 0.11\n\n\n3.96 ±\\pm 0.12\n\n3.693\n\n\n\nCosyVoice [7]\n\n25Hz\n6.8%\n4.13 ±\\pm 0.12\n4.36 ±\\pm 0.12\n4.253\n\n\nBridgeTTS (Ours)\n10Hz\n3.4%\n4.07 ±\\pm 0.11\n4.15 ±\\pm 0.09\n4.050\n\n\nLibriTTS Test Set\n\n\nGT\n/\n3.1%\n\n4.33 ±\\pm 0.11\n\n\n4.32 ±\\pm 0.09\n\n4.275\n\n\n\nVALL-E [20]\n\n50Hz\n18.5%\n\n3.64 ±\\pm 0.12\n\n\n3.49 ±\\pm 0.11\n\n2.728\n\n\n\nUniAudio [26]\n\n50Hz\n12.9%\n\n3.62 ±\\pm 0.12\n\n\n3.83 ±\\pm 0.15\n\n3.663\n\n\n\nGPT-Talker [17]\n\n50Hz\n16.4%\n\n3.78 ±\\pm 0.12\n\n\n3.84 ±\\pm 0.09\n\n3.566\n\n\n\nCosyVoice [7]\n\n25Hz\n8.0%\n4.12 ±\\pm 0.08\n4.29 ±\\pm 0.11\n4.148\n\n\nBridgeTTS (Ours)\n10Hz\n4.9%\n4.01 ±\\pm 0.12\n4.11 ±\\pm 0.13\n3.894",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Token Rate <math alttext=\"\\left(\\downarrow\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><mrow><mo>(</mo><mo lspace=\"0em\" rspace=\"0em\" stretchy=\"false\">&#8595;</mo><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">\\left(\\downarrow\\right)</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WER <math alttext=\"\\left(\\downarrow\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m2\" intent=\":literal\"><semantics><mrow><mo>(</mo><mo lspace=\"0em\" rspace=\"0em\" stretchy=\"false\">&#8595;</mo><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">\\left(\\downarrow\\right)</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SMOS <math alttext=\"\\left(\\uparrow\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m3\" intent=\":literal\"><semantics><mrow><mo>(</mo><mo lspace=\"0em\" rspace=\"0em\" stretchy=\"false\">&#8593;</mo><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">\\left(\\uparrow\\right)</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">QMOS <math alttext=\"\\left(\\uparrow\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m4\" intent=\":literal\"><semantics><mrow><mo>(</mo><mo lspace=\"0em\" rspace=\"0em\" stretchy=\"false\">&#8593;</mo><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">\\left(\\uparrow\\right)</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">UTMOS <math alttext=\"\\left(\\uparrow\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m5\" intent=\":literal\"><semantics><mrow><mo>(</mo><mo lspace=\"0em\" rspace=\"0em\" stretchy=\"false\">&#8593;</mo><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">\\left(\\uparrow\\right)</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"6\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LibriTTS Development Set</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">GT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">/</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.3%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">4.41 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.11</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">4.41 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.13</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.258</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">UniAudio </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib26\" title=\"\">26</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">50Hz</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">11.4%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.81 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.12</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.92 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.09</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.676</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">GPT-Talker </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib17\" title=\"\">17</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">50Hz</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">5.9%</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.78 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m10\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.11</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.96 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m11\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.12</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.693</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CosyVoice </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib7\" title=\"\">7</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">25Hz</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.8%</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.13 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m12\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.12</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.36 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m13\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.12</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.253</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">BridgeTTS (Ours)</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">10Hz</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.4%</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">4.07 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m14\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.11</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">4.15 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m15\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.09</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">4.050</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"6\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LibriTTS Test Set</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">GT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">/</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.1%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">4.33 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m16\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.11</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">4.32 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m17\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.09</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.275</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">VALL-E </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib20\" title=\"\">20</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">50Hz</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">18.5%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.64 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m18\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.12</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.49 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m19\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.11</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.728</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">UniAudio </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib26\" title=\"\">26</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">50Hz</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.9%</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.62 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m20\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.12</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.83 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m21\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.15</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.663</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">GPT-Talker </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib17\" title=\"\">17</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">50Hz</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">16.4%</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.78 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m22\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.12</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.84 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m23\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.09</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.566</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CosyVoice </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib7\" title=\"\">7</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">25Hz</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">8.0%</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.12 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m24\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.08</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.29 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m25\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.11</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.148</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">BridgeTTS (Ours)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">10Hz</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.9%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">4.01 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m26\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.12</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">4.11 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m27\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.13</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" style=\"font-size:90%;\">3.894</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "libritts",
            "uniaudio",
            "wer",
            "secondbest",
            "underlined",
            "reported",
            "development",
            "experiments",
            "±pm",
            "gpttalker",
            "utmos",
            "cosyvoice",
            "intervals",
            "↓leftdownarrowright",
            "token",
            "confidence",
            "best",
            "rate",
            "10hz",
            "results",
            "50hz",
            "↑leftuparrowright",
            "bold",
            "scores",
            "valle",
            "smos",
            "comparison",
            "test",
            "ours",
            "set",
            "bridgetts",
            "25hz",
            "model",
            "qmos"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We compare our BridgeTTS with state-of-the-art (SOTA) methods on the LibriSpeech dataset. We choose four SOTA methods as our baseline, including VALL-E </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, UnionAudio </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a modified version of GPT-Talker </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> adapted for zero-shot synthesis, and CosyVoice </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For fair comparison, all models are further trained on LibriSpeech using their original pre-trained weights as initialization. The results are presented in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.3 Comparison with Existing Methods &#8227; 3 Experiments &#8227; BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where GT denotes ground-truth speech.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Autoregressive (AR) frameworks have recently achieved remarkable progress in zero-shot text-to-speech (TTS) by leveraging discrete speech tokens and large language model techniques. Despite their success, existing AR-based zero-shot TTS systems face two critical limitations: (i) an inherent speed&#8211;quality trade-off, as sequential token generation either reduces frame rates at the cost of expressiveness or enriches tokens at the cost of efficiency, and (ii) a text-oriented supervision mismatch, as cross-entropy loss penalizes token errors uniformly without considering the fine-grained acoustic similarity among adjacent tokens. To address these challenges, we propose <span class=\"ltx_text ltx_font_bold\">BridgeTTS</span>, a novel AR-TTS framework built upon the dual speech representation paradigm <span class=\"ltx_text ltx_font_bold\">BridgeCode</span>. BridgeTTS reduces AR iterations by predicting sparse tokens while reconstructing rich continuous features for high-quality synthesis. Joint optimization of token-level and feature-level objectives further enhances naturalness and intelligibility. Experiments demonstrate that BridgeTTS achieves competitive quality and speaker similarity while significantly accelerating synthesis. Speech demos are available at https://test1562.github.io/demo/.</span>\n</p>\n\n",
                "matched_terms": [
                    "experiments",
                    "bridgetts",
                    "model",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">However, conventional AR zero-shot TTS systems still face two issues. First, the AR model generates discrete speech tokens sequentially, while a separate model decodes them into acoustic features and further synthesizes speech. This paradigm necessitates that the AR model iteratively processes token sequences with high generation rates, creating a computational bottleneck for real-time deployment. As illustrated in Fig.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">(A), existing methods either reduce token rates (bottom of Fig.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">(A)), sacrificing the expressiveness of generated speech </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib15\" title=\"\">15</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, or enrich tokens with additional information (top of Fig.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">(A)), reducing the token generation efficiency </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This leads to </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">issue (i): an inherent token rate-quality trade-off</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> between the token generation rate of AR and the quality of the synthesized speech. Second, existing AR zero-shot TTS methods </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> typically adopt training paradigms directly inherited from large language models, employing cross-entropy loss that solely focuses on token prediction accuracy. While this approach is well-suited for text token prediction, it is suboptimal for speech token generation. Adjacent speech tokens in the acoustic space often differ only in subtle prosodic or timbral variations, yet cross-entropy loss applies a uniform penalty regardless of token proximity to ground truth. This fails to provide the fine-grained, hierarchical supervision necessary for high-quality speech synthesis, leading to </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">issue (ii): text-oriented supervision mismatch</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "rate",
                    "model",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address issue (i), we propose </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">BridgeTTS</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a novel AR-TTS framework that reduces AR iterations while maintaining synthesis quality under zero-shot scenarios. BridgeTTS incorporates a core component, </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">BridgeCode</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which encompasses dual speech representations: sparse tokens and dense continuous features, along with two bridging modules for bidirectional conversion between them. As illustrated in Fig. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">(B), BridgeTTS enables the AR model to generate sparse tokens for efficiency, while the bridging module reconstructs detailed continuous features to ensure high-quality speech synthesis, thereby reducing prediction steps without compromising synthesis quality. Furthermore, to address issue (ii), BridgeTTS refines the training paradigm by jointly optimizing token-level and feature-level objectives, providing fine-grained, hierarchical supervision for speech token prediction, which is essential for generating speech with enhanced naturalness and intelligibility.</span>\n</p>\n\n",
                "matched_terms": [
                    "bridgetts",
                    "model",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">3) Experimental results demonstrate that BridgeTTS attains the lowest AR token rate among existing methods while achieving competitive naturalness and speaker similarity, and effectively mitigating error accumulation while accelerating synthesis speed.</span>\n</p>\n\n",
                "matched_terms": [
                    "results",
                    "rate",
                    "bridgetts",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To establish the dual speech representations required by BridgeTTS, we propose BridgeCode, a dual speech representation paradigm that incorporates sparse tokens and dense continuous features with bridging modules between them. As illustrated in Fig.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, dense continuous features are extracted by the frozen feature encoder from GPT-Talker </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, while sparse tokens are obtained by compressing dense continuous features through the proposed SparseBridge. SparseBridge and DenseBridge are two symmetrical bridging networks that perform bidirectional conversion between these two representations. Moreover, to achieve fine-grained sparse-to-dense alignment, we draw inspiration from VDVAE </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and enforce layer-wise alignment between intermediate features in both bridging modules (Fig.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), ensuring high-fidelity bidirectional conversion between sparse tokens and dense continuous features. The network architecture and training objectives of SparseBridge and DenseBridge are detailed below.</span>\n</p>\n\n",
                "matched_terms": [
                    "bridgetts",
                    "gpttalker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SparseBridge Architecture.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> SparseBridge compresses dense continuous features into sparse tokens while preserving essential information. Taking continuous speech features </span>\n  <math alttext=\"F_{0}\\in\\mathbb{R}^{T\\times 768}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">F</mi>\n          <mn mathsize=\"0.900em\">0</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mn mathsize=\"0.900em\">768</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">F_{0}\\in\\mathbb{R}^{T\\times 768}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as input, the contextual feature extractor employs multi-scale convolutional layers with kernel sizes of </span>\n  <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">1</mn>\n      <annotation encoding=\"application/x-tex\">1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">3</mn>\n      <annotation encoding=\"application/x-tex\">3</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">5</mn>\n      <annotation encoding=\"application/x-tex\">5</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to capture contextual dependencies across varying temporal spans. The extracted multi-scale features are then concatenated and denoted as </span>\n  <math alttext=\"F_{1}\\in\\mathbb{R}^{T\\times 2304}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">F</mi>\n          <mn mathsize=\"0.900em\">1</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mn mathsize=\"0.900em\">2304</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">F_{1}\\in\\mathbb{R}^{T\\times 2304}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. To obtain sparse tokens, </span>\n  <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m6\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">1</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{1}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> undergoes temporal downsampling to reduce the frame rate by a factor of </span>\n  <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m7\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">5</mn>\n      <annotation encoding=\"application/x-tex\">5</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, yielding </span>\n  <math alttext=\"F_{2}\\in\\mathbb{R}^{T/5\\times 2304}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m8\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">F</mi>\n          <mn mathsize=\"0.900em\">2</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mrow>\n              <mi mathsize=\"0.900em\">T</mi>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\" symmetric=\"true\">/</mo>\n              <mn mathsize=\"0.900em\">5</mn>\n            </mrow>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mn mathsize=\"0.900em\">2304</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">F_{2}\\in\\mathbb{R}^{T/5\\times 2304}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which is then processed by hierarchical residual vector quantization (RVQ) </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> that iteratively compresses </span>\n  <math alttext=\"F_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m9\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">2</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> into discrete codes. The Hierarchical RVQ Encoder processes the </span>\n  <math alttext=\"2304\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m10\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">2304</mn>\n      <annotation encoding=\"application/x-tex\">2304</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-dimensional vector by splitting it into three </span>\n  <math alttext=\"768\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m11\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">768</mn>\n      <annotation encoding=\"application/x-tex\">768</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-dimensional vectors, each quantized by a </span>\n  <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m12\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">3</mn>\n      <annotation encoding=\"application/x-tex\">3</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-level RVQ, resulting in a </span>\n  <math alttext=\"3\\times 3\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m13\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">3</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <mn mathsize=\"0.900em\">3</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">3\\times 3</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> code matrix. Since VALL-E </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> has demonstrated that only the first RVQ indices are crucial while other indices can be discarded without significant information loss, a code selector retains solely the first RVQ indices, producing a sparse token sequence that preserves essential speech information while achieving substantial compression.</span>\n</p>\n\n",
                "matched_terms": [
                    "rate",
                    "valle",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Based on the proposed BridgeCode, dual speech representations can be obtained for any given speech, with bidirectional conversion facilitated by the trained bridging modules. In this section, we further present BridgeTTS. As illustrated in Fig.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#S2.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 2.1 BridgeCode &#8227; 2 Method &#8227; BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the autoregressive generator in BridgeTTS is a retrained GPT-2-based </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> model. BridgeTTS allows the AR model to generate sparse tokens at a reduced frame rate to minimize iteration steps, while information-rich dense continuous features are reconstructed by the frozen DenseBridge for high-quality speech synthesis. This approach effectively addresses the aforementioned inherent speed-quality trade-off issue in AR-TTS. Moreover, to address the text-oriented supervision mismatch issue, BridgeTTS refines the training paradigm by jointly optimizing token-level and feature-level objectives, providing fine-grained supervision essential for high-quality speech synthesis.</span>\n</p>\n\n",
                "matched_terms": [
                    "rate",
                    "bridgetts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Notably, unlike traditional autoregressive models that sequentially input one token to predict the next discrete token, BridgeTTS allows the AR model to input five consecutive speech feature frames at each step to predict the next token. This enables the AR model to make more informed predictions based on richer contextual information. Meanwhile, the AR model can observe the continuous features directly used for speech generation when predicting the next token, allowing it to adjust output tokens to control the synthesis of subsequent continuous features. Consequently, BridgeTTS enables the AR model to achieve better control over the naturalness and intelligibility of synthesized speech during both training and inference.\nThe respective processes for training and inference are depicted in Fig.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#S2.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 2.1 BridgeCode &#8227; 2 Method &#8227; BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (A) and (B).</span>\n</p>\n\n",
                "matched_terms": [
                    "bridgetts",
                    "model",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">P</mi>\n      <annotation encoding=\"application/x-tex\">P</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the model&#8217;s predicted probability distribution, </span>\n  <math alttext=\"e_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">e</mi>\n        <mi mathsize=\"0.900em\">t</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">e_{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the ground truth token at time step </span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <math alttext=\"\\mathbf{f}_{&lt;t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119839;</mi>\n        <mrow>\n          <mi/>\n          <mo mathsize=\"0.900em\">&lt;</mo>\n          <mi mathsize=\"0.900em\">t</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{f}_{&lt;t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> represents all the previous features up to time step </span>\n  <math alttext=\"t-1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mo mathsize=\"0.900em\">&#8722;</mo>\n        <mn mathsize=\"0.900em\">1</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">t-1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <math alttext=\"\\mathbf{t}_{\\text{ref}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m7\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119853;</mi>\n        <mtext mathsize=\"0.900em\">ref</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{t}_{\\text{ref}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\mathbf{t}_{\\text{target}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m8\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119853;</mi>\n        <mtext mathsize=\"0.900em\">target</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{t}_{\\text{target}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are the reference and target text representations, respectively. In the BridgeTTS framework, each token prediction is conditioned on the previous features, reference text, and target text, while previous tokens do not directly influence the current prediction.</span>\n</p>\n\n",
                "matched_terms": [
                    "bridgetts",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">However, token loss computes prediction accuracy through cross-entropy loss, treating any mismatch between predicted and ground truth tokens as equally incorrect, regardless of their acoustic similarity. This poses a challenge in speech synthesis, as acoustically similar tokens may generate speech differing only in subtle acoustic details, such as prosodic variations and phonetic articulation. Consequently, a model predicting tokens closely resembling the ground truth receives the same penalty as one predicting acoustically distant tokens. To address this limitation, we introduce feature loss that computes the MSE between predicted and ground truth features, providing fine-grained, hierarchical supervision for the AR model. The feature loss is given by:</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">During inference, the AR model is conditioned on reference speech features and text input tokens, where the latter are derived by concatenating the reference transcript with the target text and encoding the result using a text encoder. At each autoregressive prediction step, the AR generator first predicts the next discrete token based on the accumulated speech features and text representations. The predicted token is then transformed into continuous speech features via the pre-trained DenseBridge. These continuous features are concatenated with the existing speech feature sequence and fed back into the AR generator for subsequent token prediction. This iterative process continues until an End-of-Speech (EOS) token is generated or the target sequence length is reached. Finally, the complete generated continuous speech features are synthesized into audio using the speech feature vocoder fine-tuned during BridgeCode training.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We conduct all experiments on the LibriTTS dataset </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with a sampling rate of 16 kHz. LibriTTS is a large-scale, multi-speaker English corpus comprising 585 hours of speech from over 2,300 speakers. For training, we combine the subsets train-clean-100, train-clean-360, and train-other-500. The development set is constructed by merging dev-clean and dev-other, while the test set consists of test-clean and test-other.</span>\n</p>\n\n",
                "matched_terms": [
                    "libritts",
                    "rate",
                    "set",
                    "development",
                    "experiments",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Implementation Details.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> For BridgeCode training, we first download the pre-trained weights for wav2vec 2.0 Base </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://github.com/eastonYi/wav2vec</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The speech feature encoder is kept frozen, while two bridging modules are trained on the LibriTTS training set for </span>\n  <math alttext=\"700k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">700</mn>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathsize=\"0.900em\">k</mi>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">700k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> steps on an NVIDIA A800 GPU with a batch size of 16. We employ the AdamW optimizer with an initial learning rate of </span>\n  <math alttext=\"1.0\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1.0</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">4</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1.0\\times 10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, decayed by a factor of </span>\n  <math alttext=\"0.999^{1/8}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mn mathsize=\"0.900em\">0.999</mn>\n        <mrow>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\" symmetric=\"true\">/</mo>\n          <mn mathsize=\"0.900em\">8</mn>\n        </mrow>\n      </msup>\n      <annotation encoding=\"application/x-tex\">0.999^{1/8}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> per epoch. After training two bridging modules, they are frozen, and the autoregressive generator is subsequently trained for </span>\n  <math alttext=\"600k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">600</mn>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathsize=\"0.900em\">k</mi>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">600k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> steps under the same conditions.</span>\n</p>\n\n",
                "matched_terms": [
                    "libritts",
                    "rate",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Subjective Evaluation Setup.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We conducted a subjective evaluation with 20 human raters, who were first trained on anonymized speech samples to familiarize them with the criteria for assessing speaker similarity and speech quality. Each rater scored randomly selected samples on a 5-point scale for both naturalness and similarity. Specifically, we performed a Speaker Similarity Mean Opinion Score (SMOS) test to measure speaker resemblance in zero-shot TTS synthesis and a Quality Mean Opinion Score (QMOS) test to evaluate overall naturalness.</span>\n</p>\n\n",
                "matched_terms": [
                    "smos",
                    "test",
                    "qmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Objective Evaluation Setup.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> For objective evaluation, we measure UTMOS </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to assess overall naturalness and quality, and Word Error Rate (WER) using a Wav2Vec 2.0-large-based ASR model </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to evaluate word-level synthesis accuracy. To further assess model performance, we introduce an additional metric, Token rate, which measures the frequency at which each AR model outputs discrete speech tokens (i.e., tokens per second).</span>\n</p>\n\n",
                "matched_terms": [
                    "token",
                    "wer",
                    "rate",
                    "utmos",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As observed, BridgeTTS delivers competitive synthesis quality compared to four baseline methods across SMOS, QMOS, and UTMOS metrics, while achieving the lowest WER and Token Rate. Compared to GPT-Talker, which shares the same base model and training data, BridgeTTS improves speech naturalness and similarity while maintaining lower Token Rate and WER. This improvement stems from the proposed novel BridgeCode and unique AR paradigm, which enables the AR model to input multiple consecutive speech feature frames at each step for next-token prediction, facilitating more informed predictions based on richer contextual information. Additionally, DenseBridge losslessly converts sparse tokens from previous steps into dense continuous features for AR input in subsequent predictions, ensuring the stability of this AR paradigm.\nCompared to CosyVoice, which employs updated codec models, advanced AR architectures, and larger training datasets, BridgeTTS achieves competitive speech naturalness and similarity while maintaining a lower WER. This superior performance is attributed to the lower frame rate of sparse tokens in the proposed BridgeCode. For synthesizing speech of equivalent duration, BridgeTTS requires fewer AR iterations than CosyVoice, resulting in reduced error accumulation during inference and higher word-level synthesis accuracy.</span>\n</p>\n\n",
                "matched_terms": [
                    "smos",
                    "token",
                    "wer",
                    "rate",
                    "bridgetts",
                    "model",
                    "gpttalker",
                    "utmos",
                    "cosyvoice",
                    "qmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Ablation studies are conducted to assess the contributions of sequence compression and feature loss, as summarized in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#S3.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 3.3 Comparison with Existing Methods &#8227; 3 Experiments &#8227; BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. w/o BridgeCode denotes training the AR generator on compressed tokens alone, without employing BridgeCode, while w/o </span>\n  <math alttext=\"\\mathcal{L}_{\\text{features}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n        <mtext mathsize=\"0.900em\">features</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{features}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes training with Bridge but excluding the feature loss.\nThe results show that compressing token sequences without BridgeCode degrades quality, as the AR generator lacks sufficiently informative prompts for accurate synthesis. Similarly, removing the feature loss results in suboptimal performance, since this objective enforces attention to fine-grained acoustic and temporal characteristics beyond semantic content.</span>\n</p>\n\n",
                "matched_terms": [
                    "results",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To further demonstrate the acceleration effect of BridgeTTS, we compare a baseline AR generator with its DenseBridge-enhanced counterpart in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.3 Comparison with Existing Methods &#8227; 3 Experiments &#8227; BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The results confirm that BridgeTTS achieves faster synthesis with improved real-time factor (RTF) while preserving comparable quality.</span>\n</p>\n\n",
                "matched_terms": [
                    "results",
                    "bridgetts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we proposed BridgeTTS, which incorporates a novel BridgeCode and a unique AR paradigm. BridgeCode introduces dual speech representations together with two bridging modules, enabling the AR model to generate sparse tokens to minimize iteration steps, while reconstructing information-rich dense continuous features for high-quality speech synthesis. In addition, BridgeTTS refines the training paradigm of AR models by providing fine-grained supervision, mitigating the mismatch between text-based and speech-based large models.\nExperiments and ablation studies verify the effectiveness of BridgeTTS. Moreover, BridgeCode can be generalized to arbitrary AR-TTS models, demonstrating strong generalization and promising potential for future applications.</span>\n</p>\n\n",
                "matched_terms": [
                    "experiments",
                    "bridgetts",
                    "model"
                ]
            }
        ]
    },
    "S3.T2": {
        "source_file": "BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis",
        "caption": "Table 2: Ablation Study Results on LibriTTS Test Set.",
        "body": "Model\n\nToken Rate (↓)\\left(\\downarrow\\right)\nWER (↓)\\left(\\downarrow\\right)\nSMOS (↑)\\left(\\uparrow\\right)\nQMOS (↑)\\left(\\uparrow\\right)\nUTMOS (↑)\\left(\\uparrow\\right)\n\n\nBridgeTTS\n10Hz\n4.9%\n4.01 ±\\pm 0.12\n4.11 ±\\pm 0.13\n3.894\n\n\n\n\n-w/o DenseBridge\n10Hz\n13.8%\n\n3.74 ±\\pm 0.11\n\n\n3.74 ±\\pm 0.12\n\n3.443\n\n\n\n-w/o ℒfeatures\\mathcal{L}_{\\text{features}}\n\n10Hz\n7.1%\n\n3.92 ±\\pm 0.13\n\n\n3.96 ±\\pm 0.12\n\n3.471",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8202;</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;Model</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Token Rate <math alttext=\"\\left(\\downarrow\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m1\" intent=\":literal\"><semantics><mrow><mo>(</mo><mo lspace=\"0em\" rspace=\"0em\" stretchy=\"false\">&#8595;</mo><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">\\left(\\downarrow\\right)</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WER <math alttext=\"\\left(\\downarrow\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m2\" intent=\":literal\"><semantics><mrow><mo>(</mo><mo lspace=\"0em\" rspace=\"0em\" stretchy=\"false\">&#8595;</mo><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">\\left(\\downarrow\\right)</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SMOS <math alttext=\"\\left(\\uparrow\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m3\" intent=\":literal\"><semantics><mrow><mo>(</mo><mo lspace=\"0em\" rspace=\"0em\" stretchy=\"false\">&#8593;</mo><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">\\left(\\uparrow\\right)</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">QMOS <math alttext=\"\\left(\\uparrow\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m4\" intent=\":literal\"><semantics><mrow><mo>(</mo><mo lspace=\"0em\" rspace=\"0em\" stretchy=\"false\">&#8593;</mo><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">\\left(\\uparrow\\right)</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">UTMOS <math alttext=\"\\left(\\uparrow\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m5\" intent=\":literal\"><semantics><mrow><mo>(</mo><mo lspace=\"0em\" rspace=\"0em\" stretchy=\"false\">&#8593;</mo><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">\\left(\\uparrow\\right)</annotation></semantics></math></span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">BridgeTTS</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">10Hz</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.9%</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.01 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m6\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.12</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.11 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m7\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.13</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.894</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-w/o DenseBridge</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">10Hz</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.8%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.74 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.11</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.74 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.12</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.443</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">-w/o </span><math alttext=\"\\mathcal{L}_{\\text{features}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m10\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi><mtext mathsize=\"0.900em\">features</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{features}}</annotation></semantics></math>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">10Hz</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.1%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.92 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m11\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.13</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.96 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m12\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.12</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.471</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "libritts",
            "wer",
            "study",
            "±pm",
            "utmos",
            "↓leftdownarrowright",
            "token",
            "rate",
            "10hz",
            "results",
            "densebridge",
            "↑leftuparrowright",
            "smos",
            "ablation",
            "test",
            "ℒfeaturesmathcalltextfeatures",
            "set",
            "bridgetts",
            "model",
            "qmos"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Ablation studies are conducted to assess the contributions of sequence compression and feature loss, as summarized in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#S3.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 3.3 Comparison with Existing Methods &#8227; 3 Experiments &#8227; BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. w/o BridgeCode denotes training the AR generator on compressed tokens alone, without employing BridgeCode, while w/o </span>\n  <math alttext=\"\\mathcal{L}_{\\text{features}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n        <mtext mathsize=\"0.900em\">features</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{features}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes training with Bridge but excluding the feature loss.\nThe results show that compressing token sequences without BridgeCode degrades quality, as the AR generator lacks sufficiently informative prompts for accurate synthesis. Similarly, removing the feature loss results in suboptimal performance, since this objective enforces attention to fine-grained acoustic and temporal characteristics beyond semantic content.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Autoregressive (AR) frameworks have recently achieved remarkable progress in zero-shot text-to-speech (TTS) by leveraging discrete speech tokens and large language model techniques. Despite their success, existing AR-based zero-shot TTS systems face two critical limitations: (i) an inherent speed&#8211;quality trade-off, as sequential token generation either reduces frame rates at the cost of expressiveness or enriches tokens at the cost of efficiency, and (ii) a text-oriented supervision mismatch, as cross-entropy loss penalizes token errors uniformly without considering the fine-grained acoustic similarity among adjacent tokens. To address these challenges, we propose <span class=\"ltx_text ltx_font_bold\">BridgeTTS</span>, a novel AR-TTS framework built upon the dual speech representation paradigm <span class=\"ltx_text ltx_font_bold\">BridgeCode</span>. BridgeTTS reduces AR iterations by predicting sparse tokens while reconstructing rich continuous features for high-quality synthesis. Joint optimization of token-level and feature-level objectives further enhances naturalness and intelligibility. Experiments demonstrate that BridgeTTS achieves competitive quality and speaker similarity while significantly accelerating synthesis. Speech demos are available at https://test1562.github.io/demo/.</span>\n</p>\n\n",
                "matched_terms": [
                    "bridgetts",
                    "model",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">However, conventional AR zero-shot TTS systems still face two issues. First, the AR model generates discrete speech tokens sequentially, while a separate model decodes them into acoustic features and further synthesizes speech. This paradigm necessitates that the AR model iteratively processes token sequences with high generation rates, creating a computational bottleneck for real-time deployment. As illustrated in Fig.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">(A), existing methods either reduce token rates (bottom of Fig.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">(A)), sacrificing the expressiveness of generated speech </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib15\" title=\"\">15</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, or enrich tokens with additional information (top of Fig.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">(A)), reducing the token generation efficiency </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This leads to </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">issue (i): an inherent token rate-quality trade-off</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> between the token generation rate of AR and the quality of the synthesized speech. Second, existing AR zero-shot TTS methods </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> typically adopt training paradigms directly inherited from large language models, employing cross-entropy loss that solely focuses on token prediction accuracy. While this approach is well-suited for text token prediction, it is suboptimal for speech token generation. Adjacent speech tokens in the acoustic space often differ only in subtle prosodic or timbral variations, yet cross-entropy loss applies a uniform penalty regardless of token proximity to ground truth. This fails to provide the fine-grained, hierarchical supervision necessary for high-quality speech synthesis, leading to </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">issue (ii): text-oriented supervision mismatch</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "rate",
                    "model",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address issue (i), we propose </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">BridgeTTS</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a novel AR-TTS framework that reduces AR iterations while maintaining synthesis quality under zero-shot scenarios. BridgeTTS incorporates a core component, </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">BridgeCode</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which encompasses dual speech representations: sparse tokens and dense continuous features, along with two bridging modules for bidirectional conversion between them. As illustrated in Fig. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">(B), BridgeTTS enables the AR model to generate sparse tokens for efficiency, while the bridging module reconstructs detailed continuous features to ensure high-quality speech synthesis, thereby reducing prediction steps without compromising synthesis quality. Furthermore, to address issue (ii), BridgeTTS refines the training paradigm by jointly optimizing token-level and feature-level objectives, providing fine-grained, hierarchical supervision for speech token prediction, which is essential for generating speech with enhanced naturalness and intelligibility.</span>\n</p>\n\n",
                "matched_terms": [
                    "bridgetts",
                    "model",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">3) Experimental results demonstrate that BridgeTTS attains the lowest AR token rate among existing methods while achieving competitive naturalness and speaker similarity, and effectively mitigating error accumulation while accelerating synthesis speed.</span>\n</p>\n\n",
                "matched_terms": [
                    "results",
                    "rate",
                    "bridgetts",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To establish the dual speech representations required by BridgeTTS, we propose BridgeCode, a dual speech representation paradigm that incorporates sparse tokens and dense continuous features with bridging modules between them. As illustrated in Fig.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, dense continuous features are extracted by the frozen feature encoder from GPT-Talker </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, while sparse tokens are obtained by compressing dense continuous features through the proposed SparseBridge. SparseBridge and DenseBridge are two symmetrical bridging networks that perform bidirectional conversion between these two representations. Moreover, to achieve fine-grained sparse-to-dense alignment, we draw inspiration from VDVAE </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and enforce layer-wise alignment between intermediate features in both bridging modules (Fig.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), ensuring high-fidelity bidirectional conversion between sparse tokens and dense continuous features. The network architecture and training objectives of SparseBridge and DenseBridge are detailed below.</span>\n</p>\n\n",
                "matched_terms": [
                    "densebridge",
                    "bridgetts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SparseBridge Architecture.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> SparseBridge compresses dense continuous features into sparse tokens while preserving essential information. Taking continuous speech features </span>\n  <math alttext=\"F_{0}\\in\\mathbb{R}^{T\\times 768}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">F</mi>\n          <mn mathsize=\"0.900em\">0</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mn mathsize=\"0.900em\">768</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">F_{0}\\in\\mathbb{R}^{T\\times 768}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as input, the contextual feature extractor employs multi-scale convolutional layers with kernel sizes of </span>\n  <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">1</mn>\n      <annotation encoding=\"application/x-tex\">1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">3</mn>\n      <annotation encoding=\"application/x-tex\">3</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">5</mn>\n      <annotation encoding=\"application/x-tex\">5</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to capture contextual dependencies across varying temporal spans. The extracted multi-scale features are then concatenated and denoted as </span>\n  <math alttext=\"F_{1}\\in\\mathbb{R}^{T\\times 2304}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">F</mi>\n          <mn mathsize=\"0.900em\">1</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mn mathsize=\"0.900em\">2304</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">F_{1}\\in\\mathbb{R}^{T\\times 2304}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. To obtain sparse tokens, </span>\n  <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m6\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">1</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{1}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> undergoes temporal downsampling to reduce the frame rate by a factor of </span>\n  <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m7\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">5</mn>\n      <annotation encoding=\"application/x-tex\">5</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, yielding </span>\n  <math alttext=\"F_{2}\\in\\mathbb{R}^{T/5\\times 2304}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m8\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">F</mi>\n          <mn mathsize=\"0.900em\">2</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mrow>\n              <mi mathsize=\"0.900em\">T</mi>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\" symmetric=\"true\">/</mo>\n              <mn mathsize=\"0.900em\">5</mn>\n            </mrow>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mn mathsize=\"0.900em\">2304</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">F_{2}\\in\\mathbb{R}^{T/5\\times 2304}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which is then processed by hierarchical residual vector quantization (RVQ) </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> that iteratively compresses </span>\n  <math alttext=\"F_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m9\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">2</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> into discrete codes. The Hierarchical RVQ Encoder processes the </span>\n  <math alttext=\"2304\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m10\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">2304</mn>\n      <annotation encoding=\"application/x-tex\">2304</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-dimensional vector by splitting it into three </span>\n  <math alttext=\"768\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m11\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">768</mn>\n      <annotation encoding=\"application/x-tex\">768</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-dimensional vectors, each quantized by a </span>\n  <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m12\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">3</mn>\n      <annotation encoding=\"application/x-tex\">3</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-level RVQ, resulting in a </span>\n  <math alttext=\"3\\times 3\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m13\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">3</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <mn mathsize=\"0.900em\">3</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">3\\times 3</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> code matrix. Since VALL-E </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> has demonstrated that only the first RVQ indices are crucial while other indices can be discarded without significant information loss, a code selector retains solely the first RVQ indices, producing a sparse token sequence that preserves essential speech information while achieving substantial compression.</span>\n</p>\n\n",
                "matched_terms": [
                    "rate",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Based on the proposed BridgeCode, dual speech representations can be obtained for any given speech, with bidirectional conversion facilitated by the trained bridging modules. In this section, we further present BridgeTTS. As illustrated in Fig.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#S2.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 2.1 BridgeCode &#8227; 2 Method &#8227; BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the autoregressive generator in BridgeTTS is a retrained GPT-2-based </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> model. BridgeTTS allows the AR model to generate sparse tokens at a reduced frame rate to minimize iteration steps, while information-rich dense continuous features are reconstructed by the frozen DenseBridge for high-quality speech synthesis. This approach effectively addresses the aforementioned inherent speed-quality trade-off issue in AR-TTS. Moreover, to address the text-oriented supervision mismatch issue, BridgeTTS refines the training paradigm by jointly optimizing token-level and feature-level objectives, providing fine-grained supervision essential for high-quality speech synthesis.</span>\n</p>\n\n",
                "matched_terms": [
                    "densebridge",
                    "rate",
                    "bridgetts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Notably, unlike traditional autoregressive models that sequentially input one token to predict the next discrete token, BridgeTTS allows the AR model to input five consecutive speech feature frames at each step to predict the next token. This enables the AR model to make more informed predictions based on richer contextual information. Meanwhile, the AR model can observe the continuous features directly used for speech generation when predicting the next token, allowing it to adjust output tokens to control the synthesis of subsequent continuous features. Consequently, BridgeTTS enables the AR model to achieve better control over the naturalness and intelligibility of synthesized speech during both training and inference.\nThe respective processes for training and inference are depicted in Fig.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#S2.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 2.1 BridgeCode &#8227; 2 Method &#8227; BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (A) and (B).</span>\n</p>\n\n",
                "matched_terms": [
                    "bridgetts",
                    "model",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">P</mi>\n      <annotation encoding=\"application/x-tex\">P</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the model&#8217;s predicted probability distribution, </span>\n  <math alttext=\"e_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">e</mi>\n        <mi mathsize=\"0.900em\">t</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">e_{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the ground truth token at time step </span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <math alttext=\"\\mathbf{f}_{&lt;t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119839;</mi>\n        <mrow>\n          <mi/>\n          <mo mathsize=\"0.900em\">&lt;</mo>\n          <mi mathsize=\"0.900em\">t</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{f}_{&lt;t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> represents all the previous features up to time step </span>\n  <math alttext=\"t-1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mo mathsize=\"0.900em\">&#8722;</mo>\n        <mn mathsize=\"0.900em\">1</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">t-1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <math alttext=\"\\mathbf{t}_{\\text{ref}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m7\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119853;</mi>\n        <mtext mathsize=\"0.900em\">ref</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{t}_{\\text{ref}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\mathbf{t}_{\\text{target}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m8\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119853;</mi>\n        <mtext mathsize=\"0.900em\">target</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{t}_{\\text{target}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are the reference and target text representations, respectively. In the BridgeTTS framework, each token prediction is conditioned on the previous features, reference text, and target text, while previous tokens do not directly influence the current prediction.</span>\n</p>\n\n",
                "matched_terms": [
                    "bridgetts",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">However, token loss computes prediction accuracy through cross-entropy loss, treating any mismatch between predicted and ground truth tokens as equally incorrect, regardless of their acoustic similarity. This poses a challenge in speech synthesis, as acoustically similar tokens may generate speech differing only in subtle acoustic details, such as prosodic variations and phonetic articulation. Consequently, a model predicting tokens closely resembling the ground truth receives the same penalty as one predicting acoustically distant tokens. To address this limitation, we introduce feature loss that computes the MSE between predicted and ground truth features, providing fine-grained, hierarchical supervision for the AR model. The feature loss is given by:</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"\\mathbf{f}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119839;</mi>\n        <mi mathsize=\"0.900em\">t</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{f}_{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the ground truth feature at time step </span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <math alttext=\"\\hat{\\mathbf{f}}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p4.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">&#119839;</mi>\n          <mo mathsize=\"0.900em\">^</mo>\n        </mover>\n        <mi mathsize=\"0.900em\">t</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\hat{\\mathbf{f}}_{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the predicted feature obtained by passing the predicted token through the pre-trained DenseBridge.</span>\n</p>\n\n",
                "matched_terms": [
                    "densebridge",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">During inference, the AR model is conditioned on reference speech features and text input tokens, where the latter are derived by concatenating the reference transcript with the target text and encoding the result using a text encoder. At each autoregressive prediction step, the AR generator first predicts the next discrete token based on the accumulated speech features and text representations. The predicted token is then transformed into continuous speech features via the pre-trained DenseBridge. These continuous features are concatenated with the existing speech feature sequence and fed back into the AR generator for subsequent token prediction. This iterative process continues until an End-of-Speech (EOS) token is generated or the target sequence length is reached. Finally, the complete generated continuous speech features are synthesized into audio using the speech feature vocoder fine-tuned during BridgeCode training.</span>\n</p>\n\n",
                "matched_terms": [
                    "densebridge",
                    "model",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We conduct all experiments on the LibriTTS dataset </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with a sampling rate of 16 kHz. LibriTTS is a large-scale, multi-speaker English corpus comprising 585 hours of speech from over 2,300 speakers. For training, we combine the subsets train-clean-100, train-clean-360, and train-other-500. The development set is constructed by merging dev-clean and dev-other, while the test set consists of test-clean and test-other.</span>\n</p>\n\n",
                "matched_terms": [
                    "libritts",
                    "rate",
                    "set",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Implementation Details.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> For BridgeCode training, we first download the pre-trained weights for wav2vec 2.0 Base </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://github.com/eastonYi/wav2vec</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The speech feature encoder is kept frozen, while two bridging modules are trained on the LibriTTS training set for </span>\n  <math alttext=\"700k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">700</mn>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathsize=\"0.900em\">k</mi>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">700k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> steps on an NVIDIA A800 GPU with a batch size of 16. We employ the AdamW optimizer with an initial learning rate of </span>\n  <math alttext=\"1.0\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1.0</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">4</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1.0\\times 10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, decayed by a factor of </span>\n  <math alttext=\"0.999^{1/8}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mn mathsize=\"0.900em\">0.999</mn>\n        <mrow>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\" symmetric=\"true\">/</mo>\n          <mn mathsize=\"0.900em\">8</mn>\n        </mrow>\n      </msup>\n      <annotation encoding=\"application/x-tex\">0.999^{1/8}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> per epoch. After training two bridging modules, they are frozen, and the autoregressive generator is subsequently trained for </span>\n  <math alttext=\"600k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">600</mn>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathsize=\"0.900em\">k</mi>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">600k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> steps under the same conditions.</span>\n</p>\n\n",
                "matched_terms": [
                    "libritts",
                    "rate",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Subjective Evaluation Setup.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We conducted a subjective evaluation with 20 human raters, who were first trained on anonymized speech samples to familiarize them with the criteria for assessing speaker similarity and speech quality. Each rater scored randomly selected samples on a 5-point scale for both naturalness and similarity. Specifically, we performed a Speaker Similarity Mean Opinion Score (SMOS) test to measure speaker resemblance in zero-shot TTS synthesis and a Quality Mean Opinion Score (QMOS) test to evaluate overall naturalness.</span>\n</p>\n\n",
                "matched_terms": [
                    "smos",
                    "test",
                    "qmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Objective Evaluation Setup.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> For objective evaluation, we measure UTMOS </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to assess overall naturalness and quality, and Word Error Rate (WER) using a Wav2Vec 2.0-large-based ASR model </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to evaluate word-level synthesis accuracy. To further assess model performance, we introduce an additional metric, Token rate, which measures the frequency at which each AR model outputs discrete speech tokens (i.e., tokens per second).</span>\n</p>\n\n",
                "matched_terms": [
                    "token",
                    "wer",
                    "rate",
                    "utmos",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We compare our BridgeTTS with state-of-the-art (SOTA) methods on the LibriSpeech dataset. We choose four SOTA methods as our baseline, including VALL-E </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, UnionAudio </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a modified version of GPT-Talker </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> adapted for zero-shot synthesis, and CosyVoice </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For fair comparison, all models are further trained on LibriSpeech using their original pre-trained weights as initialization. The results are presented in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.3 Comparison with Existing Methods &#8227; 3 Experiments &#8227; BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where GT denotes ground-truth speech.</span>\n</p>\n\n",
                "matched_terms": [
                    "results",
                    "bridgetts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As observed, BridgeTTS delivers competitive synthesis quality compared to four baseline methods across SMOS, QMOS, and UTMOS metrics, while achieving the lowest WER and Token Rate. Compared to GPT-Talker, which shares the same base model and training data, BridgeTTS improves speech naturalness and similarity while maintaining lower Token Rate and WER. This improvement stems from the proposed novel BridgeCode and unique AR paradigm, which enables the AR model to input multiple consecutive speech feature frames at each step for next-token prediction, facilitating more informed predictions based on richer contextual information. Additionally, DenseBridge losslessly converts sparse tokens from previous steps into dense continuous features for AR input in subsequent predictions, ensuring the stability of this AR paradigm.\nCompared to CosyVoice, which employs updated codec models, advanced AR architectures, and larger training datasets, BridgeTTS achieves competitive speech naturalness and similarity while maintaining a lower WER. This superior performance is attributed to the lower frame rate of sparse tokens in the proposed BridgeCode. For synthesizing speech of equivalent duration, BridgeTTS requires fewer AR iterations than CosyVoice, resulting in reduced error accumulation during inference and higher word-level synthesis accuracy.</span>\n</p>\n\n",
                "matched_terms": [
                    "smos",
                    "token",
                    "wer",
                    "rate",
                    "bridgetts",
                    "utmos",
                    "densebridge",
                    "model",
                    "qmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To further demonstrate the acceleration effect of BridgeTTS, we compare a baseline AR generator with its DenseBridge-enhanced counterpart in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.3 Comparison with Existing Methods &#8227; 3 Experiments &#8227; BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The results confirm that BridgeTTS achieves faster synthesis with improved real-time factor (RTF) while preserving comparable quality.</span>\n</p>\n\n",
                "matched_terms": [
                    "results",
                    "bridgetts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we proposed BridgeTTS, which incorporates a novel BridgeCode and a unique AR paradigm. BridgeCode introduces dual speech representations together with two bridging modules, enabling the AR model to generate sparse tokens to minimize iteration steps, while reconstructing information-rich dense continuous features for high-quality speech synthesis. In addition, BridgeTTS refines the training paradigm of AR models by providing fine-grained supervision, mitigating the mismatch between text-based and speech-based large models.\nExperiments and ablation studies verify the effectiveness of BridgeTTS. Moreover, BridgeCode can be generalized to arbitrary AR-TTS models, demonstrating strong generalization and promising potential for future applications.</span>\n</p>\n\n",
                "matched_terms": [
                    "bridgetts",
                    "model",
                    "ablation"
                ]
            }
        ]
    },
    "S3.T3": {
        "source_file": "BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis",
        "caption": "Table 3: Comparison of Baseline AR Generator vs. BridgeTTS on LibriTTS Test Set.",
        "body": "System\nRTF (↓)\\left(\\downarrow\\right)\nToken Rate (↓)\\left(\\downarrow\\right)\nWER (↓)\\left(\\downarrow\\right)\nSMOS (↑)\\left(\\uparrow\\right)\nQMOS (↑)\\left(\\uparrow\\right)\nUTMOS (↑)\\left(\\uparrow\\right)\n\n\n\n\nBaseline AR\n1×\n50Hz\n9.8%\n-\n-\n-\n\n\nBridgeTTS\n0.37×\n10Hz\n4.9%\n+0.12\n+0.09\n+0.43",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">System</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">RTF <math alttext=\"\\left(\\downarrow\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m1\" intent=\":literal\"><semantics><mrow><mo>(</mo><mo lspace=\"0em\" rspace=\"0em\" stretchy=\"false\">&#8595;</mo><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">\\left(\\downarrow\\right)</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Token Rate <math alttext=\"\\left(\\downarrow\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m2\" intent=\":literal\"><semantics><mrow><mo>(</mo><mo lspace=\"0em\" rspace=\"0em\" stretchy=\"false\">&#8595;</mo><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">\\left(\\downarrow\\right)</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WER <math alttext=\"\\left(\\downarrow\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m3\" intent=\":literal\"><semantics><mrow><mo>(</mo><mo lspace=\"0em\" rspace=\"0em\" stretchy=\"false\">&#8595;</mo><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">\\left(\\downarrow\\right)</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SMOS <math alttext=\"\\left(\\uparrow\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m4\" intent=\":literal\"><semantics><mrow><mo>(</mo><mo lspace=\"0em\" rspace=\"0em\" stretchy=\"false\">&#8593;</mo><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">\\left(\\uparrow\\right)</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">QMOS <math alttext=\"\\left(\\uparrow\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m5\" intent=\":literal\"><semantics><mrow><mo>(</mo><mo lspace=\"0em\" rspace=\"0em\" stretchy=\"false\">&#8593;</mo><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">\\left(\\uparrow\\right)</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">UTMOS <math alttext=\"\\left(\\uparrow\\right)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m6\" intent=\":literal\"><semantics><mrow><mo>(</mo><mo lspace=\"0em\" rspace=\"0em\" stretchy=\"false\">&#8593;</mo><mo>)</mo></mrow><annotation encoding=\"application/x-tex\">\\left(\\uparrow\\right)</annotation></semantics></math></span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Baseline AR</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1&#215;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">50Hz</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.8%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">BridgeTTS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.37&#215;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">10Hz</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.9%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">+0.12</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">+0.09</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">+0.43</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "libritts",
            "rtf",
            "wer",
            "utmos",
            "↓leftdownarrowright",
            "token",
            "rate",
            "10hz",
            "50hz",
            "↑leftuparrowright",
            "system",
            "smos",
            "comparison",
            "037×",
            "test",
            "baseline",
            "bridgetts",
            "set",
            "generator",
            "qmos"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To further demonstrate the acceleration effect of BridgeTTS, we compare a baseline AR generator with its DenseBridge-enhanced counterpart in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.3 Comparison with Existing Methods &#8227; 3 Experiments &#8227; BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The results confirm that BridgeTTS achieves faster synthesis with improved real-time factor (RTF) while preserving comparable quality.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Autoregressive (AR) frameworks have recently achieved remarkable progress in zero-shot text-to-speech (TTS) by leveraging discrete speech tokens and large language model techniques. Despite their success, existing AR-based zero-shot TTS systems face two critical limitations: (i) an inherent speed&#8211;quality trade-off, as sequential token generation either reduces frame rates at the cost of expressiveness or enriches tokens at the cost of efficiency, and (ii) a text-oriented supervision mismatch, as cross-entropy loss penalizes token errors uniformly without considering the fine-grained acoustic similarity among adjacent tokens. To address these challenges, we propose <span class=\"ltx_text ltx_font_bold\">BridgeTTS</span>, a novel AR-TTS framework built upon the dual speech representation paradigm <span class=\"ltx_text ltx_font_bold\">BridgeCode</span>. BridgeTTS reduces AR iterations by predicting sparse tokens while reconstructing rich continuous features for high-quality synthesis. Joint optimization of token-level and feature-level objectives further enhances naturalness and intelligibility. Experiments demonstrate that BridgeTTS achieves competitive quality and speaker similarity while significantly accelerating synthesis. Speech demos are available at https://test1562.github.io/demo/.</span>\n</p>\n\n",
                "matched_terms": [
                    "bridgetts",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;<span class=\"ltx_text ltx_font_medium\">\nZero-shot TTS, Autoregressive Generator, Token Rate-Quality Trade-off, Discrete-Continuous Representation</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "generator",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">However, conventional AR zero-shot TTS systems still face two issues. First, the AR model generates discrete speech tokens sequentially, while a separate model decodes them into acoustic features and further synthesizes speech. This paradigm necessitates that the AR model iteratively processes token sequences with high generation rates, creating a computational bottleneck for real-time deployment. As illustrated in Fig.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">(A), existing methods either reduce token rates (bottom of Fig.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">(A)), sacrificing the expressiveness of generated speech </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib15\" title=\"\">15</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, or enrich tokens with additional information (top of Fig.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">(A)), reducing the token generation efficiency </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This leads to </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">issue (i): an inherent token rate-quality trade-off</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> between the token generation rate of AR and the quality of the synthesized speech. Second, existing AR zero-shot TTS methods </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> typically adopt training paradigms directly inherited from large language models, employing cross-entropy loss that solely focuses on token prediction accuracy. While this approach is well-suited for text token prediction, it is suboptimal for speech token generation. Adjacent speech tokens in the acoustic space often differ only in subtle prosodic or timbral variations, yet cross-entropy loss applies a uniform penalty regardless of token proximity to ground truth. This fails to provide the fine-grained, hierarchical supervision necessary for high-quality speech synthesis, leading to </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">issue (ii): text-oriented supervision mismatch</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "rate",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address issue (i), we propose </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">BridgeTTS</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a novel AR-TTS framework that reduces AR iterations while maintaining synthesis quality under zero-shot scenarios. BridgeTTS incorporates a core component, </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">BridgeCode</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which encompasses dual speech representations: sparse tokens and dense continuous features, along with two bridging modules for bidirectional conversion between them. As illustrated in Fig. </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">(B), BridgeTTS enables the AR model to generate sparse tokens for efficiency, while the bridging module reconstructs detailed continuous features to ensure high-quality speech synthesis, thereby reducing prediction steps without compromising synthesis quality. Furthermore, to address issue (ii), BridgeTTS refines the training paradigm by jointly optimizing token-level and feature-level objectives, providing fine-grained, hierarchical supervision for speech token prediction, which is essential for generating speech with enhanced naturalness and intelligibility.</span>\n</p>\n\n",
                "matched_terms": [
                    "bridgetts",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">3) Experimental results demonstrate that BridgeTTS attains the lowest AR token rate among existing methods while achieving competitive naturalness and speaker similarity, and effectively mitigating error accumulation while accelerating synthesis speed.</span>\n</p>\n\n",
                "matched_terms": [
                    "rate",
                    "bridgetts",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SparseBridge Architecture.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> SparseBridge compresses dense continuous features into sparse tokens while preserving essential information. Taking continuous speech features </span>\n  <math alttext=\"F_{0}\\in\\mathbb{R}^{T\\times 768}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">F</mi>\n          <mn mathsize=\"0.900em\">0</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mn mathsize=\"0.900em\">768</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">F_{0}\\in\\mathbb{R}^{T\\times 768}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as input, the contextual feature extractor employs multi-scale convolutional layers with kernel sizes of </span>\n  <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">1</mn>\n      <annotation encoding=\"application/x-tex\">1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">3</mn>\n      <annotation encoding=\"application/x-tex\">3</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">5</mn>\n      <annotation encoding=\"application/x-tex\">5</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to capture contextual dependencies across varying temporal spans. The extracted multi-scale features are then concatenated and denoted as </span>\n  <math alttext=\"F_{1}\\in\\mathbb{R}^{T\\times 2304}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">F</mi>\n          <mn mathsize=\"0.900em\">1</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mi mathsize=\"0.900em\">T</mi>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mn mathsize=\"0.900em\">2304</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">F_{1}\\in\\mathbb{R}^{T\\times 2304}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. To obtain sparse tokens, </span>\n  <math alttext=\"F_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m6\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">1</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{1}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> undergoes temporal downsampling to reduce the frame rate by a factor of </span>\n  <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m7\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">5</mn>\n      <annotation encoding=\"application/x-tex\">5</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, yielding </span>\n  <math alttext=\"F_{2}\\in\\mathbb{R}^{T/5\\times 2304}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m8\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">F</mi>\n          <mn mathsize=\"0.900em\">2</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">&#8477;</mi>\n          <mrow>\n            <mrow>\n              <mi mathsize=\"0.900em\">T</mi>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\" symmetric=\"true\">/</mo>\n              <mn mathsize=\"0.900em\">5</mn>\n            </mrow>\n            <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n            <mn mathsize=\"0.900em\">2304</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">F_{2}\\in\\mathbb{R}^{T/5\\times 2304}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which is then processed by hierarchical residual vector quantization (RVQ) </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> that iteratively compresses </span>\n  <math alttext=\"F_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m9\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">F</mi>\n        <mn mathsize=\"0.900em\">2</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">F_{2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> into discrete codes. The Hierarchical RVQ Encoder processes the </span>\n  <math alttext=\"2304\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m10\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">2304</mn>\n      <annotation encoding=\"application/x-tex\">2304</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-dimensional vector by splitting it into three </span>\n  <math alttext=\"768\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m11\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">768</mn>\n      <annotation encoding=\"application/x-tex\">768</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-dimensional vectors, each quantized by a </span>\n  <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m12\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">3</mn>\n      <annotation encoding=\"application/x-tex\">3</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">-level RVQ, resulting in a </span>\n  <math alttext=\"3\\times 3\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m13\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">3</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <mn mathsize=\"0.900em\">3</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">3\\times 3</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> code matrix. Since VALL-E </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> has demonstrated that only the first RVQ indices are crucial while other indices can be discarded without significant information loss, a code selector retains solely the first RVQ indices, producing a sparse token sequence that preserves essential speech information while achieving substantial compression.</span>\n</p>\n\n",
                "matched_terms": [
                    "rate",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Based on the proposed BridgeCode, dual speech representations can be obtained for any given speech, with bidirectional conversion facilitated by the trained bridging modules. In this section, we further present BridgeTTS. As illustrated in Fig.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#S2.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 2.1 BridgeCode &#8227; 2 Method &#8227; BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, the autoregressive generator in BridgeTTS is a retrained GPT-2-based </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> model. BridgeTTS allows the AR model to generate sparse tokens at a reduced frame rate to minimize iteration steps, while information-rich dense continuous features are reconstructed by the frozen DenseBridge for high-quality speech synthesis. This approach effectively addresses the aforementioned inherent speed-quality trade-off issue in AR-TTS. Moreover, to address the text-oriented supervision mismatch issue, BridgeTTS refines the training paradigm by jointly optimizing token-level and feature-level objectives, providing fine-grained supervision essential for high-quality speech synthesis.</span>\n</p>\n\n",
                "matched_terms": [
                    "rate",
                    "bridgetts",
                    "generator"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Notably, unlike traditional autoregressive models that sequentially input one token to predict the next discrete token, BridgeTTS allows the AR model to input five consecutive speech feature frames at each step to predict the next token. This enables the AR model to make more informed predictions based on richer contextual information. Meanwhile, the AR model can observe the continuous features directly used for speech generation when predicting the next token, allowing it to adjust output tokens to control the synthesis of subsequent continuous features. Consequently, BridgeTTS enables the AR model to achieve better control over the naturalness and intelligibility of synthesized speech during both training and inference.\nThe respective processes for training and inference are depicted in Fig.</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#S2.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 2.1 BridgeCode &#8227; 2 Method &#8227; BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (A) and (B).</span>\n</p>\n\n",
                "matched_terms": [
                    "bridgetts",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">P</mi>\n      <annotation encoding=\"application/x-tex\">P</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the model&#8217;s predicted probability distribution, </span>\n  <math alttext=\"e_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">e</mi>\n        <mi mathsize=\"0.900em\">t</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">e_{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the ground truth token at time step </span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <math alttext=\"\\mathbf{f}_{&lt;t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119839;</mi>\n        <mrow>\n          <mi/>\n          <mo mathsize=\"0.900em\">&lt;</mo>\n          <mi mathsize=\"0.900em\">t</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{f}_{&lt;t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> represents all the previous features up to time step </span>\n  <math alttext=\"t-1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mo mathsize=\"0.900em\">&#8722;</mo>\n        <mn mathsize=\"0.900em\">1</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">t-1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <math alttext=\"\\mathbf{t}_{\\text{ref}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m7\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119853;</mi>\n        <mtext mathsize=\"0.900em\">ref</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{t}_{\\text{ref}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\mathbf{t}_{\\text{target}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p3.m8\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119853;</mi>\n        <mtext mathsize=\"0.900em\">target</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathbf{t}_{\\text{target}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are the reference and target text representations, respectively. In the BridgeTTS framework, each token prediction is conditioned on the previous features, reference text, and target text, while previous tokens do not directly influence the current prediction.</span>\n</p>\n\n",
                "matched_terms": [
                    "bridgetts",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">During inference, the AR model is conditioned on reference speech features and text input tokens, where the latter are derived by concatenating the reference transcript with the target text and encoding the result using a text encoder. At each autoregressive prediction step, the AR generator first predicts the next discrete token based on the accumulated speech features and text representations. The predicted token is then transformed into continuous speech features via the pre-trained DenseBridge. These continuous features are concatenated with the existing speech feature sequence and fed back into the AR generator for subsequent token prediction. This iterative process continues until an End-of-Speech (EOS) token is generated or the target sequence length is reached. Finally, the complete generated continuous speech features are synthesized into audio using the speech feature vocoder fine-tuned during BridgeCode training.</span>\n</p>\n\n",
                "matched_terms": [
                    "generator",
                    "token"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We conduct all experiments on the LibriTTS dataset </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with a sampling rate of 16 kHz. LibriTTS is a large-scale, multi-speaker English corpus comprising 585 hours of speech from over 2,300 speakers. For training, we combine the subsets train-clean-100, train-clean-360, and train-other-500. The development set is constructed by merging dev-clean and dev-other, while the test set consists of test-clean and test-other.</span>\n</p>\n\n",
                "matched_terms": [
                    "libritts",
                    "rate",
                    "set",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Implementation Details.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> For BridgeCode training, we first download the pre-trained weights for wav2vec 2.0 Base </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://github.com/eastonYi/wav2vec</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The speech feature encoder is kept frozen, while two bridging modules are trained on the LibriTTS training set for </span>\n  <math alttext=\"700k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">700</mn>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathsize=\"0.900em\">k</mi>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">700k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> steps on an NVIDIA A800 GPU with a batch size of 16. We employ the AdamW optimizer with an initial learning rate of </span>\n  <math alttext=\"1.0\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1.0</mn>\n        <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n        <msup>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mrow>\n            <mo mathsize=\"0.900em\">&#8722;</mo>\n            <mn mathsize=\"0.900em\">4</mn>\n          </mrow>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1.0\\times 10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, decayed by a factor of </span>\n  <math alttext=\"0.999^{1/8}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mn mathsize=\"0.900em\">0.999</mn>\n        <mrow>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\" symmetric=\"true\">/</mo>\n          <mn mathsize=\"0.900em\">8</mn>\n        </mrow>\n      </msup>\n      <annotation encoding=\"application/x-tex\">0.999^{1/8}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> per epoch. After training two bridging modules, they are frozen, and the autoregressive generator is subsequently trained for </span>\n  <math alttext=\"600k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">600</mn>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mi mathsize=\"0.900em\">k</mi>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">600k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> steps under the same conditions.</span>\n</p>\n\n",
                "matched_terms": [
                    "libritts",
                    "rate",
                    "set",
                    "generator"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Subjective Evaluation Setup.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We conducted a subjective evaluation with 20 human raters, who were first trained on anonymized speech samples to familiarize them with the criteria for assessing speaker similarity and speech quality. Each rater scored randomly selected samples on a 5-point scale for both naturalness and similarity. Specifically, we performed a Speaker Similarity Mean Opinion Score (SMOS) test to measure speaker resemblance in zero-shot TTS synthesis and a Quality Mean Opinion Score (QMOS) test to evaluate overall naturalness.</span>\n</p>\n\n",
                "matched_terms": [
                    "smos",
                    "test",
                    "qmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Objective Evaluation Setup.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> For objective evaluation, we measure UTMOS </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to assess overall naturalness and quality, and Word Error Rate (WER) using a Wav2Vec 2.0-large-based ASR model </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to evaluate word-level synthesis accuracy. To further assess model performance, we introduce an additional metric, Token rate, which measures the frequency at which each AR model outputs discrete speech tokens (i.e., tokens per second).</span>\n</p>\n\n",
                "matched_terms": [
                    "wer",
                    "rate",
                    "token",
                    "utmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We compare our BridgeTTS with state-of-the-art (SOTA) methods on the LibriSpeech dataset. We choose four SOTA methods as our baseline, including VALL-E </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, UnionAudio </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a modified version of GPT-Talker </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> adapted for zero-shot synthesis, and CosyVoice </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For fair comparison, all models are further trained on LibriSpeech using their original pre-trained weights as initialization. The results are presented in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.3 Comparison with Existing Methods &#8227; 3 Experiments &#8227; BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where GT denotes ground-truth speech.</span>\n</p>\n\n",
                "matched_terms": [
                    "bridgetts",
                    "comparison",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As observed, BridgeTTS delivers competitive synthesis quality compared to four baseline methods across SMOS, QMOS, and UTMOS metrics, while achieving the lowest WER and Token Rate. Compared to GPT-Talker, which shares the same base model and training data, BridgeTTS improves speech naturalness and similarity while maintaining lower Token Rate and WER. This improvement stems from the proposed novel BridgeCode and unique AR paradigm, which enables the AR model to input multiple consecutive speech feature frames at each step for next-token prediction, facilitating more informed predictions based on richer contextual information. Additionally, DenseBridge losslessly converts sparse tokens from previous steps into dense continuous features for AR input in subsequent predictions, ensuring the stability of this AR paradigm.\nCompared to CosyVoice, which employs updated codec models, advanced AR architectures, and larger training datasets, BridgeTTS achieves competitive speech naturalness and similarity while maintaining a lower WER. This superior performance is attributed to the lower frame rate of sparse tokens in the proposed BridgeCode. For synthesizing speech of equivalent duration, BridgeTTS requires fewer AR iterations than CosyVoice, resulting in reduced error accumulation during inference and higher word-level synthesis accuracy.</span>\n</p>\n\n",
                "matched_terms": [
                    "smos",
                    "token",
                    "baseline",
                    "wer",
                    "rate",
                    "bridgetts",
                    "utmos",
                    "qmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Ablation studies are conducted to assess the contributions of sequence compression and feature loss, as summarized in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.11646v1#S3.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 3.3 Comparison with Existing Methods &#8227; 3 Experiments &#8227; BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. w/o BridgeCode denotes training the AR generator on compressed tokens alone, without employing BridgeCode, while w/o </span>\n  <math alttext=\"\\mathcal{L}_{\\text{features}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n        <mtext mathsize=\"0.900em\">features</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{features}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes training with Bridge but excluding the feature loss.\nThe results show that compressing token sequences without BridgeCode degrades quality, as the AR generator lacks sufficiently informative prompts for accurate synthesis. Similarly, removing the feature loss results in suboptimal performance, since this objective enforces attention to fine-grained acoustic and temporal characteristics beyond semantic content.</span>\n</p>\n\n",
                "matched_terms": [
                    "generator",
                    "token"
                ]
            }
        ]
    }
}