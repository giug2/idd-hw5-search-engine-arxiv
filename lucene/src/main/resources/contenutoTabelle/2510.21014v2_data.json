{
    "S2.T1": {
        "source_file": "REFESS-QI: REFERENCE-FREE EVALUATION FOR SPEECH SEPARATION WITH JOINT QUALITY AND INTELLIGIBILITY SCORING",
        "caption": "Table 1: Distribution of the data-set splits of the WHAMR! separated\ntracks. ASR generated with Whisper large-V3.",
        "body": "Dataset\nSplit\nTotal Dur. (h)\nAvg Dur. (S)\n#Seg\nAvg #WRD\nAvg WER\nStd. Dev of WER\n\n\n\n\nREAL-M\nTest\n1.3\n5.17\n924\n11.2\n49.99%\n31.17%\n\n\nWHAMR!\nTest\n8.2\n10.7\n2,750\n16.36\n49.36%\n20.38%\n\n\nWHAMR!\nTrain\n402.7\n10.36\n140,000\n16.41\n49.84%\n31.05%",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Split</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Total Dur. (h)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Avg Dur. (S)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">#Seg</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Avg #WRD</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Avg WER</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Std. Dev of WER</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">REAL-M</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Test</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">5.17</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">924</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">11.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">49.99%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">31.17%</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">WHAMR!</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Test</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">8.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">10.7</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2,750</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">16.36</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">49.36%</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">20.38%</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">WHAMR!</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Train</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">402.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">10.36</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">140,000</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">16.41</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">49.84%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">31.05%</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "realm",
            "seg",
            "std",
            "avg",
            "split",
            "whamr",
            "test",
            "separated",
            "wer",
            "whisper",
            "dur",
            "largev3",
            "train",
            "distribution",
            "splits",
            "dataset",
            "asr",
            "dev",
            "tracks",
            "wrd",
            "total",
            "generated"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The experiments in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#S2.T1\" title=\"Table 1 &#8227; 2.3 Metrics estimators &#8227; 2 Related Works &#8227; REFESS-QI: REFERENCE-FREE EVALUATION FOR SPEECH SEPARATION WITH JOINT QUALITY AND INTELLIGIBILITY SCORING\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> show that this method can predict more than one metric. In terms of WER, that joint estimator achieves similar results than the single metrics estimator, both on the WHAMR! test set and the REAL-M datasets.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Source separation is a crucial pre-processing step for various speech processing tasks, such as automatic speech recognition (ASR).\nTraditionally, the evaluation metrics for speech separation rely on the matched reference audios and corresponding transcriptions to assess audio quality and intelligibility.\nHowever, they cannot be used to evaluate real-world mixtures for which no reference exists.\nThis paper introduces a text-free reference-free evaluation framework based on self-supervised learning (SSL) representations.\nThe proposed framework utilize the mixture and separated tracks to predict jointly audio quality, through the Scale Invariant Signal to Noise Ratio (SI-SNR) metric, and speech intelligibility through the Word Error Rate (WER) metric.\nWe conducted experiments on the WHAMR! dataset, which shows a WER estimation with a mean absolute error (MAE) of 17% and a Pearson correlation coefficient (PCC) of 0.77; and SI-SNR estimation with an MAE of 1.38 and PCC of 0.95.\nWe further demonstrate the robustness of our estimator by using various SSL representations.</p>\n\n",
                "matched_terms": [
                    "separated",
                    "tracks",
                    "dataset",
                    "whamr",
                    "wer",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance of traditional speech separation is typically evaluated using signal-based metrics, such as the SI-SNR.\nIn addition to the signal level metrics, task-oriented evaluations are widely employed to verify that the separated signals remain useful for downstream applications, such as WER for ASR.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "separated",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper introduces a novel reference-free model designed to estimate various speech separation metrics called ReFESS-QI, utilizing the Hugging Face Transformers framework for unsupervised evaluation. The network processes audio features from both the mixed input and the separated sources to predict metrics such as SI-SNR and WER in a 3-dimensional forward pass. This model outperforms existing SI-SNR estimators and achieves comparable WER baselines without relying on text hypotheses obtained from an ASR model.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "separated",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech-separation performance can be evaluated with generic signal-based metrics, such as the <span class=\"ltx_text ltx_font_italic\">Signal-to-Noise Ratio (SI-SNR)</span> and the <span class=\"ltx_text ltx_font_italic\">Perceptual Evaluation of Speech Quality (PESQ)<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text ltx_font_upright\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib11\" title=\"\">11</a><span class=\"ltx_text ltx_font_upright\">]</span></cite></span>, or with specialized downstream application-related metrics, such as the <span class=\"ltx_text ltx_font_italic\">WER</span> for ASR.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By opposition, the <span class=\"ltx_text ltx_font_italic\">WER</span> metric is measured for a given ASR system, by comparing the output of the ASR with a text reference, where the proportion of insertions, deletions, and substitutions required to align the recognized text with the reference quantifies the impact of separation on recognition performance.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The WER baseline <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib9\" title=\"\">9</a>]</cite> is a method that utilizes both audio and text features from the ASR hypothesis. These features are fed into the MLP head that predicts a WER estimate for a single audio.\nThis cascade approach presents efficiency and accuracy issues. The chaining of the results of the ASR model to the text embedding model is one point of failure when the ASR input is not accurate, and the inference of the three models is considered a heavy task.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section describes our automatic WER estimator method, as well as its extensions as an SI-SNR estimator and a joint estimator, applied to speech separation.\nFirst, we used multiple speech separation systems to extract pairs of separated audios from mixtures.\nSecond, we compute the metrics from the extracted audios. The SI-SNR is computed directly and the WER from Whisper V3-Large&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib12\" title=\"\">12</a>]</cite> predictions, using the Nemo toolkit<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib13\" title=\"\">13</a>]</cite> for text normalization.\nThird, the metric estimator model is trained on the triplets of audio (mixture and 2 outputs) to predict the corresponding metrics.</p>\n\n",
                "matched_terms": [
                    "separated",
                    "whisper",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The estimator model performs multi-output regression to estimate a target metric, depending on the training task.\nIt uses a SSL encoder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib19\" title=\"\">19</a>]</cite> to extract <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> features of dimension <math alttext=\"D_{SSL}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>D</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>L</mi></mrow></msub><annotation encoding=\"application/x-tex\">D_{SSL}</annotation></semantics></math> from the mixture <math alttext=\"Y\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mi>Y</mi><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math> and the two separated tracks <math alttext=\"\\hat{S_{1}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mover accent=\"true\"><msub><mi>S</mi><mn>1</mn></msub><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\hat{S_{1}}</annotation></semantics></math> and <math alttext=\"\\hat{S_{2}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><mover accent=\"true\"><msub><mi>S</mi><mn>2</mn></msub><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\hat{S_{2}}</annotation></semantics></math>, then concatenates the features along the feature axis and outputs an array of size <math alttext=\"(T,3\\times D_{SSL})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>T</mi><mo>,</mo><mrow><mn>3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>D</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>L</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(T,3\\times D_{SSL})</annotation></semantics></math>.\nThen, the array is processed by a transformer encoder layer, mean pooled across the time dimension, followed by a linear layer, to predict three continuous values: metric value for source 1, source 2, and their average.\nThe model is trained with a mean squared error (MSE) loss and is evaluated using MAE, and Pearson&#8217;s correlation.\nThis architecture supports reference-free estimation for any signal-level metrics or downstream model value.</p>\n\n",
                "matched_terms": [
                    "separated",
                    "tracks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the reverberant max version of WHAMR! <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib16\" title=\"\">16</a>]</cite> which is a far-field speech separation corpus designed to simulate real-world noisy and reverberant conditions. Each mixture comprises two overlapping speakers with additive environmental noise and reverberation applied through simulated room impulse responses. Additionaly, we use\nREAL-M<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib7\" title=\"\">7</a>]</cite>, a real-life speech source separation dataset for two-speaker mixtures.</p>\n\n",
                "matched_terms": [
                    "whamr",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The WHAMR! train and validation splits were used to train and validate both the speech separation models and the metric estimators. The performances of our models were evaluated on the WHAMR! test split and the Real M dataset</p>\n\n",
                "matched_terms": [
                    "test",
                    "train",
                    "splits",
                    "dataset",
                    "split",
                    "whamr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the SS systems to create a dataset of Metrics. To ensure a balanced label distribution, we selected three checkpoints from the beginning, middle, and end epochs for dataset generation. ASR was calculated using Whisper V3-Large<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib12\" title=\"\">12</a>]</cite>, and text normalization was carried out with the NeMo toolkit<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib13\" title=\"\">13</a>]</cite>. Additionally, the SISNR was computed using only the reference audio.\nThe distribution of the dataset is described in Table I and Figure 2.</p>\n\n",
                "matched_terms": [
                    "distribution",
                    "whisper",
                    "dataset",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each sample, the input to the estimator comprised three-channel audio, consisting of the mixture signal along with the two separated sources. Metric scores, computed using ground-truth references, were employed as regression targets. Initially, estimators were trained separately for the SI-SNR and WER prediction tasks, following the same architecture and data processing pipeline outlined in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#S3.SS2\" title=\"3.2 Metric estimator Architecture &#8227; 3 Methods &#8227; REFESS-QI: REFERENCE-FREE EVALUATION FOR SPEECH SEPARATION WITH JOINT QUALITY AND INTELLIGIBILITY SCORING\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "separated",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess generalization, we evaluated the trained WER estimator on the REAL-M data set <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib7\" title=\"\">7</a>]</cite>, which is a real mixture data set that contains reverberation and noise. REAL-M is for evaluation only, and therefore will be separated using our pretrained models on WHAMR!.</p>\n\n",
                "matched_terms": [
                    "realm",
                    "separated",
                    "whamr",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In terms of WER estimation, Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#S4.T2\" title=\"Table 2 &#8227; 4.3 Generalization to New Data &#8227; 4 Experiments &#8227; REFESS-QI: REFERENCE-FREE EVALUATION FOR SPEECH SEPARATION WITH JOINT QUALITY AND INTELLIGIBILITY SCORING\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows that our method that lightly trains the FE achieves better results than the baseline without using the Text hypothesis. Our method achieves an MAE of 0.17 and a PCC of 0.775 for the single metrics estimation, and an MAE of 0.14 and PCC of 0.824 for the average metrics estimation, while the baseline achieves an MAE of 0.15 and PCC of 0.746 using an additional ASR model and text embedder for the evaluation. Moreover, our estimation method with freezing the FE achieves competitive results to the baseline, which also uses frozen models, but more pretrained models in inference. We achieve our best results by utilizing Hubert representations. In the generalization experiment using new data from the Real-M dataset, our method exceeded the baseline across all metrics.</p>\n\n",
                "matched_terms": [
                    "realm",
                    "asr",
                    "dataset",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the out-of-domain evaluation (REAL-M), the WER estimator achieves an MAE of 0.25 and a PCC of 0.548.</p>\n\n",
                "matched_terms": [
                    "realm",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper presents a novel text-free reference-free model designed to estimate separately two metrics: WER and SI-SNR from the outputs of a speech separation system.\nWe proposed a new architecture leveraging self-supervised learning representations that outperformed significantly previous baselines without text nor audio references.\nWe craft our own training dataset from WHAMR! and three different speech separation systems, designed to offer a balanced distribution of WER and SISNR metrics, then train and evaluate our proposed framework on it.\nThe proposed framework demonstrates state of the art performances with different SSL representations, with and without finetuning of the encoder.\nFurthermore, we show our model can generalize to real mixtures when evaluated on the REAL-M dataset, and can work to estimate jointly both the WER and the SI-SNR.\n</p>\n\n",
                "matched_terms": [
                    "realm",
                    "train",
                    "distribution",
                    "dataset",
                    "whamr",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work has certain limitations, on the speech separation systems, the evaluation data and the applications.\nFirst, our proposed model used the same dataset to train as the speech separation models (the WHAMR! set), and the same set of speech separation models was used to generate the training and the testing data, limiting the potential generalization on new speech separation systems.\nSecond, we adopted a uniform distribution of the scores in the training data, which may not be best suited to align with the out-of-domain data, and we use a unique model to extract all transcripts for the WER evaluation (Whisper).\nFinally, the proposed metric estimator is based on self-supervised learning (SSL) systems, which typically requires large GPUs for inference, thus it will not be easy to use during training, only for evaluation.</p>\n\n",
                "matched_terms": [
                    "train",
                    "distribution",
                    "dataset",
                    "whamr",
                    "wer",
                    "whisper"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "REFESS-QI: REFERENCE-FREE EVALUATION FOR SPEECH SEPARATION WITH JOINT QUALITY AND INTELLIGIBILITY SCORING",
        "caption": "Table 2: Pearson’s correlation (PCC) and mean-absolute error (MAE) for the single and average WER estimators, for multiple Feature Extractors (FE) with FE trainable or frozen, tested on the WHAMR! dataset.",
        "body": "Models\nFE\nFE TR\nTest set\nHead\nPCC\nMAE\nA.PCC\nA.MAE\n\n\n[10]\nHuBERT+RoBERTa\n✗\nWHAMR!\nMLP\n.746\n.15\n-\n-\n\n\nOurs\nW2V2\n✓\nTr. + Lin.\n.762\n0.19\n.808\n0.17\n\n\nOurs\nHubert\n✓\nTr. + Lin.\n.775\n.17\n.824\n0.14\n\n\nOurs\nHubert\n✗\nTr. + Lin.\n.701\n.21\n.755\n0.17\n\n\nOurs\nWavLM\n✓\nTr. + Lin.\n.775\n.17\n.823\n0.14\n\n\nJoint\nHubert\n✓\nTr. + Lin.\n.769\n.17\n.819\n0.14\n\n\n[10]\nHuBERT+RoBERTa\n✗\nREAL-M\nMLP\n.413\n.28\n-\n-\n\n\nOurs\nHubert\n✓\nTr. + Lin.\n.547\n.22\n.510\n0.22\n\n\nJoint\nHubert\n✓\n.548\n.25\n.513\n0.21",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Models</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">FE</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">FE TR</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Test set</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Head</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">PCC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">MAE</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">A.PCC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">A.MAE</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib10\" title=\"\">10</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">HuBERT+RoBERTa</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"6\"><span class=\"ltx_text\" style=\"font-size:80%;\">WHAMR!</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">MLP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">.746</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">.15</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Ours</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">W2V2</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">Tr. + Lin.</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">.762</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.19</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">.808</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.17</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Ours</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">Hubert</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">Tr. + Lin.</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">.775</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">.17</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">.824</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.14</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Ours</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">Hubert</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">Tr. + Lin.</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">.701</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">.21</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">.755</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.17</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Ours</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">WavLM</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">Tr. + Lin.</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">.775</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">.17</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">.823</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.14</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Joint</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">Hubert</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">Tr. + Lin.</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">.769</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">.17</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">.819</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.14</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib10\" title=\"\">10</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">HuBERT+RoBERTa</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" rowspan=\"3\"><span class=\"ltx_text\" style=\"font-size:80%;\">REAL-M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">MLP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">.413</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">.28</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Ours</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">Hubert</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text\" style=\"font-size:80%;\">Tr. + Lin.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:80%;\">.547</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">.22</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">.510</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.22</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Joint</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">Hubert</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">.548</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:80%;\">.25</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">.513</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.21</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "mlp",
            "realm",
            "hubert",
            "ours",
            "single",
            "whamr",
            "meanabsolute",
            "mae",
            "frozen",
            "joint",
            "trainable",
            "test",
            "extractors",
            "lin",
            "estimators",
            "tested",
            "wer",
            "wavlm",
            "apcc",
            "pearson’s",
            "multiple",
            "dataset",
            "hubertroberta",
            "average",
            "w2v2",
            "amae",
            "set",
            "feature",
            "correlation",
            "models",
            "head",
            "pcc",
            "error"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">In terms of WER estimation, Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#S4.T2\" title=\"Table 2 &#8227; 4.3 Generalization to New Data &#8227; 4 Experiments &#8227; REFESS-QI: REFERENCE-FREE EVALUATION FOR SPEECH SEPARATION WITH JOINT QUALITY AND INTELLIGIBILITY SCORING\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows that our method that lightly trains the FE achieves better results than the baseline without using the Text hypothesis. Our method achieves an MAE of 0.17 and a PCC of 0.775 for the single metrics estimation, and an MAE of 0.14 and PCC of 0.824 for the average metrics estimation, while the baseline achieves an MAE of 0.15 and PCC of 0.746 using an additional ASR model and text embedder for the evaluation. Moreover, our estimation method with freezing the FE achieves competitive results to the baseline, which also uses frozen models, but more pretrained models in inference. We achieve our best results by utilizing Hubert representations. In the generalization experiment using new data from the Real-M dataset, our method exceeded the baseline across all metrics.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Source separation is a crucial pre-processing step for various speech processing tasks, such as automatic speech recognition (ASR).\nTraditionally, the evaluation metrics for speech separation rely on the matched reference audios and corresponding transcriptions to assess audio quality and intelligibility.\nHowever, they cannot be used to evaluate real-world mixtures for which no reference exists.\nThis paper introduces a text-free reference-free evaluation framework based on self-supervised learning (SSL) representations.\nThe proposed framework utilize the mixture and separated tracks to predict jointly audio quality, through the Scale Invariant Signal to Noise Ratio (SI-SNR) metric, and speech intelligibility through the Word Error Rate (WER) metric.\nWe conducted experiments on the WHAMR! dataset, which shows a WER estimation with a mean absolute error (MAE) of 17% and a Pearson correlation coefficient (PCC) of 0.77; and SI-SNR estimation with an MAE of 1.38 and PCC of 0.95.\nWe further demonstrate the robustness of our estimator by using various SSL representations.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "dataset",
                    "whamr",
                    "mae",
                    "wer",
                    "pcc",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous techniques have been proposed to estimate SI-SNR in speech separation and serve as baselines in this study <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib7\" title=\"\">7</a>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib8\" title=\"\">8</a>]</cite>. In addition, unsupervised quality estimation for speech enhancement has been explored&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib9\" title=\"\">9</a>]</cite>. This method evaluates a single output of a general enhancement system. Furthermore, a method for predicting the WER for general speech was introduced&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib10\" title=\"\">10</a>]</cite>. However, this method does not specifically predict WER based on learned relationships between representations of noisy audio and enhanced outputs. To the best of our knowledge, this work is the first to predict WER specifically in the context of speech separation.</p>\n\n",
                "matched_terms": [
                    "single",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper introduces a novel reference-free model designed to estimate various speech separation metrics called ReFESS-QI, utilizing the Hugging Face Transformers framework for unsupervised evaluation. The network processes audio features from both the mixed input and the separated sources to predict metrics such as SI-SNR and WER in a 3-dimensional forward pass. This model outperforms existing SI-SNR estimators and achieves comparable WER baselines without relying on text hypotheses obtained from an ASR model.</p>\n\n",
                "matched_terms": [
                    "estimators",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">The key contributions of this paper are as follows</span>: We introduce the first text-free, reference-free WER estimator\nfor speech-separated signals, We leverage self-supervised learning representations (SSLR) to create an improved SI-SNR estimator compared to existing baselines, We generalize our findings by training a set of joint estimators that predict WER and SISNR, indicating that this method can predict joint quality and Intelligibility scores.</p>\n\n",
                "matched_terms": [
                    "joint",
                    "set",
                    "estimators",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here we go through three related metric estimators baselines.\nIn the SI-SNR baseline <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib8\" title=\"\">8</a>]</cite>, an approach utilizing SSLR models to predict SI-SNR was introduced. This method concatenated temporally the mixture and the two separate audio tracks before feeding them in to the SSLR&#8217;s feature extractor. In contrast, our method extracts features from each audio track separately and learns the relationships between the representations in each time bin before making predictions about the metric. Moreover, the baseline is dependent on fine-tuning of the entire SSL model, which consists of millions of parameters, whereas our method does not require full fine-tuning. This allows us to preserve the knowledge from unsupervised training while only requiring light training.</p>\n\n",
                "matched_terms": [
                    "models",
                    "feature",
                    "estimators"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The WER baseline <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib9\" title=\"\">9</a>]</cite> is a method that utilizes both audio and text features from the ASR hypothesis. These features are fed into the MLP head that predicts a WER estimate for a single audio.\nThis cascade approach presents efficiency and accuracy issues. The chaining of the results of the ASR model to the text embedding model is one point of failure when the ASR input is not accurate, and the inference of the three models is considered a heavy task.</p>\n\n",
                "matched_terms": [
                    "mlp",
                    "models",
                    "single",
                    "head",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section describes our automatic WER estimator method, as well as its extensions as an SI-SNR estimator and a joint estimator, applied to speech separation.\nFirst, we used multiple speech separation systems to extract pairs of separated audios from mixtures.\nSecond, we compute the metrics from the extracted audios. The SI-SNR is computed directly and the WER from Whisper V3-Large&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib12\" title=\"\">12</a>]</cite> predictions, using the Nemo toolkit<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib13\" title=\"\">13</a>]</cite> for text normalization.\nThird, the metric estimator model is trained on the triplets of audio (mixture and 2 outputs) to predict the corresponding metrics.</p>\n\n",
                "matched_terms": [
                    "joint",
                    "multiple",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To diversify our training set, three different source separation models are used: TasNet<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib3\" title=\"\">3</a>]</cite> using espnet<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib14\" title=\"\">14</a>]</cite>, DPRNN<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib4\" title=\"\">4</a>]</cite>, and SepFormer<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib5\" title=\"\">5</a>]</cite> using speechbrain <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib15\" title=\"\">15</a>]</cite>.\nEach separator is trained on the reverberant WHAMR! Dataset<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib16\" title=\"\">16</a>]</cite> to perform two-speaker separation and de-reverberation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "set",
                    "whamr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The estimator model performs multi-output regression to estimate a target metric, depending on the training task.\nIt uses a SSL encoder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib19\" title=\"\">19</a>]</cite> to extract <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> features of dimension <math alttext=\"D_{SSL}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>D</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>L</mi></mrow></msub><annotation encoding=\"application/x-tex\">D_{SSL}</annotation></semantics></math> from the mixture <math alttext=\"Y\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mi>Y</mi><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math> and the two separated tracks <math alttext=\"\\hat{S_{1}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mover accent=\"true\"><msub><mi>S</mi><mn>1</mn></msub><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\hat{S_{1}}</annotation></semantics></math> and <math alttext=\"\\hat{S_{2}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><mover accent=\"true\"><msub><mi>S</mi><mn>2</mn></msub><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\hat{S_{2}}</annotation></semantics></math>, then concatenates the features along the feature axis and outputs an array of size <math alttext=\"(T,3\\times D_{SSL})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>T</mi><mo>,</mo><mrow><mn>3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>D</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>L</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(T,3\\times D_{SSL})</annotation></semantics></math>.\nThen, the array is processed by a transformer encoder layer, mean pooled across the time dimension, followed by a linear layer, to predict three continuous values: metric value for source 1, source 2, and their average.\nThe model is trained with a mean squared error (MSE) loss and is evaluated using MAE, and Pearson&#8217;s correlation.\nThis architecture supports reference-free estimation for any signal-level metrics or downstream model value.</p>\n\n",
                "matched_terms": [
                    "feature",
                    "correlation",
                    "mae",
                    "average",
                    "error",
                    "pearson’s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the reverberant max version of WHAMR! <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib16\" title=\"\">16</a>]</cite> which is a far-field speech separation corpus designed to simulate real-world noisy and reverberant conditions. Each mixture comprises two overlapping speakers with additive environmental noise and reverberation applied through simulated room impulse responses. Additionaly, we use\nREAL-M<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib7\" title=\"\">7</a>]</cite>, a real-life speech source separation dataset for two-speaker mixtures.</p>\n\n",
                "matched_terms": [
                    "whamr",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The WHAMR! train and validation splits were used to train and validate both the speech separation models and the metric estimators. The performances of our models were evaluated on the WHAMR! test split and the Real M dataset</p>\n\n",
                "matched_terms": [
                    "test",
                    "models",
                    "estimators",
                    "dataset",
                    "whamr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each sample, the input to the estimator comprised three-channel audio, consisting of the mixture signal along with the two separated sources. Metric scores, computed using ground-truth references, were employed as regression targets. Initially, estimators were trained separately for the SI-SNR and WER prediction tasks, following the same architecture and data processing pipeline outlined in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#S3.SS2\" title=\"3.2 Metric estimator Architecture &#8227; 3 Methods &#8227; REFESS-QI: REFERENCE-FREE EVALUATION FOR SPEECH SEPARATION WITH JOINT QUALITY AND INTELLIGIBILITY SCORING\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "estimators",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our ablation study considered three state-of-the-art self-supervised speech models as feature extractors for metric estimation: Wav2Vec 2.0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib17\" title=\"\">17</a>]</cite>, WavLM<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib18\" title=\"\">18</a>]</cite>, and HuBERT<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib19\" title=\"\">19</a>]</cite>. We conduct experiments with a trainable and frozen FE.</p>\n\n",
                "matched_terms": [
                    "feature",
                    "extractors",
                    "models",
                    "frozen",
                    "trainable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To assess generalization, we evaluated the trained WER estimator on the REAL-M data set <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib7\" title=\"\">7</a>]</cite>, which is a real mixture data set that contains reverberation and noise. REAL-M is for evaluation only, and therefore will be separated using our pretrained models on WHAMR!.</p>\n\n",
                "matched_terms": [
                    "models",
                    "realm",
                    "whamr",
                    "wer",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the SI-SNR estimation task, Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#S6.T3\" title=\"Table 3 &#8227; 6 Conclusion &#8227; REFESS-QI: REFERENCE-FREE EVALUATION FOR SPEECH SEPARATION WITH JOINT QUALITY AND INTELLIGIBILITY SCORING\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows that our method performs better than the SOTA baseline. For all the SSLRs, our methods perform better in terms of PCC and MAE.\nEven when using frozen SSL representations, our method outperforms the baseline, which was using a pre-trained SSL extractor, on both MAE and PCC.\nOur best performances are obtained using WavLM representations.</p>\n\n",
                "matched_terms": [
                    "frozen",
                    "pcc",
                    "mae",
                    "wavlm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The experiments in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#S2.T1\" title=\"Table 1 &#8227; 2.3 Metrics estimators &#8227; 2 Related Works &#8227; REFESS-QI: REFERENCE-FREE EVALUATION FOR SPEECH SEPARATION WITH JOINT QUALITY AND INTELLIGIBILITY SCORING\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> show that this method can predict more than one metric. In terms of WER, that joint estimator achieves similar results than the single metrics estimator, both on the WHAMR! test set and the REAL-M datasets.</p>\n\n",
                "matched_terms": [
                    "test",
                    "realm",
                    "single",
                    "whamr",
                    "wer",
                    "joint",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the out-of-domain evaluation (REAL-M), the WER estimator achieves an MAE of 0.25 and a PCC of 0.548.</p>\n\n",
                "matched_terms": [
                    "realm",
                    "pcc",
                    "mae",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper presents a novel text-free reference-free model designed to estimate separately two metrics: WER and SI-SNR from the outputs of a speech separation system.\nWe proposed a new architecture leveraging self-supervised learning representations that outperformed significantly previous baselines without text nor audio references.\nWe craft our own training dataset from WHAMR! and three different speech separation systems, designed to offer a balanced distribution of WER and SISNR metrics, then train and evaluate our proposed framework on it.\nThe proposed framework demonstrates state of the art performances with different SSL representations, with and without finetuning of the encoder.\nFurthermore, we show our model can generalize to real mixtures when evaluated on the REAL-M dataset, and can work to estimate jointly both the WER and the SI-SNR.\n</p>\n\n",
                "matched_terms": [
                    "realm",
                    "whamr",
                    "dataset",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work has certain limitations, on the speech separation systems, the evaluation data and the applications.\nFirst, our proposed model used the same dataset to train as the speech separation models (the WHAMR! set), and the same set of speech separation models was used to generate the training and the testing data, limiting the potential generalization on new speech separation systems.\nSecond, we adopted a uniform distribution of the scores in the training data, which may not be best suited to align with the out-of-domain data, and we use a unique model to extract all transcripts for the WER evaluation (Whisper).\nFinally, the proposed metric estimator is based on self-supervised learning (SSL) systems, which typically requires large GPUs for inference, thus it will not be easy to use during training, only for evaluation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "dataset",
                    "whamr",
                    "wer",
                    "set"
                ]
            }
        ]
    },
    "S6.T3": {
        "source_file": "REFESS-QI: REFERENCE-FREE EVALUATION FOR SPEECH SEPARATION WITH JOINT QUALITY AND INTELLIGIBILITY SCORING",
        "caption": "Table 3: Pearson’s\ncorrelation (PCC) and mean-absolute error (MAE) for single and average SI-SNR estimation, for multiple Feature Extractors (FE), with FE trainable or frozen, tested on the WHAMR! dataset.",
        "body": "Model\nFE\nFE TR\nHead\nPCC\nMAE\nA.PCC\nA.MAE\n\n\n\nBaseline[8]\n\nW2V2\n✓\nMLP\n.817\n2.01\n.916\n1.70\n\n\n\nBaseline[8]\n\nWavLM\n✓\n.823\n1.616\n.922\n1.14\n\n\n\nBaseline[8]\n\nHuBERT\n✓\n\n.823\n1.937\n.922\n1.63\n\n\nOurs\nW2V2\n✓\nTr. + Lin.\n.947\n1.475\n.952\n1.44\n\n\nOurs\nWavLM\n✓\n.951\n1.388\n.956\n1.34\n\n\nOurs\nHuBERT\n✓\n.949\n1.474\n.955\n1.446\n\n\nOurs\nHuBERT\n✗\n.936\n1.459\n.943\n1.400",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Model</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">FE</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">FE TR</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Head</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">PCC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">MAE</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">A.PCC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">A.MAE</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">Baseline</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib8\" title=\"\">8</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">W2V2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">MLP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">.817</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2.01</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">.916</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.70</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">Baseline</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib8\" title=\"\">8</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">WavLM</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">.823</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.616</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">.922</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">1.14</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">Baseline</span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib8\" title=\"\">8</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">HuBERT</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>\n<td class=\"ltx_td\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"/>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">.823</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.937</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">.922</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.63</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Ours</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">W2V2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" rowspan=\"4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Tr. + Lin.</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">.947</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.475</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">.952</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.44</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Ours</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">WavLM</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">.951</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">1.388</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">.956</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.34</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Ours</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">HuBERT</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">.949</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.474</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">.955</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.446</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Ours</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">HuBERT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">&#10007;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">.936</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.459</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">.943</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">1.400</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "mlp",
            "hubert",
            "ours",
            "estimation",
            "single",
            "whamr",
            "meanabsolute",
            "mae",
            "frozen",
            "trainable",
            "sisnr",
            "extractors",
            "lin",
            "tested",
            "wavlm",
            "apcc",
            "pearson’s",
            "model",
            "baseline8",
            "multiple",
            "dataset",
            "average",
            "w2v2",
            "amae",
            "feature",
            "correlation",
            "head",
            "pcc",
            "error"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">For the SI-SNR estimation task, Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#S6.T3\" title=\"Table 3 &#8227; 6 Conclusion &#8227; REFESS-QI: REFERENCE-FREE EVALUATION FOR SPEECH SEPARATION WITH JOINT QUALITY AND INTELLIGIBILITY SCORING\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows that our method performs better than the SOTA baseline. For all the SSLRs, our methods perform better in terms of PCC and MAE.\nEven when using frozen SSL representations, our method outperforms the baseline, which was using a pre-trained SSL extractor, on both MAE and PCC.\nOur best performances are obtained using WavLM representations.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Source separation is a crucial pre-processing step for various speech processing tasks, such as automatic speech recognition (ASR).\nTraditionally, the evaluation metrics for speech separation rely on the matched reference audios and corresponding transcriptions to assess audio quality and intelligibility.\nHowever, they cannot be used to evaluate real-world mixtures for which no reference exists.\nThis paper introduces a text-free reference-free evaluation framework based on self-supervised learning (SSL) representations.\nThe proposed framework utilize the mixture and separated tracks to predict jointly audio quality, through the Scale Invariant Signal to Noise Ratio (SI-SNR) metric, and speech intelligibility through the Word Error Rate (WER) metric.\nWe conducted experiments on the WHAMR! dataset, which shows a WER estimation with a mean absolute error (MAE) of 17% and a Pearson correlation coefficient (PCC) of 0.77; and SI-SNR estimation with an MAE of 1.38 and PCC of 0.95.\nWe further demonstrate the robustness of our estimator by using various SSL representations.</p>\n\n",
                "matched_terms": [
                    "sisnr",
                    "correlation",
                    "dataset",
                    "estimation",
                    "whamr",
                    "mae",
                    "pcc",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Previous techniques have been proposed to estimate SI-SNR in speech separation and serve as baselines in this study <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib7\" title=\"\">7</a>]</cite> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib8\" title=\"\">8</a>]</cite>. In addition, unsupervised quality estimation for speech enhancement has been explored&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib9\" title=\"\">9</a>]</cite>. This method evaluates a single output of a general enhancement system. Furthermore, a method for predicting the WER for general speech was introduced&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib10\" title=\"\">10</a>]</cite>. However, this method does not specifically predict WER based on learned relationships between representations of noisy audio and enhanced outputs. To the best of our knowledge, this work is the first to predict WER specifically in the context of speech separation.</p>\n\n",
                "matched_terms": [
                    "single",
                    "sisnr",
                    "estimation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper introduces a novel reference-free model designed to estimate various speech separation metrics called ReFESS-QI, utilizing the Hugging Face Transformers framework for unsupervised evaluation. The network processes audio features from both the mixed input and the separated sources to predict metrics such as SI-SNR and WER in a 3-dimensional forward pass. This model outperforms existing SI-SNR estimators and achieves comparable WER baselines without relying on text hypotheses obtained from an ASR model.</p>\n\n",
                "matched_terms": [
                    "sisnr",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Signal-based metrics are computed directly from the waveform by comparing with a reference. SI-SNR is estimated directly from the signal while PESQ is model-based, comparing the clean and processed signals with the aid of a built-in perceptual model to estimate perceived speech quality.</p>\n\n",
                "matched_terms": [
                    "sisnr",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here we go through three related metric estimators baselines.\nIn the SI-SNR baseline <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib8\" title=\"\">8</a>]</cite>, an approach utilizing SSLR models to predict SI-SNR was introduced. This method concatenated temporally the mixture and the two separate audio tracks before feeding them in to the SSLR&#8217;s feature extractor. In contrast, our method extracts features from each audio track separately and learns the relationships between the representations in each time bin before making predictions about the metric. Moreover, the baseline is dependent on fine-tuning of the entire SSL model, which consists of millions of parameters, whereas our method does not require full fine-tuning. This allows us to preserve the knowledge from unsupervised training while only requiring light training.</p>\n\n",
                "matched_terms": [
                    "feature",
                    "sisnr",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The WER baseline <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib9\" title=\"\">9</a>]</cite> is a method that utilizes both audio and text features from the ASR hypothesis. These features are fed into the MLP head that predicts a WER estimate for a single audio.\nThis cascade approach presents efficiency and accuracy issues. The chaining of the results of the ASR model to the text embedding model is one point of failure when the ASR input is not accurate, and the inference of the three models is considered a heavy task.</p>\n\n",
                "matched_terms": [
                    "head",
                    "single",
                    "model",
                    "mlp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section describes our automatic WER estimator method, as well as its extensions as an SI-SNR estimator and a joint estimator, applied to speech separation.\nFirst, we used multiple speech separation systems to extract pairs of separated audios from mixtures.\nSecond, we compute the metrics from the extracted audios. The SI-SNR is computed directly and the WER from Whisper V3-Large&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib12\" title=\"\">12</a>]</cite> predictions, using the Nemo toolkit<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib13\" title=\"\">13</a>]</cite> for text normalization.\nThird, the metric estimator model is trained on the triplets of audio (mixture and 2 outputs) to predict the corresponding metrics.</p>\n\n",
                "matched_terms": [
                    "sisnr",
                    "model",
                    "multiple"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The estimator model performs multi-output regression to estimate a target metric, depending on the training task.\nIt uses a SSL encoder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib19\" title=\"\">19</a>]</cite> to extract <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> features of dimension <math alttext=\"D_{SSL}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>D</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>L</mi></mrow></msub><annotation encoding=\"application/x-tex\">D_{SSL}</annotation></semantics></math> from the mixture <math alttext=\"Y\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mi>Y</mi><annotation encoding=\"application/x-tex\">Y</annotation></semantics></math> and the two separated tracks <math alttext=\"\\hat{S_{1}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mover accent=\"true\"><msub><mi>S</mi><mn>1</mn></msub><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\hat{S_{1}}</annotation></semantics></math> and <math alttext=\"\\hat{S_{2}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><mover accent=\"true\"><msub><mi>S</mi><mn>2</mn></msub><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\hat{S_{2}}</annotation></semantics></math>, then concatenates the features along the feature axis and outputs an array of size <math alttext=\"(T,3\\times D_{SSL})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>T</mi><mo>,</mo><mrow><mn>3</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msub><mi>D</mi><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>L</mi></mrow></msub></mrow><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(T,3\\times D_{SSL})</annotation></semantics></math>.\nThen, the array is processed by a transformer encoder layer, mean pooled across the time dimension, followed by a linear layer, to predict three continuous values: metric value for source 1, source 2, and their average.\nThe model is trained with a mean squared error (MSE) loss and is evaluated using MAE, and Pearson&#8217;s correlation.\nThis architecture supports reference-free estimation for any signal-level metrics or downstream model value.</p>\n\n",
                "matched_terms": [
                    "feature",
                    "model",
                    "correlation",
                    "estimation",
                    "mae",
                    "average",
                    "error",
                    "pearson’s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the reverberant max version of WHAMR! <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib16\" title=\"\">16</a>]</cite> which is a far-field speech separation corpus designed to simulate real-world noisy and reverberant conditions. Each mixture comprises two overlapping speakers with additive environmental noise and reverberation applied through simulated room impulse responses. Additionaly, we use\nREAL-M<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib7\" title=\"\">7</a>]</cite>, a real-life speech source separation dataset for two-speaker mixtures.</p>\n\n",
                "matched_terms": [
                    "whamr",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The WHAMR! train and validation splits were used to train and validate both the speech separation models and the metric estimators. The performances of our models were evaluated on the WHAMR! test split and the Real M dataset</p>\n\n",
                "matched_terms": [
                    "whamr",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the SS systems to create a dataset of Metrics. To ensure a balanced label distribution, we selected three checkpoints from the beginning, middle, and end epochs for dataset generation. ASR was calculated using Whisper V3-Large<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib12\" title=\"\">12</a>]</cite>, and text normalization was carried out with the NeMo toolkit<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib13\" title=\"\">13</a>]</cite>. Additionally, the SISNR was computed using only the reference audio.\nThe distribution of the dataset is described in Table I and Figure 2.</p>\n\n",
                "matched_terms": [
                    "sisnr",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our ablation study considered three state-of-the-art self-supervised speech models as feature extractors for metric estimation: Wav2Vec 2.0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib17\" title=\"\">17</a>]</cite>, WavLM<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib18\" title=\"\">18</a>]</cite>, and HuBERT<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#bib.bib19\" title=\"\">19</a>]</cite>. We conduct experiments with a trainable and frozen FE.</p>\n\n",
                "matched_terms": [
                    "feature",
                    "extractors",
                    "estimation",
                    "frozen",
                    "trainable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In terms of WER estimation, Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#S4.T2\" title=\"Table 2 &#8227; 4.3 Generalization to New Data &#8227; 4 Experiments &#8227; REFESS-QI: REFERENCE-FREE EVALUATION FOR SPEECH SEPARATION WITH JOINT QUALITY AND INTELLIGIBILITY SCORING\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows that our method that lightly trains the FE achieves better results than the baseline without using the Text hypothesis. Our method achieves an MAE of 0.17 and a PCC of 0.775 for the single metrics estimation, and an MAE of 0.14 and PCC of 0.824 for the average metrics estimation, while the baseline achieves an MAE of 0.15 and PCC of 0.746 using an additional ASR model and text embedder for the evaluation. Moreover, our estimation method with freezing the FE achieves competitive results to the baseline, which also uses frozen models, but more pretrained models in inference. We achieve our best results by utilizing Hubert representations. In the generalization experiment using new data from the Real-M dataset, our method exceeded the baseline across all metrics.</p>\n\n",
                "matched_terms": [
                    "model",
                    "hubert",
                    "dataset",
                    "estimation",
                    "single",
                    "pcc",
                    "mae",
                    "average",
                    "frozen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The experiments in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21014v2#S2.T1\" title=\"Table 1 &#8227; 2.3 Metrics estimators &#8227; 2 Related Works &#8227; REFESS-QI: REFERENCE-FREE EVALUATION FOR SPEECH SEPARATION WITH JOINT QUALITY AND INTELLIGIBILITY SCORING\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> show that this method can predict more than one metric. In terms of WER, that joint estimator achieves similar results than the single metrics estimator, both on the WHAMR! test set and the REAL-M datasets.</p>\n\n",
                "matched_terms": [
                    "single",
                    "whamr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the out-of-domain evaluation (REAL-M), the WER estimator achieves an MAE of 0.25 and a PCC of 0.548.</p>\n\n",
                "matched_terms": [
                    "pcc",
                    "mae"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper presents a novel text-free reference-free model designed to estimate separately two metrics: WER and SI-SNR from the outputs of a speech separation system.\nWe proposed a new architecture leveraging self-supervised learning representations that outperformed significantly previous baselines without text nor audio references.\nWe craft our own training dataset from WHAMR! and three different speech separation systems, designed to offer a balanced distribution of WER and SISNR metrics, then train and evaluate our proposed framework on it.\nThe proposed framework demonstrates state of the art performances with different SSL representations, with and without finetuning of the encoder.\nFurthermore, we show our model can generalize to real mixtures when evaluated on the REAL-M dataset, and can work to estimate jointly both the WER and the SI-SNR.\n</p>\n\n",
                "matched_terms": [
                    "sisnr",
                    "model",
                    "whamr",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work has certain limitations, on the speech separation systems, the evaluation data and the applications.\nFirst, our proposed model used the same dataset to train as the speech separation models (the WHAMR! set), and the same set of speech separation models was used to generate the training and the testing data, limiting the potential generalization on new speech separation systems.\nSecond, we adopted a uniform distribution of the scores in the training data, which may not be best suited to align with the out-of-domain data, and we use a unique model to extract all transcripts for the WER evaluation (Whisper).\nFinally, the proposed metric estimator is based on self-supervised learning (SSL) systems, which typically requires large GPUs for inference, thus it will not be easy to use during training, only for evaluation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "whamr",
                    "dataset"
                ]
            }
        ]
    }
}