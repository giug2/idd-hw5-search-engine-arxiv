{
    "S4.T1": {
        "source_file": "Teaching Audio Models to Reason: A Unified Framework for Source- and Layer-wise Distillation",
        "caption": "Table 1: Experimental results",
        "body": "75.38 / 70.36\n\n\n74.17 / 73.30",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-bottom:-1.72218pt;padding-top:1pt;padding-bottom:1pt;\">\n<span class=\"ltx_text ltx_font_bold\">75.38</span> / 70.36</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">74.17 / <span class=\"ltx_text ltx_font_bold\">73.30</span>\n</td>\n</tr>\n</table>\n",
        "informative_terms_identified": [
            "results",
            "experimental"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Experimental results are illustrated in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18579v1#S4.T1\" title=\"Table 1 &#8227; 4.4 Results and analysis &#8227; 4 Experiments &#8227; Teaching Audio Models to Reason: A Unified Framework for Source- and Layer-wise Distillation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. <span class=\"ltx_text ltx_font_italic\">Baseline</span> refers to the results reproduced using the original Qwen2.5-Omni-7B model. We report accuracies for AQA and GID. For SER, we evaluate unweighted accuracy (UA), which averages accuracies over classes (happy, anger, sad, and neutral).</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">While large audio language models excel at tasks like ASR and emotion recognition, they still struggle with complex reasoning due to the modality gap between audio and text as well as the lack of structured intermediate supervision. To address this, we propose a unified knowledge distillation framework to transfer reasoning capabilities from a high-capacity textual teacher model to a student audio models while preserving its acoustic competence. Our method introduces two key dimensions: source-wise distillation, which leverages both textual and acoustic teachers to provide complementary modality-specific supervision; and layer-wise distillation, which aligns teacher signals with appropriate student layers to improve transfer efficiency. This dual-dimensional strategy enables fine-grained control over the distillation process, effectively bridging the gap between symbolic reasoning and speech representations. Experimental results show significant improvements in audio reasoning performance, demonstrating the effectiveness of our framework as a reasoning transfer solution for audio modeling.</p>\n\n",
                "matched_terms": [
                    "results",
                    "experimental"
                ]
            }
        ]
    }
}