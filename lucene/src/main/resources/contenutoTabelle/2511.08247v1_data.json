{
    "p3": {
        "source_file": "ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech",
        "caption": "",
        "body": "Marios Koniaris  , Argyro Tsipi   Panayiotis Tsanakas \n\n\n\n\nNational Technical University of Athens\n\n\nSchool of Electrical and Computer Engineering, Division of Computer Science\n\n\nIroon Polytechniou 9, Zographou Campus, 15780 Athens, Greece\n\n\nEmail: mkoniari@central.ntua.gr, argyrotsipi@gmail.com, panag@cs.ntua.gr",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_top\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:120%;\">Marios Koniaris&#8201;<a class=\"ltx_ref ltx_orcid\" href=\"https://orcid.org/0000-0002-0679-5516\" title=\"ORCID 0000-0002-0679-5516\">&#8201;<svg class=\"ltx_orcidlogo\" height=\"1.7em\" overflow=\"visible\" version=\"1.1\" viewbox=\"0 0 72 72\" width=\"1.7em\"><g transform=\"translate(0,20)\"><path d=\"M72,36 C72,55.884375 55.884375,72 36,72 C16.115625,72 0,55.884375 0,36 C0,16.115625 16.115625,0 36,0 C55.884375,0 72,16.115625 72,36 Z\" fill=\"#A6CE39\" style=\"--ltx-fill-color:#A6CE39;\"/><g fill=\"#FFFFFF\" style=\"--ltx-fill-color:#FFFFFF;\" transform=\"translate(18.868966, 12.910345)\"><polygon points=\"5.03734929 39.1250878 0.695429861 39.1250878 0.695429861 9.14431787 5.03734929 9.14431787 5.03734929 22.6930505 5.03734929 39.1250878\"/><path d=\"M11.409257,9.14431787 L23.1380784,9.14431787 C34.303014,9.14431787 39.2088191,17.0664074 39.2088191,24.1486995 C39.2088191,31.846843 33.1470485,39.1530811 23.1944669,39.1530811 L11.409257,39.1530811 L11.409257,9.14431787 Z M15.7511765,35.2620194 L22.6587756,35.2620194 C32.49858,35.2620194 34.7541226,27.8438084 34.7541226,24.1486995 C34.7541226,18.1301509 30.8915059,13.0353795 22.4332213,13.0353795 L15.7511765,13.0353795 L15.7511765,35.2620194 Z\"/><path d=\"M5.71401206,2.90182329 C5.71401206,4.441452 4.44526937,5.72914146 2.86638958,5.72914146 C1.28750978,5.72914146 0.0187670918,4.441452 0.0187670918,2.90182329 C0.0187670918,1.33420133 1.28750978,0.0745051096 2.86638958,0.0745051096 C4.44526937,0.0745051096 5.71401206,1.36219458 5.71401206,2.90182329 Z\"/></g></g></svg></a>, Argyro Tsipi&#8201; <a class=\"ltx_ref ltx_orcid\" href=\"https://orcid.org/0009-0000-1983-9373%0A\" title=\"ORCID 0009-0000-1983-9373&#10;\"><svg class=\"ltx_orcidlogo\" height=\"1.7em\" overflow=\"visible\" version=\"1.1\" viewbox=\"0 0 72 72\" width=\"1.7em\"><g transform=\"translate(0,20)\"><path d=\"M72,36 C72,55.884375 55.884375,72 36,72 C16.115625,72 0,55.884375 0,36 C0,16.115625 16.115625,0 36,0 C55.884375,0 72,16.115625 72,36 Z\" fill=\"#A6CE39\" style=\"--ltx-fill-color:#A6CE39;\"/><g fill=\"#FFFFFF\" style=\"--ltx-fill-color:#FFFFFF;\" transform=\"translate(18.868966, 12.910345)\"><polygon points=\"5.03734929 39.1250878 0.695429861 39.1250878 0.695429861 9.14431787 5.03734929 9.14431787 5.03734929 22.6930505 5.03734929 39.1250878\"/><path d=\"M11.409257,9.14431787 L23.1380784,9.14431787 C34.303014,9.14431787 39.2088191,17.0664074 39.2088191,24.1486995 C39.2088191,31.846843 33.1470485,39.1530811 23.1944669,39.1530811 L11.409257,39.1530811 L11.409257,9.14431787 Z M15.7511765,35.2620194 L22.6587756,35.2620194 C32.49858,35.2620194 34.7541226,27.8438084 34.7541226,24.1486995 C34.7541226,18.1301509 30.8915059,13.0353795 22.4332213,13.0353795 L15.7511765,13.0353795 L15.7511765,35.2620194 Z\"/><path d=\"M5.71401206,2.90182329 C5.71401206,4.441452 4.44526937,5.72914146 2.86638958,5.72914146 C1.28750978,5.72914146 0.0187670918,4.441452 0.0187670918,2.90182329 C0.0187670918,1.33420133 1.28750978,0.0745051096 2.86638958,0.0745051096 C4.44526937,0.0745051096 5.71401206,1.36219458 5.71401206,2.90182329 Z\"/></g></g></svg></a> Panayiotis Tsanakas&#8201;<a class=\"ltx_ref ltx_orcid\" href=\"https://orcid.org/0000-0002-8503-5784\" title=\"ORCID 0000-0002-8503-5784\"><svg class=\"ltx_orcidlogo\" height=\"1.7em\" overflow=\"visible\" version=\"1.1\" viewbox=\"0 0 72 72\" width=\"1.7em\"><g transform=\"translate(0,20)\"><path d=\"M72,36 C72,55.884375 55.884375,72 36,72 C16.115625,72 0,55.884375 0,36 C0,16.115625 16.115625,0 36,0 C55.884375,0 72,16.115625 72,36 Z\" fill=\"#A6CE39\" style=\"--ltx-fill-color:#A6CE39;\"/><g fill=\"#FFFFFF\" style=\"--ltx-fill-color:#FFFFFF;\" transform=\"translate(18.868966, 12.910345)\"><polygon points=\"5.03734929 39.1250878 0.695429861 39.1250878 0.695429861 9.14431787 5.03734929 9.14431787 5.03734929 22.6930505 5.03734929 39.1250878\"/><path d=\"M11.409257,9.14431787 L23.1380784,9.14431787 C34.303014,9.14431787 39.2088191,17.0664074 39.2088191,24.1486995 C39.2088191,31.846843 33.1470485,39.1530811 23.1944669,39.1530811 L11.409257,39.1530811 L11.409257,9.14431787 Z M15.7511765,35.2620194 L22.6587756,35.2620194 C32.49858,35.2620194 34.7541226,27.8438084 34.7541226,24.1486995 C34.7541226,18.1301509 30.8915059,13.0353795 22.4332213,13.0353795 L15.7511765,13.0353795 L15.7511765,35.2620194 Z\"/><path d=\"M5.71401206,2.90182329 C5.71401206,4.441452 4.44526937,5.72914146 2.86638958,5.72914146 C1.28750978,5.72914146 0.0187670918,4.441452 0.0187670918,2.90182329 C0.0187670918,1.33420133 1.28750978,0.0745051096 2.86638958,0.0745051096 C4.44526937,0.0745051096 5.71401206,1.36219458 5.71401206,2.90182329 Z\"/></g></g></svg></a></span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">National Technical University of Athens</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">School of Electrical and Computer Engineering, Division of Computer Science</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Iroon Polytechniou 9, Zographou Campus, 15780 Athens, Greece</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Email: mkoniari@central.ntua.gr, argyrotsipi@gmail.com, panag@cs.ntua.gr</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "engineering",
            "tsipi",
            "email",
            "panagcsntuagr",
            "tsanakas",
            "greece",
            "national",
            "panayiotis",
            "argyro",
            "iroon",
            "school",
            "technical",
            "university",
            "division",
            "marios",
            "athens",
            "koniaris",
            "computer",
            "argyrotsipigmailcom",
            "zographou",
            "science",
            "polytechniou",
            "electrical",
            "campus",
            "mkoniaricentralntuagr"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Topic Difficulty Analysis</span>. Different topics posed different challenges (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F7\" title=\"Figure 7 &#8227; 6.3. Topic Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>). Science and Geography ranked as most difficult while Finance, Business, and Economics ranked lowest. Technical and natural science domains display higher cross-model disagreement than economic and political topics, consistent with greater terminological specialization and rapidly evolving concepts. In contrast, economic and political discussions employs more stable conceptual frameworks aligned with core parliamentary functions.</p>\n\n",
                "matched_terms": [
                    "science",
                    "technical"
                ]
            }
        ]
    },
    "S4.T1": {
        "source_file": "ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech",
        "caption": "Table 1: Multi-Dimensional Evaluation Framework.",
        "body": "Evaluation Level\nAssessment Category\nComputational Metrics\nLLM-Judge Metrics\n\n\nSpeech\nLinguistic Quality\nPPL, Dist-N, Self-BLEU\nJ_Coh, J_Conc\n\n\nSemantic Coherence\nGRUEN, BERTScore, MoverScore\nJ_Rel\n\n\nPolitical Authenticity\nPSA, Party Align\nJ_Auth, J_PolApp, J_Qual\n\n\nConsistency\nCross-Context Stability\nApplied to all speech evaluation metrics",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding:-0.35pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Evaluation Level</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding:-0.35pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Assessment Category</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding:-0.35pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Computational Metrics</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding:-0.35pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">LLM-Judge Metrics</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"3\" style=\"padding:-0.35pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Speech</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:-0.35pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Linguistic Quality</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:-0.35pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">PPL, Dist-N, Self-BLEU</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:-0.35pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">J_Coh, J_Conc</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:-0.35pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Semantic Coherence</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:-0.35pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">GRUEN, BERTScore, MoverScore</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:-0.35pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">J_Rel</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:-0.35pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Political Authenticity</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:-0.35pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">PSA, Party Align</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:-0.35pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">J_Auth, J_PolApp, J_Qual</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" style=\"padding:-0.35pt 3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Consistency</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" style=\"padding:-0.35pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Cross-Context Stability</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" colspan=\"2\" style=\"padding:-0.35pt 3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Applied to all speech evaluation metrics</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "quality",
            "coherence",
            "political",
            "linguistic",
            "computational",
            "speech",
            "applied",
            "all",
            "metrics",
            "jauth",
            "jpolapp",
            "psa",
            "crosscontext",
            "gruen",
            "bertscore",
            "evaluation",
            "selfbleu",
            "moverscore",
            "align",
            "consistency",
            "jconc",
            "semantic",
            "level",
            "authenticity",
            "llmjudge",
            "stability",
            "category",
            "jqual",
            "assessment",
            "framework",
            "multidimensional",
            "jcoh",
            "jrel",
            "ppl",
            "party",
            "distn"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.T1\" title=\"Table 1 &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows our evaluation structure. We assess three quality dimensions: (i) linguistic quality, (ii) semantic coherence, and (iii) political authenticity using both computational and LLM-judge metrics.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Parliamentary speech generation presents specific challenges for large language models beyond standard text generation tasks. Unlike general text generation, parliamentary speeches require not only linguistic quality but also political authenticity and ideological consistency. Current language models lack specialized training for parliamentary contexts, and existing evaluation methods focus on standard NLP metrics rather than political authenticity. To address this, we present ParliaBench, a benchmark for parliamentary speech generation. We constructed a dataset of speeches from UK Parliament to enable systematic model training. We introduce an evaluation framework combining computational metrics with LLM-as-a-judge assessments for measuring generation quality across three dimensions: linguistic quality, semantic coherence, and political authenticity. We propose two novel embedding-based metrics, Political Spectrum Alignment and Party Alignment, to quantify ideological positioning. We fine-tuned five large language models (LLMs), generated 28k speeches, and evaluated them\nusing our framework, comparing baseline and fine-tuned models. Results show that fine-tuning produces statistically significant improvements across the majority of metrics and our novel metrics demonstrate strong discriminative power for political dimensions.\n\n\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Parliamentary Speech Generation,LLM Evaluation,Political Authenticity,Benchmark Evaluation</p>\n\n",
                "matched_terms": [
                    "speech",
                    "quality",
                    "coherence",
                    "political",
                    "metrics",
                    "evaluation",
                    "linguistic",
                    "framework",
                    "computational",
                    "party",
                    "consistency",
                    "semantic",
                    "authenticity"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "framework",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Democracy thrives through debate. Democratic parliaments are open forums where elected representatives engage in arguments over policy <cite class=\"ltx_cite ltx_citemacro_cite\">Back et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib4\" title=\"\">2021</a>)</cite>. These debates provide unique insights into political reasoning and ideological positioning. Researchers in political science and computational linguistics increasingly seek to understand and model parliamentary debates.</p>\n\n",
                "matched_terms": [
                    "computational",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Yet generating authentic parliamentary speech presents significant challenges that extend well beyond typical text generation tasks. The complexity becomes apparent when considering specific examples. A Labour MP discussing taxation policy must sound distinctly different from a Conservative counterpart not just in policy position, but most significantly, in their fundamental approach to governance. Large language models have shown promise across political applications, from sentiment analysis <cite class=\"ltx_cite ltx_citemacro_cite\">Bestvater and Monroe (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib6\" title=\"\">2023</a>)</cite> and election forecasting <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib22\" title=\"\">2024b</a>)</cite> to synthetic survey data generation <cite class=\"ltx_cite ltx_citemacro_cite\">Argyle et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib3\" title=\"\">2023</a>); Bisbee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib7\" title=\"\">2024</a>)</cite>. Authentic parliamentary speech generation, however, remains challenging.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Evaluation approaches for generated political text fall short for these specialized requirements. Although evaluation methods for text generation tasks have evolved from simple overlap metrics <cite class=\"ltx_cite ltx_citemacro_cite\">Papineni et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib27\" title=\"\">2002</a>); Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib23\" title=\"\">2004</a>)</cite> toward more sophisticated embedding-based approaches <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib37\" title=\"\">2020</a>); Zhao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib39\" title=\"\">2019</a>)</cite>, they still focus on surface-level similarity rather than political authenticity.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "evaluation",
                    "political",
                    "authenticity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Similarly, domain-specific benchmarks have emerged for legislative summarization <cite class=\"ltx_cite ltx_citemacro_cite\">Kornilova and Eidelman (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib19\" title=\"\">2019</a>)</cite>, opinion alignment <cite class=\"ltx_cite ltx_citemacro_cite\">Santurkar et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib31\" title=\"\">2023</a>)</cite>, and other specialized fields <cite class=\"ltx_cite ltx_citemacro_cite\">Xie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib36\" title=\"\">2024</a>); Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib20\" title=\"\">2024a</a>)</cite>. Parliamentary speech generation, however, lacks the evaluation framework needed to assess ideological consistency and parliamentary conventions simultaneously.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "consistency",
                    "framework",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contributions.</span> We address these limitations by establishing a benchmark resource designed\nspecifically for evaluating parliamentary speech generation. First, we develop a curated dataset containing 448k speeches from UK Parliament (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S3\" title=\"3. ParliaBench Dataset &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Second, we introduce a multi-dimensional evaluation framework that assesses both linguistic quality and political authenticity (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4\" title=\"4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). Third, we fine-tune five large language models and generate 28k parliamentary speeches to establish baseline performance (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S5\" title=\"5. Experimental Setup &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). Finally, we demonstrate the framework&#8217;s effectiveness through systematic evaluation (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6\" title=\"6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>). Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S1.F1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides an overview of the complete ParliaBench framework and experimental methodology.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "quality",
                    "political",
                    "evaluation",
                    "linguistic",
                    "framework",
                    "multidimensional",
                    "authenticity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in large language models (LLMs) have enabled a wide range of applications across political domains. <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib22\" title=\"\">2024b</a>)</cite> outlines several applications of LLM in political contexts, covering predictive, generative, and simulation-based approaches. The use of LLMs as substitutes for human experts in annotating political texts across multiple languages is explored in <cite class=\"ltx_cite ltx_citemacro_cite\">Heseltine and von Hohenberg (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib16\" title=\"\">2024</a>)</cite>, while <cite class=\"ltx_cite ltx_citemacro_cite\">Gunes and Florczak (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib14\" title=\"\">2023</a>)</cite> employ LLMs for classifying U.S. Congressional bills. <cite class=\"ltx_cite ltx_citemacro_cite\">Argyle et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib3\" title=\"\">2023</a>)</cite> investigate LLMs as proxies for specific human subpopulations in social science research and <cite class=\"ltx_cite ltx_citemacro_cite\">Bisbee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib7\" title=\"\">2024</a>)</cite> raise concerns about the quality, reliability, and reproducibility of synthetic survey data generated by LLMs.\nAgent-based LLMs are utilized as coalition negotiators <cite class=\"ltx_cite ltx_citemacro_cite\">Moghimifar et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib25\" title=\"\">2024</a>)</cite> and as U.S. senators\nsimulating legislative processes <cite class=\"ltx_cite ltx_citemacro_cite\">Baker and Azher (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib5\" title=\"\">2024</a>)</cite>. However, these approaches predominantly emphasize analytical and simulation capabilities rather than authentic speech generation quality.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "quality",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Traditional text generation evaluation has evolved from reference-based metrics like BLEU <cite class=\"ltx_cite ltx_citemacro_cite\">Papineni et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib27\" title=\"\">2002</a>)</cite> and ROUGE <cite class=\"ltx_cite ltx_citemacro_cite\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib23\" title=\"\">2004</a>)</cite> toward embedding based approaches that better capture semantic similarity. BERTScore <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib37\" title=\"\">2020</a>)</cite> uses contextualized embeddings to compute token-level similarity, while MoverScore <cite class=\"ltx_cite ltx_citemacro_cite\">Zhao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib39\" title=\"\">2019</a>)</cite> measures semantic transportation cost using Earth Mover&#8217;s Distance. For reference-free evaluation, Zhu and Bhat <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu and Bhat (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib41\" title=\"\">2020</a>)</cite> propose GRUEN, assessing grammaticality and semantic coherence. Domain-specific datasets like BillSum <cite class=\"ltx_cite ltx_citemacro_cite\">Kornilova and Eidelman (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib19\" title=\"\">2019</a>)</cite> for legislative summarization and OpinionQA <cite class=\"ltx_cite ltx_citemacro_cite\">Santurkar et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib31\" title=\"\">2023</a>)</cite> for opinion alignment provide targeted evaluation resources, though gaps remain in generative parliamentary speech assessment.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "coherence",
                    "metrics",
                    "evaluation",
                    "assessment",
                    "moverscore",
                    "bertscore",
                    "gruen",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The emergence of LLM-as-a-Judge evaluation <cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib40\" title=\"\">2023</a>)</cite> offers scalable alternatives for nuanced assessment, achieving over 80% agreement with human evaluators in complex judgment tasks. <cite class=\"ltx_cite ltx_citemacro_cite\">Liu and Sun (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib24\" title=\"\">2023</a>)</cite> further validate this approach, demonstrating GPT-4&#8217;s high alignment with human thematic coding in political analysis. This methodology has been successfully applied across diverse contexts, from general LLM benchmarking through competitive debates <cite class=\"ltx_cite ltx_citemacro_cite\">Moniri et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib26\" title=\"\">2025</a>)</cite> to long-context reasoning evaluation in parliamentary debates <cite class=\"ltx_cite ltx_citemacro_cite\">Tiwari et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib34\" title=\"\">2025</a>)</cite>. These approaches demonstrate the viability of automated evaluation for argumentative and political content, though models exhibit documented biases toward Western, educated populations <cite class=\"ltx_cite ltx_citemacro_cite\">Durmus et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib12\" title=\"\">2023</a>)</cite> and systematic preferences in political simulations <cite class=\"ltx_cite ltx_citemacro_cite\">Qi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib29\" title=\"\">2024</a>)</cite>. Recent advances in retrieval-augmented generation and chain-of-thought reasoning provide enhanced capabilities for contextually-grounded political text generation, though their application to parliamentary speech evaluation remains underexplored.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "applied",
                    "political",
                    "evaluation",
                    "assessment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding parliamentary speech data, structured corpora like ParlaMint <cite class=\"ltx_cite ltx_citemacro_cite\">Erjavec et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib13\" title=\"\">2025</a>)</cite> provide multi-lingual parliamentary proceedings. Embedding-based approaches for political analysis, introduced in <cite class=\"ltx_cite ltx_citemacro_cite\">Rheault and Cochrane (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib30\" title=\"\">2020</a>)</cite>, demonstrate that embeddings can capture ideological positioning in parliamentary text.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Domain-specific evaluation frameworks have emerged across professional fields, including FinBen <cite class=\"ltx_cite ltx_citemacro_cite\">Xie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib36\" title=\"\">2024</a>)</cite> and LexEval <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib20\" title=\"\">2024a</a>)</cite>. Political science applications have developed specialized benchmarks for election prediction and legislative analysis, yet these focus primarily on classification and analysis tasks. Parliamentary speech generation has attracted recent computational interest, with work exploring European Parliament consensus building <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib38\" title=\"\">2025</a>)</cite> and political impersonation authenticity <cite class=\"ltx_cite ltx_citemacro_cite\">Herbold et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib15\" title=\"\">2024</a>)</cite>, but evaluation frameworks remain underdeveloped. Existing approaches focus on narrow aspects like style mimicry rather than systematic quality assessment across linguistic and political authenticity dimensions that parliamentary speech generation requires.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "quality",
                    "political",
                    "evaluation",
                    "linguistic",
                    "assessment",
                    "computational",
                    "authenticity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 2: Metadata Extraction &amp; Temporal Alignment</span>. Enriched speeches with speaker identity, political affiliation, chamber designation, and session dates. As parliamentary speakers frequently change affiliations and roles during their careers, we employed temporal alignment by cross-referencing speech dates with affiliation histories from corpus metadata.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 3: Content Processing &amp; Filtering</span>. Distinguished substantive content from procedural elements, separating parliamentary prompts from speech responses. Filtered procedural noise and non-substantive speeches. For political affiliations, we applied 1000-speech minimum threshold, reducing from 28 original affiliations to 11 to ensure stable model training.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "applied",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 4: Thematic Classification</span>. While ParlaMint uses CAP classification, we selected EuroVoc <cite class=\"ltx_cite ltx_citemacro_cite\">Publications Office of the European Union (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib28\" title=\"\">2025</a>)</cite> as the standard classification system for European parliamentary systems. For policy domains with clear semantic correspondence between CAP and EuroVoc taxonomies, we applied direct mapping rules. For semantically complex or ambiguous categories, we employed the methodology provided by <cite class=\"ltx_cite ltx_citemacro_cite\">Bocchi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib8\" title=\"\">2024</a>)</cite>. We argue that this approach is particularly well-suited for our dataset because it was specifically designed for legal and governmental texts. For speeches yielding multiple concepts, we selected the highest individual concept score. (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S9.SS2\" title=\"9.2. Hybrid Classification Strategy &#8227; 9. Dataset Details &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">9.2</span></a> documents the hybrid classification strategy)</p>\n\n",
                "matched_terms": [
                    "applied",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S3.F2\" title=\"Figure 2 &#8227; 3.2. Statistics &#8227; 3. ParliaBench Dataset &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates key dataset characteristics. Panel (a) reveals a highly right-skewed distribution of speech lengths, indicating that the dataset is dominated by relatively short speeches while containing a smaller number of substantially longer ones. Panel (b) presents topic distribution showing that speeches most frequently address International Relations, Law, and Politics, which together account for over 45% of the corpus. Panel (c) demonstrates ideological balance across the political spectrum. While most parties by count are in the centre-left to left spectrum, the centre-right to right spectrum produces a larger amount of speeches by 110,000. Panel (d) displays temporal patterns and institutional differences. The House of Commons consistently produces 3-4x more speeches than the House of Lords across 2015-2022.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generated Dataset.</span> We release 27,560 speeches (model outputs) produced during evaluation (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S5\" title=\"5. Experimental Setup &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). The generated dataset follows the same format with additional fields: <span class=\"ltx_text ltx_font_italic\">Model</span> (architecture identifier), <span class=\"ltx_text ltx_font_italic\">Type</span> (baseline/fine-tuned), <span class=\"ltx_text ltx_font_italic\">Generated Speech</span> (model output), and <span class=\"ltx_text ltx_font_italic\">Evaluation Scores</span> (computed metrics). To ensure a fair comparison, we used the same input prompts across all models and model types, allowing evaluation on identical inputs.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "all",
                    "metrics",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Parliamentary speech evaluation requires assessment across multiple levels that generic benchmarks cannot capture. Our framework operates on two levels: (i) <span class=\"ltx_text ltx_font_bold\">speech evaluation metrics</span> measuring generation quality across three dimensions, and (ii) <span class=\"ltx_text ltx_font_bold\">consistency evaluation metrics</span> measuring performance reliability across political contexts. This dual-track approach also combines computational metrics with LLM-judge evaluation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "llmjudge",
                    "quality",
                    "political",
                    "metrics",
                    "evaluation",
                    "assessment",
                    "framework",
                    "computational",
                    "consistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our framework introduces two novel computational metrics (Political Spectrum Alignment and Party Alignment, Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.SS1.SSS1\" title=\"4.1.1. Novel Political Authenticity Metrics &#8227; 4.1. Speech Evaluation Metrics &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4.1.1</span></a>) alongside established metrics from both computational and LLM-judge traditions.</p>\n\n",
                "matched_terms": [
                    "llmjudge",
                    "political",
                    "metrics",
                    "framework",
                    "computational",
                    "party"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Computational metrics provide deterministic assessment through established NLP measures, while LLM-judge metrics capture nuanced qualities requiring contextual understanding. For LLM-judge evaluation, we adapt the methodology from <cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib40\" title=\"\">2023</a>)</cite>. We employed Flow-Judge-v0.1 as our LLM judge, an LLM specialized in system evaluation tasks. It inherits it&#8217;s architecture from Phi-3.5-mini instruct <cite class=\"ltx_cite ltx_citemacro_cite\">Abdin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib1\" title=\"\">2024</a>)</cite>, thus ensuring complete architectural and training data independence from the evaluated models. Our judge rates each speech on a 1-10 scale with written explanations across six parliamentary specific dimensions. We acknowledge this introduces bias through single model judgment. We employ consistent prompt formatting and evaluate speeches sampled across diverse political contexts. Specific prompts for each dimension are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S10\" title=\"10. LLM-as-a-Judge Evaluation Prompts &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "llmjudge",
                    "political",
                    "metrics",
                    "evaluation",
                    "assessment",
                    "computational"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Linguistic Quality</span>\n</p>\n\n",
                "matched_terms": [
                    "quality",
                    "linguistic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Computational Metrics</span>\n</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "computational"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Perplexity</span> (PPL)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jelinek et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib17\" title=\"\">1977</a>)</cite> measures text naturalness. We compute perplexity using GPT-2 as a fixed reference model across all generated speeches, ensuring cross-model comparability. Lower scores indicate more natural-sounding text according to GPT-2&#8217;s language distribution. (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i1.I1.i1.I1.i1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> better)</p>\n\n",
                "matched_terms": [
                    "all",
                    "ppl"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">LLM-Judge Metrics</span>\n</p>\n\n",
                "matched_terms": [
                    "llmjudge",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Coherence</span> (J_Coh) evaluates logical argument flow and structural organization (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i1.I1.i2.I1.i1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> better).</p>\n\n",
                "matched_terms": [
                    "jcoh",
                    "coherence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conciseness</span> (J_Conc) assesses communication efficiency, drawing from debate evaluation criteria&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Moniri et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib26\" title=\"\">2025</a>)</cite> (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i1.I1.i2.I1.i2.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> better).</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "jconc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Semantic Coherence</span>\n</p>\n\n",
                "matched_terms": [
                    "semantic",
                    "coherence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Computational Metrics</span>\n</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "computational"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">GRUEN</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zhu and Bhat (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib41\" title=\"\">2020</a>)</cite> evaluates linguistic quality through grammatical correctness and semantic coherence (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.I1.i1.I1.i1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> better), aggregating four quality dimensions: grammaticality, non-redundancy, focus, and structural coherence.</p>\n\n",
                "matched_terms": [
                    "quality",
                    "coherence",
                    "linguistic",
                    "gruen",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">BERTScore</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib37\" title=\"\">2020</a>)</cite> measures semantic similarity using RoBERTa-large embeddings (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.I1.i1.I1.i2.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> better), reporting F1-score to balance precision and recall.</p>\n\n",
                "matched_terms": [
                    "bertscore",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MoverScore</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zhao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib39\" title=\"\">2019</a>)</cite> quantifies semantic distance via Earth Mover&#8217;s Distance (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.I1.i1.I1.i3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> better).</p>\n\n",
                "matched_terms": [
                    "moverscore",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Note:</span> For both BERTScore and MoverScore, generated speeches are compared against the top-5 human speeches from the training set matching the same context.</p>\n\n",
                "matched_terms": [
                    "moverscore",
                    "bertscore"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">LLM-Judge Metrics</span>\n</p>\n\n",
                "matched_terms": [
                    "llmjudge",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Political Authenticity</span>\n</p>\n\n",
                "matched_terms": [
                    "political",
                    "authenticity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Computational Metrics</span>\n</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "computational"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ two novel embedding-based metrics for political authenticity assessment: Political Spectrum Alignment (PSA) and Party Alignment (Party Align). Detailed calculation methodology provided in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.SS1.SSS1\" title=\"4.1.1. Novel Political Authenticity Metrics &#8227; 4.1. Speech Evaluation Metrics &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4.1.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "political",
                    "metrics",
                    "assessment",
                    "party",
                    "psa",
                    "align",
                    "authenticity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">LLM-Judge Metrics</span>\n</p>\n\n",
                "matched_terms": [
                    "llmjudge",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Authenticity</span> (J_Auth) assesses whether content reflects genuine political speech (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i3.I1.i2.I1.i1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> better).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "jauth",
                    "political",
                    "authenticity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Political Appropriateness</span> (J_PolApp) evaluates whether tone is suitable for political speech (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i3.I1.i2.I1.i2.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> better).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "jpolapp",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall Quality</span> (J_Qual) assesses sophistication, persuasiveness, and communicative effectiveness (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i3.I1.i2.I1.i3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> better).</p>\n\n",
                "matched_terms": [
                    "jqual",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Political Spectrum Alignment</span> (PSA) evaluates ideological positioning on the left-right spectrum. We adapt semantic embedding approaches <cite class=\"ltx_cite ltx_citemacro_cite\">Rheault and Cochrane (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib30\" title=\"\">2020</a>)</cite> for LLM-generated speech evaluation, drawing on\nthe Left-Right (RILE) scale methodology <cite class=\"ltx_cite ltx_citemacro_cite\">Volkens et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib35\" title=\"\">2013</a>); Budge (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib9\" title=\"\">2013</a>)</cite>. Our metric employs a two-stage approach combining semantic similarity with ideological distance. We create reference embeddings by grouping parliamentary speeches by political orientations (Far-left through Far-right, including intermediate positions) and computing centroid embeddings using sentence transformers. Orientations map to numerical values where Far-left = -6, Centre = 0, Far-right = +6.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "political",
                    "evaluation",
                    "psa",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"po^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p4.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>o</mi><mo>&#8727;</mo></msup></mrow><annotation encoding=\"application/x-tex\">po^{*}</annotation></semantics></math> is the closest matching orientation, <math alttext=\"\\mathcal{PO}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p4.m2\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119978;</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{PO}</annotation></semantics></math>\nthe set of all political orientations, <math alttext=\"c_{po}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p4.m3\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{po}</annotation></semantics></math> the orientation centroid, and <math alttext=\"\\text{sim}(s,c_{po})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p4.m4\" intent=\":literal\"><semantics><mrow><mtext>sim</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo>,</mo><msub><mi>c</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{sim}(s,c_{po})</annotation></semantics></math> the cosine similarity between generated speech <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p4.m5\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> and centroid.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "all",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The PSA score combines semantic similarity with orientation distance:</p>\n\n",
                "matched_terms": [
                    "psa",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Party Alignment</span> (Party Align) applies the same embedding methodology to party-specific alignment, using party affiliation rather than political orientation for centroid construction. The alignment score measures cosine similarity between generated speech and expected party centroid:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "align",
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Context Stability</span> measures performance consistency using coefficient of variation, with higher scores indicating more consistent performance across political contexts. This meta-evaluation applies to all speech evaluation metrics, providing diagnostic insight into model reliability. For cross-metric comparison, all metrics, Computational &amp; LLM-Judge, are normalized to 0-1 scale.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "stability",
                    "llmjudge",
                    "political",
                    "all",
                    "metrics",
                    "evaluation",
                    "computational",
                    "crosscontext",
                    "consistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We examine consistency across three dimensions: political parties, topic domains, and political orientations. The stability calculation quantifies performance variability:</p>\n\n",
                "matched_terms": [
                    "stability",
                    "consistency",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed Quantized Low-Rank Adaptation (QLoRA) <cite class=\"ltx_cite ltx_citemacro_cite\">Dettmers et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib10\" title=\"\">2023</a>)</cite> for parameter-efficient fine-tuning. (<math alttext=\"r=16\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">r=16</annotation></semantics></math>, <math alttext=\"\\alpha=16\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=16</annotation></semantics></math>, <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math> epochs). Model-specific chat templates structure training inputs with political metadata (party affiliation, topic classification, orientation, section, and house). Training used SFTTrainer from TRL with 80%-20% train-test splits and automated checkpointing. Fine-tuned models were saved with adapter weights for subsequent evaluation, ensuring consistent model states across experiments.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We planned to generate 30,000 speeches (3,000 per model-type combination across 10 models) using stratified sampling from the held-out test set. To maintain consistency with training conditions, our prompt distribution matches the ParliaBench Dataset structure: 90% generic instruction prompts formatted with political context (party, topic, orientation, section, house) and 10% specific parliamentary questions from the test set.</p>\n\n",
                "matched_terms": [
                    "consistency",
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generation employed nucleus sampling (<math alttext=\"temperature=0.7\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>=</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">temperature=0.7</annotation></semantics></math>, <math alttext=\"top\\-p=0.85\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><mo>=</mo><mn>0.85</mn></mrow><annotation encoding=\"application/x-tex\">top\\-p=0.85</annotation></semantics></math>, <math alttext=\"repetition_{p}enalty=1.2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>n</mi><mi>p</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>y</mi></mrow><mo>=</mo><mn>1.2</mn></mrow><annotation encoding=\"application/x-tex\">repetition_{p}enalty=1.2</annotation></semantics></math>). Generated speeches underwent validation for template leakage, encoding corruption, semantic relevance, and length constraints. Invalid outputs were automatically regenerated (max 3 attempts). Baseline models exhibited higher failure rates, suggesting fine-tuning improved output quality. To ensure fair cross-model comparison, we retained only speeches successfully generated by all 10 model-type combinations, yielding 29,220 speeches.</p>\n\n",
                "matched_terms": [
                    "all",
                    "quality",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generated speeches were then evaluated using our LLM-judge framework. Speeches with incomplete dimension ratings were excluded, resulting in 27,560 fully evaluated speeches, across all six dimensions, for all models. All subsequent results use the 27,560 fully evaluated speeches, which maintain balanced representation across political affiliations and topics. Complete implementation details, including QLoRA configuration, training parameters, chat templates for all architectures, and speech generation validation methodology, are provided in the Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S11\" title=\"11. Setup Implementation Details &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "llmjudge",
                    "political",
                    "all",
                    "framework"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated 27,560 generated speeches using our evaluation framework. This section presents fine-tuning effectiveness and performance patterns across political parties, topic domains, and ideological orientations. Representative examples of generated speeches are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S12\" title=\"12. Representative Generated Speeches &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>.</p>\n\n",
                "matched_terms": [
                    "framework",
                    "evaluation",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.T2\" title=\"Table 2 &#8227; 6.1. Overview and Fine-Tuning Impact &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents metric results organized by our framework assessment categories. Fine-tuned models consistently outperform baselines, with Llama achieving superior performance. Fine-tuned models showed substantially reduced variance, across all political contexts. Extended context windows (128k tokens) and larger vocabularies contribute to architectural advantages.\n</p>\n\n",
                "matched_terms": [
                    "framework",
                    "assessment",
                    "all",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, our novel political authenticity metrics (PSA and Party Align) displayed strong responsiveness to fine-tuning. All five models significantly improved PSA (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>), with effect sizes ranging from small to very large (d=0.141-1.045). Party Align showed similar patterns (4 of 5 models improved, d=0.099-1.221). These substantial effects validate that our embedding based metrics capture critical political authenticity dimensions unavailable to conventional evaluation.\nFor complete t-test results including effect sizes, see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T13\" title=\"Table 13 &#8227; 13.5. Statistical Significance Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "political",
                    "all",
                    "metrics",
                    "evaluation",
                    "psa",
                    "party",
                    "align",
                    "authenticity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F3\" title=\"Figure 3 &#8227; 6.1. Overview and Fine-Tuning Impact &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows fine-tuning impact across evaluation categories. <span class=\"ltx_text ltx_font_italic\">YI</span> achieved the strongest improvements across all dimensions , while <span class=\"ltx_text ltx_font_italic\">Llama</span> had consistent gains. <span class=\"ltx_text ltx_font_italic\">Gemma2</span> and <span class=\"ltx_text ltx_font_italic\">Qwen2</span> exhibited quality trade-offs, with improvements in one category accompanied by declines in others, suggesting architectural differences in how models balance competing objectives during fine-tuning. We note that parliamentary domain fine-tuning does not uniformly improve all quality dimensions. Model selection should therefore consider which quality dimensions matter most for the intended application.</p>\n\n",
                "matched_terms": [
                    "category",
                    "all",
                    "evaluation",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-context stability analysis (Eq.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.E4\" title=\"In 4.2. Consistency Evaluation Metrics &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) revealed fine-tuned models maintained consistent performance across political contexts (composite stability 91.4-96.2). Mistral achieved highest consistency (96.2) despite trade-offs in absolute performance, while Llama (95.1) balanced strong performance with stability.\nDetailed scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T12\" title=\"Table 12 &#8227; 13.4. Cross-Context Stability Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "consistency",
                    "stability",
                    "political",
                    "crosscontext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both new political authenticity metrics (PSA and Party Align) successfully discriminate their target political dimensions. Party Align distinguishes parties while PSA distinguishes orientations (both <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>). Our analysis reveals that Party Align performance depends primarily on data abundance and ideological coherence rather than party size alone. Models successfully learn party-specific language patterns when training data provides clear stylistic signals, indicating targeted data collection for under-represented parties could improve coverage.</p>\n\n",
                "matched_terms": [
                    "coherence",
                    "political",
                    "metrics",
                    "psa",
                    "party",
                    "align",
                    "authenticity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Party alignment Difficulty Analysis</span>. Applying cross-context stability analysis (Eq.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.E4\" title=\"In 4.2. Consistency Evaluation Metrics &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), party difficulty scores ranged narrowly (0.382-0.456), with no statistically significant differences. This suggests relatively consistent modeling challenges across parties regardless of size or ideological composition. Results are presented in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F5\" title=\"Figure 5 &#8227; 6.2. Political Context Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Detailed scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T9\" title=\"Table 9 &#8227; 13.2. Metric Validation: Political Discrimination Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "stability",
                    "party",
                    "crosscontext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results establish several key findings: (i) Architectural design impacts political authenticity, with extended context windows enabling consistent improvements; (ii) Domain-specific fine-tuning proves essential as 45 of 70 metric comparisons showed statistically significant improvements and (iii) Novel political authenticity metrics (PSA, Party Align) successfully capture dimensions unavailable to conventional NLP metrics, validated through both fine-tuning responsiveness and discrimination testing (both <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "political",
                    "metrics",
                    "psa",
                    "party",
                    "align",
                    "authenticity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To the best of our knowledge, ParliaBench represents the first benchmark resource addressing the specialized challenges of parliamentary speech generation, comprising dataset, evaluation framework, novel metrics, and baseline benchmarks. ParliaBench provides standardized evaluation protocols and baseline performance results that\nsupport systematic comparison and reproducible research in the field.\nOur results demonstrate that domain specific fine-tuning produces significant quality improvements, while\nour novel political authenticity metrics successfully capture ideological dimensions absent from conventional evaluation approaches.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "quality",
                    "political",
                    "metrics",
                    "evaluation",
                    "framework",
                    "authenticity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Future Directions:</span> Extensions include: (i) multilingual evaluation for European parliamentary systems, (ii) human evaluation protocols for validation, and (iii) systematic assessment of political bias and perspective maintenance across viewpoints.</p>\n\n",
                "matched_terms": [
                    "assessment",
                    "evaluation",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation measures linguistic quality and political authenticity but does not assess argument structure or verify factual accuracy against parliamentary records. Our methods rely entirely on automated metrics without human validation, and LLM-as-a-Judge approaches may carry inherent biases.\nThis work is intended for research and educational purposes, not deployment in actual democratic processes.</p>\n\n",
                "matched_terms": [
                    "quality",
                    "political",
                    "metrics",
                    "evaluation",
                    "linguistic",
                    "authenticity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work establishes a benchmark resource for evaluating LLM-generated parliamentary speech in research and educational contexts. These resources should only be used as assistance to human experts with consideration of their limitations and biases. The parliamentary data is derived from publicly available UK parliamentary proceedings (ParlaMint corpus) licensed under Creative Commons Attribution 4.0 International, which our derived datasets maintain.\nGenerated parliamentary speeches must be clearly identified as AI-generated content and not misrepresented as authentic political speeches from actual parliamentarians.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This subsection documents the hybrid classification strategy employed to map Comparative Agendas Project (CAP) categories to EuroVoc domains for topic assignment in our parliamentary speech dataset. We employed direct semantic mapping for 16 categories, while 6 remaining categories required automated classification, as detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S9.T4\" title=\"Table 4 &#8227; 9.2. Hybrid Classification Strategy &#8227; 9. Dataset Details &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This appendix documents the automated evaluation system used to assess the quality of generated parliamentary speeches. The system employs Flow-Judge-v0.1, a 3.8B parameter evaluation model, to score speeches across six dimensions using a 10-point scale.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All speeches are evaluated on six metrics:</p>\n\n",
                "matched_terms": [
                    "all",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Coherence (j_coh)</span>: Logical flow and structural clarity</p>\n\n",
                "matched_terms": [
                    "jcoh",
                    "coherence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Authenticity (j_auth)</span>: Naturalness of Westminster discourse</p>\n\n",
                "matched_terms": [
                    "jauth",
                    "authenticity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Political Appropriateness (j_polapp)</span>: Alignment with party positions</p>\n\n",
                "matched_terms": [
                    "jpolapp",
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall Quality (j_qual)</span>: Persuasiveness and argumentation strength</p>\n\n",
                "matched_terms": [
                    "jqual",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each metric uses a structured prompt template with explicit evaluation criteria, a 10-point scoring rubric, and detailed instructions for the judge model. The system processes speeches in batches of 32 for computational efficiency.</p>\n\n",
                "matched_terms": [
                    "computational",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Does the speech directly address the prompt/question and cover all core concerns specified in the instruction?</p>\n\n",
                "matched_terms": [
                    "speech",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The QLoRA configuration parameters were selected based on established best practices for parameter-efficient fine-tuning in specialized domains. The rank value of 16 provides sufficient adaptation capacity while maintaining computational efficiency. Setting LoRA Alpha equal to the rank ensures reliable baseline performance, while disabling dropout enables Unsloth framework optimizations essential for efficient training on A100 hardware.</p>\n\n",
                "matched_terms": [
                    "framework",
                    "computational"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Target modules encompass all linear transformation layers (q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj) to achieve performance comparable to full fine-tuning while requiring only a fraction of the computational resources. The consistent random state across all architectures ensures reproducible results essential for systematic model comparison.</p>\n\n",
                "matched_terms": [
                    "all",
                    "computational"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_typewriter\">You are a seasoned UK parliamentary member.\nUse proper British parliamentary language appropriate for the specified House.\n<br class=\"ltx_break\"/>The speech should reflect the political orientation and typical positions of the specified party on the given topic.\n<br class=\"ltx_break\"/></span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">You are a seasoned UK parliamentary member. Generate a coherent speech of\nmin_words - max_words words in standard English (no Unicode artifacts, no special characters).</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_typewriter\">\nUse proper British parliamentary language appropriate for the specified House. \n<br class=\"ltx_break\"/>The speech should reflect the political orientation and typical positions of the specified party on the given topic. \n<br class=\"ltx_break\"/></span></p>\n\n",
                "matched_terms": [
                    "speech",
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PARTY:</span> Political party affiliation (e.g., Conservative)</p>\n\n",
                "matched_terms": [
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T9\" title=\"Table 9 &#8227; 13.2. Metric Validation: Political Discrimination Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T10\" title=\"Table 10 &#8227; 13.2. Metric Validation: Political Discrimination Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> present ANOVA results validating the discriminative power of the novel political authenticity metrics.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "political",
                    "authenticity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T12\" title=\"Table 12 &#8227; 13.4. Cross-Context Stability Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> presents cross-context stability scores. Fine-tuned models maintain high consistency across political contexts (91.4-96.2), with Mistral achieving highest overall stability (96.2).</p>\n\n",
                "matched_terms": [
                    "stability",
                    "consistency",
                    "political",
                    "crosscontext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T13\" title=\"Table 13 &#8227; 13.5. Statistical Significance Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> presents complete pairwise t-test results comparing baseline and fine-tuned models across all evaluation metrics, including p-values, effect sizes, and significance after Bonferroni correction.</p>\n\n",
                "matched_terms": [
                    "all",
                    "metrics",
                    "evaluation"
                ]
            }
        ]
    },
    "S6.T2": {
        "source_file": "ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech",
        "caption": "Table 2: Complete Performance Comparison between Baseline (B) and Fine-Tuned (F) Models across Evaluation Metrics. Asterisks (*) indicate statistically significant improvements (p < 0.05).",
        "body": "Model\nLinguistic Quality\nSemantic Coherence\nPolitical Authenticity\n\n\n\n\nPPL ↓\\downarrow\n\n\nDist-N ↑\\uparrow\n\n\nSelf-BLEU ↓\\downarrow\n\n\nJ_Coh ↑\\uparrow\n\n\nJ_Conc ↑\\uparrow\n\n\nGRUEN ↑\\uparrow\n\n\nBERTScore ↑\\uparrow\n\n\nMoverScore ↑\\uparrow\n\n\nJ_Rel ↑\\uparrow\n\n\nPSA ↑\\uparrow\n\n\nParty Align ↑\\uparrow\n\n\nJ_Auth ↑\\uparrow\n\n\nJ_PolApp ↑\\uparrow\n\n\nJ_Qual ↑\\uparrow\n\n\n\n\n\nLlama 3.1 8B (B)\n61.150 ±35.890\\pm 35.890\n\n0.988 ±0.019\\pm 0.019\n\n0.006 ±0.003\\pm 0.003\n\n0.591 ±0.085\\pm 0.085\n\n0.799 ±0.014\\pm 0.014\n\n0.504 ±0.007\\pm 0.007\n\n0.397 ±0.141\\pm 0.141\n\n0.502 ±0.134\\pm 0.134\n\n7.043 ±2.481\\pm 2.481\n\n5.889 ±3.101\\pm 3.101\n\n5.518 ±2.948\\pm 2.948\n\n4.364 ±2.647\\pm 2.647\n\n6.184 ±2.585\\pm 2.585\n\n4.796 ±2.403\\pm 2.403\n\n\n\nLlama 3.1 8B (F)\n31.623∗ ±8.251\\pm 8.251\n\n0.974 ±0.018\\pm 0.018\n\n0.018 ±0.011\\pm 0.011\n\n0.537 ±0.146\\pm 0.146\n\n0.814∗ ±0.010\\pm 0.010\n\n0.509∗ ±0.006\\pm 0.006\n\n0.488∗ ±0.139\\pm 0.139\n\n0.576∗ ±0.107\\pm 0.107\n\n7.902∗ ±1.142\\pm 1.142\n\n7.125∗ ±2.608\\pm 2.608\n\n6.186∗ ±2.543\\pm 2.543\n\n6.133∗ ±3.001\\pm 3.001\n\n7.277∗ ±1.577\\pm 1.577\n\n5.415∗ ±2.235\\pm 2.235\n\n\n\nGemma 2 9B (B)\n89.783 ±47.619\\pm 47.619\n\n0.992 ±0.007\\pm 0.007\n\n0.008 ±0.004\\pm 0.004\n\n0.579 ±0.074\\pm 0.074\n\n0.800 ±0.016\\pm 0.016\n\n0.508 ±0.007\\pm 0.007\n\n0.443 ±0.142\\pm 0.142\n\n0.542 ±0.118\\pm 0.118\n\n7.782 ±1.312\\pm 1.312\n\n4.756 ±3.361\\pm 3.361\n\n5.822 ±2.473\\pm 2.473\n\n3.842 ±2.357\\pm 2.357\n\n6.513 ±2.044\\pm 2.044\n\n4.469 ±1.987\\pm 1.987\n\n\n\nGemma 2 9B (F)\n102.382 ±60.592\\pm 60.592\n\n0.990 ±0.021\\pm 0.021\n\n0.010 ±0.006\\pm 0.006\n\n0.532 ±0.078\\pm 0.078\n\n0.799 ±0.018\\pm 0.018\n\n0.508 ±0.006\\pm 0.006\n\n0.497∗ ±0.137\\pm 0.137\n\n0.589∗ ±0.102\\pm 0.102\n\n7.502 ±1.487\\pm 1.487\n\n4.970 ±3.489\\pm 3.489\n\n5.598 ±2.463\\pm 2.463\n\n4.215∗ ±2.529\\pm 2.529\n\n7.314∗ ±1.617\\pm 1.617\n\n4.976∗ ±1.915\\pm 1.915\n\n\n\nMistral 7B v0.3 (B)\n31.428 ±18.844\\pm 18.844\n\n0.966 ±0.052\\pm 0.052\n\n0.008 ±0.005\\pm 0.005\n\n0.598 ±0.108\\pm 0.108\n\n0.806 ±0.012\\pm 0.012\n\n0.505 ±0.007\\pm 0.007\n\n0.417 ±0.146\\pm 0.146\n\n0.522 ±0.131\\pm 0.131\n\n6.582 ±2.963\\pm 2.963\n\n6.850 ±3.212\\pm 3.212\n\n5.428 ±3.184\\pm 3.184\n\n4.234 ±2.780\\pm 2.780\n\n5.627 ±2.732\\pm 2.732\n\n4.182 ±2.647\\pm 2.647\n\n\n\nMistral 7B v0.3 (F)\n29.379∗ ±10.852\\pm 10.852\n\n0.972∗ ±0.019\\pm 0.019\n\n0.017 ±0.012\\pm 0.012\n\n0.590 ±0.082\\pm 0.082\n\n0.820∗ ±0.009\\pm 0.009\n\n0.506∗ ±0.006\\pm 0.006\n\n0.437∗ ±0.139\\pm 0.139\n\n0.506 ±0.122\\pm 0.122\n\n7.954∗ ±1.085\\pm 1.085\n\n8.942∗ ±2.051\\pm 2.051\n\n5.744∗ ±2.904\\pm 2.904\n\n3.986 ±2.735\\pm 2.735\n\n6.388∗ ±2.097\\pm 2.097\n\n3.745 ±2.089\\pm 2.089\n\n\n\nQwen2 7B (B)\n44.927 ±21.739\\pm 21.739\n\n0.981 ±0.026\\pm 0.026\n\n0.020 ±0.014\\pm 0.014\n\n0.534 ±0.142\\pm 0.142\n\n0.799 ±0.009\\pm 0.009\n\n0.507 ±0.006\\pm 0.006\n\n0.444 ±0.140\\pm 0.140\n\n0.562 ±0.114\\pm 0.114\n\n7.919 ±1.222\\pm 1.222\n\n5.937 ±1.652\\pm 1.652\n\n6.938 ±2.345\\pm 2.345\n\n6.558 ±2.583\\pm 2.583\n\n7.324 ±1.751\\pm 1.751\n\n6.369 ±1.627\\pm 1.627\n\n\n\nQwen2 7B (F)\n36.040∗ ±9.144\\pm 9.144\n\n0.982 ±0.013\\pm 0.013\n\n0.017∗ ±0.009\\pm 0.009\n\n0.570∗ ±0.086\\pm 0.086\n\n0.815∗ ±0.009\\pm 0.009\n\n0.510∗ ±0.006\\pm 0.006\n\n0.489∗ ±0.135\\pm 0.135\n\n0.573∗ ±0.106\\pm 0.106\n\n8.056∗ ±0.887\\pm 0.887\n\n7.617∗ ±2.697\\pm 2.697\n\n6.081 ±2.641\\pm 2.641\n\n5.718 ±3.017\\pm 3.017\n\n7.146 ±1.617\\pm 1.617\n\n5.015 ±2.100\\pm 2.100\n\n\n\nYI 6B (B)\n81.424 ±56.676\\pm 56.676\n\n0.990 ±0.013\\pm 0.013\n\n0.006 ±0.005\\pm 0.005\n\n0.616 ±0.052\\pm 0.052\n\n0.794 ±0.019\\pm 0.019\n\n0.503 ±0.007\\pm 0.007\n\n0.344 ±0.144\\pm 0.144\n\n0.424 ±0.150\\pm 0.150\n\n6.737 ±2.608\\pm 2.608\n\n4.322 ±3.652\\pm 3.652\n\n4.503 ±2.864\\pm 2.864\n\n2.969 ±1.965\\pm 1.965\n\n5.387 ±2.634\\pm 2.634\n\n3.100 ±2.118\\pm 2.118\n\n\n\nYI 6B (F)\n42.835∗ ±13.527\\pm 13.527\n\n0.986 ±0.011\\pm 0.011\n\n0.016 ±0.009\\pm 0.009\n\n0.566 ±0.082\\pm 0.082\n\n0.812∗ ±0.010\\pm 0.010\n\n0.509∗ ±0.006\\pm 0.006\n\n0.492∗ ±0.139\\pm 0.139\n\n0.581∗ ±0.102\\pm 0.102\n\n8.039∗ ±0.746\\pm 0.746\n\n6.838∗ ±2.822\\pm 2.822\n\n6.062∗ ±2.555\\pm 2.555\n\n6.128∗ ±2.867\\pm 2.867\n\n7.334∗ ±1.479\\pm 1.479\n\n5.409∗ ±2.061\\pm 2.061",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"5\"><span class=\"ltx_text ltx_font_bold\">Linguistic Quality</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\">Semantic Coherence</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"5\"><span class=\"ltx_text ltx_font_bold\">Political Authenticity</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">PPL</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">Dist-N</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">Self-BLEU</span> <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">J_Coh</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">J_Conc</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">GRUEN</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">BERTScore</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">MoverScore</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">J_Rel</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m9\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">PSA</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m10\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">Party Align</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m11\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">J_Auth</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m12\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">J_PolApp</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m13\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">J_Qual</span> <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m14\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Llama 3.1 8B (B)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">61.150 <math alttext=\"\\pm 35.890\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m15\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">35.890</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 35.890</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.988 <math alttext=\"\\pm 0.019\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m16\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.019</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.019</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.006 <math alttext=\"\\pm 0.003\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m17\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.003</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.003</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.591 <math alttext=\"\\pm 0.085\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m18\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.085</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.085</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.799 <math alttext=\"\\pm 0.014\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m19\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.014</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.014</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.504 <math alttext=\"\\pm 0.007\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m20\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.007</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.007</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.397 <math alttext=\"\\pm 0.141\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m21\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.141</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.141</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.502 <math alttext=\"\\pm 0.134\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m22\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.134</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.134</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">7.043 <math alttext=\"\\pm 2.481\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m23\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.481</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.481</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.889 <math alttext=\"\\pm 3.101\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m24\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">3.101</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 3.101</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.518 <math alttext=\"\\pm 2.948\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m25\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.948</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.948</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.364 <math alttext=\"\\pm 2.647\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m26\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.647</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.647</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.184 <math alttext=\"\\pm 2.585\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m27\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.585</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.585</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.796 <math alttext=\"\\pm 2.403\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m28\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.403</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.403</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Llama 3.1 8B (F)</th>\n<td class=\"ltx_td ltx_align_center\">31.623<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 8.251\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m30\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">8.251</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 8.251</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.974 <math alttext=\"\\pm 0.018\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m31\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.018</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.018</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.018 <math alttext=\"\\pm 0.011\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m32\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.011</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.011</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.537 <math alttext=\"\\pm 0.146\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m33\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.146</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.146</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.814<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 0.010\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m35\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.010</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.010</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.509<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 0.006\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m37\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.006</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.006</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.488<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 0.139\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m39\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.139</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.139</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.576<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 0.107\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m41\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.107</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.107</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">7.902<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 1.142\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m43\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">1.142</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 1.142</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">7.125<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 2.608\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m45\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.608</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.608</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">6.186<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 2.543\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m47\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.543</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.543</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">6.133<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 3.001\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m49\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">3.001</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 3.001</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">7.277<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 1.577\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m51\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">1.577</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 1.577</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">5.415<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 2.235\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m53\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.235</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.235</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Gemma 2 9B (B)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">89.783 <math alttext=\"\\pm 47.619\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m54\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">47.619</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 47.619</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.992 <math alttext=\"\\pm 0.007\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m55\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.007</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.007</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.008 <math alttext=\"\\pm 0.004\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m56\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.004</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.004</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.579 <math alttext=\"\\pm 0.074\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m57\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.074</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.074</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.800 <math alttext=\"\\pm 0.016\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m58\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.016</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.016</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.508 <math alttext=\"\\pm 0.007\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m59\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.007</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.007</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.443 <math alttext=\"\\pm 0.142\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m60\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.142</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.142</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.542 <math alttext=\"\\pm 0.118\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m61\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.118</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.118</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">7.782 <math alttext=\"\\pm 1.312\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m62\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">1.312</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 1.312</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.756 <math alttext=\"\\pm 3.361\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m63\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">3.361</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 3.361</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.822 <math alttext=\"\\pm 2.473\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m64\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.473</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.473</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.842 <math alttext=\"\\pm 2.357\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m65\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.357</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.357</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.513 <math alttext=\"\\pm 2.044\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m66\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.044</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.044</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.469 <math alttext=\"\\pm 1.987\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m67\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">1.987</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 1.987</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Gemma 2 9B (F)</th>\n<td class=\"ltx_td ltx_align_center\">102.382 <math alttext=\"\\pm 60.592\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m68\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">60.592</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 60.592</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.990 <math alttext=\"\\pm 0.021\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m69\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.021</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.021</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.010 <math alttext=\"\\pm 0.006\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m70\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.006</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.006</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.532 <math alttext=\"\\pm 0.078\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m71\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.078</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.078</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.799 <math alttext=\"\\pm 0.018\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m72\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.018</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.018</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.508 <math alttext=\"\\pm 0.006\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m73\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.006</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.006</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.497<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 0.137\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m75\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.137</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.137</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.589<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 0.102\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m77\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.102</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.102</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">7.502 <math alttext=\"\\pm 1.487\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m78\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">1.487</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 1.487</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">4.970 <math alttext=\"\\pm 3.489\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m79\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">3.489</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 3.489</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">5.598 <math alttext=\"\\pm 2.463\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m80\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.463</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.463</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">4.215<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 2.529\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m82\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.529</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.529</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">7.314<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 1.617\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m84\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">1.617</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 1.617</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">4.976<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 1.915\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m86\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">1.915</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 1.915</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Mistral 7B v0.3 (B)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">31.428 <math alttext=\"\\pm 18.844\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m87\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">18.844</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 18.844</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.966 <math alttext=\"\\pm 0.052\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m88\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.052</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.052</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.008 <math alttext=\"\\pm 0.005\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m89\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.005</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.005</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.598 <math alttext=\"\\pm 0.108\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m90\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.108</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.108</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.806 <math alttext=\"\\pm 0.012\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m91\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.012</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.012</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.505 <math alttext=\"\\pm 0.007\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m92\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.007</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.007</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.417 <math alttext=\"\\pm 0.146\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m93\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.146</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.146</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.522 <math alttext=\"\\pm 0.131\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m94\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.131</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.131</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.582 <math alttext=\"\\pm 2.963\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m95\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.963</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.963</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.850 <math alttext=\"\\pm 3.212\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m96\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">3.212</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 3.212</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.428 <math alttext=\"\\pm 3.184\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m97\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">3.184</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 3.184</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.234 <math alttext=\"\\pm 2.780\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m98\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.780</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.780</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.627 <math alttext=\"\\pm 2.732\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m99\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.732</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.732</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.182 <math alttext=\"\\pm 2.647\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m100\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.647</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.647</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Mistral 7B v0.3 (F)</th>\n<td class=\"ltx_td ltx_align_center\">29.379<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 10.852\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m102\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">10.852</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 10.852</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.972<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 0.019\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m104\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.019</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.019</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.017 <math alttext=\"\\pm 0.012\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m105\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.012</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.012</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.590 <math alttext=\"\\pm 0.082\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m106\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.082</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.082</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.820<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 0.009\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m108\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.009</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.009</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.506<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 0.006\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m110\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.006</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.006</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.437<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 0.139\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m112\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.139</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.139</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.506 <math alttext=\"\\pm 0.122\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m113\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.122</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.122</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">7.954<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 1.085\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m115\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">1.085</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 1.085</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">8.942<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 2.051\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m117\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.051</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.051</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">5.744<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 2.904\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m119\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.904</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.904</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">3.986 <math alttext=\"\\pm 2.735\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m120\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.735</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.735</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">6.388<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 2.097\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m122\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.097</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.097</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">3.745 <math alttext=\"\\pm 2.089\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m123\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.089</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.089</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Qwen2 7B (B)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">44.927 <math alttext=\"\\pm 21.739\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m124\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">21.739</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 21.739</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.981 <math alttext=\"\\pm 0.026\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m125\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.026</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.026</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.020 <math alttext=\"\\pm 0.014\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m126\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.014</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.014</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.534 <math alttext=\"\\pm 0.142\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m127\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.142</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.142</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.799 <math alttext=\"\\pm 0.009\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m128\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.009</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.009</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.507 <math alttext=\"\\pm 0.006\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m129\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.006</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.006</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.444 <math alttext=\"\\pm 0.140\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m130\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.140</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.140</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.562 <math alttext=\"\\pm 0.114\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m131\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.114</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.114</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">7.919 <math alttext=\"\\pm 1.222\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m132\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">1.222</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 1.222</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.937 <math alttext=\"\\pm 1.652\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m133\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">1.652</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 1.652</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.938 <math alttext=\"\\pm 2.345\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m134\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.345</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.345</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.558 <math alttext=\"\\pm 2.583\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m135\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.583</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.583</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">7.324 <math alttext=\"\\pm 1.751\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m136\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">1.751</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 1.751</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.369 <math alttext=\"\\pm 1.627\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m137\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">1.627</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 1.627</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Qwen2 7B (F)</th>\n<td class=\"ltx_td ltx_align_center\">36.040<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 9.144\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m139\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">9.144</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 9.144</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.982 <math alttext=\"\\pm 0.013\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m140\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.013</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.013</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.017<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 0.009\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m142\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.009</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.009</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.570<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 0.086\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m144\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.086</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.086</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.815<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 0.009\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m146\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.009</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.009</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.510<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 0.006\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m148\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.006</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.006</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.489<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 0.135\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m150\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.135</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.135</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">0.573<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 0.106\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m152\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.106</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.106</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">8.056<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 0.887\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m154\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.887</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.887</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">7.617<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 2.697\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m156\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.697</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.697</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">6.081 <math alttext=\"\\pm 2.641\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m157\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.641</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.641</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">5.718 <math alttext=\"\\pm 3.017\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m158\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">3.017</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 3.017</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">7.146 <math alttext=\"\\pm 1.617\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m159\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">1.617</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 1.617</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\">5.015 <math alttext=\"\\pm 2.100\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m160\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.100</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.100</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">YI 6B (B)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">81.424 <math alttext=\"\\pm 56.676\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m161\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">56.676</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 56.676</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.990 <math alttext=\"\\pm 0.013\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m162\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.013</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.013</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.006 <math alttext=\"\\pm 0.005\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m163\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.005</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.005</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.616 <math alttext=\"\\pm 0.052\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m164\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.052</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.052</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.794 <math alttext=\"\\pm 0.019\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m165\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.019</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.019</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.503 <math alttext=\"\\pm 0.007\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m166\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.007</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.007</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.344 <math alttext=\"\\pm 0.144\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m167\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.144</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.144</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.424 <math alttext=\"\\pm 0.150\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m168\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.150</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.150</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">6.737 <math alttext=\"\\pm 2.608\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m169\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.608</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.608</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.322 <math alttext=\"\\pm 3.652\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m170\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">3.652</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 3.652</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.503 <math alttext=\"\\pm 2.864\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m171\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.864</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.864</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.969 <math alttext=\"\\pm 1.965\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m172\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">1.965</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 1.965</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.387 <math alttext=\"\\pm 2.634\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m173\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.634</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.634</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.100 <math alttext=\"\\pm 2.118\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m174\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.118</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.118</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">YI 6B (F)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">42.835<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 13.527\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m176\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">13.527</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 13.527</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.986 <math alttext=\"\\pm 0.011\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m177\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.011</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.011</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.016 <math alttext=\"\\pm 0.009\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m178\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.009</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.009</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.566 <math alttext=\"\\pm 0.082\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m179\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.082</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.082</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.812<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 0.010\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m181\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.010</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.010</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.509<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 0.006\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m183\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.006</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.006</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.492<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 0.139\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m185\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.139</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.139</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.581<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 0.102\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m187\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.102</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.102</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">8.039<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 0.746\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m189\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">0.746</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 0.746</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">6.838<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 2.822\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m191\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.822</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.822</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">6.062<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 2.555\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m193\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.555</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.555</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">6.128<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 2.867\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m195\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.867</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.867</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">7.334<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 1.479\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m197\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">1.479</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 1.479</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">5.409<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> <math alttext=\"\\pm 2.061\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m199\" intent=\":literal\"><semantics><mrow><mo mathsize=\"0.500em\">&#177;</mo><mn mathsize=\"0.500em\">2.061</mn></mrow><annotation encoding=\"application/x-tex\">\\pm 2.061</annotation></semantics></math>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "±0005pm",
            "coherence",
            "↓downarrow",
            "±0144pm",
            "±35890pm",
            "±1965pm",
            "±1617pm",
            "0815∗",
            "0489∗",
            "±2641pm",
            "±0107pm",
            "±3489pm",
            "±0114pm",
            "0581∗",
            "±0013pm",
            "±0122pm",
            "±10852pm",
            "36040∗",
            "8039∗",
            "0570∗",
            "±3101pm",
            "29379∗",
            "±2097pm",
            "6388∗",
            "±0102pm",
            "±0135pm",
            "±0106pm",
            "evaluation",
            "±3212pm",
            "5415∗",
            "significant",
            "±0006pm",
            "±2061pm",
            "semantic",
            "7902∗",
            "6838∗",
            "±1142pm",
            "±1312pm",
            "±1487pm",
            "party",
            "±0009pm",
            "6062∗",
            "5409∗",
            "±0078pm",
            "±2780pm",
            "±0086pm",
            "0509∗",
            "±0003pm",
            "improvements",
            "jconc",
            "4215∗",
            "7334∗",
            "±2529pm",
            "±0012pm",
            "±2403pm",
            "7125∗",
            "0820∗",
            "±0134pm",
            "±3001pm",
            "0972∗",
            "across",
            "±0150pm",
            "±21739pm",
            "±1652pm",
            "±8251pm",
            "jauth",
            "between",
            "42835∗",
            "gruen",
            "±2647pm",
            "7617∗",
            "±56676pm",
            "±1987pm",
            "selfbleu",
            "±0141pm",
            "±2481pm",
            "31623∗",
            "±9144pm",
            "performance",
            "↑uparrow",
            "gemma",
            "±2089pm",
            "authenticity",
            "±2235pm",
            "±2697pm",
            "statistically",
            "jcoh",
            "jrel",
            "±0007pm",
            "comparison",
            "distn",
            "±1222pm",
            "±3017pm",
            "±2555pm",
            "±1915pm",
            "political",
            "llama",
            "±2118pm",
            "±1627pm",
            "±0142pm",
            "v03",
            "±13527pm",
            "±0014pm",
            "7314∗",
            "0576∗",
            "±0131pm",
            "±2051pm",
            "metrics",
            "±0085pm",
            "±0016pm",
            "±2867pm",
            "complete",
            "jpolapp",
            "±2473pm",
            "±0011pm",
            "baseline",
            "psa",
            "±2585pm",
            "0812∗",
            "±0108pm",
            "±3361pm",
            "6133∗",
            "±1085pm",
            "moverscore",
            "±2583pm",
            "align",
            "±2732pm",
            "±0746pm",
            "0017∗",
            "finetuned",
            "0492∗",
            "±2735pm",
            "models",
            "jqual",
            "±0139pm",
            "±0026pm",
            "ppl",
            "±0052pm",
            "±0118pm",
            "±3184pm",
            "±0146pm",
            "quality",
            "±0021pm",
            "±2634pm",
            "7954∗",
            "6128∗",
            "mistral",
            "5744∗",
            "linguistic",
            "0589∗",
            "±2044pm",
            "±0140pm",
            "±2963pm",
            "±2357pm",
            "±2608pm",
            "±2100pm",
            "±0137pm",
            "±18844pm",
            "±47619pm",
            "0497∗",
            "±2345pm",
            "±2822pm",
            "8056∗",
            "±2948pm",
            "±60592pm",
            "±0019pm",
            "6186∗",
            "8942∗",
            "indicate",
            "asterisks",
            "bertscore",
            "±0018pm",
            "±0004pm",
            "0488∗",
            "model",
            "qwen2",
            "±0074pm",
            "0510∗",
            "±2904pm",
            "0437∗",
            "7277∗",
            "0573∗",
            "±0887pm",
            "±1577pm",
            "±1479pm",
            "±2463pm",
            "±1751pm",
            "0814∗",
            "±3652pm",
            "0506∗",
            "±2864pm",
            "4976∗",
            "±0010pm",
            "±0082pm",
            "±2543pm"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.T2\" title=\"Table 2 &#8227; 6.1. Overview and Fine-Tuning Impact &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents metric results organized by our framework assessment categories. Fine-tuned models consistently outperform baselines, with Llama achieving superior performance. Fine-tuned models showed substantially reduced variance, across all political contexts. Extended context windows (128k tokens) and larger vocabularies contribute to architectural advantages.\n</p>\n\n",
            "<p class=\"ltx_p\">Pairwise t-tests confirm statistical significance of fine-tuning effects (45 out of 70 comparisons). Model architectures exhibited differential responsiveness: <span class=\"ltx_text ltx_font_italic\">YI</span> and <span class=\"ltx_text ltx_font_italic\">Llama</span> achieved notable improvements (11/14 metrics, 79%), while others showed more selective gains (improvements marked with <sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.T2\" title=\"Table 2 &#8227; 6.1. Overview and Fine-Tuning Impact &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>).</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Parliamentary speech generation presents specific challenges for large language models beyond standard text generation tasks. Unlike general text generation, parliamentary speeches require not only linguistic quality but also political authenticity and ideological consistency. Current language models lack specialized training for parliamentary contexts, and existing evaluation methods focus on standard NLP metrics rather than political authenticity. To address this, we present ParliaBench, a benchmark for parliamentary speech generation. We constructed a dataset of speeches from UK Parliament to enable systematic model training. We introduce an evaluation framework combining computational metrics with LLM-as-a-judge assessments for measuring generation quality across three dimensions: linguistic quality, semantic coherence, and political authenticity. We propose two novel embedding-based metrics, Political Spectrum Alignment and Party Alignment, to quantify ideological positioning. We fine-tuned five large language models (LLMs), generated 28k speeches, and evaluated them\nusing our framework, comparing baseline and fine-tuned models. Results show that fine-tuning produces statistically significant improvements across the majority of metrics and our novel metrics demonstrate strong discriminative power for political dimensions.\n\n\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Parliamentary Speech Generation,LLM Evaluation,Political Authenticity,Benchmark Evaluation</p>\n\n",
                "matched_terms": [
                    "improvements",
                    "model",
                    "finetuned",
                    "coherence",
                    "across",
                    "political",
                    "models",
                    "quality",
                    "metrics",
                    "evaluation",
                    "linguistic",
                    "statistically",
                    "significant",
                    "party",
                    "baseline",
                    "semantic",
                    "authenticity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Democracy thrives through debate. Democratic parliaments are open forums where elected representatives engage in arguments over policy <cite class=\"ltx_cite ltx_citemacro_cite\">Back et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib4\" title=\"\">2021</a>)</cite>. These debates provide unique insights into political reasoning and ideological positioning. Researchers in political science and computational linguistics increasingly seek to understand and model parliamentary debates.</p>\n\n",
                "matched_terms": [
                    "model",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Yet generating authentic parliamentary speech presents significant challenges that extend well beyond typical text generation tasks. The complexity becomes apparent when considering specific examples. A Labour MP discussing taxation policy must sound distinctly different from a Conservative counterpart not just in policy position, but most significantly, in their fundamental approach to governance. Large language models have shown promise across political applications, from sentiment analysis <cite class=\"ltx_cite ltx_citemacro_cite\">Bestvater and Monroe (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib6\" title=\"\">2023</a>)</cite> and election forecasting <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib22\" title=\"\">2024b</a>)</cite> to synthetic survey data generation <cite class=\"ltx_cite ltx_citemacro_cite\">Argyle et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib3\" title=\"\">2023</a>); Bisbee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib7\" title=\"\">2024</a>)</cite>. Authentic parliamentary speech generation, however, remains challenging.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "political",
                    "significant"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Evaluation approaches for generated political text fall short for these specialized requirements. Although evaluation methods for text generation tasks have evolved from simple overlap metrics <cite class=\"ltx_cite ltx_citemacro_cite\">Papineni et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib27\" title=\"\">2002</a>); Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib23\" title=\"\">2004</a>)</cite> toward more sophisticated embedding-based approaches <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib37\" title=\"\">2020</a>); Zhao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib39\" title=\"\">2019</a>)</cite>, they still focus on surface-level similarity rather than political authenticity.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "evaluation",
                    "political",
                    "authenticity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contributions.</span> We address these limitations by establishing a benchmark resource designed\nspecifically for evaluating parliamentary speech generation. First, we develop a curated dataset containing 448k speeches from UK Parliament (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S3\" title=\"3. ParliaBench Dataset &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Second, we introduce a multi-dimensional evaluation framework that assesses both linguistic quality and political authenticity (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4\" title=\"4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). Third, we fine-tune five large language models and generate 28k parliamentary speeches to establish baseline performance (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S5\" title=\"5. Experimental Setup &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). Finally, we demonstrate the framework&#8217;s effectiveness through systematic evaluation (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6\" title=\"6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>). Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S1.F1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides an overview of the complete ParliaBench framework and experimental methodology.</p>\n\n",
                "matched_terms": [
                    "quality",
                    "political",
                    "models",
                    "evaluation",
                    "linguistic",
                    "complete",
                    "baseline",
                    "performance",
                    "authenticity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in large language models (LLMs) have enabled a wide range of applications across political domains. <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib22\" title=\"\">2024b</a>)</cite> outlines several applications of LLM in political contexts, covering predictive, generative, and simulation-based approaches. The use of LLMs as substitutes for human experts in annotating political texts across multiple languages is explored in <cite class=\"ltx_cite ltx_citemacro_cite\">Heseltine and von Hohenberg (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib16\" title=\"\">2024</a>)</cite>, while <cite class=\"ltx_cite ltx_citemacro_cite\">Gunes and Florczak (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib14\" title=\"\">2023</a>)</cite> employ LLMs for classifying U.S. Congressional bills. <cite class=\"ltx_cite ltx_citemacro_cite\">Argyle et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib3\" title=\"\">2023</a>)</cite> investigate LLMs as proxies for specific human subpopulations in social science research and <cite class=\"ltx_cite ltx_citemacro_cite\">Bisbee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib7\" title=\"\">2024</a>)</cite> raise concerns about the quality, reliability, and reproducibility of synthetic survey data generated by LLMs.\nAgent-based LLMs are utilized as coalition negotiators <cite class=\"ltx_cite ltx_citemacro_cite\">Moghimifar et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib25\" title=\"\">2024</a>)</cite> and as U.S. senators\nsimulating legislative processes <cite class=\"ltx_cite ltx_citemacro_cite\">Baker and Azher (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib5\" title=\"\">2024</a>)</cite>. However, these approaches predominantly emphasize analytical and simulation capabilities rather than authentic speech generation quality.</p>\n\n",
                "matched_terms": [
                    "models",
                    "quality",
                    "across",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Traditional text generation evaluation has evolved from reference-based metrics like BLEU <cite class=\"ltx_cite ltx_citemacro_cite\">Papineni et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib27\" title=\"\">2002</a>)</cite> and ROUGE <cite class=\"ltx_cite ltx_citemacro_cite\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib23\" title=\"\">2004</a>)</cite> toward embedding based approaches that better capture semantic similarity. BERTScore <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib37\" title=\"\">2020</a>)</cite> uses contextualized embeddings to compute token-level similarity, while MoverScore <cite class=\"ltx_cite ltx_citemacro_cite\">Zhao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib39\" title=\"\">2019</a>)</cite> measures semantic transportation cost using Earth Mover&#8217;s Distance. For reference-free evaluation, Zhu and Bhat <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu and Bhat (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib41\" title=\"\">2020</a>)</cite> propose GRUEN, assessing grammaticality and semantic coherence. Domain-specific datasets like BillSum <cite class=\"ltx_cite ltx_citemacro_cite\">Kornilova and Eidelman (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib19\" title=\"\">2019</a>)</cite> for legislative summarization and OpinionQA <cite class=\"ltx_cite ltx_citemacro_cite\">Santurkar et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib31\" title=\"\">2023</a>)</cite> for opinion alignment provide targeted evaluation resources, though gaps remain in generative parliamentary speech assessment.</p>\n\n",
                "matched_terms": [
                    "coherence",
                    "metrics",
                    "evaluation",
                    "gruen",
                    "moverscore",
                    "bertscore",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The emergence of LLM-as-a-Judge evaluation <cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib40\" title=\"\">2023</a>)</cite> offers scalable alternatives for nuanced assessment, achieving over 80% agreement with human evaluators in complex judgment tasks. <cite class=\"ltx_cite ltx_citemacro_cite\">Liu and Sun (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib24\" title=\"\">2023</a>)</cite> further validate this approach, demonstrating GPT-4&#8217;s high alignment with human thematic coding in political analysis. This methodology has been successfully applied across diverse contexts, from general LLM benchmarking through competitive debates <cite class=\"ltx_cite ltx_citemacro_cite\">Moniri et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib26\" title=\"\">2025</a>)</cite> to long-context reasoning evaluation in parliamentary debates <cite class=\"ltx_cite ltx_citemacro_cite\">Tiwari et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib34\" title=\"\">2025</a>)</cite>. These approaches demonstrate the viability of automated evaluation for argumentative and political content, though models exhibit documented biases toward Western, educated populations <cite class=\"ltx_cite ltx_citemacro_cite\">Durmus et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib12\" title=\"\">2023</a>)</cite> and systematic preferences in political simulations <cite class=\"ltx_cite ltx_citemacro_cite\">Qi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib29\" title=\"\">2024</a>)</cite>. Recent advances in retrieval-augmented generation and chain-of-thought reasoning provide enhanced capabilities for contextually-grounded political text generation, though their application to parliamentary speech evaluation remains underexplored.</p>\n\n",
                "matched_terms": [
                    "models",
                    "evaluation",
                    "across",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Domain-specific evaluation frameworks have emerged across professional fields, including FinBen <cite class=\"ltx_cite ltx_citemacro_cite\">Xie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib36\" title=\"\">2024</a>)</cite> and LexEval <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib20\" title=\"\">2024a</a>)</cite>. Political science applications have developed specialized benchmarks for election prediction and legislative analysis, yet these focus primarily on classification and analysis tasks. Parliamentary speech generation has attracted recent computational interest, with work exploring European Parliament consensus building <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib38\" title=\"\">2025</a>)</cite> and political impersonation authenticity <cite class=\"ltx_cite ltx_citemacro_cite\">Herbold et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib15\" title=\"\">2024</a>)</cite>, but evaluation frameworks remain underdeveloped. Existing approaches focus on narrow aspects like style mimicry rather than systematic quality assessment across linguistic and political authenticity dimensions that parliamentary speech generation requires.</p>\n\n",
                "matched_terms": [
                    "quality",
                    "across",
                    "political",
                    "evaluation",
                    "linguistic",
                    "authenticity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 3: Content Processing &amp; Filtering</span>. Distinguished substantive content from procedural elements, separating parliamentary prompts from speech responses. Filtered procedural noise and non-substantive speeches. For political affiliations, we applied 1000-speech minimum threshold, reducing from 28 original affiliations to 11 to ensure stable model training.</p>\n\n",
                "matched_terms": [
                    "model",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 4: Thematic Classification</span>. While ParlaMint uses CAP classification, we selected EuroVoc <cite class=\"ltx_cite ltx_citemacro_cite\">Publications Office of the European Union (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib28\" title=\"\">2025</a>)</cite> as the standard classification system for European parliamentary systems. For policy domains with clear semantic correspondence between CAP and EuroVoc taxonomies, we applied direct mapping rules. For semantically complex or ambiguous categories, we employed the methodology provided by <cite class=\"ltx_cite ltx_citemacro_cite\">Bocchi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib8\" title=\"\">2024</a>)</cite>. We argue that this approach is particularly well-suited for our dataset because it was specifically designed for legal and governmental texts. For speeches yielding multiple concepts, we selected the highest individual concept score. (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S9.SS2\" title=\"9.2. Hybrid Classification Strategy &#8227; 9. Dataset Details &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">9.2</span></a> documents the hybrid classification strategy)</p>\n\n",
                "matched_terms": [
                    "between",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final dataset contains 447,778 speeches from 1,901 unique speakers across 11 political affiliations, with major parties (Conservative: 263,513; Labour: 108,831) dominating representation. The dataset contains approximately 99.94 million words, with speeches averaging 223 words (median: 99 words). This distribution reflects natural variation in parliamentary speeches, from brief procedural statements to extended policy expositions. Temporal coverage captures significant political events including Brexit debates, and the COVID-19 pandemic response, ensuring exposure to diverse political contexts and rhetorical situations. Note: \"Bishops\", \"Crossbench\", and \"Non-Affiliated\" are not political parties in the traditional sense but formal affiliations in Parliament.</p>\n\n",
                "matched_terms": [
                    "across",
                    "political",
                    "significant"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S3.F2\" title=\"Figure 2 &#8227; 3.2. Statistics &#8227; 3. ParliaBench Dataset &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates key dataset characteristics. Panel (a) reveals a highly right-skewed distribution of speech lengths, indicating that the dataset is dominated by relatively short speeches while containing a smaller number of substantially longer ones. Panel (b) presents topic distribution showing that speeches most frequently address International Relations, Law, and Politics, which together account for over 45% of the corpus. Panel (c) demonstrates ideological balance across the political spectrum. While most parties by count are in the centre-left to left spectrum, the centre-right to right spectrum produces a larger amount of speeches by 110,000. Panel (d) displays temporal patterns and institutional differences. The House of Commons consistently produces 3-4x more speeches than the House of Lords across 2015-2022.</p>\n\n",
                "matched_terms": [
                    "across",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generated Dataset.</span> We release 27,560 speeches (model outputs) produced during evaluation (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S5\" title=\"5. Experimental Setup &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). The generated dataset follows the same format with additional fields: <span class=\"ltx_text ltx_font_italic\">Model</span> (architecture identifier), <span class=\"ltx_text ltx_font_italic\">Type</span> (baseline/fine-tuned), <span class=\"ltx_text ltx_font_italic\">Generated Speech</span> (model output), and <span class=\"ltx_text ltx_font_italic\">Evaluation Scores</span> (computed metrics). To ensure a fair comparison, we used the same input prompts across all models and model types, allowing evaluation on identical inputs.</p>\n\n",
                "matched_terms": [
                    "model",
                    "across",
                    "models",
                    "metrics",
                    "evaluation",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both datasets (Training and Generated Data) and finetuned models are available under a CC BY License at the ParliaBench collection on <a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/collections/argyrotsipi/parliabench\" title=\"\">Hugging Face</a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "finetuned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Parliamentary speech evaluation requires assessment across multiple levels that generic benchmarks cannot capture. Our framework operates on two levels: (i) <span class=\"ltx_text ltx_font_bold\">speech evaluation metrics</span> measuring generation quality across three dimensions, and (ii) <span class=\"ltx_text ltx_font_bold\">consistency evaluation metrics</span> measuring performance reliability across political contexts. This dual-track approach also combines computational metrics with LLM-judge evaluation.</p>\n\n",
                "matched_terms": [
                    "quality",
                    "across",
                    "political",
                    "metrics",
                    "evaluation",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our framework introduces two novel computational metrics (Political Spectrum Alignment and Party Alignment, Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.SS1.SSS1\" title=\"4.1.1. Novel Political Authenticity Metrics &#8227; 4.1. Speech Evaluation Metrics &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4.1.1</span></a>) alongside established metrics from both computational and LLM-judge traditions.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Computational metrics provide deterministic assessment through established NLP measures, while LLM-judge metrics capture nuanced qualities requiring contextual understanding. For LLM-judge evaluation, we adapt the methodology from <cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib40\" title=\"\">2023</a>)</cite>. We employed Flow-Judge-v0.1 as our LLM judge, an LLM specialized in system evaluation tasks. It inherits it&#8217;s architecture from Phi-3.5-mini instruct <cite class=\"ltx_cite ltx_citemacro_cite\">Abdin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib1\" title=\"\">2024</a>)</cite>, thus ensuring complete architectural and training data independence from the evaluated models. Our judge rates each speech on a 1-10 scale with written explanations across six parliamentary specific dimensions. We acknowledge this introduces bias through single model judgment. We employ consistent prompt formatting and evaluate speeches sampled across diverse political contexts. Specific prompts for each dimension are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S10\" title=\"10. LLM-as-a-Judge Evaluation Prompts &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "across",
                    "political",
                    "models",
                    "metrics",
                    "evaluation",
                    "complete"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.T1\" title=\"Table 1 &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows our evaluation structure. We assess three quality dimensions: (i) linguistic quality, (ii) semantic coherence, and (iii) political authenticity using both computational and LLM-judge metrics.</p>\n\n",
                "matched_terms": [
                    "quality",
                    "coherence",
                    "political",
                    "metrics",
                    "evaluation",
                    "linguistic",
                    "semantic",
                    "authenticity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Linguistic Quality</span>\n</p>\n\n",
                "matched_terms": [
                    "quality",
                    "linguistic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Perplexity</span> (PPL)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jelinek et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib17\" title=\"\">1977</a>)</cite> measures text naturalness. We compute perplexity using GPT-2 as a fixed reference model across all generated speeches, ensuring cross-model comparability. Lower scores indicate more natural-sounding text according to GPT-2&#8217;s language distribution. (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i1.I1.i1.I1.i1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> better)</p>\n\n",
                "matched_terms": [
                    "model",
                    "across",
                    "↓downarrow",
                    "indicate",
                    "ppl"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Distinct-<math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i1.I1.i1.I1.i2.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span> (Dist-N)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib21\" title=\"\">2016</a>)</cite> measures lexical diversity through unique bigram proportions (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i1.I1.i1.I1.i2.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> better), capturing variety in phrase transitions without repetition.</p>\n\n",
                "matched_terms": [
                    "↑uparrow",
                    "distn"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Self-BLEU</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zhu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib42\" title=\"\">2018</a>)</cite> measures intra-model diversity by computing BLEU similarity among variants generated under the same prompt. Lower scores indicate greater output variety. (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i1.I1.i1.I1.i3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> better)</p>\n\n",
                "matched_terms": [
                    "selfbleu",
                    "↓downarrow",
                    "indicate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Coherence</span> (J_Coh) evaluates logical argument flow and structural organization (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i1.I1.i2.I1.i1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> better).</p>\n\n",
                "matched_terms": [
                    "↑uparrow",
                    "jcoh",
                    "coherence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conciseness</span> (J_Conc) assesses communication efficiency, drawing from debate evaluation criteria&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Moniri et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib26\" title=\"\">2025</a>)</cite> (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i1.I1.i2.I1.i2.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> better).</p>\n\n",
                "matched_terms": [
                    "↑uparrow",
                    "evaluation",
                    "jconc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Semantic Coherence</span>\n</p>\n\n",
                "matched_terms": [
                    "semantic",
                    "coherence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">GRUEN</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zhu and Bhat (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib41\" title=\"\">2020</a>)</cite> evaluates linguistic quality through grammatical correctness and semantic coherence (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.I1.i1.I1.i1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> better), aggregating four quality dimensions: grammaticality, non-redundancy, focus, and structural coherence.</p>\n\n",
                "matched_terms": [
                    "quality",
                    "coherence",
                    "linguistic",
                    "↑uparrow",
                    "gruen",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">BERTScore</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib37\" title=\"\">2020</a>)</cite> measures semantic similarity using RoBERTa-large embeddings (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.I1.i1.I1.i2.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> better), reporting F1-score to balance precision and recall.</p>\n\n",
                "matched_terms": [
                    "↑uparrow",
                    "bertscore",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MoverScore</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zhao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib39\" title=\"\">2019</a>)</cite> quantifies semantic distance via Earth Mover&#8217;s Distance (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.I1.i1.I1.i3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> better).</p>\n\n",
                "matched_terms": [
                    "↑uparrow",
                    "moverscore",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Note:</span> For both BERTScore and MoverScore, generated speeches are compared against the top-5 human speeches from the training set matching the same context.</p>\n\n",
                "matched_terms": [
                    "moverscore",
                    "bertscore"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Relevance</span> (J_Rel) evaluates whether responses address given prompts (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.I1.i2.I1.i1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> better).</p>\n\n",
                "matched_terms": [
                    "↑uparrow",
                    "jrel"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Political Authenticity</span>\n</p>\n\n",
                "matched_terms": [
                    "political",
                    "authenticity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ two novel embedding-based metrics for political authenticity assessment: Political Spectrum Alignment (PSA) and Party Alignment (Party Align). Detailed calculation methodology provided in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.SS1.SSS1\" title=\"4.1.1. Novel Political Authenticity Metrics &#8227; 4.1. Speech Evaluation Metrics &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4.1.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "political",
                    "metrics",
                    "party",
                    "psa",
                    "align",
                    "authenticity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Authenticity</span> (J_Auth) assesses whether content reflects genuine political speech (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i3.I1.i2.I1.i1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> better).</p>\n\n",
                "matched_terms": [
                    "jauth",
                    "↑uparrow",
                    "political",
                    "authenticity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Political Appropriateness</span> (J_PolApp) evaluates whether tone is suitable for political speech (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i3.I1.i2.I1.i2.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> better).</p>\n\n",
                "matched_terms": [
                    "jpolapp",
                    "↑uparrow",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall Quality</span> (J_Qual) assesses sophistication, persuasiveness, and communicative effectiveness (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i3.I1.i2.I1.i3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> better).</p>\n\n",
                "matched_terms": [
                    "↑uparrow",
                    "jqual",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Political Spectrum Alignment</span> (PSA) evaluates ideological positioning on the left-right spectrum. We adapt semantic embedding approaches <cite class=\"ltx_cite ltx_citemacro_cite\">Rheault and Cochrane (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib30\" title=\"\">2020</a>)</cite> for LLM-generated speech evaluation, drawing on\nthe Left-Right (RILE) scale methodology <cite class=\"ltx_cite ltx_citemacro_cite\">Volkens et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib35\" title=\"\">2013</a>); Budge (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib9\" title=\"\">2013</a>)</cite>. Our metric employs a two-stage approach combining semantic similarity with ideological distance. We create reference embeddings by grouping parliamentary speeches by political orientations (Far-left through Far-right, including intermediate positions) and computing centroid embeddings using sentence transformers. Orientations map to numerical values where Far-left = -6, Centre = 0, Far-right = +6.</p>\n\n",
                "matched_terms": [
                    "semantic",
                    "evaluation",
                    "psa",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"po^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p4.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>o</mi><mo>&#8727;</mo></msup></mrow><annotation encoding=\"application/x-tex\">po^{*}</annotation></semantics></math> is the closest matching orientation, <math alttext=\"\\mathcal{PO}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p4.m2\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119978;</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{PO}</annotation></semantics></math>\nthe set of all political orientations, <math alttext=\"c_{po}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p4.m3\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{po}</annotation></semantics></math> the orientation centroid, and <math alttext=\"\\text{sim}(s,c_{po})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p4.m4\" intent=\":literal\"><semantics><mrow><mtext>sim</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo>,</mo><msub><mi>c</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{sim}(s,c_{po})</annotation></semantics></math> the cosine similarity between generated speech <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p4.m5\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> and centroid.</p>\n\n",
                "matched_terms": [
                    "between",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The PSA score combines semantic similarity with orientation distance:</p>\n\n",
                "matched_terms": [
                    "psa",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Party Alignment</span> (Party Align) applies the same embedding methodology to party-specific alignment, using party affiliation rather than political orientation for centroid construction. The alignment score measures cosine similarity between generated speech and expected party centroid:</p>\n\n",
                "matched_terms": [
                    "align",
                    "between",
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"c_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p10.m1\" intent=\":literal\"><semantics><msub><mi>c</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">c_{p}</annotation></semantics></math> is the party-specific centroid and <math alttext=\"\\text{sim}(s,c_{p})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p10.m2\" intent=\":literal\"><semantics><mrow><mtext>sim</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo>,</mo><msub><mi>c</mi><mi>p</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{sim}(s,c_{p})</annotation></semantics></math> the cosine similarity between speech <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p10.m3\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> and centroid (0-1 scale). Higher scores indicate stronger alignment with party-specific language.</p>\n\n",
                "matched_terms": [
                    "between",
                    "indicate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Context Stability</span> measures performance consistency using coefficient of variation, with higher scores indicating more consistent performance across political contexts. This meta-evaluation applies to all speech evaluation metrics, providing diagnostic insight into model reliability. For cross-metric comparison, all metrics, Computational &amp; LLM-Judge, are normalized to 0-1 scale.</p>\n\n",
                "matched_terms": [
                    "model",
                    "across",
                    "political",
                    "metrics",
                    "evaluation",
                    "performance",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We examine consistency across three dimensions: political parties, topic domains, and political orientations. The stability calculation quantifies performance variability:</p>\n\n",
                "matched_terms": [
                    "performance",
                    "across",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We selected five language models representing distinct architectural approaches to establish baseline performance for parliamentary speech generation:\n<span class=\"ltx_text ltx_font_bold\">Mistral 7B v0.3</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib18\" title=\"\">2023</a>)</cite> uses Grouped Query Attention and Sliding Window Attention for efficient long-context processing.\n<span class=\"ltx_text ltx_font_bold\">Llama 3.1 8B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Dubey et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib11\" title=\"\">2024</a>)</cite> features 128k-token context window and expanded vocabulary.\n<span class=\"ltx_text ltx_font_bold\">Gemma 2 9B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Team et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib32\" title=\"\">2024</a>)</cite> employs alternating local/global attention across 42 layers with logit soft-capping.\n<span class=\"ltx_text ltx_font_bold\">Qwen2 7B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Team (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib33\" title=\"\">2024</a>)</cite> is a multilingual model with enhanced reasoning capabilities.\n<span class=\"ltx_text ltx_font_bold\">YI 6B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">AI et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib2\" title=\"\">2025</a>)</cite> emphasizes strong reasoning and coding performance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "qwen2",
                    "across",
                    "models",
                    "llama",
                    "mistral",
                    "baseline",
                    "v03",
                    "performance",
                    "gemma"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed Quantized Low-Rank Adaptation (QLoRA) <cite class=\"ltx_cite ltx_citemacro_cite\">Dettmers et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib10\" title=\"\">2023</a>)</cite> for parameter-efficient fine-tuning. (<math alttext=\"r=16\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">r=16</annotation></semantics></math>, <math alttext=\"\\alpha=16\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=16</annotation></semantics></math>, <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math> epochs). Model-specific chat templates structure training inputs with political metadata (party affiliation, topic classification, orientation, section, and house). Training used SFTTrainer from TRL with 80%-20% train-test splits and automated checkpointing. Fine-tuned models were saved with adapter weights for subsequent evaluation, ensuring consistent model states across experiments.</p>\n\n",
                "matched_terms": [
                    "model",
                    "finetuned",
                    "across",
                    "political",
                    "models",
                    "evaluation",
                    "party"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We planned to generate 30,000 speeches (3,000 per model-type combination across 10 models) using stratified sampling from the held-out test set. To maintain consistency with training conditions, our prompt distribution matches the ParliaBench Dataset structure: 90% generic instruction prompts formatted with political context (party, topic, orientation, section, house) and 10% specific parliamentary questions from the test set.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generation employed nucleus sampling (<math alttext=\"temperature=0.7\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>=</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">temperature=0.7</annotation></semantics></math>, <math alttext=\"top\\-p=0.85\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><mo>=</mo><mn>0.85</mn></mrow><annotation encoding=\"application/x-tex\">top\\-p=0.85</annotation></semantics></math>, <math alttext=\"repetition_{p}enalty=1.2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>n</mi><mi>p</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>y</mi></mrow><mo>=</mo><mn>1.2</mn></mrow><annotation encoding=\"application/x-tex\">repetition_{p}enalty=1.2</annotation></semantics></math>). Generated speeches underwent validation for template leakage, encoding corruption, semantic relevance, and length constraints. Invalid outputs were automatically regenerated (max 3 attempts). Baseline models exhibited higher failure rates, suggesting fine-tuning improved output quality. To ensure fair cross-model comparison, we retained only speeches successfully generated by all 10 model-type combinations, yielding 29,220 speeches.</p>\n\n",
                "matched_terms": [
                    "quality",
                    "models",
                    "baseline",
                    "comparison",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generated speeches were then evaluated using our LLM-judge framework. Speeches with incomplete dimension ratings were excluded, resulting in 27,560 fully evaluated speeches, across all six dimensions, for all models. All subsequent results use the 27,560 fully evaluated speeches, which maintain balanced representation across political affiliations and topics. Complete implementation details, including QLoRA configuration, training parameters, chat templates for all architectures, and speech generation validation methodology, are provided in the Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S11\" title=\"11. Setup Implementation Details &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "complete",
                    "models",
                    "across",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated 27,560 generated speeches using our evaluation framework. This section presents fine-tuning effectiveness and performance patterns across political parties, topic domains, and ideological orientations. Representative examples of generated speeches are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S12\" title=\"12. Representative Generated Speeches &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "across",
                    "political",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, our novel political authenticity metrics (PSA and Party Align) displayed strong responsiveness to fine-tuning. All five models significantly improved PSA (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>), with effect sizes ranging from small to very large (d=0.141-1.045). Party Align showed similar patterns (4 of 5 models improved, d=0.099-1.221). These substantial effects validate that our embedding based metrics capture critical political authenticity dimensions unavailable to conventional evaluation.\nFor complete t-test results including effect sizes, see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T13\" title=\"Table 13 &#8227; 13.5. Statistical Significance Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "political",
                    "models",
                    "metrics",
                    "evaluation",
                    "complete",
                    "psa",
                    "party",
                    "align",
                    "authenticity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F3\" title=\"Figure 3 &#8227; 6.1. Overview and Fine-Tuning Impact &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows fine-tuning impact across evaluation categories. <span class=\"ltx_text ltx_font_italic\">YI</span> achieved the strongest improvements across all dimensions , while <span class=\"ltx_text ltx_font_italic\">Llama</span> had consistent gains. <span class=\"ltx_text ltx_font_italic\">Gemma2</span> and <span class=\"ltx_text ltx_font_italic\">Qwen2</span> exhibited quality trade-offs, with improvements in one category accompanied by declines in others, suggesting architectural differences in how models balance competing objectives during fine-tuning. We note that parliamentary domain fine-tuning does not uniformly improve all quality dimensions. Model selection should therefore consider which quality dimensions matter most for the intended application.</p>\n\n",
                "matched_terms": [
                    "improvements",
                    "model",
                    "quality",
                    "across",
                    "qwen2",
                    "models",
                    "llama",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-context stability analysis (Eq.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.E4\" title=\"In 4.2. Consistency Evaluation Metrics &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) revealed fine-tuned models maintained consistent performance across political contexts (composite stability 91.4-96.2). Mistral achieved highest consistency (96.2) despite trade-offs in absolute performance, while Llama (95.1) balanced strong performance with stability.\nDetailed scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T12\" title=\"Table 12 &#8227; 13.4. Cross-Context Stability Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "finetuned",
                    "political",
                    "across",
                    "models",
                    "llama",
                    "mistral",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Party alignment Patterns</span>. Party alignment performance varied substantially across models (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F4\" title=\"Figure 4 &#8227; 6.2. Political Context Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). Major parties (Conservative, Labour) achieved stable performance across models, benefiting from substantial training data (58.9%, 24.3%). Minor parties exhibited greater variability. <span class=\"ltx_text ltx_font_italic\">Mistral</span> struggled with heterogeneous groups (Non-Affiliated: 0.436), while <span class=\"ltx_text ltx_font_italic\">Qwen</span> excelled with ideologically coherent minorities (Bishops: 0.664). <span class=\"ltx_text ltx_font_italic\">YI</span> demonstrated robust cross-party performance (0.614-0.633).\nDetailed scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T8\" title=\"Table 8 &#8227; 13.1. Party-Specific Performance &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "models",
                    "mistral",
                    "party",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both new political authenticity metrics (PSA and Party Align) successfully discriminate their target political dimensions. Party Align distinguishes parties while PSA distinguishes orientations (both <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>). Our analysis reveals that Party Align performance depends primarily on data abundance and ideological coherence rather than party size alone. Models successfully learn party-specific language patterns when training data provides clear stylistic signals, indicating targeted data collection for under-represented parties could improve coverage.</p>\n\n",
                "matched_terms": [
                    "political",
                    "coherence",
                    "models",
                    "metrics",
                    "psa",
                    "party",
                    "align",
                    "performance",
                    "authenticity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Party alignment Difficulty Analysis</span>. Applying cross-context stability analysis (Eq.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.E4\" title=\"In 4.2. Consistency Evaluation Metrics &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), party difficulty scores ranged narrowly (0.382-0.456), with no statistically significant differences. This suggests relatively consistent modeling challenges across parties regardless of size or ideological composition. Results are presented in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F5\" title=\"Figure 5 &#8227; 6.2. Political Context Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Detailed scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T9\" title=\"Table 9 &#8227; 13.2. Metric Validation: Political Discrimination Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "statistically",
                    "significant",
                    "party",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Topic Performance Patterns</span>. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F6\" title=\"Figure 6 &#8227; 6.3. Topic Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows model performance across topic domains. Science achieved lowest scores (avg 0.516), while Economics (0.610) and European Union (0.606) showed highest performance. Detailed scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T11\" title=\"Table 11 &#8227; 13.3. Topic-Specific Performance &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Performance across political orientations showed expected patterns. Centrist positions (Centre-left: 0.607, Centre-right: 0.551) dominated the dataset (88%) and achieved higher scores. Model-specific strengths emerged as both <span class=\"ltx_text ltx_font_italic\">Gemma</span> and <span class=\"ltx_text ltx_font_italic\">Qwen</span> achieved highest scores on Right positions and <span class=\"ltx_text ltx_font_italic\">Mistral</span> underperformed consistently, indicating architectural rather than ideological limitations. As models are optimized for mainstream parliamentary speeches, extreme positions may require specialized training approaches. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F8\" title=\"Figure 8 &#8227; 6.4. Political Orientations Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> illustrates these patterns. Detailed orientation difficulty rankings are provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T10\" title=\"Table 10 &#8227; 13.2. Metric Validation: Political Discrimination Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a></p>\n\n",
                "matched_terms": [
                    "political",
                    "across",
                    "models",
                    "mistral",
                    "performance",
                    "gemma"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results establish several key findings: (i) Architectural design impacts political authenticity, with extended context windows enabling consistent improvements; (ii) Domain-specific fine-tuning proves essential as 45 of 70 metric comparisons showed statistically significant improvements and (iii) Novel political authenticity metrics (PSA, Party Align) successfully capture dimensions unavailable to conventional NLP metrics, validated through both fine-tuning responsiveness and discrimination testing (both <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "improvements",
                    "political",
                    "metrics",
                    "statistically",
                    "significant",
                    "psa",
                    "party",
                    "align",
                    "authenticity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To the best of our knowledge, ParliaBench represents the first benchmark resource addressing the specialized challenges of parliamentary speech generation, comprising dataset, evaluation framework, novel metrics, and baseline benchmarks. ParliaBench provides standardized evaluation protocols and baseline performance results that\nsupport systematic comparison and reproducible research in the field.\nOur results demonstrate that domain specific fine-tuning produces significant quality improvements, while\nour novel political authenticity metrics successfully capture ideological dimensions absent from conventional evaluation approaches.</p>\n\n",
                "matched_terms": [
                    "improvements",
                    "quality",
                    "political",
                    "metrics",
                    "evaluation",
                    "baseline",
                    "significant",
                    "performance",
                    "comparison",
                    "authenticity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Future Directions:</span> Extensions include: (i) multilingual evaluation for European parliamentary systems, (ii) human evaluation protocols for validation, and (iii) systematic assessment of political bias and perspective maintenance across viewpoints.</p>\n\n",
                "matched_terms": [
                    "across",
                    "political",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both datasets (Training and Generated Data) and finetuned models are available under a CC BY License at the ParliaBench collection on <a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/collections/argyrotsipi/parliabench\" title=\"\">Hugging Face</a>. \n<br class=\"ltx_break\"/>Resources are openly accessible on <a class=\"ltx_ref ltx_href\" href=\"https://argyrotsipi.github.io/ParliaBench/\" title=\"\">Website</a> and <a class=\"ltx_ref ltx_href\" href=\"https://github.com/ArgyroTsipi/ParliaBench\" title=\"\">GitHub</a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "finetuned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation measures linguistic quality and political authenticity but does not assess argument structure or verify factual accuracy against parliamentary records. Our methods rely entirely on automated metrics without human validation, and LLM-as-a-Judge approaches may carry inherent biases.\nThis work is intended for research and educational purposes, not deployment in actual democratic processes.</p>\n\n",
                "matched_terms": [
                    "quality",
                    "political",
                    "metrics",
                    "evaluation",
                    "linguistic",
                    "authenticity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This appendix documents the automated evaluation system used to assess the quality of generated parliamentary speeches. The system employs Flow-Judge-v0.1, a 3.8B parameter evaluation model, to score speeches across six dimensions using a 10-point scale.</p>\n\n",
                "matched_terms": [
                    "model",
                    "quality",
                    "across",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Coherence (j_coh)</span>: Logical flow and structural clarity</p>\n\n",
                "matched_terms": [
                    "jcoh",
                    "coherence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Authenticity (j_auth)</span>: Naturalness of Westminster discourse</p>\n\n",
                "matched_terms": [
                    "jauth",
                    "authenticity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Political Appropriateness (j_polapp)</span>: Alignment with party positions</p>\n\n",
                "matched_terms": [
                    "jpolapp",
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall Quality (j_qual)</span>: Persuasiveness and argumentation strength</p>\n\n",
                "matched_terms": [
                    "jqual",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each metric uses a structured prompt template with explicit evaluation criteria, a 10-point scoring rubric, and detailed instructions for the judge model. The system processes speeches in batches of 32 for computational efficiency.</p>\n\n",
                "matched_terms": [
                    "model",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section provides complete technical specifications for the Quantized Low-Rank Adaptation (QLoRA) implementation used across all model architectures.</p>\n\n",
                "matched_terms": [
                    "complete",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The QLoRA configuration parameters were selected based on established best practices for parameter-efficient fine-tuning in specialized domains. The rank value of 16 provides sufficient adaptation capacity while maintaining computational efficiency. Setting LoRA Alpha equal to the rank ensures reliable baseline performance, while disabling dropout enables Unsloth framework optimizations essential for efficient training on A100 hardware.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Target modules encompass all linear transformation layers (q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj) to achieve performance comparable to full fine-tuning while requiring only a fraction of the computational resources. The consistent random state across all architectures ensures reproducible results essential for systematic model comparison.</p>\n\n",
                "matched_terms": [
                    "comparison",
                    "performance",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section provides the complete chat template specifications used for training and generation across all model architectures, ensuring reproducibility of experimental results.</p>\n\n",
                "matched_terms": [
                    "complete",
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_typewriter\">You are a seasoned UK parliamentary member.\nUse proper British parliamentary language appropriate for the specified House.\n<br class=\"ltx_break\"/>The speech should reflect the political orientation and typical positions of the specified party on the given topic.\n<br class=\"ltx_break\"/></span>\n</p>\n\n",
                "matched_terms": [
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We generated the speeches for the finetuned and the baseline models using the following prompt for all models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "baseline",
                    "finetuned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">You are a seasoned UK parliamentary member. Generate a coherent speech of\nmin_words - max_words words in standard English (no Unicode artifacts, no special characters).</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_typewriter\">\nUse proper British parliamentary language appropriate for the specified House. \n<br class=\"ltx_break\"/>The speech should reflect the political orientation and typical positions of the specified party on the given topic. \n<br class=\"ltx_break\"/></span></p>\n\n",
                "matched_terms": [
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PARTY:</span> Political party affiliation (e.g., Conservative)</p>\n\n",
                "matched_terms": [
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This setup helps fine-tuned models learn to give responses that match a party&#8217;s views, stay on topic, follow parliamentary rules and political views.</p>\n\n",
                "matched_terms": [
                    "models",
                    "finetuned",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present various speeches generated by the models (baseline and finetuned).</p>\n\n",
                "matched_terms": [
                    "models",
                    "baseline",
                    "finetuned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T8\" title=\"Table 8 &#8227; 13.1. Party-Specific Performance &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents detailed performance scores for all UK parliamentary parties across fine-tuned models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "finetuned",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T9\" title=\"Table 9 &#8227; 13.2. Metric Validation: Political Discrimination Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T10\" title=\"Table 10 &#8227; 13.2. Metric Validation: Political Discrimination Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> present ANOVA results validating the discriminative power of the novel political authenticity metrics.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "political",
                    "authenticity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T11\" title=\"Table 11 &#8227; 13.3. Topic-Specific Performance &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> presents model performance across topic domains for fine-tuned models.</p>\n\n",
                "matched_terms": [
                    "model",
                    "finetuned",
                    "across",
                    "models",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T12\" title=\"Table 12 &#8227; 13.4. Cross-Context Stability Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> presents cross-context stability scores. Fine-tuned models maintain high consistency across political contexts (91.4-96.2), with Mistral achieving highest overall stability (96.2).</p>\n\n",
                "matched_terms": [
                    "finetuned",
                    "across",
                    "political",
                    "models",
                    "mistral"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T13\" title=\"Table 13 &#8227; 13.5. Statistical Significance Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> presents complete pairwise t-test results comparing baseline and fine-tuned models across all evaluation metrics, including p-values, effect sizes, and significance after Bonferroni correction.</p>\n\n",
                "matched_terms": [
                    "finetuned",
                    "across",
                    "models",
                    "metrics",
                    "evaluation",
                    "complete",
                    "baseline"
                ]
            }
        ]
    },
    "S9.T3": {
        "source_file": "ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech",
        "caption": "Table 3: Political Parties Represented in Dataset",
        "body": "Party\nOrientation\nSpeeches\nSpeakers\nActive Period\n\n\n\n\nConservative\nCentre-right\n263,513\n792\n2015–2022\n\n\nLabour\nCentre-left\n108,831\n592\n2015–2022\n\n\nScottish National Party\nCentre-left\n23,562\n67\n2015–2022\n\n\nLiberal Democrats\nCentre to centre-left\n23,517\n168\n2015–2022\n\n\nCrossbench\nUnknown\n11,878\n215\n2015–2022\n\n\nDemocratic Unionist Party\nRight\n6,610\n15\n2015–2022\n\n\nIndependent\nUnknown\n2,783\n45\n2015–2022\n\n\nPlaid Cymru\nCentre-left to left\n2,229\n7\n2015–2022\n\n\nGreen Party\nLeft\n1,992\n3\n2015–2022\n\n\nNon-Affiliated\nUnknown\n1,713\n60\n2015–2022\n\n\nBishops\nUnknown\n1,150\n41\n2015–2022\n\n\nTotal\n\n447,778\n1901",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Party</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Orientation</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Speeches</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Speakers</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Active Period</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Conservative</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Centre-right</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">263,513</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">792</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">2015&#8211;2022</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Labour</td>\n<td class=\"ltx_td ltx_align_left\">Centre-left</td>\n<td class=\"ltx_td ltx_align_right\">108,831</td>\n<td class=\"ltx_td ltx_align_right\">592</td>\n<td class=\"ltx_td ltx_align_right\">2015&#8211;2022</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Scottish National Party</td>\n<td class=\"ltx_td ltx_align_left\">Centre-left</td>\n<td class=\"ltx_td ltx_align_right\">23,562</td>\n<td class=\"ltx_td ltx_align_right\">67</td>\n<td class=\"ltx_td ltx_align_right\">2015&#8211;2022</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Liberal Democrats</td>\n<td class=\"ltx_td ltx_align_left\">Centre to centre-left</td>\n<td class=\"ltx_td ltx_align_right\">23,517</td>\n<td class=\"ltx_td ltx_align_right\">168</td>\n<td class=\"ltx_td ltx_align_right\">2015&#8211;2022</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Crossbench</td>\n<td class=\"ltx_td ltx_align_left\">Unknown</td>\n<td class=\"ltx_td ltx_align_right\">11,878</td>\n<td class=\"ltx_td ltx_align_right\">215</td>\n<td class=\"ltx_td ltx_align_right\">2015&#8211;2022</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Democratic Unionist Party</td>\n<td class=\"ltx_td ltx_align_left\">Right</td>\n<td class=\"ltx_td ltx_align_right\">6,610</td>\n<td class=\"ltx_td ltx_align_right\">15</td>\n<td class=\"ltx_td ltx_align_right\">2015&#8211;2022</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Independent</td>\n<td class=\"ltx_td ltx_align_left\">Unknown</td>\n<td class=\"ltx_td ltx_align_right\">2,783</td>\n<td class=\"ltx_td ltx_align_right\">45</td>\n<td class=\"ltx_td ltx_align_right\">2015&#8211;2022</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Plaid Cymru</td>\n<td class=\"ltx_td ltx_align_left\">Centre-left to left</td>\n<td class=\"ltx_td ltx_align_right\">2,229</td>\n<td class=\"ltx_td ltx_align_right\">7</td>\n<td class=\"ltx_td ltx_align_right\">2015&#8211;2022</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Green Party</td>\n<td class=\"ltx_td ltx_align_left\">Left</td>\n<td class=\"ltx_td ltx_align_right\">1,992</td>\n<td class=\"ltx_td ltx_align_right\">3</td>\n<td class=\"ltx_td ltx_align_right\">2015&#8211;2022</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Non-Affiliated</td>\n<td class=\"ltx_td ltx_align_left\">Unknown</td>\n<td class=\"ltx_td ltx_align_right\">1,713</td>\n<td class=\"ltx_td ltx_align_right\">60</td>\n<td class=\"ltx_td ltx_align_right\">2015&#8211;2022</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Bishops</td>\n<td class=\"ltx_td ltx_align_left\">Unknown</td>\n<td class=\"ltx_td ltx_align_right\">1,150</td>\n<td class=\"ltx_td ltx_align_right\">41</td>\n<td class=\"ltx_td ltx_align_right\">2015&#8211;2022</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Total</span></td>\n<td class=\"ltx_td ltx_border_bb ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">447,778</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">1901</span></td>\n<td class=\"ltx_td ltx_border_bb ltx_border_t\"/>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "centreright",
            "political",
            "crossbench",
            "democratic",
            "unionist",
            "centreleft",
            "national",
            "right",
            "conservative",
            "bishops",
            "left",
            "cymru",
            "period",
            "independent",
            "plaid",
            "labour",
            "nonaffiliated",
            "dataset",
            "speakers",
            "centre",
            "green",
            "parties",
            "2015–2022",
            "liberal",
            "democrats",
            "orientation",
            "active",
            "scottish",
            "total",
            "party",
            "speeches",
            "unknown",
            "represented"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S9.T3\" title=\"Table 3 &#8227; 9.1. Dataset Statistics &#8227; 9. Dataset Details &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents detailed breakdown of speeches by political affiliation in the Parliamentary Debates Benchmark.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Parliamentary speech generation presents specific challenges for large language models beyond standard text generation tasks. Unlike general text generation, parliamentary speeches require not only linguistic quality but also political authenticity and ideological consistency. Current language models lack specialized training for parliamentary contexts, and existing evaluation methods focus on standard NLP metrics rather than political authenticity. To address this, we present ParliaBench, a benchmark for parliamentary speech generation. We constructed a dataset of speeches from UK Parliament to enable systematic model training. We introduce an evaluation framework combining computational metrics with LLM-as-a-judge assessments for measuring generation quality across three dimensions: linguistic quality, semantic coherence, and political authenticity. We propose two novel embedding-based metrics, Political Spectrum Alignment and Party Alignment, to quantify ideological positioning. We fine-tuned five large language models (LLMs), generated 28k speeches, and evaluated them\nusing our framework, comparing baseline and fine-tuned models. Results show that fine-tuning produces statistically significant improvements across the majority of metrics and our novel metrics demonstrate strong discriminative power for political dimensions.\n\n\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Parliamentary Speech Generation,LLM Evaluation,Political Authenticity,Benchmark Evaluation</p>\n\n",
                "matched_terms": [
                    "speeches",
                    "party",
                    "dataset",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Democracy thrives through debate. Democratic parliaments are open forums where elected representatives engage in arguments over policy <cite class=\"ltx_cite ltx_citemacro_cite\">Back et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib4\" title=\"\">2021</a>)</cite>. These debates provide unique insights into political reasoning and ideological positioning. Researchers in political science and computational linguistics increasingly seek to understand and model parliamentary debates.</p>\n\n",
                "matched_terms": [
                    "democratic",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Yet generating authentic parliamentary speech presents significant challenges that extend well beyond typical text generation tasks. The complexity becomes apparent when considering specific examples. A Labour MP discussing taxation policy must sound distinctly different from a Conservative counterpart not just in policy position, but most significantly, in their fundamental approach to governance. Large language models have shown promise across political applications, from sentiment analysis <cite class=\"ltx_cite ltx_citemacro_cite\">Bestvater and Monroe (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib6\" title=\"\">2023</a>)</cite> and election forecasting <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib22\" title=\"\">2024b</a>)</cite> to synthetic survey data generation <cite class=\"ltx_cite ltx_citemacro_cite\">Argyle et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib3\" title=\"\">2023</a>); Bisbee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib7\" title=\"\">2024</a>)</cite>. Authentic parliamentary speech generation, however, remains challenging.</p>\n\n",
                "matched_terms": [
                    "labour",
                    "conservative",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contributions.</span> We address these limitations by establishing a benchmark resource designed\nspecifically for evaluating parliamentary speech generation. First, we develop a curated dataset containing 448k speeches from UK Parliament (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S3\" title=\"3. ParliaBench Dataset &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Second, we introduce a multi-dimensional evaluation framework that assesses both linguistic quality and political authenticity (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4\" title=\"4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). Third, we fine-tune five large language models and generate 28k parliamentary speeches to establish baseline performance (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S5\" title=\"5. Experimental Setup &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). Finally, we demonstrate the framework&#8217;s effectiveness through systematic evaluation (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6\" title=\"6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>). Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S1.F1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides an overview of the complete ParliaBench framework and experimental methodology.</p>\n\n",
                "matched_terms": [
                    "speeches",
                    "dataset",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 2: Metadata Extraction &amp; Temporal Alignment</span>. Enriched speeches with speaker identity, political affiliation, chamber designation, and session dates. As parliamentary speakers frequently change affiliations and roles during their careers, we employed temporal alignment by cross-referencing speech dates with affiliation histories from corpus metadata.</p>\n\n",
                "matched_terms": [
                    "speeches",
                    "speakers",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 3: Content Processing &amp; Filtering</span>. Distinguished substantive content from procedural elements, separating parliamentary prompts from speech responses. Filtered procedural noise and non-substantive speeches. For political affiliations, we applied 1000-speech minimum threshold, reducing from 28 original affiliations to 11 to ensure stable model training.</p>\n\n",
                "matched_terms": [
                    "speeches",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 4: Thematic Classification</span>. While ParlaMint uses CAP classification, we selected EuroVoc <cite class=\"ltx_cite ltx_citemacro_cite\">Publications Office of the European Union (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib28\" title=\"\">2025</a>)</cite> as the standard classification system for European parliamentary systems. For policy domains with clear semantic correspondence between CAP and EuroVoc taxonomies, we applied direct mapping rules. For semantically complex or ambiguous categories, we employed the methodology provided by <cite class=\"ltx_cite ltx_citemacro_cite\">Bocchi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib8\" title=\"\">2024</a>)</cite>. We argue that this approach is particularly well-suited for our dataset because it was specifically designed for legal and governmental texts. For speeches yielding multiple concepts, we selected the highest individual concept score. (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S9.SS2\" title=\"9.2. Hybrid Classification Strategy &#8227; 9. Dataset Details &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">9.2</span></a> documents the hybrid classification strategy)</p>\n\n",
                "matched_terms": [
                    "speeches",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final dataset contains 447,778 speeches from 1,901 unique speakers across 11 political affiliations, with major parties (Conservative: 263,513; Labour: 108,831) dominating representation. The dataset contains approximately 99.94 million words, with speeches averaging 223 words (median: 99 words). This distribution reflects natural variation in parliamentary speeches, from brief procedural statements to extended policy expositions. Temporal coverage captures significant political events including Brexit debates, and the COVID-19 pandemic response, ensuring exposure to diverse political contexts and rhetorical situations. Note: \"Bishops\", \"Crossbench\", and \"Non-Affiliated\" are not political parties in the traditional sense but formal affiliations in Parliament.</p>\n\n",
                "matched_terms": [
                    "parties",
                    "bishops",
                    "political",
                    "crossbench",
                    "dataset",
                    "speeches",
                    "labour",
                    "speakers",
                    "conservative",
                    "nonaffiliated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S3.F2\" title=\"Figure 2 &#8227; 3.2. Statistics &#8227; 3. ParliaBench Dataset &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates key dataset characteristics. Panel (a) reveals a highly right-skewed distribution of speech lengths, indicating that the dataset is dominated by relatively short speeches while containing a smaller number of substantially longer ones. Panel (b) presents topic distribution showing that speeches most frequently address International Relations, Law, and Politics, which together account for over 45% of the corpus. Panel (c) demonstrates ideological balance across the political spectrum. While most parties by count are in the centre-left to left spectrum, the centre-right to right spectrum produces a larger amount of speeches by 110,000. Panel (d) displays temporal patterns and institutional differences. The House of Commons consistently produces 3-4x more speeches than the House of Lords across 2015-2022.</p>\n\n",
                "matched_terms": [
                    "parties",
                    "centreright",
                    "left",
                    "political",
                    "dataset",
                    "centreleft",
                    "speeches",
                    "right"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generated Dataset.</span> We release 27,560 speeches (model outputs) produced during evaluation (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S5\" title=\"5. Experimental Setup &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). The generated dataset follows the same format with additional fields: <span class=\"ltx_text ltx_font_italic\">Model</span> (architecture identifier), <span class=\"ltx_text ltx_font_italic\">Type</span> (baseline/fine-tuned), <span class=\"ltx_text ltx_font_italic\">Generated Speech</span> (model output), and <span class=\"ltx_text ltx_font_italic\">Evaluation Scores</span> (computed metrics). To ensure a fair comparison, we used the same input prompts across all models and model types, allowing evaluation on identical inputs.</p>\n\n",
                "matched_terms": [
                    "speeches",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our framework introduces two novel computational metrics (Political Spectrum Alignment and Party Alignment, Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.SS1.SSS1\" title=\"4.1.1. Novel Political Authenticity Metrics &#8227; 4.1. Speech Evaluation Metrics &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4.1.1</span></a>) alongside established metrics from both computational and LLM-judge traditions.</p>\n\n",
                "matched_terms": [
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Computational metrics provide deterministic assessment through established NLP measures, while LLM-judge metrics capture nuanced qualities requiring contextual understanding. For LLM-judge evaluation, we adapt the methodology from <cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib40\" title=\"\">2023</a>)</cite>. We employed Flow-Judge-v0.1 as our LLM judge, an LLM specialized in system evaluation tasks. It inherits it&#8217;s architecture from Phi-3.5-mini instruct <cite class=\"ltx_cite ltx_citemacro_cite\">Abdin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib1\" title=\"\">2024</a>)</cite>, thus ensuring complete architectural and training data independence from the evaluated models. Our judge rates each speech on a 1-10 scale with written explanations across six parliamentary specific dimensions. We acknowledge this introduces bias through single model judgment. We employ consistent prompt formatting and evaluate speeches sampled across diverse political contexts. Specific prompts for each dimension are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S10\" title=\"10. LLM-as-a-Judge Evaluation Prompts &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.</p>\n\n",
                "matched_terms": [
                    "speeches",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ two novel embedding-based metrics for political authenticity assessment: Political Spectrum Alignment (PSA) and Party Alignment (Party Align). Detailed calculation methodology provided in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.SS1.SSS1\" title=\"4.1.1. Novel Political Authenticity Metrics &#8227; 4.1. Speech Evaluation Metrics &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4.1.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Political Spectrum Alignment</span> (PSA) evaluates ideological positioning on the left-right spectrum. We adapt semantic embedding approaches <cite class=\"ltx_cite ltx_citemacro_cite\">Rheault and Cochrane (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib30\" title=\"\">2020</a>)</cite> for LLM-generated speech evaluation, drawing on\nthe Left-Right (RILE) scale methodology <cite class=\"ltx_cite ltx_citemacro_cite\">Volkens et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib35\" title=\"\">2013</a>); Budge (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib9\" title=\"\">2013</a>)</cite>. Our metric employs a two-stage approach combining semantic similarity with ideological distance. We create reference embeddings by grouping parliamentary speeches by political orientations (Far-left through Far-right, including intermediate positions) and computing centroid embeddings using sentence transformers. Orientations map to numerical values where Far-left = -6, Centre = 0, Far-right = +6.</p>\n\n",
                "matched_terms": [
                    "speeches",
                    "centre",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first identify the closest matching political orientation:</p>\n\n",
                "matched_terms": [
                    "political",
                    "orientation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"po^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p4.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>o</mi><mo>&#8727;</mo></msup></mrow><annotation encoding=\"application/x-tex\">po^{*}</annotation></semantics></math> is the closest matching orientation, <math alttext=\"\\mathcal{PO}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p4.m2\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119978;</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{PO}</annotation></semantics></math>\nthe set of all political orientations, <math alttext=\"c_{po}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p4.m3\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{po}</annotation></semantics></math> the orientation centroid, and <math alttext=\"\\text{sim}(s,c_{po})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p4.m4\" intent=\":literal\"><semantics><mrow><mtext>sim</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo>,</mo><msub><mi>c</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{sim}(s,c_{po})</annotation></semantics></math> the cosine similarity between generated speech <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p4.m5\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> and centroid.</p>\n\n",
                "matched_terms": [
                    "political",
                    "orientation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Parties with unknown orientation are excluded from PSA analysis as their ideological position cannot be reliably mapped to the left-right spectrum.</p>\n\n",
                "matched_terms": [
                    "parties",
                    "unknown",
                    "orientation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Party Alignment</span> (Party Align) applies the same embedding methodology to party-specific alignment, using party affiliation rather than political orientation for centroid construction. The alignment score measures cosine similarity between generated speech and expected party centroid:</p>\n\n",
                "matched_terms": [
                    "party",
                    "political",
                    "orientation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We examine consistency across three dimensions: political parties, topic domains, and political orientations. The stability calculation quantifies performance variability:</p>\n\n",
                "matched_terms": [
                    "parties",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed Quantized Low-Rank Adaptation (QLoRA) <cite class=\"ltx_cite ltx_citemacro_cite\">Dettmers et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib10\" title=\"\">2023</a>)</cite> for parameter-efficient fine-tuning. (<math alttext=\"r=16\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">r=16</annotation></semantics></math>, <math alttext=\"\\alpha=16\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=16</annotation></semantics></math>, <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math> epochs). Model-specific chat templates structure training inputs with political metadata (party affiliation, topic classification, orientation, section, and house). Training used SFTTrainer from TRL with 80%-20% train-test splits and automated checkpointing. Fine-tuned models were saved with adapter weights for subsequent evaluation, ensuring consistent model states across experiments.</p>\n\n",
                "matched_terms": [
                    "party",
                    "political",
                    "orientation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We planned to generate 30,000 speeches (3,000 per model-type combination across 10 models) using stratified sampling from the held-out test set. To maintain consistency with training conditions, our prompt distribution matches the ParliaBench Dataset structure: 90% generic instruction prompts formatted with political context (party, topic, orientation, section, house) and 10% specific parliamentary questions from the test set.</p>\n\n",
                "matched_terms": [
                    "political",
                    "orientation",
                    "dataset",
                    "party",
                    "speeches"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generated speeches were then evaluated using our LLM-judge framework. Speeches with incomplete dimension ratings were excluded, resulting in 27,560 fully evaluated speeches, across all six dimensions, for all models. All subsequent results use the 27,560 fully evaluated speeches, which maintain balanced representation across political affiliations and topics. Complete implementation details, including QLoRA configuration, training parameters, chat templates for all architectures, and speech generation validation methodology, are provided in the Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S11\" title=\"11. Setup Implementation Details &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "speeches",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated 27,560 generated speeches using our evaluation framework. This section presents fine-tuning effectiveness and performance patterns across political parties, topic domains, and ideological orientations. Representative examples of generated speeches are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S12\" title=\"12. Representative Generated Speeches &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>.</p>\n\n",
                "matched_terms": [
                    "speeches",
                    "parties",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, our novel political authenticity metrics (PSA and Party Align) displayed strong responsiveness to fine-tuning. All five models significantly improved PSA (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>), with effect sizes ranging from small to very large (d=0.141-1.045). Party Align showed similar patterns (4 of 5 models improved, d=0.099-1.221). These substantial effects validate that our embedding based metrics capture critical political authenticity dimensions unavailable to conventional evaluation.\nFor complete t-test results including effect sizes, see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T13\" title=\"Table 13 &#8227; 13.5. Statistical Significance Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Party alignment Patterns</span>. Party alignment performance varied substantially across models (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F4\" title=\"Figure 4 &#8227; 6.2. Political Context Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). Major parties (Conservative, Labour) achieved stable performance across models, benefiting from substantial training data (58.9%, 24.3%). Minor parties exhibited greater variability. <span class=\"ltx_text ltx_font_italic\">Mistral</span> struggled with heterogeneous groups (Non-Affiliated: 0.436), while <span class=\"ltx_text ltx_font_italic\">Qwen</span> excelled with ideologically coherent minorities (Bishops: 0.664). <span class=\"ltx_text ltx_font_italic\">YI</span> demonstrated robust cross-party performance (0.614-0.633).\nDetailed scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T8\" title=\"Table 8 &#8227; 13.1. Party-Specific Performance &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "parties",
                    "bishops",
                    "party",
                    "labour",
                    "conservative",
                    "nonaffiliated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both new political authenticity metrics (PSA and Party Align) successfully discriminate their target political dimensions. Party Align distinguishes parties while PSA distinguishes orientations (both <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>). Our analysis reveals that Party Align performance depends primarily on data abundance and ideological coherence rather than party size alone. Models successfully learn party-specific language patterns when training data provides clear stylistic signals, indicating targeted data collection for under-represented parties could improve coverage.</p>\n\n",
                "matched_terms": [
                    "parties",
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Party alignment Difficulty Analysis</span>. Applying cross-context stability analysis (Eq.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.E4\" title=\"In 4.2. Consistency Evaluation Metrics &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), party difficulty scores ranged narrowly (0.382-0.456), with no statistically significant differences. This suggests relatively consistent modeling challenges across parties regardless of size or ideological composition. Results are presented in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F5\" title=\"Figure 5 &#8227; 6.2. Political Context Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Detailed scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T9\" title=\"Table 9 &#8227; 13.2. Metric Validation: Political Discrimination Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "parties",
                    "party"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Performance across political orientations showed expected patterns. Centrist positions (Centre-left: 0.607, Centre-right: 0.551) dominated the dataset (88%) and achieved higher scores. Model-specific strengths emerged as both <span class=\"ltx_text ltx_font_italic\">Gemma</span> and <span class=\"ltx_text ltx_font_italic\">Qwen</span> achieved highest scores on Right positions and <span class=\"ltx_text ltx_font_italic\">Mistral</span> underperformed consistently, indicating architectural rather than ideological limitations. As models are optimized for mainstream parliamentary speeches, extreme positions may require specialized training approaches. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F8\" title=\"Figure 8 &#8227; 6.4. Political Orientations Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> illustrates these patterns. Detailed orientation difficulty rankings are provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T10\" title=\"Table 10 &#8227; 13.2. Metric Validation: Political Discrimination Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a></p>\n\n",
                "matched_terms": [
                    "centreright",
                    "political",
                    "orientation",
                    "dataset",
                    "centreleft",
                    "speeches",
                    "right"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results establish several key findings: (i) Architectural design impacts political authenticity, with extended context windows enabling consistent improvements; (ii) Domain-specific fine-tuning proves essential as 45 of 70 metric comparisons showed statistically significant improvements and (iii) Novel political authenticity metrics (PSA, Party Align) successfully capture dimensions unavailable to conventional NLP metrics, validated through both fine-tuning responsiveness and discrimination testing (both <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To the best of our knowledge, ParliaBench represents the first benchmark resource addressing the specialized challenges of parliamentary speech generation, comprising dataset, evaluation framework, novel metrics, and baseline benchmarks. ParliaBench provides standardized evaluation protocols and baseline performance results that\nsupport systematic comparison and reproducible research in the field.\nOur results demonstrate that domain specific fine-tuning produces significant quality improvements, while\nour novel political authenticity metrics successfully capture ideological dimensions absent from conventional evaluation approaches.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation measures linguistic quality and political authenticity but does not assess argument structure or verify factual accuracy against parliamentary records. Our methods rely entirely on automated metrics without human validation, and LLM-as-a-Judge approaches may carry inherent biases.\nThis work is intended for research and educational purposes, not deployment in actual democratic processes.</p>\n\n",
                "matched_terms": [
                    "democratic",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work establishes a benchmark resource for evaluating LLM-generated parliamentary speech in research and educational contexts. These resources should only be used as assistance to human experts with consideration of their limitations and biases. The parliamentary data is derived from publicly available UK parliamentary proceedings (ParlaMint corpus) licensed under Creative Commons Attribution 4.0 International, which our derived datasets maintain.\nGenerated parliamentary speeches must be clearly identified as AI-generated content and not misrepresented as authentic political speeches from actual parliamentarians.</p>\n\n",
                "matched_terms": [
                    "speeches",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Political Appropriateness (j_polapp)</span>: Alignment with party positions</p>\n\n",
                "matched_terms": [
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_typewriter\">You are a seasoned UK parliamentary member.\nUse proper British parliamentary language appropriate for the specified House.\n<br class=\"ltx_break\"/>The speech should reflect the political orientation and typical positions of the specified party on the given topic.\n<br class=\"ltx_break\"/></span>\n</p>\n\n",
                "matched_terms": [
                    "party",
                    "political",
                    "orientation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">You are a seasoned UK parliamentary member. Generate a coherent speech of\nmin_words - max_words words in standard English (no Unicode artifacts, no special characters).</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_typewriter\">\nUse proper British parliamentary language appropriate for the specified House. \n<br class=\"ltx_break\"/>The speech should reflect the political orientation and typical positions of the specified party on the given topic. \n<br class=\"ltx_break\"/></span></p>\n\n",
                "matched_terms": [
                    "party",
                    "political",
                    "orientation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PARTY:</span> Political party affiliation (e.g., Conservative)</p>\n\n",
                "matched_terms": [
                    "conservative",
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">POLITICAL ORIENTATION:</span> Political orientation label (e.g., Right)</p>\n\n",
                "matched_terms": [
                    "right",
                    "political",
                    "orientation"
                ]
            }
        ]
    },
    "S9.T4": {
        "source_file": "ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech",
        "caption": "Table 4: Mapping between CAP Categories and EuroVoc Categories",
        "body": "CAP Category\nEuroVoc Category\nMethod\nRationale\n\n\n\n\nEnergy\nEnergy\nDirect\nExact match\n\n\nEnvironment\nEnvironment\nDirect\nExact match\n\n\nTransportation\nTransport\nDirect\nExact match\n\n\nForeign Trade\nTrade\nDirect\nClear semantic alignment\n\n\nGovernment Operations\nPolitics\nDirect\nClear semantic alignment\n\n\nInternational Affairs\nInternational Relations\nDirect\nClear semantic alignment\n\n\nLabor\nEmployment and Working Conditions\nDirect\nClear semantic alignment\n\n\nLaw and Crime\nLaw\nDirect\nClear semantic alignment\n\n\nAgriculture\nAgriculture, Forestry, Fisheries\nDirect\nClear semantic alignment\n\n\nEducation\nEducation and Communications\nDirect\nClear semantic alignment\n\n\nMacroeconomics\nEconomy\nDirect\nClear semantic alignment\n\n\nSocial Welfare\nSocial Questions\nDirect\nClear semantic alignment\n\n\nTechnology\nProduction, Technology and Research\nDirect\nClear semantic alignment\n\n\nCivil Rights\n—\nAutomated\nComplex assignment\n\n\nDomestic Commerce\n—\nAutomated\nComplex assignment\n\n\nCulture\n—\nAutomated\nComplex assignment\n\n\nHealth\n—\nAutomated\nComplex assignment\n\n\nDefense\n—\nAutomated\nComplex assignment\n\n\nHousing\n—\nAutomated\nComplex assignment\n\n\nImmigration\n—\nAutomated\nComplex assignment\n\n\nPublic Lands\n—\nAutomated\nComplex assignment\n\n\nMix/Other\n—\nAutomated\nComplex assignment",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">CAP Category</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">EuroVoc Category</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Method</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Rationale</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Energy</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Energy</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Direct</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Exact match</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Environment</td>\n<td class=\"ltx_td ltx_align_left\">Environment</td>\n<td class=\"ltx_td ltx_align_left\">Direct</td>\n<td class=\"ltx_td ltx_align_left\">Exact match</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Transportation</td>\n<td class=\"ltx_td ltx_align_left\">Transport</td>\n<td class=\"ltx_td ltx_align_left\">Direct</td>\n<td class=\"ltx_td ltx_align_left\">Exact match</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Foreign Trade</td>\n<td class=\"ltx_td ltx_align_left\">Trade</td>\n<td class=\"ltx_td ltx_align_left\">Direct</td>\n<td class=\"ltx_td ltx_align_left\">Clear semantic alignment</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Government Operations</td>\n<td class=\"ltx_td ltx_align_left\">Politics</td>\n<td class=\"ltx_td ltx_align_left\">Direct</td>\n<td class=\"ltx_td ltx_align_left\">Clear semantic alignment</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">International Affairs</td>\n<td class=\"ltx_td ltx_align_left\">International Relations</td>\n<td class=\"ltx_td ltx_align_left\">Direct</td>\n<td class=\"ltx_td ltx_align_left\">Clear semantic alignment</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Labor</td>\n<td class=\"ltx_td ltx_align_left\">Employment and Working Conditions</td>\n<td class=\"ltx_td ltx_align_left\">Direct</td>\n<td class=\"ltx_td ltx_align_left\">Clear semantic alignment</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Law and Crime</td>\n<td class=\"ltx_td ltx_align_left\">Law</td>\n<td class=\"ltx_td ltx_align_left\">Direct</td>\n<td class=\"ltx_td ltx_align_left\">Clear semantic alignment</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Agriculture</td>\n<td class=\"ltx_td ltx_align_left\">Agriculture, Forestry, Fisheries</td>\n<td class=\"ltx_td ltx_align_left\">Direct</td>\n<td class=\"ltx_td ltx_align_left\">Clear semantic alignment</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Education</td>\n<td class=\"ltx_td ltx_align_left\">Education and Communications</td>\n<td class=\"ltx_td ltx_align_left\">Direct</td>\n<td class=\"ltx_td ltx_align_left\">Clear semantic alignment</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Macroeconomics</td>\n<td class=\"ltx_td ltx_align_left\">Economy</td>\n<td class=\"ltx_td ltx_align_left\">Direct</td>\n<td class=\"ltx_td ltx_align_left\">Clear semantic alignment</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Social Welfare</td>\n<td class=\"ltx_td ltx_align_left\">Social Questions</td>\n<td class=\"ltx_td ltx_align_left\">Direct</td>\n<td class=\"ltx_td ltx_align_left\">Clear semantic alignment</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Technology</td>\n<td class=\"ltx_td ltx_align_left\">Production, Technology and Research</td>\n<td class=\"ltx_td ltx_align_left\">Direct</td>\n<td class=\"ltx_td ltx_align_left\">Clear semantic alignment</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Civil Rights</td>\n<td class=\"ltx_td ltx_align_left\">&#8212;</td>\n<td class=\"ltx_td ltx_align_left\">Automated</td>\n<td class=\"ltx_td ltx_align_left\">Complex assignment</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Domestic Commerce</td>\n<td class=\"ltx_td ltx_align_left\">&#8212;</td>\n<td class=\"ltx_td ltx_align_left\">Automated</td>\n<td class=\"ltx_td ltx_align_left\">Complex assignment</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Culture</td>\n<td class=\"ltx_td ltx_align_left\">&#8212;</td>\n<td class=\"ltx_td ltx_align_left\">Automated</td>\n<td class=\"ltx_td ltx_align_left\">Complex assignment</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Health</td>\n<td class=\"ltx_td ltx_align_left\">&#8212;</td>\n<td class=\"ltx_td ltx_align_left\">Automated</td>\n<td class=\"ltx_td ltx_align_left\">Complex assignment</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Defense</td>\n<td class=\"ltx_td ltx_align_left\">&#8212;</td>\n<td class=\"ltx_td ltx_align_left\">Automated</td>\n<td class=\"ltx_td ltx_align_left\">Complex assignment</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Housing</td>\n<td class=\"ltx_td ltx_align_left\">&#8212;</td>\n<td class=\"ltx_td ltx_align_left\">Automated</td>\n<td class=\"ltx_td ltx_align_left\">Complex assignment</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Immigration</td>\n<td class=\"ltx_td ltx_align_left\">&#8212;</td>\n<td class=\"ltx_td ltx_align_left\">Automated</td>\n<td class=\"ltx_td ltx_align_left\">Complex assignment</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Public Lands</td>\n<td class=\"ltx_td ltx_align_left\">&#8212;</td>\n<td class=\"ltx_td ltx_align_left\">Automated</td>\n<td class=\"ltx_td ltx_align_left\">Complex assignment</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\">Mix/Other</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\">&#8212;</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\">Automated</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\">Complex assignment</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "eurovoc",
            "civil",
            "mapping",
            "lands",
            "categories",
            "production",
            "health",
            "employment",
            "public",
            "foreign",
            "economy",
            "questions",
            "rights",
            "government",
            "welfare",
            "operations",
            "assignment",
            "conditions",
            "macroeconomics",
            "relations",
            "commerce",
            "education",
            "communications",
            "between",
            "trade",
            "rationale",
            "housing",
            "mixother",
            "cap",
            "exact",
            "clear",
            "law",
            "match",
            "crime",
            "immigration",
            "international",
            "defense",
            "fisheries",
            "automated",
            "forestry",
            "alignment",
            "working",
            "technology",
            "affairs",
            "politics",
            "domestic",
            "semantic",
            "labor",
            "transport",
            "energy",
            "category",
            "culture",
            "social",
            "research",
            "method",
            "agriculture",
            "environment",
            "direct",
            "complex",
            "transportation"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">This subsection documents the hybrid classification strategy employed to map Comparative Agendas Project (CAP) categories to EuroVoc domains for topic assignment in our parliamentary speech dataset. We employed direct semantic mapping for 16 categories, while 6 remaining categories required automated classification, as detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S9.T4\" title=\"Table 4 &#8227; 9.2. Hybrid Classification Strategy &#8227; 9. Dataset Details &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Parliamentary speech generation presents specific challenges for large language models beyond standard text generation tasks. Unlike general text generation, parliamentary speeches require not only linguistic quality but also political authenticity and ideological consistency. Current language models lack specialized training for parliamentary contexts, and existing evaluation methods focus on standard NLP metrics rather than political authenticity. To address this, we present ParliaBench, a benchmark for parliamentary speech generation. We constructed a dataset of speeches from UK Parliament to enable systematic model training. We introduce an evaluation framework combining computational metrics with LLM-as-a-judge assessments for measuring generation quality across three dimensions: linguistic quality, semantic coherence, and political authenticity. We propose two novel embedding-based metrics, Political Spectrum Alignment and Party Alignment, to quantify ideological positioning. We fine-tuned five large language models (LLMs), generated 28k speeches, and evaluated them\nusing our framework, comparing baseline and fine-tuned models. Results show that fine-tuning produces statistically significant improvements across the majority of metrics and our novel metrics demonstrate strong discriminative power for political dimensions.\n\n\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Parliamentary Speech Generation,LLM Evaluation,Political Authenticity,Benchmark Evaluation</p>\n\n",
                "matched_terms": [
                    "semantic",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in large language models (LLMs) have enabled a wide range of applications across political domains. <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib22\" title=\"\">2024b</a>)</cite> outlines several applications of LLM in political contexts, covering predictive, generative, and simulation-based approaches. The use of LLMs as substitutes for human experts in annotating political texts across multiple languages is explored in <cite class=\"ltx_cite ltx_citemacro_cite\">Heseltine and von Hohenberg (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib16\" title=\"\">2024</a>)</cite>, while <cite class=\"ltx_cite ltx_citemacro_cite\">Gunes and Florczak (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib14\" title=\"\">2023</a>)</cite> employ LLMs for classifying U.S. Congressional bills. <cite class=\"ltx_cite ltx_citemacro_cite\">Argyle et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib3\" title=\"\">2023</a>)</cite> investigate LLMs as proxies for specific human subpopulations in social science research and <cite class=\"ltx_cite ltx_citemacro_cite\">Bisbee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib7\" title=\"\">2024</a>)</cite> raise concerns about the quality, reliability, and reproducibility of synthetic survey data generated by LLMs.\nAgent-based LLMs are utilized as coalition negotiators <cite class=\"ltx_cite ltx_citemacro_cite\">Moghimifar et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib25\" title=\"\">2024</a>)</cite> and as U.S. senators\nsimulating legislative processes <cite class=\"ltx_cite ltx_citemacro_cite\">Baker and Azher (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib5\" title=\"\">2024</a>)</cite>. However, these approaches predominantly emphasize analytical and simulation capabilities rather than authentic speech generation quality.</p>\n\n",
                "matched_terms": [
                    "research",
                    "social"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Traditional text generation evaluation has evolved from reference-based metrics like BLEU <cite class=\"ltx_cite ltx_citemacro_cite\">Papineni et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib27\" title=\"\">2002</a>)</cite> and ROUGE <cite class=\"ltx_cite ltx_citemacro_cite\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib23\" title=\"\">2004</a>)</cite> toward embedding based approaches that better capture semantic similarity. BERTScore <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib37\" title=\"\">2020</a>)</cite> uses contextualized embeddings to compute token-level similarity, while MoverScore <cite class=\"ltx_cite ltx_citemacro_cite\">Zhao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib39\" title=\"\">2019</a>)</cite> measures semantic transportation cost using Earth Mover&#8217;s Distance. For reference-free evaluation, Zhu and Bhat <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu and Bhat (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib41\" title=\"\">2020</a>)</cite> propose GRUEN, assessing grammaticality and semantic coherence. Domain-specific datasets like BillSum <cite class=\"ltx_cite ltx_citemacro_cite\">Kornilova and Eidelman (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib19\" title=\"\">2019</a>)</cite> for legislative summarization and OpinionQA <cite class=\"ltx_cite ltx_citemacro_cite\">Santurkar et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib31\" title=\"\">2023</a>)</cite> for opinion alignment provide targeted evaluation resources, though gaps remain in generative parliamentary speech assessment.</p>\n\n",
                "matched_terms": [
                    "semantic",
                    "alignment",
                    "transportation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The emergence of LLM-as-a-Judge evaluation <cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib40\" title=\"\">2023</a>)</cite> offers scalable alternatives for nuanced assessment, achieving over 80% agreement with human evaluators in complex judgment tasks. <cite class=\"ltx_cite ltx_citemacro_cite\">Liu and Sun (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib24\" title=\"\">2023</a>)</cite> further validate this approach, demonstrating GPT-4&#8217;s high alignment with human thematic coding in political analysis. This methodology has been successfully applied across diverse contexts, from general LLM benchmarking through competitive debates <cite class=\"ltx_cite ltx_citemacro_cite\">Moniri et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib26\" title=\"\">2025</a>)</cite> to long-context reasoning evaluation in parliamentary debates <cite class=\"ltx_cite ltx_citemacro_cite\">Tiwari et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib34\" title=\"\">2025</a>)</cite>. These approaches demonstrate the viability of automated evaluation for argumentative and political content, though models exhibit documented biases toward Western, educated populations <cite class=\"ltx_cite ltx_citemacro_cite\">Durmus et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib12\" title=\"\">2023</a>)</cite> and systematic preferences in political simulations <cite class=\"ltx_cite ltx_citemacro_cite\">Qi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib29\" title=\"\">2024</a>)</cite>. Recent advances in retrieval-augmented generation and chain-of-thought reasoning provide enhanced capabilities for contextually-grounded political text generation, though their application to parliamentary speech evaluation remains underexplored.</p>\n\n",
                "matched_terms": [
                    "automated",
                    "complex",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 4: Thematic Classification</span>. While ParlaMint uses CAP classification, we selected EuroVoc <cite class=\"ltx_cite ltx_citemacro_cite\">Publications Office of the European Union (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib28\" title=\"\">2025</a>)</cite> as the standard classification system for European parliamentary systems. For policy domains with clear semantic correspondence between CAP and EuroVoc taxonomies, we applied direct mapping rules. For semantically complex or ambiguous categories, we employed the methodology provided by <cite class=\"ltx_cite ltx_citemacro_cite\">Bocchi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib8\" title=\"\">2024</a>)</cite>. We argue that this approach is particularly well-suited for our dataset because it was specifically designed for legal and governmental texts. For speeches yielding multiple concepts, we selected the highest individual concept score. (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S9.SS2\" title=\"9.2. Hybrid Classification Strategy &#8227; 9. Dataset Details &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">9.2</span></a> documents the hybrid classification strategy)</p>\n\n",
                "matched_terms": [
                    "eurovoc",
                    "cap",
                    "clear",
                    "mapping",
                    "categories",
                    "between",
                    "direct",
                    "complex",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S3.F2\" title=\"Figure 2 &#8227; 3.2. Statistics &#8227; 3. ParliaBench Dataset &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates key dataset characteristics. Panel (a) reveals a highly right-skewed distribution of speech lengths, indicating that the dataset is dominated by relatively short speeches while containing a smaller number of substantially longer ones. Panel (b) presents topic distribution showing that speeches most frequently address International Relations, Law, and Politics, which together account for over 45% of the corpus. Panel (c) demonstrates ideological balance across the political spectrum. While most parties by count are in the centre-left to left spectrum, the centre-right to right spectrum produces a larger amount of speeches by 110,000. Panel (d) displays temporal patterns and institutional differences. The House of Commons consistently produces 3-4x more speeches than the House of Lords across 2015-2022.</p>\n\n",
                "matched_terms": [
                    "politics",
                    "law",
                    "international",
                    "relations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Political Spectrum Alignment</span> (PSA) evaluates ideological positioning on the left-right spectrum. We adapt semantic embedding approaches <cite class=\"ltx_cite ltx_citemacro_cite\">Rheault and Cochrane (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib30\" title=\"\">2020</a>)</cite> for LLM-generated speech evaluation, drawing on\nthe Left-Right (RILE) scale methodology <cite class=\"ltx_cite ltx_citemacro_cite\">Volkens et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib35\" title=\"\">2013</a>); Budge (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib9\" title=\"\">2013</a>)</cite>. Our metric employs a two-stage approach combining semantic similarity with ideological distance. We create reference embeddings by grouping parliamentary speeches by political orientations (Far-left through Far-right, including intermediate positions) and computing centroid embeddings using sentence transformers. Orientations map to numerical values where Far-left = -6, Centre = 0, Far-right = +6.</p>\n\n",
                "matched_terms": [
                    "semantic",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Party Alignment</span> (Party Align) applies the same embedding methodology to party-specific alignment, using party affiliation rather than political orientation for centroid construction. The alignment score measures cosine similarity between generated speech and expected party centroid:</p>\n\n",
                "matched_terms": [
                    "between",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"c_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p10.m1\" intent=\":literal\"><semantics><msub><mi>c</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">c_{p}</annotation></semantics></math> is the party-specific centroid and <math alttext=\"\\text{sim}(s,c_{p})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p10.m2\" intent=\":literal\"><semantics><mrow><mtext>sim</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo>,</mo><msub><mi>c</mi><mi>p</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{sim}(s,c_{p})</annotation></semantics></math> the cosine similarity between speech <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p10.m3\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> and centroid (0-1 scale). Higher scores indicate stronger alignment with party-specific language.</p>\n\n",
                "matched_terms": [
                    "between",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We planned to generate 30,000 speeches (3,000 per model-type combination across 10 models) using stratified sampling from the held-out test set. To maintain consistency with training conditions, our prompt distribution matches the ParliaBench Dataset structure: 90% generic instruction prompts formatted with political context (party, topic, orientation, section, house) and 10% specific parliamentary questions from the test set.</p>\n\n",
                "matched_terms": [
                    "conditions",
                    "questions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F3\" title=\"Figure 3 &#8227; 6.1. Overview and Fine-Tuning Impact &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows fine-tuning impact across evaluation categories. <span class=\"ltx_text ltx_font_italic\">YI</span> achieved the strongest improvements across all dimensions , while <span class=\"ltx_text ltx_font_italic\">Llama</span> had consistent gains. <span class=\"ltx_text ltx_font_italic\">Gemma2</span> and <span class=\"ltx_text ltx_font_italic\">Qwen2</span> exhibited quality trade-offs, with improvements in one category accompanied by declines in others, suggesting architectural differences in how models balance competing objectives during fine-tuning. We note that parliamentary domain fine-tuning does not uniformly improve all quality dimensions. Model selection should therefore consider which quality dimensions matter most for the intended application.</p>\n\n",
                "matched_terms": [
                    "category",
                    "categories"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation measures linguistic quality and political authenticity but does not assess argument structure or verify factual accuracy against parliamentary records. Our methods rely entirely on automated metrics without human validation, and LLM-as-a-Judge approaches may carry inherent biases.\nThis work is intended for research and educational purposes, not deployment in actual democratic processes.</p>\n\n",
                "matched_terms": [
                    "research",
                    "automated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AWS resources were provided by the National Infrastructures for Research and Technology GRNET and funded by the EU Recovery and Resiliency Facility.</p>\n\n",
                "matched_terms": [
                    "technology",
                    "research"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work establishes a benchmark resource for evaluating LLM-generated parliamentary speech in research and educational contexts. These resources should only be used as assistance to human experts with consideration of their limitations and biases. The parliamentary data is derived from publicly available UK parliamentary proceedings (ParlaMint corpus) licensed under Creative Commons Attribution 4.0 International, which our derived datasets maintain.\nGenerated parliamentary speeches must be clearly identified as AI-generated content and not misrepresented as authentic political speeches from actual parliamentarians.</p>\n\n",
                "matched_terms": [
                    "research",
                    "international"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">TOPIC:</span> EuroVoc classification (e.g., TRADE)</p>\n\n",
                "matched_terms": [
                    "eurovoc",
                    "trade"
                ]
            }
        ]
    },
    "S11.T5": {
        "source_file": "ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech",
        "caption": "Table 5: Model Technical Specifications and Quantization Details",
        "body": "Model\nQuantized Version\nMemory Reduction\nInference Speed\nKey Features\n\n\n\n\nMistral 7B v0.3\nmistral-7b-v0.3-bnb-4bit\n62%\n2.2×\nGQA, SWA\n\n\nLlama 3.1 8B\nMeta-Llama-3.1-8B-bnb-4bit\n58%\n2.4×\n128k context\n\n\nGemma 2 9B\ngemma-2-9b-bnb-4bit\n58%\n2.2×\nAlternating attention\n\n\nQwen2 7B\nQwen2-7B-bnb-4bit\nN/A\nN/A\nMultilingual\n\n\nFalcon-H1 7B\nfalcon-7b-bnb-4bit\nN/A\nN/A\nTraining efficiency",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Quantized Version</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Memory Reduction</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Inference Speed</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Key Features</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Mistral 7B v0.3</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">mistral-7b-v0.3-bnb-4bit</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">62%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.2&#215;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">GQA, SWA</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Llama 3.1 8B</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Meta-Llama-3.1-8B-bnb-4bit</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">58%</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.4&#215;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">128k context</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Gemma 2 9B</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">gemma-2-9b-bnb-4bit</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">58%</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.2&#215;</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Alternating attention</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Qwen2 7B</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Qwen2-7B-bnb-4bit</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">N/A</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">N/A</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Multilingual</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Falcon-H1 7B</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">falcon-7b-bnb-4bit</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">N/A</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">N/A</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Training efficiency</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "efficiency",
            "quantization",
            "training",
            "features",
            "gqa",
            "llama",
            "mistral",
            "falcon7bbnb4bit",
            "gemma29bbnb4bit",
            "v03",
            "memory",
            "details",
            "key",
            "alternating",
            "technical",
            "multilingual",
            "speed",
            "quantized",
            "qwen27bbnb4bit",
            "model",
            "qwen2",
            "22×",
            "24×",
            "version",
            "mistral7bv03bnb4bit",
            "swa",
            "metallama318bbnb4bit",
            "context",
            "gemma",
            "128k",
            "falconh1",
            "specifications",
            "reduction",
            "attention",
            "inference"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S11.T5\" title=\"Table 5 &#8227; 11.1. Model Architecture Details &#8227; 11. Setup Implementation Details &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> summarizes technical specifications and quantization details for the selected models.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Parliamentary speech generation presents specific challenges for large language models beyond standard text generation tasks. Unlike general text generation, parliamentary speeches require not only linguistic quality but also political authenticity and ideological consistency. Current language models lack specialized training for parliamentary contexts, and existing evaluation methods focus on standard NLP metrics rather than political authenticity. To address this, we present ParliaBench, a benchmark for parliamentary speech generation. We constructed a dataset of speeches from UK Parliament to enable systematic model training. We introduce an evaluation framework combining computational metrics with LLM-as-a-judge assessments for measuring generation quality across three dimensions: linguistic quality, semantic coherence, and political authenticity. We propose two novel embedding-based metrics, Political Spectrum Alignment and Party Alignment, to quantify ideological positioning. We fine-tuned five large language models (LLMs), generated 28k speeches, and evaluated them\nusing our framework, comparing baseline and fine-tuned models. Results show that fine-tuning produces statistically significant improvements across the majority of metrics and our novel metrics demonstrate strong discriminative power for political dimensions.\n\n\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Parliamentary Speech Generation,LLM Evaluation,Political Authenticity,Benchmark Evaluation</p>\n\n",
                "matched_terms": [
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 3: Content Processing &amp; Filtering</span>. Distinguished substantive content from procedural elements, separating parliamentary prompts from speech responses. Filtered procedural noise and non-substantive speeches. For political affiliations, we applied 1000-speech minimum threshold, reducing from 28 original affiliations to 11 to ensure stable model training.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Computational metrics provide deterministic assessment through established NLP measures, while LLM-judge metrics capture nuanced qualities requiring contextual understanding. For LLM-judge evaluation, we adapt the methodology from <cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib40\" title=\"\">2023</a>)</cite>. We employed Flow-Judge-v0.1 as our LLM judge, an LLM specialized in system evaluation tasks. It inherits it&#8217;s architecture from Phi-3.5-mini instruct <cite class=\"ltx_cite ltx_citemacro_cite\">Abdin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib1\" title=\"\">2024</a>)</cite>, thus ensuring complete architectural and training data independence from the evaluated models. Our judge rates each speech on a 1-10 scale with written explanations across six parliamentary specific dimensions. We acknowledge this introduces bias through single model judgment. We employ consistent prompt formatting and evaluate speeches sampled across diverse political contexts. Specific prompts for each dimension are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S10\" title=\"10. LLM-as-a-Judge Evaluation Prompts &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Note:</span> For both BERTScore and MoverScore, generated speeches are compared against the top-5 human speeches from the training set matching the same context.</p>\n\n",
                "matched_terms": [
                    "context",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We selected five language models representing distinct architectural approaches to establish baseline performance for parliamentary speech generation:\n<span class=\"ltx_text ltx_font_bold\">Mistral 7B v0.3</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib18\" title=\"\">2023</a>)</cite> uses Grouped Query Attention and Sliding Window Attention for efficient long-context processing.\n<span class=\"ltx_text ltx_font_bold\">Llama 3.1 8B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Dubey et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib11\" title=\"\">2024</a>)</cite> features 128k-token context window and expanded vocabulary.\n<span class=\"ltx_text ltx_font_bold\">Gemma 2 9B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Team et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib32\" title=\"\">2024</a>)</cite> employs alternating local/global attention across 42 layers with logit soft-capping.\n<span class=\"ltx_text ltx_font_bold\">Qwen2 7B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Team (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib33\" title=\"\">2024</a>)</cite> is a multilingual model with enhanced reasoning capabilities.\n<span class=\"ltx_text ltx_font_bold\">YI 6B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">AI et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib2\" title=\"\">2025</a>)</cite> emphasizes strong reasoning and coding performance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "qwen2",
                    "features",
                    "llama",
                    "mistral",
                    "alternating",
                    "attention",
                    "context",
                    "gemma",
                    "v03",
                    "multilingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed Quantized Low-Rank Adaptation (QLoRA) <cite class=\"ltx_cite ltx_citemacro_cite\">Dettmers et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib10\" title=\"\">2023</a>)</cite> for parameter-efficient fine-tuning. (<math alttext=\"r=16\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">r=16</annotation></semantics></math>, <math alttext=\"\\alpha=16\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=16</annotation></semantics></math>, <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math> epochs). Model-specific chat templates structure training inputs with political metadata (party affiliation, topic classification, orientation, section, and house). Training used SFTTrainer from TRL with 80%-20% train-test splits and automated checkpointing. Fine-tuned models were saved with adapter weights for subsequent evaluation, ensuring consistent model states across experiments.</p>\n\n",
                "matched_terms": [
                    "model",
                    "training",
                    "quantized"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We planned to generate 30,000 speeches (3,000 per model-type combination across 10 models) using stratified sampling from the held-out test set. To maintain consistency with training conditions, our prompt distribution matches the ParliaBench Dataset structure: 90% generic instruction prompts formatted with political context (party, topic, orientation, section, house) and 10% specific parliamentary questions from the test set.</p>\n\n",
                "matched_terms": [
                    "context",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generated speeches were then evaluated using our LLM-judge framework. Speeches with incomplete dimension ratings were excluded, resulting in 27,560 fully evaluated speeches, across all six dimensions, for all models. All subsequent results use the 27,560 fully evaluated speeches, which maintain balanced representation across political affiliations and topics. Complete implementation details, including QLoRA configuration, training parameters, chat templates for all architectures, and speech generation validation methodology, are provided in the Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S11\" title=\"11. Setup Implementation Details &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "details",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.T2\" title=\"Table 2 &#8227; 6.1. Overview and Fine-Tuning Impact &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents metric results organized by our framework assessment categories. Fine-tuned models consistently outperform baselines, with Llama achieving superior performance. Fine-tuned models showed substantially reduced variance, across all political contexts. Extended context windows (128k tokens) and larger vocabularies contribute to architectural advantages.\n</p>\n\n",
                "matched_terms": [
                    "context",
                    "llama",
                    "128k"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Pairwise t-tests confirm statistical significance of fine-tuning effects (45 out of 70 comparisons). Model architectures exhibited differential responsiveness: <span class=\"ltx_text ltx_font_italic\">YI</span> and <span class=\"ltx_text ltx_font_italic\">Llama</span> achieved notable improvements (11/14 metrics, 79%), while others showed more selective gains (improvements marked with <sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.T2\" title=\"Table 2 &#8227; 6.1. Overview and Fine-Tuning Impact &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>).</p>\n\n",
                "matched_terms": [
                    "llama",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F3\" title=\"Figure 3 &#8227; 6.1. Overview and Fine-Tuning Impact &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows fine-tuning impact across evaluation categories. <span class=\"ltx_text ltx_font_italic\">YI</span> achieved the strongest improvements across all dimensions , while <span class=\"ltx_text ltx_font_italic\">Llama</span> had consistent gains. <span class=\"ltx_text ltx_font_italic\">Gemma2</span> and <span class=\"ltx_text ltx_font_italic\">Qwen2</span> exhibited quality trade-offs, with improvements in one category accompanied by declines in others, suggesting architectural differences in how models balance competing objectives during fine-tuning. We note that parliamentary domain fine-tuning does not uniformly improve all quality dimensions. Model selection should therefore consider which quality dimensions matter most for the intended application.</p>\n\n",
                "matched_terms": [
                    "llama",
                    "model",
                    "qwen2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-context stability analysis (Eq.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.E4\" title=\"In 4.2. Consistency Evaluation Metrics &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) revealed fine-tuned models maintained consistent performance across political contexts (composite stability 91.4-96.2). Mistral achieved highest consistency (96.2) despite trade-offs in absolute performance, while Llama (95.1) balanced strong performance with stability.\nDetailed scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T12\" title=\"Table 12 &#8227; 13.4. Cross-Context Stability Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "llama",
                    "mistral"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Party alignment Patterns</span>. Party alignment performance varied substantially across models (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F4\" title=\"Figure 4 &#8227; 6.2. Political Context Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). Major parties (Conservative, Labour) achieved stable performance across models, benefiting from substantial training data (58.9%, 24.3%). Minor parties exhibited greater variability. <span class=\"ltx_text ltx_font_italic\">Mistral</span> struggled with heterogeneous groups (Non-Affiliated: 0.436), while <span class=\"ltx_text ltx_font_italic\">Qwen</span> excelled with ideologically coherent minorities (Bishops: 0.664). <span class=\"ltx_text ltx_font_italic\">YI</span> demonstrated robust cross-party performance (0.614-0.633).\nDetailed scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T8\" title=\"Table 8 &#8227; 13.1. Party-Specific Performance &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "mistral",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Performance across political orientations showed expected patterns. Centrist positions (Centre-left: 0.607, Centre-right: 0.551) dominated the dataset (88%) and achieved higher scores. Model-specific strengths emerged as both <span class=\"ltx_text ltx_font_italic\">Gemma</span> and <span class=\"ltx_text ltx_font_italic\">Qwen</span> achieved highest scores on Right positions and <span class=\"ltx_text ltx_font_italic\">Mistral</span> underperformed consistently, indicating architectural rather than ideological limitations. As models are optimized for mainstream parliamentary speeches, extreme positions may require specialized training approaches. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F8\" title=\"Figure 8 &#8227; 6.4. Political Orientations Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> illustrates these patterns. Detailed orientation difficulty rankings are provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T10\" title=\"Table 10 &#8227; 13.2. Metric Validation: Political Discrimination Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a></p>\n\n",
                "matched_terms": [
                    "gemma",
                    "mistral",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results establish several key findings: (i) Architectural design impacts political authenticity, with extended context windows enabling consistent improvements; (ii) Domain-specific fine-tuning proves essential as 45 of 70 metric comparisons showed statistically significant improvements and (iii) Novel political authenticity metrics (PSA, Party Align) successfully capture dimensions unavailable to conventional NLP metrics, validated through both fine-tuning responsiveness and discrimination testing (both <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "context",
                    "key"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Conciseness (j_conc)</span>: Efficiency of expression within parliamentary context</p>\n\n",
                "matched_terms": [
                    "context",
                    "efficiency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Each metric uses a structured prompt template with explicit evaluation criteria, a 10-point scoring rubric, and detailed instructions for the judge model. The system processes speeches in batches of 32 for computational efficiency.</p>\n\n",
                "matched_terms": [
                    "model",
                    "efficiency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Judge Model</span>: Flow-Judge-v0.1 (3.8B parameters, 4-bit quantization)</p>\n\n",
                "matched_terms": [
                    "model",
                    "quantization"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section provides complete technical specifications for the Quantized Low-Rank Adaptation (QLoRA) implementation used across all model architectures.</p>\n\n",
                "matched_terms": [
                    "technical",
                    "specifications",
                    "model",
                    "quantized"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The QLoRA configuration parameters were selected based on established best practices for parameter-efficient fine-tuning in specialized domains. The rank value of 16 provides sufficient adaptation capacity while maintaining computational efficiency. Setting LoRA Alpha equal to the rank ensures reliable baseline performance, while disabling dropout enables Unsloth framework optimizations essential for efficient training on A100 hardware.</p>\n\n",
                "matched_terms": [
                    "efficiency",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section provides the complete chat template specifications used for training and generation across all model architectures, ensuring reproducibility of experimental results.</p>\n\n",
                "matched_terms": [
                    "specifications",
                    "model",
                    "training"
                ]
            }
        ]
    },
    "S11.T6": {
        "source_file": "ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech",
        "caption": "Table 6: Complete QLoRA Configuration Parameters",
        "body": "Parameter\n\n\n\n\nValue\n\n\n\n\nRationale\n\n\n\n\n\n\n\n\nLoRA Rank (r)\n\n\n\n\n16\n\n\n\n\nOptimal balance for fast fine-tuning\n\n\n\n\n\n\nLoRA Alpha\n\n\n\n\n16\n\n\n\n\nSet equal to rank (α/r=1\\alpha/r=1) for baseline\n\n\n\n\n\n\nTarget Modules\n\n\n\n\n7 layers\n\n\n\n\nAll linear transformations\n\n\n\n\n\n\nLoRA Dropout\n\n\n\n\n0\n\n\n\n\nEnable Unsloth optimizations\n\n\n\n\n\n\nBias Configuration\n\n\n\n\nnone\n\n\n\n\nFaster training, reduced memory\n\n\n\n\n\n\nRandom State\n\n\n\n\n3407\n\n\n\n\nReproducibility across architectures",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:130.1pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Parameter</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:108.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Value</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:151.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Rationale</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:130.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">LoRA Rank (r)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:108.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:151.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Optimal balance for fast fine-tuning</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:130.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">LoRA Alpha</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:108.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">16</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:151.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Set equal to rank (</span><math alttext=\"\\alpha/r=1\" class=\"ltx_Math\" display=\"inline\" id=\"S11.T6.m1\" intent=\":literal\"><semantics><mrow><mrow><mi mathsize=\"0.900em\">&#945;</mi><mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\" symmetric=\"true\">/</mo><mi mathsize=\"0.900em\">r</mi></mrow><mo mathsize=\"0.900em\">=</mo><mn mathsize=\"0.900em\">1</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha/r=1</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">) for baseline</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:130.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Target Modules</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:108.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">7 layers</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:151.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">All linear transformations</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:130.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">LoRA Dropout</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:108.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:151.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Enable Unsloth optimizations</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:130.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Bias Configuration</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:108.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">none</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:151.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Faster training, reduced memory</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:130.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Random State</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:108.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3407</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:151.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Reproducibility across architectures</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "training",
            "parameter",
            "equal",
            "target",
            "optimizations",
            "optimal",
            "memory",
            "finetuning",
            "random",
            "faster",
            "across",
            "αr1alphar1",
            "all",
            "complete",
            "baseline",
            "rationale",
            "lora",
            "rank",
            "none",
            "unsloth",
            "qlora",
            "layers",
            "state",
            "modules",
            "bias",
            "configuration",
            "fast",
            "dropout",
            "alpha",
            "set",
            "balance",
            "reproducibility",
            "transformations",
            "value",
            "architectures",
            "linear",
            "enable",
            "parameters",
            "reduced"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Parliamentary speech generation presents specific challenges for large language models beyond standard text generation tasks. Unlike general text generation, parliamentary speeches require not only linguistic quality but also political authenticity and ideological consistency. Current language models lack specialized training for parliamentary contexts, and existing evaluation methods focus on standard NLP metrics rather than political authenticity. To address this, we present ParliaBench, a benchmark for parliamentary speech generation. We constructed a dataset of speeches from UK Parliament to enable systematic model training. We introduce an evaluation framework combining computational metrics with LLM-as-a-judge assessments for measuring generation quality across three dimensions: linguistic quality, semantic coherence, and political authenticity. We propose two novel embedding-based metrics, Political Spectrum Alignment and Party Alignment, to quantify ideological positioning. We fine-tuned five large language models (LLMs), generated 28k speeches, and evaluated them\nusing our framework, comparing baseline and fine-tuned models. Results show that fine-tuning produces statistically significant improvements across the majority of metrics and our novel metrics demonstrate strong discriminative power for political dimensions.\n\n\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Parliamentary Speech Generation,LLM Evaluation,Political Authenticity,Benchmark Evaluation</p>\n\n",
                "matched_terms": [
                    "training",
                    "across",
                    "baseline",
                    "enable",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contributions.</span> We address these limitations by establishing a benchmark resource designed\nspecifically for evaluating parliamentary speech generation. First, we develop a curated dataset containing 448k speeches from UK Parliament (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S3\" title=\"3. ParliaBench Dataset &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Second, we introduce a multi-dimensional evaluation framework that assesses both linguistic quality and political authenticity (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4\" title=\"4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). Third, we fine-tune five large language models and generate 28k parliamentary speeches to establish baseline performance (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S5\" title=\"5. Experimental Setup &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). Finally, we demonstrate the framework&#8217;s effectiveness through systematic evaluation (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6\" title=\"6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>). Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S1.F1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides an overview of the complete ParliaBench framework and experimental methodology.</p>\n\n",
                "matched_terms": [
                    "complete",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in large language models (LLMs) have enabled a wide range of applications across political domains. <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib22\" title=\"\">2024b</a>)</cite> outlines several applications of LLM in political contexts, covering predictive, generative, and simulation-based approaches. The use of LLMs as substitutes for human experts in annotating political texts across multiple languages is explored in <cite class=\"ltx_cite ltx_citemacro_cite\">Heseltine and von Hohenberg (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib16\" title=\"\">2024</a>)</cite>, while <cite class=\"ltx_cite ltx_citemacro_cite\">Gunes and Florczak (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib14\" title=\"\">2023</a>)</cite> employ LLMs for classifying U.S. Congressional bills. <cite class=\"ltx_cite ltx_citemacro_cite\">Argyle et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib3\" title=\"\">2023</a>)</cite> investigate LLMs as proxies for specific human subpopulations in social science research and <cite class=\"ltx_cite ltx_citemacro_cite\">Bisbee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib7\" title=\"\">2024</a>)</cite> raise concerns about the quality, reliability, and reproducibility of synthetic survey data generated by LLMs.\nAgent-based LLMs are utilized as coalition negotiators <cite class=\"ltx_cite ltx_citemacro_cite\">Moghimifar et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib25\" title=\"\">2024</a>)</cite> and as U.S. senators\nsimulating legislative processes <cite class=\"ltx_cite ltx_citemacro_cite\">Baker and Azher (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib5\" title=\"\">2024</a>)</cite>. However, these approaches predominantly emphasize analytical and simulation capabilities rather than authentic speech generation quality.</p>\n\n",
                "matched_terms": [
                    "across",
                    "reproducibility"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S3.F2\" title=\"Figure 2 &#8227; 3.2. Statistics &#8227; 3. ParliaBench Dataset &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates key dataset characteristics. Panel (a) reveals a highly right-skewed distribution of speech lengths, indicating that the dataset is dominated by relatively short speeches while containing a smaller number of substantially longer ones. Panel (b) presents topic distribution showing that speeches most frequently address International Relations, Law, and Politics, which together account for over 45% of the corpus. Panel (c) demonstrates ideological balance across the political spectrum. While most parties by count are in the centre-left to left spectrum, the centre-right to right spectrum produces a larger amount of speeches by 110,000. Panel (d) displays temporal patterns and institutional differences. The House of Commons consistently produces 3-4x more speeches than the House of Lords across 2015-2022.</p>\n\n",
                "matched_terms": [
                    "balance",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generated Dataset.</span> We release 27,560 speeches (model outputs) produced during evaluation (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S5\" title=\"5. Experimental Setup &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). The generated dataset follows the same format with additional fields: <span class=\"ltx_text ltx_font_italic\">Model</span> (architecture identifier), <span class=\"ltx_text ltx_font_italic\">Type</span> (baseline/fine-tuned), <span class=\"ltx_text ltx_font_italic\">Generated Speech</span> (model output), and <span class=\"ltx_text ltx_font_italic\">Evaluation Scores</span> (computed metrics). To ensure a fair comparison, we used the same input prompts across all models and model types, allowing evaluation on identical inputs.</p>\n\n",
                "matched_terms": [
                    "all",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Computational metrics provide deterministic assessment through established NLP measures, while LLM-judge metrics capture nuanced qualities requiring contextual understanding. For LLM-judge evaluation, we adapt the methodology from <cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib40\" title=\"\">2023</a>)</cite>. We employed Flow-Judge-v0.1 as our LLM judge, an LLM specialized in system evaluation tasks. It inherits it&#8217;s architecture from Phi-3.5-mini instruct <cite class=\"ltx_cite ltx_citemacro_cite\">Abdin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib1\" title=\"\">2024</a>)</cite>, thus ensuring complete architectural and training data independence from the evaluated models. Our judge rates each speech on a 1-10 scale with written explanations across six parliamentary specific dimensions. We acknowledge this introduces bias through single model judgment. We employ consistent prompt formatting and evaluate speeches sampled across diverse political contexts. Specific prompts for each dimension are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S10\" title=\"10. LLM-as-a-Judge Evaluation Prompts &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.</p>\n\n",
                "matched_terms": [
                    "complete",
                    "bias",
                    "training",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Perplexity</span> (PPL)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jelinek et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib17\" title=\"\">1977</a>)</cite> measures text naturalness. We compute perplexity using GPT-2 as a fixed reference model across all generated speeches, ensuring cross-model comparability. Lower scores indicate more natural-sounding text according to GPT-2&#8217;s language distribution. (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i1.I1.i1.I1.i1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> better)</p>\n\n",
                "matched_terms": [
                    "all",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Note:</span> For both BERTScore and MoverScore, generated speeches are compared against the top-5 human speeches from the training set matching the same context.</p>\n\n",
                "matched_terms": [
                    "set",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"po^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p4.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>o</mi><mo>&#8727;</mo></msup></mrow><annotation encoding=\"application/x-tex\">po^{*}</annotation></semantics></math> is the closest matching orientation, <math alttext=\"\\mathcal{PO}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p4.m2\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119978;</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{PO}</annotation></semantics></math>\nthe set of all political orientations, <math alttext=\"c_{po}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p4.m3\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{po}</annotation></semantics></math> the orientation centroid, and <math alttext=\"\\text{sim}(s,c_{po})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p4.m4\" intent=\":literal\"><semantics><mrow><mtext>sim</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo>,</mo><msub><mi>c</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{sim}(s,c_{po})</annotation></semantics></math> the cosine similarity between generated speech <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p4.m5\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> and centroid.</p>\n\n",
                "matched_terms": [
                    "all",
                    "set"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Context Stability</span> measures performance consistency using coefficient of variation, with higher scores indicating more consistent performance across political contexts. This meta-evaluation applies to all speech evaluation metrics, providing diagnostic insight into model reliability. For cross-metric comparison, all metrics, Computational &amp; LLM-Judge, are normalized to 0-1 scale.</p>\n\n",
                "matched_terms": [
                    "all",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We selected five language models representing distinct architectural approaches to establish baseline performance for parliamentary speech generation:\n<span class=\"ltx_text ltx_font_bold\">Mistral 7B v0.3</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib18\" title=\"\">2023</a>)</cite> uses Grouped Query Attention and Sliding Window Attention for efficient long-context processing.\n<span class=\"ltx_text ltx_font_bold\">Llama 3.1 8B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Dubey et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib11\" title=\"\">2024</a>)</cite> features 128k-token context window and expanded vocabulary.\n<span class=\"ltx_text ltx_font_bold\">Gemma 2 9B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Team et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib32\" title=\"\">2024</a>)</cite> employs alternating local/global attention across 42 layers with logit soft-capping.\n<span class=\"ltx_text ltx_font_bold\">Qwen2 7B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Team (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib33\" title=\"\">2024</a>)</cite> is a multilingual model with enhanced reasoning capabilities.\n<span class=\"ltx_text ltx_font_bold\">YI 6B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">AI et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib2\" title=\"\">2025</a>)</cite> emphasizes strong reasoning and coding performance.</p>\n\n",
                "matched_terms": [
                    "layers",
                    "baseline",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed Quantized Low-Rank Adaptation (QLoRA) <cite class=\"ltx_cite ltx_citemacro_cite\">Dettmers et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib10\" title=\"\">2023</a>)</cite> for parameter-efficient fine-tuning. (<math alttext=\"r=16\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">r=16</annotation></semantics></math>, <math alttext=\"\\alpha=16\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=16</annotation></semantics></math>, <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math> epochs). Model-specific chat templates structure training inputs with political metadata (party affiliation, topic classification, orientation, section, and house). Training used SFTTrainer from TRL with 80%-20% train-test splits and automated checkpointing. Fine-tuned models were saved with adapter weights for subsequent evaluation, ensuring consistent model states across experiments.</p>\n\n",
                "matched_terms": [
                    "qlora",
                    "finetuning",
                    "training",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We planned to generate 30,000 speeches (3,000 per model-type combination across 10 models) using stratified sampling from the held-out test set. To maintain consistency with training conditions, our prompt distribution matches the ParliaBench Dataset structure: 90% generic instruction prompts formatted with political context (party, topic, orientation, section, house) and 10% specific parliamentary questions from the test set.</p>\n\n",
                "matched_terms": [
                    "set",
                    "training",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generation employed nucleus sampling (<math alttext=\"temperature=0.7\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>=</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">temperature=0.7</annotation></semantics></math>, <math alttext=\"top\\-p=0.85\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><mo>=</mo><mn>0.85</mn></mrow><annotation encoding=\"application/x-tex\">top\\-p=0.85</annotation></semantics></math>, <math alttext=\"repetition_{p}enalty=1.2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>n</mi><mi>p</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>y</mi></mrow><mo>=</mo><mn>1.2</mn></mrow><annotation encoding=\"application/x-tex\">repetition_{p}enalty=1.2</annotation></semantics></math>). Generated speeches underwent validation for template leakage, encoding corruption, semantic relevance, and length constraints. Invalid outputs were automatically regenerated (max 3 attempts). Baseline models exhibited higher failure rates, suggesting fine-tuning improved output quality. To ensure fair cross-model comparison, we retained only speeches successfully generated by all 10 model-type combinations, yielding 29,220 speeches.</p>\n\n",
                "matched_terms": [
                    "all",
                    "finetuning",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generated speeches were then evaluated using our LLM-judge framework. Speeches with incomplete dimension ratings were excluded, resulting in 27,560 fully evaluated speeches, across all six dimensions, for all models. All subsequent results use the 27,560 fully evaluated speeches, which maintain balanced representation across political affiliations and topics. Complete implementation details, including QLoRA configuration, training parameters, chat templates for all architectures, and speech generation validation methodology, are provided in the Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S11\" title=\"11. Setup Implementation Details &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "qlora",
                    "training",
                    "across",
                    "all",
                    "complete",
                    "configuration",
                    "architectures",
                    "parameters"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated 27,560 generated speeches using our evaluation framework. This section presents fine-tuning effectiveness and performance patterns across political parties, topic domains, and ideological orientations. Representative examples of generated speeches are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S12\" title=\"12. Representative Generated Speeches &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.T2\" title=\"Table 2 &#8227; 6.1. Overview and Fine-Tuning Impact &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents metric results organized by our framework assessment categories. Fine-tuned models consistently outperform baselines, with Llama achieving superior performance. Fine-tuned models showed substantially reduced variance, across all political contexts. Extended context windows (128k tokens) and larger vocabularies contribute to architectural advantages.\n</p>\n\n",
                "matched_terms": [
                    "all",
                    "across",
                    "reduced"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Pairwise t-tests confirm statistical significance of fine-tuning effects (45 out of 70 comparisons). Model architectures exhibited differential responsiveness: <span class=\"ltx_text ltx_font_italic\">YI</span> and <span class=\"ltx_text ltx_font_italic\">Llama</span> achieved notable improvements (11/14 metrics, 79%), while others showed more selective gains (improvements marked with <sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.T2\" title=\"Table 2 &#8227; 6.1. Overview and Fine-Tuning Impact &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>).</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "architectures"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, our novel political authenticity metrics (PSA and Party Align) displayed strong responsiveness to fine-tuning. All five models significantly improved PSA (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>), with effect sizes ranging from small to very large (d=0.141-1.045). Party Align showed similar patterns (4 of 5 models improved, d=0.099-1.221). These substantial effects validate that our embedding based metrics capture critical political authenticity dimensions unavailable to conventional evaluation.\nFor complete t-test results including effect sizes, see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T13\" title=\"Table 13 &#8227; 13.5. Statistical Significance Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "all",
                    "complete",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F3\" title=\"Figure 3 &#8227; 6.1. Overview and Fine-Tuning Impact &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows fine-tuning impact across evaluation categories. <span class=\"ltx_text ltx_font_italic\">YI</span> achieved the strongest improvements across all dimensions , while <span class=\"ltx_text ltx_font_italic\">Llama</span> had consistent gains. <span class=\"ltx_text ltx_font_italic\">Gemma2</span> and <span class=\"ltx_text ltx_font_italic\">Qwen2</span> exhibited quality trade-offs, with improvements in one category accompanied by declines in others, suggesting architectural differences in how models balance competing objectives during fine-tuning. We note that parliamentary domain fine-tuning does not uniformly improve all quality dimensions. Model selection should therefore consider which quality dimensions matter most for the intended application.</p>\n\n",
                "matched_terms": [
                    "all",
                    "finetuning",
                    "balance",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Party alignment Patterns</span>. Party alignment performance varied substantially across models (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F4\" title=\"Figure 4 &#8227; 6.2. Political Context Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). Major parties (Conservative, Labour) achieved stable performance across models, benefiting from substantial training data (58.9%, 24.3%). Minor parties exhibited greater variability. <span class=\"ltx_text ltx_font_italic\">Mistral</span> struggled with heterogeneous groups (Non-Affiliated: 0.436), while <span class=\"ltx_text ltx_font_italic\">Qwen</span> excelled with ideologically coherent minorities (Bishops: 0.664). <span class=\"ltx_text ltx_font_italic\">YI</span> demonstrated robust cross-party performance (0.614-0.633).\nDetailed scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T8\" title=\"Table 8 &#8227; 13.1. Party-Specific Performance &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "training",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both new political authenticity metrics (PSA and Party Align) successfully discriminate their target political dimensions. Party Align distinguishes parties while PSA distinguishes orientations (both <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>). Our analysis reveals that Party Align performance depends primarily on data abundance and ideological coherence rather than party size alone. Models successfully learn party-specific language patterns when training data provides clear stylistic signals, indicating targeted data collection for under-represented parties could improve coverage.</p>\n\n",
                "matched_terms": [
                    "training",
                    "target"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Performance across political orientations showed expected patterns. Centrist positions (Centre-left: 0.607, Centre-right: 0.551) dominated the dataset (88%) and achieved higher scores. Model-specific strengths emerged as both <span class=\"ltx_text ltx_font_italic\">Gemma</span> and <span class=\"ltx_text ltx_font_italic\">Qwen</span> achieved highest scores on Right positions and <span class=\"ltx_text ltx_font_italic\">Mistral</span> underperformed consistently, indicating architectural rather than ideological limitations. As models are optimized for mainstream parliamentary speeches, extreme positions may require specialized training approaches. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F8\" title=\"Figure 8 &#8227; 6.4. Political Orientations Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> illustrates these patterns. Detailed orientation difficulty rankings are provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T10\" title=\"Table 10 &#8227; 13.2. Metric Validation: Political Discrimination Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a></p>\n\n",
                "matched_terms": [
                    "training",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To the best of our knowledge, ParliaBench represents the first benchmark resource addressing the specialized challenges of parliamentary speech generation, comprising dataset, evaluation framework, novel metrics, and baseline benchmarks. ParliaBench provides standardized evaluation protocols and baseline performance results that\nsupport systematic comparison and reproducible research in the field.\nOur results demonstrate that domain specific fine-tuning produces significant quality improvements, while\nour novel political authenticity metrics successfully capture ideological dimensions absent from conventional evaluation approaches.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Future Directions:</span> Extensions include: (i) multilingual evaluation for European parliamentary systems, (ii) human evaluation protocols for validation, and (iii) systematic assessment of political bias and perspective maintenance across viewpoints.</p>\n\n",
                "matched_terms": [
                    "bias",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This appendix documents the automated evaluation system used to assess the quality of generated parliamentary speeches. The system employs Flow-Judge-v0.1, a 3.8B parameter evaluation model, to score speeches across six dimensions using a 10-point scale.</p>\n\n",
                "matched_terms": [
                    "parameter",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section provides complete technical specifications for the Quantized Low-Rank Adaptation (QLoRA) implementation used across all model architectures.</p>\n\n",
                "matched_terms": [
                    "qlora",
                    "across",
                    "all",
                    "complete",
                    "architectures"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The QLoRA configuration parameters were selected based on established best practices for parameter-efficient fine-tuning in specialized domains. The rank value of 16 provides sufficient adaptation capacity while maintaining computational efficiency. Setting LoRA Alpha equal to the rank ensures reliable baseline performance, while disabling dropout enables Unsloth framework optimizations essential for efficient training on A100 hardware.</p>\n\n",
                "matched_terms": [
                    "qlora",
                    "training",
                    "alpha",
                    "value",
                    "rank",
                    "equal",
                    "configuration",
                    "baseline",
                    "parameters",
                    "optimizations",
                    "dropout",
                    "finetuning",
                    "lora",
                    "unsloth"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Target modules encompass all linear transformation layers (q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj) to achieve performance comparable to full fine-tuning while requiring only a fraction of the computational resources. The consistent random state across all architectures ensures reproducible results essential for systematic model comparison.</p>\n\n",
                "matched_terms": [
                    "random",
                    "state",
                    "layers",
                    "across",
                    "all",
                    "modules",
                    "architectures",
                    "target",
                    "linear",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section provides the complete chat template specifications used for training and generation across all model architectures, ensuring reproducibility of experimental results.</p>\n\n",
                "matched_terms": [
                    "training",
                    "across",
                    "reproducibility",
                    "all",
                    "complete",
                    "architectures"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We generated the speeches for the finetuned and the baseline models using the following prompt for all models.</p>\n\n",
                "matched_terms": [
                    "all",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T8\" title=\"Table 8 &#8227; 13.1. Party-Specific Performance &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents detailed performance scores for all UK parliamentary parties across fine-tuned models.</p>\n\n",
                "matched_terms": [
                    "all",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T13\" title=\"Table 13 &#8227; 13.5. Statistical Significance Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> presents complete pairwise t-test results comparing baseline and fine-tuned models across all evaluation metrics, including p-values, effect sizes, and significance after Bonferroni correction.</p>\n\n",
                "matched_terms": [
                    "complete",
                    "all",
                    "baseline",
                    "across"
                ]
            }
        ]
    },
    "S11.T7": {
        "source_file": "ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech",
        "caption": "Table 7: Complete Training Configuration for Parliamentary Speech Generation",
        "body": "Parameter\n\n\n\n\nValue\n\n\n\n\nJustification\n\n\n\n\n\n\n\n\nBatch Size\n\n\n\n\n64\n\n\n\n\nGPU memory optimization\n\n\n\n\n\n\nGradient Accumulation\n\n\n\n\n1\n\n\n\n\nNo gradient accumulation\n\n\n\n\n\n\nLearning Rate\n\n\n\n\n2e-4\n\n\n\n\nStandard for LoRA fine-tuning\n\n\n\n\n\n\nMax Steps\n\n\n\n\n11194\n\n\n\n\n2 epochs (Prevents overfitting)\n\n\n\n\n\n\nWarmup Steps\n\n\n\n\n336\n\n\n\n\n3% of max steps\n\n\n\n\n\n\nOptimizer\n\n\n\n\nadamw\n\n\n\n\nMemory-efficient\n\n\n\n\n\n\nWeight Decay\n\n\n\n\n0.01\n\n\n\n\nPrevents overfitting on political data\n\n\n\n\n\n\nMax Sequence Length\n\n\n\n\n1024\n\n\n\n\nAccommodates speech lengths\n\n\n\n\n\n\nScheduler\n\n\n\n\nlinear\n\n\n\n\nLinear learning rate schedule",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:130.1pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Parameter</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:108.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Value</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:151.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Justification</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:130.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Batch Size</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:108.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">64</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:151.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">GPU memory optimization</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:130.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Gradient Accumulation</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:108.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:151.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">No gradient accumulation</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:130.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Learning Rate</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:108.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2e-4</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:151.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Standard for LoRA fine-tuning</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:130.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Max Steps</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:108.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">11194</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:151.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2 epochs (Prevents overfitting)</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:130.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Warmup Steps</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:108.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">336</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:151.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3% of max steps</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:130.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Optimizer</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:108.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">adamw</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:151.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Memory-efficient</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:130.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Weight Decay</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:108.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.01</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:151.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Prevents overfitting on political data</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:130.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Max Sequence Length</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:108.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1024</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:151.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Accommodates speech lengths</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:130.1pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Scheduler</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:108.4pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">linear</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:151.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Linear learning rate schedule</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "parliamentary",
            "training",
            "size",
            "political",
            "rate",
            "parameter",
            "memoryefficient",
            "max",
            "prevents",
            "length",
            "memory",
            "finetuning",
            "speech",
            "standard",
            "decay",
            "learning",
            "complete",
            "overfitting",
            "scheduler",
            "lora",
            "accommodates",
            "generation",
            "sequence",
            "weight",
            "optimization",
            "accumulation",
            "adamw",
            "lengths",
            "schedule",
            "configuration",
            "epochs",
            "gpu",
            "steps",
            "2e4",
            "value",
            "optimizer",
            "batch",
            "gradient",
            "linear",
            "warmup",
            "data",
            "justification"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Parliamentary speech generation presents specific challenges for large language models beyond standard text generation tasks. Unlike general text generation, parliamentary speeches require not only linguistic quality but also political authenticity and ideological consistency. Current language models lack specialized training for parliamentary contexts, and existing evaluation methods focus on standard NLP metrics rather than political authenticity. To address this, we present ParliaBench, a benchmark for parliamentary speech generation. We constructed a dataset of speeches from UK Parliament to enable systematic model training. We introduce an evaluation framework combining computational metrics with LLM-as-a-judge assessments for measuring generation quality across three dimensions: linguistic quality, semantic coherence, and political authenticity. We propose two novel embedding-based metrics, Political Spectrum Alignment and Party Alignment, to quantify ideological positioning. We fine-tuned five large language models (LLMs), generated 28k speeches, and evaluated them\nusing our framework, comparing baseline and fine-tuned models. Results show that fine-tuning produces statistically significant improvements across the majority of metrics and our novel metrics demonstrate strong discriminative power for political dimensions.\n\n\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Parliamentary Speech Generation,LLM Evaluation,Political Authenticity,Benchmark Evaluation</p>\n\n",
                "matched_terms": [
                    "speech",
                    "parliamentary",
                    "standard",
                    "generation",
                    "training",
                    "political",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:144%;\">ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "parliamentary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Democracy thrives through debate. Democratic parliaments are open forums where elected representatives engage in arguments over policy <cite class=\"ltx_cite ltx_citemacro_cite\">Back et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib4\" title=\"\">2021</a>)</cite>. These debates provide unique insights into political reasoning and ideological positioning. Researchers in political science and computational linguistics increasingly seek to understand and model parliamentary debates.</p>\n\n",
                "matched_terms": [
                    "parliamentary",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Yet generating authentic parliamentary speech presents significant challenges that extend well beyond typical text generation tasks. The complexity becomes apparent when considering specific examples. A Labour MP discussing taxation policy must sound distinctly different from a Conservative counterpart not just in policy position, but most significantly, in their fundamental approach to governance. Large language models have shown promise across political applications, from sentiment analysis <cite class=\"ltx_cite ltx_citemacro_cite\">Bestvater and Monroe (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib6\" title=\"\">2023</a>)</cite> and election forecasting <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib22\" title=\"\">2024b</a>)</cite> to synthetic survey data generation <cite class=\"ltx_cite ltx_citemacro_cite\">Argyle et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib3\" title=\"\">2023</a>); Bisbee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib7\" title=\"\">2024</a>)</cite>. Authentic parliamentary speech generation, however, remains challenging.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "parliamentary",
                    "generation",
                    "political",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Evaluation approaches for generated political text fall short for these specialized requirements. Although evaluation methods for text generation tasks have evolved from simple overlap metrics <cite class=\"ltx_cite ltx_citemacro_cite\">Papineni et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib27\" title=\"\">2002</a>); Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib23\" title=\"\">2004</a>)</cite> toward more sophisticated embedding-based approaches <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib37\" title=\"\">2020</a>); Zhao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib39\" title=\"\">2019</a>)</cite>, they still focus on surface-level similarity rather than political authenticity.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Similarly, domain-specific benchmarks have emerged for legislative summarization <cite class=\"ltx_cite ltx_citemacro_cite\">Kornilova and Eidelman (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib19\" title=\"\">2019</a>)</cite>, opinion alignment <cite class=\"ltx_cite ltx_citemacro_cite\">Santurkar et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib31\" title=\"\">2023</a>)</cite>, and other specialized fields <cite class=\"ltx_cite ltx_citemacro_cite\">Xie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib36\" title=\"\">2024</a>); Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib20\" title=\"\">2024a</a>)</cite>. Parliamentary speech generation, however, lacks the evaluation framework needed to assess ideological consistency and parliamentary conventions simultaneously.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "parliamentary",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contributions.</span> We address these limitations by establishing a benchmark resource designed\nspecifically for evaluating parliamentary speech generation. First, we develop a curated dataset containing 448k speeches from UK Parliament (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S3\" title=\"3. ParliaBench Dataset &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Second, we introduce a multi-dimensional evaluation framework that assesses both linguistic quality and political authenticity (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4\" title=\"4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). Third, we fine-tune five large language models and generate 28k parliamentary speeches to establish baseline performance (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S5\" title=\"5. Experimental Setup &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). Finally, we demonstrate the framework&#8217;s effectiveness through systematic evaluation (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6\" title=\"6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>). Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S1.F1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides an overview of the complete ParliaBench framework and experimental methodology.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "parliamentary",
                    "generation",
                    "political",
                    "complete"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in large language models (LLMs) have enabled a wide range of applications across political domains. <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib22\" title=\"\">2024b</a>)</cite> outlines several applications of LLM in political contexts, covering predictive, generative, and simulation-based approaches. The use of LLMs as substitutes for human experts in annotating political texts across multiple languages is explored in <cite class=\"ltx_cite ltx_citemacro_cite\">Heseltine and von Hohenberg (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib16\" title=\"\">2024</a>)</cite>, while <cite class=\"ltx_cite ltx_citemacro_cite\">Gunes and Florczak (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib14\" title=\"\">2023</a>)</cite> employ LLMs for classifying U.S. Congressional bills. <cite class=\"ltx_cite ltx_citemacro_cite\">Argyle et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib3\" title=\"\">2023</a>)</cite> investigate LLMs as proxies for specific human subpopulations in social science research and <cite class=\"ltx_cite ltx_citemacro_cite\">Bisbee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib7\" title=\"\">2024</a>)</cite> raise concerns about the quality, reliability, and reproducibility of synthetic survey data generated by LLMs.\nAgent-based LLMs are utilized as coalition negotiators <cite class=\"ltx_cite ltx_citemacro_cite\">Moghimifar et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib25\" title=\"\">2024</a>)</cite> and as U.S. senators\nsimulating legislative processes <cite class=\"ltx_cite ltx_citemacro_cite\">Baker and Azher (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib5\" title=\"\">2024</a>)</cite>. However, these approaches predominantly emphasize analytical and simulation capabilities rather than authentic speech generation quality.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation",
                    "data",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Traditional text generation evaluation has evolved from reference-based metrics like BLEU <cite class=\"ltx_cite ltx_citemacro_cite\">Papineni et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib27\" title=\"\">2002</a>)</cite> and ROUGE <cite class=\"ltx_cite ltx_citemacro_cite\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib23\" title=\"\">2004</a>)</cite> toward embedding based approaches that better capture semantic similarity. BERTScore <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib37\" title=\"\">2020</a>)</cite> uses contextualized embeddings to compute token-level similarity, while MoverScore <cite class=\"ltx_cite ltx_citemacro_cite\">Zhao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib39\" title=\"\">2019</a>)</cite> measures semantic transportation cost using Earth Mover&#8217;s Distance. For reference-free evaluation, Zhu and Bhat <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu and Bhat (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib41\" title=\"\">2020</a>)</cite> propose GRUEN, assessing grammaticality and semantic coherence. Domain-specific datasets like BillSum <cite class=\"ltx_cite ltx_citemacro_cite\">Kornilova and Eidelman (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib19\" title=\"\">2019</a>)</cite> for legislative summarization and OpinionQA <cite class=\"ltx_cite ltx_citemacro_cite\">Santurkar et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib31\" title=\"\">2023</a>)</cite> for opinion alignment provide targeted evaluation resources, though gaps remain in generative parliamentary speech assessment.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "parliamentary",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The emergence of LLM-as-a-Judge evaluation <cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib40\" title=\"\">2023</a>)</cite> offers scalable alternatives for nuanced assessment, achieving over 80% agreement with human evaluators in complex judgment tasks. <cite class=\"ltx_cite ltx_citemacro_cite\">Liu and Sun (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib24\" title=\"\">2023</a>)</cite> further validate this approach, demonstrating GPT-4&#8217;s high alignment with human thematic coding in political analysis. This methodology has been successfully applied across diverse contexts, from general LLM benchmarking through competitive debates <cite class=\"ltx_cite ltx_citemacro_cite\">Moniri et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib26\" title=\"\">2025</a>)</cite> to long-context reasoning evaluation in parliamentary debates <cite class=\"ltx_cite ltx_citemacro_cite\">Tiwari et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib34\" title=\"\">2025</a>)</cite>. These approaches demonstrate the viability of automated evaluation for argumentative and political content, though models exhibit documented biases toward Western, educated populations <cite class=\"ltx_cite ltx_citemacro_cite\">Durmus et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib12\" title=\"\">2023</a>)</cite> and systematic preferences in political simulations <cite class=\"ltx_cite ltx_citemacro_cite\">Qi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib29\" title=\"\">2024</a>)</cite>. Recent advances in retrieval-augmented generation and chain-of-thought reasoning provide enhanced capabilities for contextually-grounded political text generation, though their application to parliamentary speech evaluation remains underexplored.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "parliamentary",
                    "generation",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding parliamentary speech data, structured corpora like ParlaMint <cite class=\"ltx_cite ltx_citemacro_cite\">Erjavec et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib13\" title=\"\">2025</a>)</cite> provide multi-lingual parliamentary proceedings. Embedding-based approaches for political analysis, introduced in <cite class=\"ltx_cite ltx_citemacro_cite\">Rheault and Cochrane (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib30\" title=\"\">2020</a>)</cite>, demonstrate that embeddings can capture ideological positioning in parliamentary text.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "parliamentary",
                    "data",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Domain-specific evaluation frameworks have emerged across professional fields, including FinBen <cite class=\"ltx_cite ltx_citemacro_cite\">Xie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib36\" title=\"\">2024</a>)</cite> and LexEval <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib20\" title=\"\">2024a</a>)</cite>. Political science applications have developed specialized benchmarks for election prediction and legislative analysis, yet these focus primarily on classification and analysis tasks. Parliamentary speech generation has attracted recent computational interest, with work exploring European Parliament consensus building <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib38\" title=\"\">2025</a>)</cite> and political impersonation authenticity <cite class=\"ltx_cite ltx_citemacro_cite\">Herbold et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib15\" title=\"\">2024</a>)</cite>, but evaluation frameworks remain underdeveloped. Existing approaches focus on narrow aspects like style mimicry rather than systematic quality assessment across linguistic and political authenticity dimensions that parliamentary speech generation requires.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "parliamentary",
                    "generation",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work addresses these gaps by establishing a benchmark resource specifically designed for parliamentary speech generation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "parliamentary",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 2: Metadata Extraction &amp; Temporal Alignment</span>. Enriched speeches with speaker identity, political affiliation, chamber designation, and session dates. As parliamentary speakers frequently change affiliations and roles during their careers, we employed temporal alignment by cross-referencing speech dates with affiliation histories from corpus metadata.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "parliamentary",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 3: Content Processing &amp; Filtering</span>. Distinguished substantive content from procedural elements, separating parliamentary prompts from speech responses. Filtered procedural noise and non-substantive speeches. For political affiliations, we applied 1000-speech minimum threshold, reducing from 28 original affiliations to 11 to ensure stable model training.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "parliamentary",
                    "training",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 4: Thematic Classification</span>. While ParlaMint uses CAP classification, we selected EuroVoc <cite class=\"ltx_cite ltx_citemacro_cite\">Publications Office of the European Union (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib28\" title=\"\">2025</a>)</cite> as the standard classification system for European parliamentary systems. For policy domains with clear semantic correspondence between CAP and EuroVoc taxonomies, we applied direct mapping rules. For semantically complex or ambiguous categories, we employed the methodology provided by <cite class=\"ltx_cite ltx_citemacro_cite\">Bocchi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib8\" title=\"\">2024</a>)</cite>. We argue that this approach is particularly well-suited for our dataset because it was specifically designed for legal and governmental texts. For speeches yielding multiple concepts, we selected the highest individual concept score. (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S9.SS2\" title=\"9.2. Hybrid Classification Strategy &#8227; 9. Dataset Details &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">9.2</span></a> documents the hybrid classification strategy)</p>\n\n",
                "matched_terms": [
                    "parliamentary",
                    "standard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final dataset contains 447,778 speeches from 1,901 unique speakers across 11 political affiliations, with major parties (Conservative: 263,513; Labour: 108,831) dominating representation. The dataset contains approximately 99.94 million words, with speeches averaging 223 words (median: 99 words). This distribution reflects natural variation in parliamentary speeches, from brief procedural statements to extended policy expositions. Temporal coverage captures significant political events including Brexit debates, and the COVID-19 pandemic response, ensuring exposure to diverse political contexts and rhetorical situations. Note: \"Bishops\", \"Crossbench\", and \"Non-Affiliated\" are not political parties in the traditional sense but formal affiliations in Parliament.</p>\n\n",
                "matched_terms": [
                    "parliamentary",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S3.F2\" title=\"Figure 2 &#8227; 3.2. Statistics &#8227; 3. ParliaBench Dataset &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates key dataset characteristics. Panel (a) reveals a highly right-skewed distribution of speech lengths, indicating that the dataset is dominated by relatively short speeches while containing a smaller number of substantially longer ones. Panel (b) presents topic distribution showing that speeches most frequently address International Relations, Law, and Politics, which together account for over 45% of the corpus. Panel (c) demonstrates ideological balance across the political spectrum. While most parties by count are in the centre-left to left spectrum, the centre-right to right spectrum produces a larger amount of speeches by 110,000. Panel (d) displays temporal patterns and institutional differences. The House of Commons consistently produces 3-4x more speeches than the House of Lords across 2015-2022.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "lengths",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both datasets (Training and Generated Data) and finetuned models are available under a CC BY License at the ParliaBench collection on <a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/collections/argyrotsipi/parliabench\" title=\"\">Hugging Face</a>.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Parliamentary speech evaluation requires assessment across multiple levels that generic benchmarks cannot capture. Our framework operates on two levels: (i) <span class=\"ltx_text ltx_font_bold\">speech evaluation metrics</span> measuring generation quality across three dimensions, and (ii) <span class=\"ltx_text ltx_font_bold\">consistency evaluation metrics</span> measuring performance reliability across political contexts. This dual-track approach also combines computational metrics with LLM-judge evaluation.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "parliamentary",
                    "generation",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Computational metrics provide deterministic assessment through established NLP measures, while LLM-judge metrics capture nuanced qualities requiring contextual understanding. For LLM-judge evaluation, we adapt the methodology from <cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib40\" title=\"\">2023</a>)</cite>. We employed Flow-Judge-v0.1 as our LLM judge, an LLM specialized in system evaluation tasks. It inherits it&#8217;s architecture from Phi-3.5-mini instruct <cite class=\"ltx_cite ltx_citemacro_cite\">Abdin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib1\" title=\"\">2024</a>)</cite>, thus ensuring complete architectural and training data independence from the evaluated models. Our judge rates each speech on a 1-10 scale with written explanations across six parliamentary specific dimensions. We acknowledge this introduces bias through single model judgment. We employ consistent prompt formatting and evaluate speeches sampled across diverse political contexts. Specific prompts for each dimension are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S10\" title=\"10. LLM-as-a-Judge Evaluation Prompts &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "parliamentary",
                    "training",
                    "political",
                    "complete",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Authenticity</span> (J_Auth) assesses whether content reflects genuine political speech (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i3.I1.i2.I1.i1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> better).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Political Appropriateness</span> (J_PolApp) evaluates whether tone is suitable for political speech (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i3.I1.i2.I1.i2.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math> better).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Political Spectrum Alignment</span> (PSA) evaluates ideological positioning on the left-right spectrum. We adapt semantic embedding approaches <cite class=\"ltx_cite ltx_citemacro_cite\">Rheault and Cochrane (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib30\" title=\"\">2020</a>)</cite> for LLM-generated speech evaluation, drawing on\nthe Left-Right (RILE) scale methodology <cite class=\"ltx_cite ltx_citemacro_cite\">Volkens et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib35\" title=\"\">2013</a>); Budge (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib9\" title=\"\">2013</a>)</cite>. Our metric employs a two-stage approach combining semantic similarity with ideological distance. We create reference embeddings by grouping parliamentary speeches by political orientations (Far-left through Far-right, including intermediate positions) and computing centroid embeddings using sentence transformers. Orientations map to numerical values where Far-left = -6, Centre = 0, Far-right = +6.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "parliamentary",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"po^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p4.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>o</mi><mo>&#8727;</mo></msup></mrow><annotation encoding=\"application/x-tex\">po^{*}</annotation></semantics></math> is the closest matching orientation, <math alttext=\"\\mathcal{PO}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p4.m2\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119978;</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{PO}</annotation></semantics></math>\nthe set of all political orientations, <math alttext=\"c_{po}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p4.m3\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{po}</annotation></semantics></math> the orientation centroid, and <math alttext=\"\\text{sim}(s,c_{po})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p4.m4\" intent=\":literal\"><semantics><mrow><mtext>sim</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo>,</mo><msub><mi>c</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{sim}(s,c_{po})</annotation></semantics></math> the cosine similarity between generated speech <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p4.m5\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> and centroid.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Party Alignment</span> (Party Align) applies the same embedding methodology to party-specific alignment, using party affiliation rather than political orientation for centroid construction. The alignment score measures cosine similarity between generated speech and expected party centroid:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Context Stability</span> measures performance consistency using coefficient of variation, with higher scores indicating more consistent performance across political contexts. This meta-evaluation applies to all speech evaluation metrics, providing diagnostic insight into model reliability. For cross-metric comparison, all metrics, Computational &amp; LLM-Judge, are normalized to 0-1 scale.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We selected five language models representing distinct architectural approaches to establish baseline performance for parliamentary speech generation:\n<span class=\"ltx_text ltx_font_bold\">Mistral 7B v0.3</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib18\" title=\"\">2023</a>)</cite> uses Grouped Query Attention and Sliding Window Attention for efficient long-context processing.\n<span class=\"ltx_text ltx_font_bold\">Llama 3.1 8B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Dubey et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib11\" title=\"\">2024</a>)</cite> features 128k-token context window and expanded vocabulary.\n<span class=\"ltx_text ltx_font_bold\">Gemma 2 9B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Team et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib32\" title=\"\">2024</a>)</cite> employs alternating local/global attention across 42 layers with logit soft-capping.\n<span class=\"ltx_text ltx_font_bold\">Qwen2 7B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Team (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib33\" title=\"\">2024</a>)</cite> is a multilingual model with enhanced reasoning capabilities.\n<span class=\"ltx_text ltx_font_bold\">YI 6B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">AI et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib2\" title=\"\">2025</a>)</cite> emphasizes strong reasoning and coding performance.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "parliamentary",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed Quantized Low-Rank Adaptation (QLoRA) <cite class=\"ltx_cite ltx_citemacro_cite\">Dettmers et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib10\" title=\"\">2023</a>)</cite> for parameter-efficient fine-tuning. (<math alttext=\"r=16\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">r=16</annotation></semantics></math>, <math alttext=\"\\alpha=16\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=16</annotation></semantics></math>, <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math> epochs). Model-specific chat templates structure training inputs with political metadata (party affiliation, topic classification, orientation, section, and house). Training used SFTTrainer from TRL with 80%-20% train-test splits and automated checkpointing. Fine-tuned models were saved with adapter weights for subsequent evaluation, ensuring consistent model states across experiments.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "epochs",
                    "training",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We planned to generate 30,000 speeches (3,000 per model-type combination across 10 models) using stratified sampling from the held-out test set. To maintain consistency with training conditions, our prompt distribution matches the ParliaBench Dataset structure: 90% generic instruction prompts formatted with political context (party, topic, orientation, section, house) and 10% specific parliamentary questions from the test set.</p>\n\n",
                "matched_terms": [
                    "parliamentary",
                    "training",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generation employed nucleus sampling (<math alttext=\"temperature=0.7\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>=</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">temperature=0.7</annotation></semantics></math>, <math alttext=\"top\\-p=0.85\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><mo>=</mo><mn>0.85</mn></mrow><annotation encoding=\"application/x-tex\">top\\-p=0.85</annotation></semantics></math>, <math alttext=\"repetition_{p}enalty=1.2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>n</mi><mi>p</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>y</mi></mrow><mo>=</mo><mn>1.2</mn></mrow><annotation encoding=\"application/x-tex\">repetition_{p}enalty=1.2</annotation></semantics></math>). Generated speeches underwent validation for template leakage, encoding corruption, semantic relevance, and length constraints. Invalid outputs were automatically regenerated (max 3 attempts). Baseline models exhibited higher failure rates, suggesting fine-tuning improved output quality. To ensure fair cross-model comparison, we retained only speeches successfully generated by all 10 model-type combinations, yielding 29,220 speeches.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "max",
                    "finetuning",
                    "length"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generated speeches were then evaluated using our LLM-judge framework. Speeches with incomplete dimension ratings were excluded, resulting in 27,560 fully evaluated speeches, across all six dimensions, for all models. All subsequent results use the 27,560 fully evaluated speeches, which maintain balanced representation across political affiliations and topics. Complete implementation details, including QLoRA configuration, training parameters, chat templates for all architectures, and speech generation validation methodology, are provided in the Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S11\" title=\"11. Setup Implementation Details &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation",
                    "training",
                    "political",
                    "complete",
                    "configuration"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated 27,560 generated speeches using our evaluation framework. This section presents fine-tuning effectiveness and performance patterns across political parties, topic domains, and ideological orientations. Representative examples of generated speeches are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S12\" title=\"12. Representative Generated Speeches &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, our novel political authenticity metrics (PSA and Party Align) displayed strong responsiveness to fine-tuning. All five models significantly improved PSA (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>), with effect sizes ranging from small to very large (d=0.141-1.045). Party Align showed similar patterns (4 of 5 models improved, d=0.099-1.221). These substantial effects validate that our embedding based metrics capture critical political authenticity dimensions unavailable to conventional evaluation.\nFor complete t-test results including effect sizes, see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T13\" title=\"Table 13 &#8227; 13.5. Statistical Significance Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "complete",
                    "finetuning",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F3\" title=\"Figure 3 &#8227; 6.1. Overview and Fine-Tuning Impact &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows fine-tuning impact across evaluation categories. <span class=\"ltx_text ltx_font_italic\">YI</span> achieved the strongest improvements across all dimensions , while <span class=\"ltx_text ltx_font_italic\">Llama</span> had consistent gains. <span class=\"ltx_text ltx_font_italic\">Gemma2</span> and <span class=\"ltx_text ltx_font_italic\">Qwen2</span> exhibited quality trade-offs, with improvements in one category accompanied by declines in others, suggesting architectural differences in how models balance competing objectives during fine-tuning. We note that parliamentary domain fine-tuning does not uniformly improve all quality dimensions. Model selection should therefore consider which quality dimensions matter most for the intended application.</p>\n\n",
                "matched_terms": [
                    "parliamentary",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Party alignment Patterns</span>. Party alignment performance varied substantially across models (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F4\" title=\"Figure 4 &#8227; 6.2. Political Context Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). Major parties (Conservative, Labour) achieved stable performance across models, benefiting from substantial training data (58.9%, 24.3%). Minor parties exhibited greater variability. <span class=\"ltx_text ltx_font_italic\">Mistral</span> struggled with heterogeneous groups (Non-Affiliated: 0.436), while <span class=\"ltx_text ltx_font_italic\">Qwen</span> excelled with ideologically coherent minorities (Bishops: 0.664). <span class=\"ltx_text ltx_font_italic\">YI</span> demonstrated robust cross-party performance (0.614-0.633).\nDetailed scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T8\" title=\"Table 8 &#8227; 13.1. Party-Specific Performance &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both new political authenticity metrics (PSA and Party Align) successfully discriminate their target political dimensions. Party Align distinguishes parties while PSA distinguishes orientations (both <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>). Our analysis reveals that Party Align performance depends primarily on data abundance and ideological coherence rather than party size alone. Models successfully learn party-specific language patterns when training data provides clear stylistic signals, indicating targeted data collection for under-represented parties could improve coverage.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training",
                    "size",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Topic Difficulty Analysis</span>. Different topics posed different challenges (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F7\" title=\"Figure 7 &#8227; 6.3. Topic Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>). Science and Geography ranked as most difficult while Finance, Business, and Economics ranked lowest. Technical and natural science domains display higher cross-model disagreement than economic and political topics, consistent with greater terminological specialization and rapidly evolving concepts. In contrast, economic and political discussions employs more stable conceptual frameworks aligned with core parliamentary functions.</p>\n\n",
                "matched_terms": [
                    "parliamentary",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Performance across political orientations showed expected patterns. Centrist positions (Centre-left: 0.607, Centre-right: 0.551) dominated the dataset (88%) and achieved higher scores. Model-specific strengths emerged as both <span class=\"ltx_text ltx_font_italic\">Gemma</span> and <span class=\"ltx_text ltx_font_italic\">Qwen</span> achieved highest scores on Right positions and <span class=\"ltx_text ltx_font_italic\">Mistral</span> underperformed consistently, indicating architectural rather than ideological limitations. As models are optimized for mainstream parliamentary speeches, extreme positions may require specialized training approaches. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F8\" title=\"Figure 8 &#8227; 6.4. Political Orientations Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> illustrates these patterns. Detailed orientation difficulty rankings are provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T10\" title=\"Table 10 &#8227; 13.2. Metric Validation: Political Discrimination Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a></p>\n\n",
                "matched_terms": [
                    "parliamentary",
                    "training",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results establish several key findings: (i) Architectural design impacts political authenticity, with extended context windows enabling consistent improvements; (ii) Domain-specific fine-tuning proves essential as 45 of 70 metric comparisons showed statistically significant improvements and (iii) Novel political authenticity metrics (PSA, Party Align) successfully capture dimensions unavailable to conventional NLP metrics, validated through both fine-tuning responsiveness and discrimination testing (both <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To the best of our knowledge, ParliaBench represents the first benchmark resource addressing the specialized challenges of parliamentary speech generation, comprising dataset, evaluation framework, novel metrics, and baseline benchmarks. ParliaBench provides standardized evaluation protocols and baseline performance results that\nsupport systematic comparison and reproducible research in the field.\nOur results demonstrate that domain specific fine-tuning produces significant quality improvements, while\nour novel political authenticity metrics successfully capture ideological dimensions absent from conventional evaluation approaches.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "parliamentary",
                    "generation",
                    "political",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Future Directions:</span> Extensions include: (i) multilingual evaluation for European parliamentary systems, (ii) human evaluation protocols for validation, and (iii) systematic assessment of political bias and perspective maintenance across viewpoints.</p>\n\n",
                "matched_terms": [
                    "parliamentary",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both datasets (Training and Generated Data) and finetuned models are available under a CC BY License at the ParliaBench collection on <a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/collections/argyrotsipi/parliabench\" title=\"\">Hugging Face</a>. \n<br class=\"ltx_break\"/>Resources are openly accessible on <a class=\"ltx_ref ltx_href\" href=\"https://argyrotsipi.github.io/ParliaBench/\" title=\"\">Website</a> and <a class=\"ltx_ref ltx_href\" href=\"https://github.com/ArgyroTsipi/ParliaBench\" title=\"\">GitHub</a>.</p>\n\n",
                "matched_terms": [
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation measures linguistic quality and political authenticity but does not assess argument structure or verify factual accuracy against parliamentary records. Our methods rely entirely on automated metrics without human validation, and LLM-as-a-Judge approaches may carry inherent biases.\nThis work is intended for research and educational purposes, not deployment in actual democratic processes.</p>\n\n",
                "matched_terms": [
                    "parliamentary",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work establishes a benchmark resource for evaluating LLM-generated parliamentary speech in research and educational contexts. These resources should only be used as assistance to human experts with consideration of their limitations and biases. The parliamentary data is derived from publicly available UK parliamentary proceedings (ParlaMint corpus) licensed under Creative Commons Attribution 4.0 International, which our derived datasets maintain.\nGenerated parliamentary speeches must be clearly identified as AI-generated content and not misrepresented as authentic political speeches from actual parliamentarians.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "parliamentary",
                    "data",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S9.T3\" title=\"Table 3 &#8227; 9.1. Dataset Statistics &#8227; 9. Dataset Details &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> presents detailed breakdown of speeches by political affiliation in the Parliamentary Debates Benchmark.</p>\n\n",
                "matched_terms": [
                    "parliamentary",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This subsection documents the hybrid classification strategy employed to map Comparative Agendas Project (CAP) categories to EuroVoc domains for topic assignment in our parliamentary speech dataset. We employed direct semantic mapping for 16 categories, while 6 remaining categories required automated classification, as detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S9.T4\" title=\"Table 4 &#8227; 9.2. Hybrid Classification Strategy &#8227; 9. Dataset Details &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "parliamentary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This appendix documents the automated evaluation system used to assess the quality of generated parliamentary speeches. The system employs Flow-Judge-v0.1, a 3.8B parameter evaluation model, to score speeches across six dimensions using a 10-point scale.</p>\n\n",
                "matched_terms": [
                    "parliamentary",
                    "parameter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Does the speech follow a clear logical progression? Are arguments well-connected and ideas flow naturally with appropriate parliamentary structure?</p>\n\n",
                "matched_terms": [
                    "speech",
                    "parliamentary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Is the message conveyed efficiently without excessive verbosity, within the context of formal parliamentary speech where longer discourse is expected?</p>\n\n",
                "matched_terms": [
                    "speech",
                    "parliamentary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Batch Size</span>: 32 speeches per batch</p>\n\n",
                "matched_terms": [
                    "size",
                    "batch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The QLoRA configuration parameters were selected based on established best practices for parameter-efficient fine-tuning in specialized domains. The rank value of 16 provides sufficient adaptation capacity while maintaining computational efficiency. Setting LoRA Alpha equal to the rank ensures reliable baseline performance, while disabling dropout enables Unsloth framework optimizations essential for efficient training on A100 hardware.</p>\n\n",
                "matched_terms": [
                    "training",
                    "value",
                    "configuration",
                    "finetuning",
                    "lora"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Target modules encompass all linear transformation layers (q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj) to achieve performance comparable to full fine-tuning while requiring only a fraction of the computational resources. The consistent random state across all architectures ensures reproducible results essential for systematic model comparison.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "linear"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section provides the complete chat template specifications used for training and generation across all model architectures, ensuring reproducibility of experimental results.</p>\n\n",
                "matched_terms": [
                    "complete",
                    "generation",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_typewriter\">You are a seasoned UK parliamentary member.\nUse proper British parliamentary language appropriate for the specified House.\n<br class=\"ltx_break\"/>The speech should reflect the political orientation and typical positions of the specified party on the given topic.\n<br class=\"ltx_break\"/></span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "parliamentary",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">You are a seasoned UK parliamentary member. Generate a coherent speech of\nmin_words - max_words words in standard English (no Unicode artifacts, no special characters).</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_typewriter\">\nUse proper British parliamentary language appropriate for the specified House. \n<br class=\"ltx_break\"/>The speech should reflect the political orientation and typical positions of the specified party on the given topic. \n<br class=\"ltx_break\"/></span></p>\n\n",
                "matched_terms": [
                    "speech",
                    "parliamentary",
                    "standard",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This setup helps fine-tuned models learn to give responses that match a party&#8217;s views, stay on topic, follow parliamentary rules and political views.</p>\n\n",
                "matched_terms": [
                    "parliamentary",
                    "political"
                ]
            }
        ]
    },
    "S13.T8": {
        "source_file": "ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech",
        "caption": "Table 8: Model Performance by Political Party (Fine-Tuned Models Only)",
        "body": "Party\nGemma 2 9B\nLlama 3.1 8B\nMistral 7B v0.3\nQwen2 7B\nYI 6B\nAverage\nStd\n\n\n\n\nBishops\n0.609\n0.624\n0.550\n0.664\n0.562\n0.602\n0.042\n\n\nConservative\n0.576\n0.561\n0.487\n0.560\n0.569\n0.551\n0.032\n\n\nCrossbench\n0.599\n0.623\n0.453\n0.574\n0.592\n0.568\n0.060\n\n\nDemocratic Unionist Party\n0.630\n0.559\n0.593\n0.624\n0.594\n0.600\n0.025\n\n\nGreen Party\n0.614\n0.585\n0.522\n0.572\n0.586\n0.576\n0.030\n\n\nIndependent\n0.587\n0.541\n0.482\n0.549\n0.521\n0.536\n0.034\n\n\nLabour\n0.612\n0.607\n0.540\n0.600\n0.607\n0.593\n0.027\n\n\nLiberal Democrats\n0.598\n0.580\n0.524\n0.558\n0.581\n0.568\n0.025\n\n\nNon-Affiliated\n0.620\n0.554\n0.436\n0.526\n0.633\n0.554\n0.071\n\n\nPlaid Cymru\n0.619\n0.558\n0.530\n0.572\n0.577\n0.571\n0.029\n\n\nScottish National Party\n0.597\n0.622\n0.543\n0.596\n0.614\n0.594\n0.028",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Party</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">Gemma 2 9B</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">Llama 3.1 8B</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">Mistral 7B v0.3</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">Qwen2 7B</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">YI 6B</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">Average</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">Std</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Bishops</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.609</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.624</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.550</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.664</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.562</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.602</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.042</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Conservative</th>\n<td class=\"ltx_td ltx_align_right\">0.576</td>\n<td class=\"ltx_td ltx_align_right\">0.561</td>\n<td class=\"ltx_td ltx_align_right\">0.487</td>\n<td class=\"ltx_td ltx_align_right\">0.560</td>\n<td class=\"ltx_td ltx_align_right\">0.569</td>\n<td class=\"ltx_td ltx_align_right\">0.551</td>\n<td class=\"ltx_td ltx_align_right\">0.032</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Crossbench</th>\n<td class=\"ltx_td ltx_align_right\">0.599</td>\n<td class=\"ltx_td ltx_align_right\">0.623</td>\n<td class=\"ltx_td ltx_align_right\">0.453</td>\n<td class=\"ltx_td ltx_align_right\">0.574</td>\n<td class=\"ltx_td ltx_align_right\">0.592</td>\n<td class=\"ltx_td ltx_align_right\">0.568</td>\n<td class=\"ltx_td ltx_align_right\">0.060</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Democratic Unionist Party</th>\n<td class=\"ltx_td ltx_align_right\">0.630</td>\n<td class=\"ltx_td ltx_align_right\">0.559</td>\n<td class=\"ltx_td ltx_align_right\">0.593</td>\n<td class=\"ltx_td ltx_align_right\">0.624</td>\n<td class=\"ltx_td ltx_align_right\">0.594</td>\n<td class=\"ltx_td ltx_align_right\">0.600</td>\n<td class=\"ltx_td ltx_align_right\">0.025</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Green Party</th>\n<td class=\"ltx_td ltx_align_right\">0.614</td>\n<td class=\"ltx_td ltx_align_right\">0.585</td>\n<td class=\"ltx_td ltx_align_right\">0.522</td>\n<td class=\"ltx_td ltx_align_right\">0.572</td>\n<td class=\"ltx_td ltx_align_right\">0.586</td>\n<td class=\"ltx_td ltx_align_right\">0.576</td>\n<td class=\"ltx_td ltx_align_right\">0.030</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Independent</th>\n<td class=\"ltx_td ltx_align_right\">0.587</td>\n<td class=\"ltx_td ltx_align_right\">0.541</td>\n<td class=\"ltx_td ltx_align_right\">0.482</td>\n<td class=\"ltx_td ltx_align_right\">0.549</td>\n<td class=\"ltx_td ltx_align_right\">0.521</td>\n<td class=\"ltx_td ltx_align_right\">0.536</td>\n<td class=\"ltx_td ltx_align_right\">0.034</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Labour</th>\n<td class=\"ltx_td ltx_align_right\">0.612</td>\n<td class=\"ltx_td ltx_align_right\">0.607</td>\n<td class=\"ltx_td ltx_align_right\">0.540</td>\n<td class=\"ltx_td ltx_align_right\">0.600</td>\n<td class=\"ltx_td ltx_align_right\">0.607</td>\n<td class=\"ltx_td ltx_align_right\">0.593</td>\n<td class=\"ltx_td ltx_align_right\">0.027</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Liberal Democrats</th>\n<td class=\"ltx_td ltx_align_right\">0.598</td>\n<td class=\"ltx_td ltx_align_right\">0.580</td>\n<td class=\"ltx_td ltx_align_right\">0.524</td>\n<td class=\"ltx_td ltx_align_right\">0.558</td>\n<td class=\"ltx_td ltx_align_right\">0.581</td>\n<td class=\"ltx_td ltx_align_right\">0.568</td>\n<td class=\"ltx_td ltx_align_right\">0.025</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Non-Affiliated</th>\n<td class=\"ltx_td ltx_align_right\">0.620</td>\n<td class=\"ltx_td ltx_align_right\">0.554</td>\n<td class=\"ltx_td ltx_align_right\">0.436</td>\n<td class=\"ltx_td ltx_align_right\">0.526</td>\n<td class=\"ltx_td ltx_align_right\">0.633</td>\n<td class=\"ltx_td ltx_align_right\">0.554</td>\n<td class=\"ltx_td ltx_align_right\">0.071</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Plaid Cymru</th>\n<td class=\"ltx_td ltx_align_right\">0.619</td>\n<td class=\"ltx_td ltx_align_right\">0.558</td>\n<td class=\"ltx_td ltx_align_right\">0.530</td>\n<td class=\"ltx_td ltx_align_right\">0.572</td>\n<td class=\"ltx_td ltx_align_right\">0.577</td>\n<td class=\"ltx_td ltx_align_right\">0.571</td>\n<td class=\"ltx_td ltx_align_right\">0.029</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Scottish National Party</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">0.597</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">0.622</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">0.543</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">0.596</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">0.614</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">0.594</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">0.028</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "political",
            "crossbench",
            "llama",
            "mistral",
            "democratic",
            "std",
            "unionist",
            "national",
            "v03",
            "conservative",
            "bishops",
            "cymru",
            "independent",
            "plaid",
            "labour",
            "nonaffiliated",
            "model",
            "qwen2",
            "average",
            "only",
            "performance",
            "gemma",
            "green",
            "finetuned",
            "liberal",
            "democrats",
            "models",
            "scottish",
            "party"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Party alignment Patterns</span>. Party alignment performance varied substantially across models (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F4\" title=\"Figure 4 &#8227; 6.2. Political Context Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). Major parties (Conservative, Labour) achieved stable performance across models, benefiting from substantial training data (58.9%, 24.3%). Minor parties exhibited greater variability. <span class=\"ltx_text ltx_font_italic\">Mistral</span> struggled with heterogeneous groups (Non-Affiliated: 0.436), while <span class=\"ltx_text ltx_font_italic\">Qwen</span> excelled with ideologically coherent minorities (Bishops: 0.664). <span class=\"ltx_text ltx_font_italic\">YI</span> demonstrated robust cross-party performance (0.614-0.633).\nDetailed scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T8\" title=\"Table 8 &#8227; 13.1. Party-Specific Performance &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T8\" title=\"Table 8 &#8227; 13.1. Party-Specific Performance &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents detailed performance scores for all UK parliamentary parties across fine-tuned models.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Parliamentary speech generation presents specific challenges for large language models beyond standard text generation tasks. Unlike general text generation, parliamentary speeches require not only linguistic quality but also political authenticity and ideological consistency. Current language models lack specialized training for parliamentary contexts, and existing evaluation methods focus on standard NLP metrics rather than political authenticity. To address this, we present ParliaBench, a benchmark for parliamentary speech generation. We constructed a dataset of speeches from UK Parliament to enable systematic model training. We introduce an evaluation framework combining computational metrics with LLM-as-a-judge assessments for measuring generation quality across three dimensions: linguistic quality, semantic coherence, and political authenticity. We propose two novel embedding-based metrics, Political Spectrum Alignment and Party Alignment, to quantify ideological positioning. We fine-tuned five large language models (LLMs), generated 28k speeches, and evaluated them\nusing our framework, comparing baseline and fine-tuned models. Results show that fine-tuning produces statistically significant improvements across the majority of metrics and our novel metrics demonstrate strong discriminative power for political dimensions.\n\n\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Parliamentary Speech Generation,LLM Evaluation,Political Authenticity,Benchmark Evaluation</p>\n\n",
                "matched_terms": [
                    "model",
                    "finetuned",
                    "political",
                    "models",
                    "party",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Democracy thrives through debate. Democratic parliaments are open forums where elected representatives engage in arguments over policy <cite class=\"ltx_cite ltx_citemacro_cite\">Back et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib4\" title=\"\">2021</a>)</cite>. These debates provide unique insights into political reasoning and ideological positioning. Researchers in political science and computational linguistics increasingly seek to understand and model parliamentary debates.</p>\n\n",
                "matched_terms": [
                    "democratic",
                    "model",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Yet generating authentic parliamentary speech presents significant challenges that extend well beyond typical text generation tasks. The complexity becomes apparent when considering specific examples. A Labour MP discussing taxation policy must sound distinctly different from a Conservative counterpart not just in policy position, but most significantly, in their fundamental approach to governance. Large language models have shown promise across political applications, from sentiment analysis <cite class=\"ltx_cite ltx_citemacro_cite\">Bestvater and Monroe (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib6\" title=\"\">2023</a>)</cite> and election forecasting <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib22\" title=\"\">2024b</a>)</cite> to synthetic survey data generation <cite class=\"ltx_cite ltx_citemacro_cite\">Argyle et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib3\" title=\"\">2023</a>); Bisbee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib7\" title=\"\">2024</a>)</cite>. Authentic parliamentary speech generation, however, remains challenging.</p>\n\n",
                "matched_terms": [
                    "models",
                    "labour",
                    "conservative",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contributions.</span> We address these limitations by establishing a benchmark resource designed\nspecifically for evaluating parliamentary speech generation. First, we develop a curated dataset containing 448k speeches from UK Parliament (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S3\" title=\"3. ParliaBench Dataset &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Second, we introduce a multi-dimensional evaluation framework that assesses both linguistic quality and political authenticity (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4\" title=\"4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). Third, we fine-tune five large language models and generate 28k parliamentary speeches to establish baseline performance (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S5\" title=\"5. Experimental Setup &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). Finally, we demonstrate the framework&#8217;s effectiveness through systematic evaluation (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6\" title=\"6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>). Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S1.F1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> provides an overview of the complete ParliaBench framework and experimental methodology.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in large language models (LLMs) have enabled a wide range of applications across political domains. <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib22\" title=\"\">2024b</a>)</cite> outlines several applications of LLM in political contexts, covering predictive, generative, and simulation-based approaches. The use of LLMs as substitutes for human experts in annotating political texts across multiple languages is explored in <cite class=\"ltx_cite ltx_citemacro_cite\">Heseltine and von Hohenberg (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib16\" title=\"\">2024</a>)</cite>, while <cite class=\"ltx_cite ltx_citemacro_cite\">Gunes and Florczak (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib14\" title=\"\">2023</a>)</cite> employ LLMs for classifying U.S. Congressional bills. <cite class=\"ltx_cite ltx_citemacro_cite\">Argyle et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib3\" title=\"\">2023</a>)</cite> investigate LLMs as proxies for specific human subpopulations in social science research and <cite class=\"ltx_cite ltx_citemacro_cite\">Bisbee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib7\" title=\"\">2024</a>)</cite> raise concerns about the quality, reliability, and reproducibility of synthetic survey data generated by LLMs.\nAgent-based LLMs are utilized as coalition negotiators <cite class=\"ltx_cite ltx_citemacro_cite\">Moghimifar et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib25\" title=\"\">2024</a>)</cite> and as U.S. senators\nsimulating legislative processes <cite class=\"ltx_cite ltx_citemacro_cite\">Baker and Azher (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib5\" title=\"\">2024</a>)</cite>. However, these approaches predominantly emphasize analytical and simulation capabilities rather than authentic speech generation quality.</p>\n\n",
                "matched_terms": [
                    "models",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The emergence of LLM-as-a-Judge evaluation <cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib40\" title=\"\">2023</a>)</cite> offers scalable alternatives for nuanced assessment, achieving over 80% agreement with human evaluators in complex judgment tasks. <cite class=\"ltx_cite ltx_citemacro_cite\">Liu and Sun (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib24\" title=\"\">2023</a>)</cite> further validate this approach, demonstrating GPT-4&#8217;s high alignment with human thematic coding in political analysis. This methodology has been successfully applied across diverse contexts, from general LLM benchmarking through competitive debates <cite class=\"ltx_cite ltx_citemacro_cite\">Moniri et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib26\" title=\"\">2025</a>)</cite> to long-context reasoning evaluation in parliamentary debates <cite class=\"ltx_cite ltx_citemacro_cite\">Tiwari et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib34\" title=\"\">2025</a>)</cite>. These approaches demonstrate the viability of automated evaluation for argumentative and political content, though models exhibit documented biases toward Western, educated populations <cite class=\"ltx_cite ltx_citemacro_cite\">Durmus et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib12\" title=\"\">2023</a>)</cite> and systematic preferences in political simulations <cite class=\"ltx_cite ltx_citemacro_cite\">Qi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib29\" title=\"\">2024</a>)</cite>. Recent advances in retrieval-augmented generation and chain-of-thought reasoning provide enhanced capabilities for contextually-grounded political text generation, though their application to parliamentary speech evaluation remains underexplored.</p>\n\n",
                "matched_terms": [
                    "models",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 3: Content Processing &amp; Filtering</span>. Distinguished substantive content from procedural elements, separating parliamentary prompts from speech responses. Filtered procedural noise and non-substantive speeches. For political affiliations, we applied 1000-speech minimum threshold, reducing from 28 original affiliations to 11 to ensure stable model training.</p>\n\n",
                "matched_terms": [
                    "model",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final dataset contains 447,778 speeches from 1,901 unique speakers across 11 political affiliations, with major parties (Conservative: 263,513; Labour: 108,831) dominating representation. The dataset contains approximately 99.94 million words, with speeches averaging 223 words (median: 99 words). This distribution reflects natural variation in parliamentary speeches, from brief procedural statements to extended policy expositions. Temporal coverage captures significant political events including Brexit debates, and the COVID-19 pandemic response, ensuring exposure to diverse political contexts and rhetorical situations. Note: \"Bishops\", \"Crossbench\", and \"Non-Affiliated\" are not political parties in the traditional sense but formal affiliations in Parliament.</p>\n\n",
                "matched_terms": [
                    "bishops",
                    "political",
                    "crossbench",
                    "labour",
                    "conservative",
                    "nonaffiliated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generated Dataset.</span> We release 27,560 speeches (model outputs) produced during evaluation (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S5\" title=\"5. Experimental Setup &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). The generated dataset follows the same format with additional fields: <span class=\"ltx_text ltx_font_italic\">Model</span> (architecture identifier), <span class=\"ltx_text ltx_font_italic\">Type</span> (baseline/fine-tuned), <span class=\"ltx_text ltx_font_italic\">Generated Speech</span> (model output), and <span class=\"ltx_text ltx_font_italic\">Evaluation Scores</span> (computed metrics). To ensure a fair comparison, we used the same input prompts across all models and model types, allowing evaluation on identical inputs.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both datasets (Training and Generated Data) and finetuned models are available under a CC BY License at the ParliaBench collection on <a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/collections/argyrotsipi/parliabench\" title=\"\">Hugging Face</a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "finetuned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Parliamentary speech evaluation requires assessment across multiple levels that generic benchmarks cannot capture. Our framework operates on two levels: (i) <span class=\"ltx_text ltx_font_bold\">speech evaluation metrics</span> measuring generation quality across three dimensions, and (ii) <span class=\"ltx_text ltx_font_bold\">consistency evaluation metrics</span> measuring performance reliability across political contexts. This dual-track approach also combines computational metrics with LLM-judge evaluation.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our framework introduces two novel computational metrics (Political Spectrum Alignment and Party Alignment, Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.SS1.SSS1\" title=\"4.1.1. Novel Political Authenticity Metrics &#8227; 4.1. Speech Evaluation Metrics &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4.1.1</span></a>) alongside established metrics from both computational and LLM-judge traditions.</p>\n\n",
                "matched_terms": [
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Computational metrics provide deterministic assessment through established NLP measures, while LLM-judge metrics capture nuanced qualities requiring contextual understanding. For LLM-judge evaluation, we adapt the methodology from <cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib40\" title=\"\">2023</a>)</cite>. We employed Flow-Judge-v0.1 as our LLM judge, an LLM specialized in system evaluation tasks. It inherits it&#8217;s architecture from Phi-3.5-mini instruct <cite class=\"ltx_cite ltx_citemacro_cite\">Abdin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib1\" title=\"\">2024</a>)</cite>, thus ensuring complete architectural and training data independence from the evaluated models. Our judge rates each speech on a 1-10 scale with written explanations across six parliamentary specific dimensions. We acknowledge this introduces bias through single model judgment. We employ consistent prompt formatting and evaluate speeches sampled across diverse political contexts. Specific prompts for each dimension are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S10\" title=\"10. LLM-as-a-Judge Evaluation Prompts &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ two novel embedding-based metrics for political authenticity assessment: Political Spectrum Alignment (PSA) and Party Alignment (Party Align). Detailed calculation methodology provided in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.SS1.SSS1\" title=\"4.1.1. Novel Political Authenticity Metrics &#8227; 4.1. Speech Evaluation Metrics &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4.1.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Party Alignment</span> (Party Align) applies the same embedding methodology to party-specific alignment, using party affiliation rather than political orientation for centroid construction. The alignment score measures cosine similarity between generated speech and expected party centroid:</p>\n\n",
                "matched_terms": [
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Context Stability</span> measures performance consistency using coefficient of variation, with higher scores indicating more consistent performance across political contexts. This meta-evaluation applies to all speech evaluation metrics, providing diagnostic insight into model reliability. For cross-metric comparison, all metrics, Computational &amp; LLM-Judge, are normalized to 0-1 scale.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We examine consistency across three dimensions: political parties, topic domains, and political orientations. The stability calculation quantifies performance variability:</p>\n\n",
                "matched_terms": [
                    "performance",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We selected five language models representing distinct architectural approaches to establish baseline performance for parliamentary speech generation:\n<span class=\"ltx_text ltx_font_bold\">Mistral 7B v0.3</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib18\" title=\"\">2023</a>)</cite> uses Grouped Query Attention and Sliding Window Attention for efficient long-context processing.\n<span class=\"ltx_text ltx_font_bold\">Llama 3.1 8B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Dubey et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib11\" title=\"\">2024</a>)</cite> features 128k-token context window and expanded vocabulary.\n<span class=\"ltx_text ltx_font_bold\">Gemma 2 9B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Team et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib32\" title=\"\">2024</a>)</cite> employs alternating local/global attention across 42 layers with logit soft-capping.\n<span class=\"ltx_text ltx_font_bold\">Qwen2 7B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Team (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib33\" title=\"\">2024</a>)</cite> is a multilingual model with enhanced reasoning capabilities.\n<span class=\"ltx_text ltx_font_bold\">YI 6B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">AI et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib2\" title=\"\">2025</a>)</cite> emphasizes strong reasoning and coding performance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "qwen2",
                    "models",
                    "llama",
                    "mistral",
                    "v03",
                    "performance",
                    "gemma"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed Quantized Low-Rank Adaptation (QLoRA) <cite class=\"ltx_cite ltx_citemacro_cite\">Dettmers et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib10\" title=\"\">2023</a>)</cite> for parameter-efficient fine-tuning. (<math alttext=\"r=16\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">r=16</annotation></semantics></math>, <math alttext=\"\\alpha=16\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=16</annotation></semantics></math>, <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math> epochs). Model-specific chat templates structure training inputs with political metadata (party affiliation, topic classification, orientation, section, and house). Training used SFTTrainer from TRL with 80%-20% train-test splits and automated checkpointing. Fine-tuned models were saved with adapter weights for subsequent evaluation, ensuring consistent model states across experiments.</p>\n\n",
                "matched_terms": [
                    "model",
                    "finetuned",
                    "political",
                    "models",
                    "party"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We planned to generate 30,000 speeches (3,000 per model-type combination across 10 models) using stratified sampling from the held-out test set. To maintain consistency with training conditions, our prompt distribution matches the ParliaBench Dataset structure: 90% generic instruction prompts formatted with political context (party, topic, orientation, section, house) and 10% specific parliamentary questions from the test set.</p>\n\n",
                "matched_terms": [
                    "models",
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generation employed nucleus sampling (<math alttext=\"temperature=0.7\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>=</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">temperature=0.7</annotation></semantics></math>, <math alttext=\"top\\-p=0.85\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><mo>=</mo><mn>0.85</mn></mrow><annotation encoding=\"application/x-tex\">top\\-p=0.85</annotation></semantics></math>, <math alttext=\"repetition_{p}enalty=1.2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>n</mi><mi>p</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>y</mi></mrow><mo>=</mo><mn>1.2</mn></mrow><annotation encoding=\"application/x-tex\">repetition_{p}enalty=1.2</annotation></semantics></math>). Generated speeches underwent validation for template leakage, encoding corruption, semantic relevance, and length constraints. Invalid outputs were automatically regenerated (max 3 attempts). Baseline models exhibited higher failure rates, suggesting fine-tuning improved output quality. To ensure fair cross-model comparison, we retained only speeches successfully generated by all 10 model-type combinations, yielding 29,220 speeches.</p>\n\n",
                "matched_terms": [
                    "models",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generated speeches were then evaluated using our LLM-judge framework. Speeches with incomplete dimension ratings were excluded, resulting in 27,560 fully evaluated speeches, across all six dimensions, for all models. All subsequent results use the 27,560 fully evaluated speeches, which maintain balanced representation across political affiliations and topics. Complete implementation details, including QLoRA configuration, training parameters, chat templates for all architectures, and speech generation validation methodology, are provided in the Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S11\" title=\"11. Setup Implementation Details &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated 27,560 generated speeches using our evaluation framework. This section presents fine-tuning effectiveness and performance patterns across political parties, topic domains, and ideological orientations. Representative examples of generated speeches are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S12\" title=\"12. Representative Generated Speeches &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.T2\" title=\"Table 2 &#8227; 6.1. Overview and Fine-Tuning Impact &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents metric results organized by our framework assessment categories. Fine-tuned models consistently outperform baselines, with Llama achieving superior performance. Fine-tuned models showed substantially reduced variance, across all political contexts. Extended context windows (128k tokens) and larger vocabularies contribute to architectural advantages.\n</p>\n\n",
                "matched_terms": [
                    "finetuned",
                    "political",
                    "models",
                    "llama",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Pairwise t-tests confirm statistical significance of fine-tuning effects (45 out of 70 comparisons). Model architectures exhibited differential responsiveness: <span class=\"ltx_text ltx_font_italic\">YI</span> and <span class=\"ltx_text ltx_font_italic\">Llama</span> achieved notable improvements (11/14 metrics, 79%), while others showed more selective gains (improvements marked with <sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.T2\" title=\"Table 2 &#8227; 6.1. Overview and Fine-Tuning Impact &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>).</p>\n\n",
                "matched_terms": [
                    "llama",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, our novel political authenticity metrics (PSA and Party Align) displayed strong responsiveness to fine-tuning. All five models significantly improved PSA (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>), with effect sizes ranging from small to very large (d=0.141-1.045). Party Align showed similar patterns (4 of 5 models improved, d=0.099-1.221). These substantial effects validate that our embedding based metrics capture critical political authenticity dimensions unavailable to conventional evaluation.\nFor complete t-test results including effect sizes, see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T13\" title=\"Table 13 &#8227; 13.5. Statistical Significance Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F3\" title=\"Figure 3 &#8227; 6.1. Overview and Fine-Tuning Impact &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows fine-tuning impact across evaluation categories. <span class=\"ltx_text ltx_font_italic\">YI</span> achieved the strongest improvements across all dimensions , while <span class=\"ltx_text ltx_font_italic\">Llama</span> had consistent gains. <span class=\"ltx_text ltx_font_italic\">Gemma2</span> and <span class=\"ltx_text ltx_font_italic\">Qwen2</span> exhibited quality trade-offs, with improvements in one category accompanied by declines in others, suggesting architectural differences in how models balance competing objectives during fine-tuning. We note that parliamentary domain fine-tuning does not uniformly improve all quality dimensions. Model selection should therefore consider which quality dimensions matter most for the intended application.</p>\n\n",
                "matched_terms": [
                    "models",
                    "llama",
                    "model",
                    "qwen2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-context stability analysis (Eq.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.E4\" title=\"In 4.2. Consistency Evaluation Metrics &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) revealed fine-tuned models maintained consistent performance across political contexts (composite stability 91.4-96.2). Mistral achieved highest consistency (96.2) despite trade-offs in absolute performance, while Llama (95.1) balanced strong performance with stability.\nDetailed scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T12\" title=\"Table 12 &#8227; 13.4. Cross-Context Stability Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "finetuned",
                    "political",
                    "models",
                    "llama",
                    "mistral",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both new political authenticity metrics (PSA and Party Align) successfully discriminate their target political dimensions. Party Align distinguishes parties while PSA distinguishes orientations (both <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>). Our analysis reveals that Party Align performance depends primarily on data abundance and ideological coherence rather than party size alone. Models successfully learn party-specific language patterns when training data provides clear stylistic signals, indicating targeted data collection for under-represented parties could improve coverage.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Topic Performance Patterns</span>. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F6\" title=\"Figure 6 &#8227; 6.3. Topic Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows model performance across topic domains. Science achieved lowest scores (avg 0.516), while Economics (0.610) and European Union (0.606) showed highest performance. Detailed scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T11\" title=\"Table 11 &#8227; 13.3. Topic-Specific Performance &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Performance across political orientations showed expected patterns. Centrist positions (Centre-left: 0.607, Centre-right: 0.551) dominated the dataset (88%) and achieved higher scores. Model-specific strengths emerged as both <span class=\"ltx_text ltx_font_italic\">Gemma</span> and <span class=\"ltx_text ltx_font_italic\">Qwen</span> achieved highest scores on Right positions and <span class=\"ltx_text ltx_font_italic\">Mistral</span> underperformed consistently, indicating architectural rather than ideological limitations. As models are optimized for mainstream parliamentary speeches, extreme positions may require specialized training approaches. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F8\" title=\"Figure 8 &#8227; 6.4. Political Orientations Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> illustrates these patterns. Detailed orientation difficulty rankings are provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T10\" title=\"Table 10 &#8227; 13.2. Metric Validation: Political Discrimination Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a></p>\n\n",
                "matched_terms": [
                    "political",
                    "models",
                    "mistral",
                    "performance",
                    "gemma"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results establish several key findings: (i) Architectural design impacts political authenticity, with extended context windows enabling consistent improvements; (ii) Domain-specific fine-tuning proves essential as 45 of 70 metric comparisons showed statistically significant improvements and (iii) Novel political authenticity metrics (PSA, Party Align) successfully capture dimensions unavailable to conventional NLP metrics, validated through both fine-tuning responsiveness and discrimination testing (both <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To the best of our knowledge, ParliaBench represents the first benchmark resource addressing the specialized challenges of parliamentary speech generation, comprising dataset, evaluation framework, novel metrics, and baseline benchmarks. ParliaBench provides standardized evaluation protocols and baseline performance results that\nsupport systematic comparison and reproducible research in the field.\nOur results demonstrate that domain specific fine-tuning produces significant quality improvements, while\nour novel political authenticity metrics successfully capture ideological dimensions absent from conventional evaluation approaches.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both datasets (Training and Generated Data) and finetuned models are available under a CC BY License at the ParliaBench collection on <a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/collections/argyrotsipi/parliabench\" title=\"\">Hugging Face</a>. \n<br class=\"ltx_break\"/>Resources are openly accessible on <a class=\"ltx_ref ltx_href\" href=\"https://argyrotsipi.github.io/ParliaBench/\" title=\"\">Website</a> and <a class=\"ltx_ref ltx_href\" href=\"https://github.com/ArgyroTsipi/ParliaBench\" title=\"\">GitHub</a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "finetuned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation measures linguistic quality and political authenticity but does not assess argument structure or verify factual accuracy against parliamentary records. Our methods rely entirely on automated metrics without human validation, and LLM-as-a-Judge approaches may carry inherent biases.\nThis work is intended for research and educational purposes, not deployment in actual democratic processes.</p>\n\n",
                "matched_terms": [
                    "democratic",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work establishes a benchmark resource for evaluating LLM-generated parliamentary speech in research and educational contexts. These resources should only be used as assistance to human experts with consideration of their limitations and biases. The parliamentary data is derived from publicly available UK parliamentary proceedings (ParlaMint corpus) licensed under Creative Commons Attribution 4.0 International, which our derived datasets maintain.\nGenerated parliamentary speeches must be clearly identified as AI-generated content and not misrepresented as authentic political speeches from actual parliamentarians.</p>\n\n",
                "matched_terms": [
                    "only",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Political Appropriateness (j_polapp)</span>: Alignment with party positions</p>\n\n",
                "matched_terms": [
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Target modules encompass all linear transformation layers (q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj) to achieve performance comparable to full fine-tuning while requiring only a fraction of the computational resources. The consistent random state across all architectures ensures reproducible results essential for systematic model comparison.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "only",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_typewriter\">You are a seasoned UK parliamentary member.\nUse proper British parliamentary language appropriate for the specified House.\n<br class=\"ltx_break\"/>The speech should reflect the political orientation and typical positions of the specified party on the given topic.\n<br class=\"ltx_break\"/></span>\n</p>\n\n",
                "matched_terms": [
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We generated the speeches for the finetuned and the baseline models using the following prompt for all models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "finetuned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">You are a seasoned UK parliamentary member. Generate a coherent speech of\nmin_words - max_words words in standard English (no Unicode artifacts, no special characters).</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_typewriter\">\nUse proper British parliamentary language appropriate for the specified House. \n<br class=\"ltx_break\"/>The speech should reflect the political orientation and typical positions of the specified party on the given topic. \n<br class=\"ltx_break\"/></span></p>\n\n",
                "matched_terms": [
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PARTY:</span> Political party affiliation (e.g., Conservative)</p>\n\n",
                "matched_terms": [
                    "conservative",
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This setup helps fine-tuned models learn to give responses that match a party&#8217;s views, stay on topic, follow parliamentary rules and political views.</p>\n\n",
                "matched_terms": [
                    "models",
                    "finetuned",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present various speeches generated by the models (baseline and finetuned).</p>\n\n",
                "matched_terms": [
                    "models",
                    "finetuned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T11\" title=\"Table 11 &#8227; 13.3. Topic-Specific Performance &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> presents model performance across topic domains for fine-tuned models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "performance",
                    "model",
                    "finetuned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T12\" title=\"Table 12 &#8227; 13.4. Cross-Context Stability Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> presents cross-context stability scores. Fine-tuned models maintain high consistency across political contexts (91.4-96.2), with Mistral achieving highest overall stability (96.2).</p>\n\n",
                "matched_terms": [
                    "models",
                    "mistral",
                    "finetuned",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T13\" title=\"Table 13 &#8227; 13.5. Statistical Significance Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> presents complete pairwise t-test results comparing baseline and fine-tuned models across all evaluation metrics, including p-values, effect sizes, and significance after Bonferroni correction.</p>\n\n",
                "matched_terms": [
                    "models",
                    "finetuned"
                ]
            }
        ]
    },
    "S13.T9": {
        "source_file": "ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech",
        "caption": "Table 9: Party Difficulty Rankings: Political Parties Ranked by Modeling Difficulty",
        "body": "Rank\nPolitical Party\nDifficulty Score\nConsistency Score\n\n\n\n\n1\nDemocratic Unionist Party\n0.456355\n17.207189\n\n\n2\nPlaid Cymru\n0.454357\n17.664821\n\n\n3\nIndependent\n0.450960\n16.126553\n\n\n4\nGreen Party\n0.449039\n18.088600\n\n\n5\nConservative\n0.447174\n18.570069\n\n\n6\nLabour\n0.445414\n17.817318\n\n\n7\nLiberal Democrats\n0.445108\n18.165583\n\n\n8\nScottish National Party\n0.441675\n18.100135\n\n\n9\nBishops\n0.428796\n17.796382\n\n\n10\nCrossbench\n0.425935\n20.236259\n\n\n11\nNon-Affiliated\n0.381897\n19.673312",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Rank</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Political Party</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Difficulty Score</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Consistency Score</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">1</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Democratic Unionist Party</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.456355</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">17.207189</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">2</th>\n<td class=\"ltx_td ltx_align_center\">Plaid Cymru</td>\n<td class=\"ltx_td ltx_align_center\">0.454357</td>\n<td class=\"ltx_td ltx_align_center\">17.664821</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">3</th>\n<td class=\"ltx_td ltx_align_center\">Independent</td>\n<td class=\"ltx_td ltx_align_center\">0.450960</td>\n<td class=\"ltx_td ltx_align_center\">16.126553</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">4</th>\n<td class=\"ltx_td ltx_align_center\">Green Party</td>\n<td class=\"ltx_td ltx_align_center\">0.449039</td>\n<td class=\"ltx_td ltx_align_center\">18.088600</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">5</th>\n<td class=\"ltx_td ltx_align_center\">Conservative</td>\n<td class=\"ltx_td ltx_align_center\">0.447174</td>\n<td class=\"ltx_td ltx_align_center\">18.570069</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">6</th>\n<td class=\"ltx_td ltx_align_center\">Labour</td>\n<td class=\"ltx_td ltx_align_center\">0.445414</td>\n<td class=\"ltx_td ltx_align_center\">17.817318</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">7</th>\n<td class=\"ltx_td ltx_align_center\">Liberal Democrats</td>\n<td class=\"ltx_td ltx_align_center\">0.445108</td>\n<td class=\"ltx_td ltx_align_center\">18.165583</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">8</th>\n<td class=\"ltx_td ltx_align_center\">Scottish National Party</td>\n<td class=\"ltx_td ltx_align_center\">0.441675</td>\n<td class=\"ltx_td ltx_align_center\">18.100135</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">9</th>\n<td class=\"ltx_td ltx_align_center\">Bishops</td>\n<td class=\"ltx_td ltx_align_center\">0.428796</td>\n<td class=\"ltx_td ltx_align_center\">17.796382</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">10</th>\n<td class=\"ltx_td ltx_align_center\">Crossbench</td>\n<td class=\"ltx_td ltx_align_center\">0.425935</td>\n<td class=\"ltx_td ltx_align_center\">20.236259</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">11</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Non-Affiliated</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.381897</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">19.673312</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "political",
            "crossbench",
            "democratic",
            "unionist",
            "national",
            "score",
            "conservative",
            "rankings",
            "bishops",
            "ranked",
            "cymru",
            "modeling",
            "independent",
            "plaid",
            "labour",
            "rank",
            "nonaffiliated",
            "consistency",
            "green",
            "parties",
            "difficulty",
            "liberal",
            "democrats",
            "scottish",
            "party"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Party alignment Difficulty Analysis</span>. Applying cross-context stability analysis (Eq.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.E4\" title=\"In 4.2. Consistency Evaluation Metrics &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), party difficulty scores ranged narrowly (0.382-0.456), with no statistically significant differences. This suggests relatively consistent modeling challenges across parties regardless of size or ideological composition. Results are presented in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F5\" title=\"Figure 5 &#8227; 6.2. Political Context Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Detailed scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T9\" title=\"Table 9 &#8227; 13.2. Metric Validation: Political Discrimination Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T9\" title=\"Table 9 &#8227; 13.2. Metric Validation: Political Discrimination Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T10\" title=\"Table 10 &#8227; 13.2. Metric Validation: Political Discrimination Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> present ANOVA results validating the discriminative power of the novel political authenticity metrics.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Parliamentary speech generation presents specific challenges for large language models beyond standard text generation tasks. Unlike general text generation, parliamentary speeches require not only linguistic quality but also political authenticity and ideological consistency. Current language models lack specialized training for parliamentary contexts, and existing evaluation methods focus on standard NLP metrics rather than political authenticity. To address this, we present ParliaBench, a benchmark for parliamentary speech generation. We constructed a dataset of speeches from UK Parliament to enable systematic model training. We introduce an evaluation framework combining computational metrics with LLM-as-a-judge assessments for measuring generation quality across three dimensions: linguistic quality, semantic coherence, and political authenticity. We propose two novel embedding-based metrics, Political Spectrum Alignment and Party Alignment, to quantify ideological positioning. We fine-tuned five large language models (LLMs), generated 28k speeches, and evaluated them\nusing our framework, comparing baseline and fine-tuned models. Results show that fine-tuning produces statistically significant improvements across the majority of metrics and our novel metrics demonstrate strong discriminative power for political dimensions.\n\n\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Parliamentary Speech Generation,LLM Evaluation,Political Authenticity,Benchmark Evaluation</p>\n\n",
                "matched_terms": [
                    "consistency",
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Democracy thrives through debate. Democratic parliaments are open forums where elected representatives engage in arguments over policy <cite class=\"ltx_cite ltx_citemacro_cite\">Back et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib4\" title=\"\">2021</a>)</cite>. These debates provide unique insights into political reasoning and ideological positioning. Researchers in political science and computational linguistics increasingly seek to understand and model parliamentary debates.</p>\n\n",
                "matched_terms": [
                    "democratic",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Yet generating authentic parliamentary speech presents significant challenges that extend well beyond typical text generation tasks. The complexity becomes apparent when considering specific examples. A Labour MP discussing taxation policy must sound distinctly different from a Conservative counterpart not just in policy position, but most significantly, in their fundamental approach to governance. Large language models have shown promise across political applications, from sentiment analysis <cite class=\"ltx_cite ltx_citemacro_cite\">Bestvater and Monroe (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib6\" title=\"\">2023</a>)</cite> and election forecasting <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib22\" title=\"\">2024b</a>)</cite> to synthetic survey data generation <cite class=\"ltx_cite ltx_citemacro_cite\">Argyle et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib3\" title=\"\">2023</a>); Bisbee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib7\" title=\"\">2024</a>)</cite>. Authentic parliamentary speech generation, however, remains challenging.</p>\n\n",
                "matched_terms": [
                    "labour",
                    "conservative",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The final dataset contains 447,778 speeches from 1,901 unique speakers across 11 political affiliations, with major parties (Conservative: 263,513; Labour: 108,831) dominating representation. The dataset contains approximately 99.94 million words, with speeches averaging 223 words (median: 99 words). This distribution reflects natural variation in parliamentary speeches, from brief procedural statements to extended policy expositions. Temporal coverage captures significant political events including Brexit debates, and the COVID-19 pandemic response, ensuring exposure to diverse political contexts and rhetorical situations. Note: \"Bishops\", \"Crossbench\", and \"Non-Affiliated\" are not political parties in the traditional sense but formal affiliations in Parliament.</p>\n\n",
                "matched_terms": [
                    "parties",
                    "bishops",
                    "political",
                    "crossbench",
                    "labour",
                    "conservative",
                    "nonaffiliated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S3.F2\" title=\"Figure 2 &#8227; 3.2. Statistics &#8227; 3. ParliaBench Dataset &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates key dataset characteristics. Panel (a) reveals a highly right-skewed distribution of speech lengths, indicating that the dataset is dominated by relatively short speeches while containing a smaller number of substantially longer ones. Panel (b) presents topic distribution showing that speeches most frequently address International Relations, Law, and Politics, which together account for over 45% of the corpus. Panel (c) demonstrates ideological balance across the political spectrum. While most parties by count are in the centre-left to left spectrum, the centre-right to right spectrum produces a larger amount of speeches by 110,000. Panel (d) displays temporal patterns and institutional differences. The House of Commons consistently produces 3-4x more speeches than the House of Lords across 2015-2022.</p>\n\n",
                "matched_terms": [
                    "parties",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Parliamentary speech evaluation requires assessment across multiple levels that generic benchmarks cannot capture. Our framework operates on two levels: (i) <span class=\"ltx_text ltx_font_bold\">speech evaluation metrics</span> measuring generation quality across three dimensions, and (ii) <span class=\"ltx_text ltx_font_bold\">consistency evaluation metrics</span> measuring performance reliability across political contexts. This dual-track approach also combines computational metrics with LLM-judge evaluation.</p>\n\n",
                "matched_terms": [
                    "consistency",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our framework introduces two novel computational metrics (Political Spectrum Alignment and Party Alignment, Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.SS1.SSS1\" title=\"4.1.1. Novel Political Authenticity Metrics &#8227; 4.1. Speech Evaluation Metrics &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4.1.1</span></a>) alongside established metrics from both computational and LLM-judge traditions.</p>\n\n",
                "matched_terms": [
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ two novel embedding-based metrics for political authenticity assessment: Political Spectrum Alignment (PSA) and Party Alignment (Party Align). Detailed calculation methodology provided in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.SS1.SSS1\" title=\"4.1.1. Novel Political Authenticity Metrics &#8227; 4.1. Speech Evaluation Metrics &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4.1.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Party Alignment</span> (Party Align) applies the same embedding methodology to party-specific alignment, using party affiliation rather than political orientation for centroid construction. The alignment score measures cosine similarity between generated speech and expected party centroid:</p>\n\n",
                "matched_terms": [
                    "party",
                    "political",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Context Stability</span> measures performance consistency using coefficient of variation, with higher scores indicating more consistent performance across political contexts. This meta-evaluation applies to all speech evaluation metrics, providing diagnostic insight into model reliability. For cross-metric comparison, all metrics, Computational &amp; LLM-Judge, are normalized to 0-1 scale.</p>\n\n",
                "matched_terms": [
                    "consistency",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We examine consistency across three dimensions: political parties, topic domains, and political orientations. The stability calculation quantifies performance variability:</p>\n\n",
                "matched_terms": [
                    "parties",
                    "consistency",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed Quantized Low-Rank Adaptation (QLoRA) <cite class=\"ltx_cite ltx_citemacro_cite\">Dettmers et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib10\" title=\"\">2023</a>)</cite> for parameter-efficient fine-tuning. (<math alttext=\"r=16\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">r=16</annotation></semantics></math>, <math alttext=\"\\alpha=16\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=16</annotation></semantics></math>, <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math> epochs). Model-specific chat templates structure training inputs with political metadata (party affiliation, topic classification, orientation, section, and house). Training used SFTTrainer from TRL with 80%-20% train-test splits and automated checkpointing. Fine-tuned models were saved with adapter weights for subsequent evaluation, ensuring consistent model states across experiments.</p>\n\n",
                "matched_terms": [
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We planned to generate 30,000 speeches (3,000 per model-type combination across 10 models) using stratified sampling from the held-out test set. To maintain consistency with training conditions, our prompt distribution matches the ParliaBench Dataset structure: 90% generic instruction prompts formatted with political context (party, topic, orientation, section, house) and 10% specific parliamentary questions from the test set.</p>\n\n",
                "matched_terms": [
                    "consistency",
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated 27,560 generated speeches using our evaluation framework. This section presents fine-tuning effectiveness and performance patterns across political parties, topic domains, and ideological orientations. Representative examples of generated speeches are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S12\" title=\"12. Representative Generated Speeches &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>.</p>\n\n",
                "matched_terms": [
                    "parties",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Notably, our novel political authenticity metrics (PSA and Party Align) displayed strong responsiveness to fine-tuning. All five models significantly improved PSA (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>), with effect sizes ranging from small to very large (d=0.141-1.045). Party Align showed similar patterns (4 of 5 models improved, d=0.099-1.221). These substantial effects validate that our embedding based metrics capture critical political authenticity dimensions unavailable to conventional evaluation.\nFor complete t-test results including effect sizes, see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T13\" title=\"Table 13 &#8227; 13.5. Statistical Significance Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-context stability analysis (Eq.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.E4\" title=\"In 4.2. Consistency Evaluation Metrics &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) revealed fine-tuned models maintained consistent performance across political contexts (composite stability 91.4-96.2). Mistral achieved highest consistency (96.2) despite trade-offs in absolute performance, while Llama (95.1) balanced strong performance with stability.\nDetailed scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T12\" title=\"Table 12 &#8227; 13.4. Cross-Context Stability Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "consistency",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Party alignment Patterns</span>. Party alignment performance varied substantially across models (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F4\" title=\"Figure 4 &#8227; 6.2. Political Context Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). Major parties (Conservative, Labour) achieved stable performance across models, benefiting from substantial training data (58.9%, 24.3%). Minor parties exhibited greater variability. <span class=\"ltx_text ltx_font_italic\">Mistral</span> struggled with heterogeneous groups (Non-Affiliated: 0.436), while <span class=\"ltx_text ltx_font_italic\">Qwen</span> excelled with ideologically coherent minorities (Bishops: 0.664). <span class=\"ltx_text ltx_font_italic\">YI</span> demonstrated robust cross-party performance (0.614-0.633).\nDetailed scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T8\" title=\"Table 8 &#8227; 13.1. Party-Specific Performance &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "parties",
                    "bishops",
                    "party",
                    "labour",
                    "conservative",
                    "nonaffiliated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both new political authenticity metrics (PSA and Party Align) successfully discriminate their target political dimensions. Party Align distinguishes parties while PSA distinguishes orientations (both <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>). Our analysis reveals that Party Align performance depends primarily on data abundance and ideological coherence rather than party size alone. Models successfully learn party-specific language patterns when training data provides clear stylistic signals, indicating targeted data collection for under-represented parties could improve coverage.</p>\n\n",
                "matched_terms": [
                    "parties",
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Topic Difficulty Analysis</span>. Different topics posed different challenges (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F7\" title=\"Figure 7 &#8227; 6.3. Topic Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>). Science and Geography ranked as most difficult while Finance, Business, and Economics ranked lowest. Technical and natural science domains display higher cross-model disagreement than economic and political topics, consistent with greater terminological specialization and rapidly evolving concepts. In contrast, economic and political discussions employs more stable conceptual frameworks aligned with core parliamentary functions.</p>\n\n",
                "matched_terms": [
                    "ranked",
                    "difficulty",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Performance across political orientations showed expected patterns. Centrist positions (Centre-left: 0.607, Centre-right: 0.551) dominated the dataset (88%) and achieved higher scores. Model-specific strengths emerged as both <span class=\"ltx_text ltx_font_italic\">Gemma</span> and <span class=\"ltx_text ltx_font_italic\">Qwen</span> achieved highest scores on Right positions and <span class=\"ltx_text ltx_font_italic\">Mistral</span> underperformed consistently, indicating architectural rather than ideological limitations. As models are optimized for mainstream parliamentary speeches, extreme positions may require specialized training approaches. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F8\" title=\"Figure 8 &#8227; 6.4. Political Orientations Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> illustrates these patterns. Detailed orientation difficulty rankings are provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T10\" title=\"Table 10 &#8227; 13.2. Metric Validation: Political Discrimination Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a></p>\n\n",
                "matched_terms": [
                    "rankings",
                    "difficulty",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results establish several key findings: (i) Architectural design impacts political authenticity, with extended context windows enabling consistent improvements; (ii) Domain-specific fine-tuning proves essential as 45 of 70 metric comparisons showed statistically significant improvements and (iii) Novel political authenticity metrics (PSA, Party Align) successfully capture dimensions unavailable to conventional NLP metrics, validated through both fine-tuning responsiveness and discrimination testing (both <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluation measures linguistic quality and political authenticity but does not assess argument structure or verify factual accuracy against parliamentary records. Our methods rely entirely on automated metrics without human validation, and LLM-as-a-Judge approaches may carry inherent biases.\nThis work is intended for research and educational purposes, not deployment in actual democratic processes.</p>\n\n",
                "matched_terms": [
                    "democratic",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Political Appropriateness (j_polapp)</span>: Alignment with party positions</p>\n\n",
                "matched_terms": [
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_typewriter\">You are a seasoned UK parliamentary member.\nUse proper British parliamentary language appropriate for the specified House.\n<br class=\"ltx_break\"/>The speech should reflect the political orientation and typical positions of the specified party on the given topic.\n<br class=\"ltx_break\"/></span>\n</p>\n\n",
                "matched_terms": [
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">You are a seasoned UK parliamentary member. Generate a coherent speech of\nmin_words - max_words words in standard English (no Unicode artifacts, no special characters).</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_typewriter\">\nUse proper British parliamentary language appropriate for the specified House. \n<br class=\"ltx_break\"/>The speech should reflect the political orientation and typical positions of the specified party on the given topic. \n<br class=\"ltx_break\"/></span></p>\n\n",
                "matched_terms": [
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PARTY:</span> Political party affiliation (e.g., Conservative)</p>\n\n",
                "matched_terms": [
                    "conservative",
                    "party",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T12\" title=\"Table 12 &#8227; 13.4. Cross-Context Stability Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> presents cross-context stability scores. Fine-tuned models maintain high consistency across political contexts (91.4-96.2), with Mistral achieving highest overall stability (96.2).</p>\n\n",
                "matched_terms": [
                    "consistency",
                    "political"
                ]
            }
        ]
    },
    "S13.T10": {
        "source_file": "ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech",
        "caption": "Table 10: Orientation Difficulty Rankings: Political Orientations Ranked by Modeling Difficulty",
        "body": "Rank\nPolitical Orientation\nDifficulty Score\nConsistency Score\n\n\n\n\n1\nRight\n0.456355\n17.207189\n\n\n2\nCentre-left to left\n0.454357\n17.664821\n\n\n3\nLeft\n0.449039\n18.088600\n\n\n4\nCentre-right\n0.447174\n18.570069\n\n\n5\nCentre to centre-left\n0.445108\n18.165583\n\n\n6\nCentre-left\n0.445095\n17.800365\n\n\n7\nUnknown\n0.439036\n17.085963",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Rank</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Political Orientation</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Difficulty Score</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Consistency Score</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">1</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Right</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.456355</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">17.207189</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">2</th>\n<td class=\"ltx_td ltx_align_center\">Centre-left to left</td>\n<td class=\"ltx_td ltx_align_center\">0.454357</td>\n<td class=\"ltx_td ltx_align_center\">17.664821</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">3</th>\n<td class=\"ltx_td ltx_align_center\">Left</td>\n<td class=\"ltx_td ltx_align_center\">0.449039</td>\n<td class=\"ltx_td ltx_align_center\">18.088600</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">4</th>\n<td class=\"ltx_td ltx_align_center\">Centre-right</td>\n<td class=\"ltx_td ltx_align_center\">0.447174</td>\n<td class=\"ltx_td ltx_align_center\">18.570069</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">5</th>\n<td class=\"ltx_td ltx_align_center\">Centre to centre-left</td>\n<td class=\"ltx_td ltx_align_center\">0.445108</td>\n<td class=\"ltx_td ltx_align_center\">18.165583</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">6</th>\n<td class=\"ltx_td ltx_align_center\">Centre-left</td>\n<td class=\"ltx_td ltx_align_center\">0.445095</td>\n<td class=\"ltx_td ltx_align_center\">17.800365</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">7</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Unknown</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.439036</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">17.085963</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "rankings",
            "difficulty",
            "centreright",
            "political",
            "orientation",
            "left",
            "ranked",
            "modeling",
            "centreleft",
            "centre",
            "right",
            "unknown",
            "score",
            "consistency",
            "rank",
            "orientations"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Performance across political orientations showed expected patterns. Centrist positions (Centre-left: 0.607, Centre-right: 0.551) dominated the dataset (88%) and achieved higher scores. Model-specific strengths emerged as both <span class=\"ltx_text ltx_font_italic\">Gemma</span> and <span class=\"ltx_text ltx_font_italic\">Qwen</span> achieved highest scores on Right positions and <span class=\"ltx_text ltx_font_italic\">Mistral</span> underperformed consistently, indicating architectural rather than ideological limitations. As models are optimized for mainstream parliamentary speeches, extreme positions may require specialized training approaches. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F8\" title=\"Figure 8 &#8227; 6.4. Political Orientations Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> illustrates these patterns. Detailed orientation difficulty rankings are provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T10\" title=\"Table 10 &#8227; 13.2. Metric Validation: Political Discrimination Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a></p>\n\n",
            "<p class=\"ltx_p\">Tables <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T9\" title=\"Table 9 &#8227; 13.2. Metric Validation: Political Discrimination Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T10\" title=\"Table 10 &#8227; 13.2. Metric Validation: Political Discrimination Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a> present ANOVA results validating the discriminative power of the novel political authenticity metrics.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Parliamentary speech generation presents specific challenges for large language models beyond standard text generation tasks. Unlike general text generation, parliamentary speeches require not only linguistic quality but also political authenticity and ideological consistency. Current language models lack specialized training for parliamentary contexts, and existing evaluation methods focus on standard NLP metrics rather than political authenticity. To address this, we present ParliaBench, a benchmark for parliamentary speech generation. We constructed a dataset of speeches from UK Parliament to enable systematic model training. We introduce an evaluation framework combining computational metrics with LLM-as-a-judge assessments for measuring generation quality across three dimensions: linguistic quality, semantic coherence, and political authenticity. We propose two novel embedding-based metrics, Political Spectrum Alignment and Party Alignment, to quantify ideological positioning. We fine-tuned five large language models (LLMs), generated 28k speeches, and evaluated them\nusing our framework, comparing baseline and fine-tuned models. Results show that fine-tuning produces statistically significant improvements across the majority of metrics and our novel metrics demonstrate strong discriminative power for political dimensions.\n\n\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Parliamentary Speech Generation,LLM Evaluation,Political Authenticity,Benchmark Evaluation</p>\n\n",
                "matched_terms": [
                    "consistency",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S3.F2\" title=\"Figure 2 &#8227; 3.2. Statistics &#8227; 3. ParliaBench Dataset &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates key dataset characteristics. Panel (a) reveals a highly right-skewed distribution of speech lengths, indicating that the dataset is dominated by relatively short speeches while containing a smaller number of substantially longer ones. Panel (b) presents topic distribution showing that speeches most frequently address International Relations, Law, and Politics, which together account for over 45% of the corpus. Panel (c) demonstrates ideological balance across the political spectrum. While most parties by count are in the centre-left to left spectrum, the centre-right to right spectrum produces a larger amount of speeches by 110,000. Panel (d) displays temporal patterns and institutional differences. The House of Commons consistently produces 3-4x more speeches than the House of Lords across 2015-2022.</p>\n\n",
                "matched_terms": [
                    "centreright",
                    "left",
                    "political",
                    "centreleft",
                    "right"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Parliamentary speech evaluation requires assessment across multiple levels that generic benchmarks cannot capture. Our framework operates on two levels: (i) <span class=\"ltx_text ltx_font_bold\">speech evaluation metrics</span> measuring generation quality across three dimensions, and (ii) <span class=\"ltx_text ltx_font_bold\">consistency evaluation metrics</span> measuring performance reliability across political contexts. This dual-track approach also combines computational metrics with LLM-judge evaluation.</p>\n\n",
                "matched_terms": [
                    "consistency",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Political Spectrum Alignment</span> (PSA) evaluates ideological positioning on the left-right spectrum. We adapt semantic embedding approaches <cite class=\"ltx_cite ltx_citemacro_cite\">Rheault and Cochrane (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib30\" title=\"\">2020</a>)</cite> for LLM-generated speech evaluation, drawing on\nthe Left-Right (RILE) scale methodology <cite class=\"ltx_cite ltx_citemacro_cite\">Volkens et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib35\" title=\"\">2013</a>); Budge (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib9\" title=\"\">2013</a>)</cite>. Our metric employs a two-stage approach combining semantic similarity with ideological distance. We create reference embeddings by grouping parliamentary speeches by political orientations (Far-left through Far-right, including intermediate positions) and computing centroid embeddings using sentence transformers. Orientations map to numerical values where Far-left = -6, Centre = 0, Far-right = +6.</p>\n\n",
                "matched_terms": [
                    "orientations",
                    "centre",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first identify the closest matching political orientation:</p>\n\n",
                "matched_terms": [
                    "political",
                    "orientation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"po^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p4.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>o</mi><mo>&#8727;</mo></msup></mrow><annotation encoding=\"application/x-tex\">po^{*}</annotation></semantics></math> is the closest matching orientation, <math alttext=\"\\mathcal{PO}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p4.m2\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119979;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119978;</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal{PO}</annotation></semantics></math>\nthe set of all political orientations, <math alttext=\"c_{po}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p4.m3\" intent=\":literal\"><semantics><msub><mi>c</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi></mrow></msub><annotation encoding=\"application/x-tex\">c_{po}</annotation></semantics></math> the orientation centroid, and <math alttext=\"\\text{sim}(s,c_{po})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p4.m4\" intent=\":literal\"><semantics><mrow><mtext>sim</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo>,</mo><msub><mi>c</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{sim}(s,c_{po})</annotation></semantics></math> the cosine similarity between generated speech <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p4.m5\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> and centroid.</p>\n\n",
                "matched_terms": [
                    "orientations",
                    "political",
                    "orientation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The PSA score combines semantic similarity with orientation distance:</p>\n\n",
                "matched_terms": [
                    "score",
                    "orientation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\Delta_{\\phi}=|\\phi(po_{e})-\\phi(po^{*})|\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p6.m1\" intent=\":literal\"><semantics><mrow><msub><mi mathvariant=\"normal\">&#916;</mi><mi>&#981;</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">|</mo><mrow><mrow><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>o</mi><mi>e</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&#8722;</mo><mrow><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>o</mi><mo>&#8727;</mo></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">|</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta_{\\phi}=|\\phi(po_{e})-\\phi(po^{*})|</annotation></semantics></math>, <math alttext=\"po_{e}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p6.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>o</mi><mi>e</mi></msub></mrow><annotation encoding=\"application/x-tex\">po_{e}</annotation></semantics></math> is the expected orientation, <math alttext=\"\\phi(po)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p6.m3\" intent=\":literal\"><semantics><mrow><mi>&#981;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\phi(po)</annotation></semantics></math> maps orientations to numerical values, and <math alttext=\"\\text{sim}(s,c_{po^{*}})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p6.m4\" intent=\":literal\"><semantics><mrow><mtext>sim</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo>,</mo><msub><mi>c</mi><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msup><mi>o</mi><mo>&#8727;</mo></msup></mrow></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{sim}(s,c_{po^{*}})</annotation></semantics></math> is the cosine similarity to the matched centroid. The maximum distance of 12 (from Far-left to Far-right) normalizes to 0-1 scale. Perfect ideological alignment approaches 1, while misalignment approaches 0.</p>\n\n",
                "matched_terms": [
                    "orientations",
                    "orientation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Parties with unknown orientation are excluded from PSA analysis as their ideological position cannot be reliably mapped to the left-right spectrum.</p>\n\n",
                "matched_terms": [
                    "unknown",
                    "orientation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Party Alignment</span> (Party Align) applies the same embedding methodology to party-specific alignment, using party affiliation rather than political orientation for centroid construction. The alignment score measures cosine similarity between generated speech and expected party centroid:</p>\n\n",
                "matched_terms": [
                    "score",
                    "political",
                    "orientation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Context Stability</span> measures performance consistency using coefficient of variation, with higher scores indicating more consistent performance across political contexts. This meta-evaluation applies to all speech evaluation metrics, providing diagnostic insight into model reliability. For cross-metric comparison, all metrics, Computational &amp; LLM-Judge, are normalized to 0-1 scale.</p>\n\n",
                "matched_terms": [
                    "consistency",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We examine consistency across three dimensions: political parties, topic domains, and political orientations. The stability calculation quantifies performance variability:</p>\n\n",
                "matched_terms": [
                    "consistency",
                    "orientations",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed Quantized Low-Rank Adaptation (QLoRA) <cite class=\"ltx_cite ltx_citemacro_cite\">Dettmers et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib10\" title=\"\">2023</a>)</cite> for parameter-efficient fine-tuning. (<math alttext=\"r=16\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">r=16</annotation></semantics></math>, <math alttext=\"\\alpha=16\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=16</annotation></semantics></math>, <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math> epochs). Model-specific chat templates structure training inputs with political metadata (party affiliation, topic classification, orientation, section, and house). Training used SFTTrainer from TRL with 80%-20% train-test splits and automated checkpointing. Fine-tuned models were saved with adapter weights for subsequent evaluation, ensuring consistent model states across experiments.</p>\n\n",
                "matched_terms": [
                    "political",
                    "orientation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We planned to generate 30,000 speeches (3,000 per model-type combination across 10 models) using stratified sampling from the held-out test set. To maintain consistency with training conditions, our prompt distribution matches the ParliaBench Dataset structure: 90% generic instruction prompts formatted with political context (party, topic, orientation, section, house) and 10% specific parliamentary questions from the test set.</p>\n\n",
                "matched_terms": [
                    "consistency",
                    "political",
                    "orientation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated 27,560 generated speeches using our evaluation framework. This section presents fine-tuning effectiveness and performance patterns across political parties, topic domains, and ideological orientations. Representative examples of generated speeches are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S12\" title=\"12. Representative Generated Speeches &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>.</p>\n\n",
                "matched_terms": [
                    "orientations",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-context stability analysis (Eq.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.E4\" title=\"In 4.2. Consistency Evaluation Metrics &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) revealed fine-tuned models maintained consistent performance across political contexts (composite stability 91.4-96.2). Mistral achieved highest consistency (96.2) despite trade-offs in absolute performance, while Llama (95.1) balanced strong performance with stability.\nDetailed scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T12\" title=\"Table 12 &#8227; 13.4. Cross-Context Stability Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "consistency",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both new political authenticity metrics (PSA and Party Align) successfully discriminate their target political dimensions. Party Align distinguishes parties while PSA distinguishes orientations (both <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>). Our analysis reveals that Party Align performance depends primarily on data abundance and ideological coherence rather than party size alone. Models successfully learn party-specific language patterns when training data provides clear stylistic signals, indicating targeted data collection for under-represented parties could improve coverage.</p>\n\n",
                "matched_terms": [
                    "orientations",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Party alignment Difficulty Analysis</span>. Applying cross-context stability analysis (Eq.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.E4\" title=\"In 4.2. Consistency Evaluation Metrics &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), party difficulty scores ranged narrowly (0.382-0.456), with no statistically significant differences. This suggests relatively consistent modeling challenges across parties regardless of size or ideological composition. Results are presented in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F5\" title=\"Figure 5 &#8227; 6.2. Political Context Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Detailed scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T9\" title=\"Table 9 &#8227; 13.2. Metric Validation: Political Discrimination Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "difficulty",
                    "modeling"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Topic Difficulty Analysis</span>. Different topics posed different challenges (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F7\" title=\"Figure 7 &#8227; 6.3. Topic Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>). Science and Geography ranked as most difficult while Finance, Business, and Economics ranked lowest. Technical and natural science domains display higher cross-model disagreement than economic and political topics, consistent with greater terminological specialization and rapidly evolving concepts. In contrast, economic and political discussions employs more stable conceptual frameworks aligned with core parliamentary functions.</p>\n\n",
                "matched_terms": [
                    "ranked",
                    "difficulty",
                    "political"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_typewriter\">You are a seasoned UK parliamentary member.\nUse proper British parliamentary language appropriate for the specified House.\n<br class=\"ltx_break\"/>The speech should reflect the political orientation and typical positions of the specified party on the given topic.\n<br class=\"ltx_break\"/></span>\n</p>\n\n",
                "matched_terms": [
                    "political",
                    "orientation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">You are a seasoned UK parliamentary member. Generate a coherent speech of\nmin_words - max_words words in standard English (no Unicode artifacts, no special characters).</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_typewriter\">\nUse proper British parliamentary language appropriate for the specified House. \n<br class=\"ltx_break\"/>The speech should reflect the political orientation and typical positions of the specified party on the given topic. \n<br class=\"ltx_break\"/></span></p>\n\n",
                "matched_terms": [
                    "political",
                    "orientation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">POLITICAL ORIENTATION:</span> Political orientation label (e.g., Right)</p>\n\n",
                "matched_terms": [
                    "right",
                    "political",
                    "orientation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T12\" title=\"Table 12 &#8227; 13.4. Cross-Context Stability Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> presents cross-context stability scores. Fine-tuned models maintain high consistency across political contexts (91.4-96.2), with Mistral achieving highest overall stability (96.2).</p>\n\n",
                "matched_terms": [
                    "consistency",
                    "political"
                ]
            }
        ]
    },
    "S13.T11": {
        "source_file": "ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech",
        "caption": "Table 11: Model Performance by Topic Domain (Party Alignment Scores)",
        "body": "Topic\nGemma 2 9B\nLlama 3.1 8B\nMistral 7B v0.3\nQwen2 7B\nYi 6B\nAverage\nStd\n\n\n\n\nAgri-Foodstuffs\n0.549\n0.538\n0.475\n0.535\n0.565\n0.532\n0.031\n\n\nAgriculture, Forestry and Fisheries\n0.587\n0.572\n0.491\n0.579\n0.568\n0.559\n0.035\n\n\nBusiness and Competition\n0.606\n0.566\n0.528\n0.577\n0.582\n0.572\n0.026\n\n\nEconomics\n0.626\n0.621\n0.553\n0.615\n0.636\n0.610\n0.029\n\n\nEducation and Communications\n0.581\n0.575\n0.492\n0.546\n0.565\n0.552\n0.032\n\n\nEmployment and Working Conditions\n0.587\n0.583\n0.507\n0.569\n0.585\n0.566\n0.030\n\n\nEnergy\n0.584\n0.564\n0.494\n0.563\n0.566\n0.554\n0.031\n\n\nEnvironment\n0.553\n0.550\n0.483\n0.546\n0.562\n0.539\n0.028\n\n\nEuropean Union\n0.633\n0.612\n0.539\n0.609\n0.637\n0.606\n0.035\n\n\nFinance\n0.614\n0.601\n0.546\n0.608\n0.599\n0.594\n0.024\n\n\nGeography\n0.615\n0.593\n0.532\n0.594\n0.607\n0.588\n0.029\n\n\nIndustry\n0.591\n0.564\n0.481\n0.565\n0.580\n0.556\n0.039\n\n\nInternational Organisations\n0.583\n0.577\n0.490\n0.575\n0.586\n0.562\n0.036\n\n\nInternational Relations\n0.585\n0.560\n0.489\n0.552\n0.559\n0.549\n0.032\n\n\nLaw\n0.592\n0.580\n0.505\n0.575\n0.579\n0.566\n0.031\n\n\nPolitics\n0.617\n0.605\n0.549\n0.597\n0.592\n0.592\n0.023\n\n\nProduction, Technology and Research\n0.562\n0.561\n0.482\n0.555\n0.563\n0.545\n0.031\n\n\nScience\n0.539\n0.521\n0.460\n0.545\n0.513\n0.516\n0.030\n\n\nSocial Questions\n0.592\n0.583\n0.511\n0.589\n0.600\n0.575\n0.032\n\n\nTrade\n0.591\n0.576\n0.514\n0.553\n0.574\n0.562\n0.027\n\n\nTransport\n0.581\n0.581\n0.509\n0.572\n0.574\n0.563\n0.027",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Topic</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">Gemma 2 9B</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">Llama 3.1 8B</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">Mistral 7B v0.3</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">Qwen2 7B</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">Yi 6B</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">Average</th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">Std</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Agri-Foodstuffs</th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.549</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.538</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.475</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.535</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.565</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.532</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.031</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Agriculture, Forestry and Fisheries</th>\n<td class=\"ltx_td ltx_align_right\">0.587</td>\n<td class=\"ltx_td ltx_align_right\">0.572</td>\n<td class=\"ltx_td ltx_align_right\">0.491</td>\n<td class=\"ltx_td ltx_align_right\">0.579</td>\n<td class=\"ltx_td ltx_align_right\">0.568</td>\n<td class=\"ltx_td ltx_align_right\">0.559</td>\n<td class=\"ltx_td ltx_align_right\">0.035</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Business and Competition</th>\n<td class=\"ltx_td ltx_align_right\">0.606</td>\n<td class=\"ltx_td ltx_align_right\">0.566</td>\n<td class=\"ltx_td ltx_align_right\">0.528</td>\n<td class=\"ltx_td ltx_align_right\">0.577</td>\n<td class=\"ltx_td ltx_align_right\">0.582</td>\n<td class=\"ltx_td ltx_align_right\">0.572</td>\n<td class=\"ltx_td ltx_align_right\">0.026</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Economics</th>\n<td class=\"ltx_td ltx_align_right\">0.626</td>\n<td class=\"ltx_td ltx_align_right\">0.621</td>\n<td class=\"ltx_td ltx_align_right\">0.553</td>\n<td class=\"ltx_td ltx_align_right\">0.615</td>\n<td class=\"ltx_td ltx_align_right\">0.636</td>\n<td class=\"ltx_td ltx_align_right\">0.610</td>\n<td class=\"ltx_td ltx_align_right\">0.029</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Education and Communications</th>\n<td class=\"ltx_td ltx_align_right\">0.581</td>\n<td class=\"ltx_td ltx_align_right\">0.575</td>\n<td class=\"ltx_td ltx_align_right\">0.492</td>\n<td class=\"ltx_td ltx_align_right\">0.546</td>\n<td class=\"ltx_td ltx_align_right\">0.565</td>\n<td class=\"ltx_td ltx_align_right\">0.552</td>\n<td class=\"ltx_td ltx_align_right\">0.032</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Employment and Working Conditions</th>\n<td class=\"ltx_td ltx_align_right\">0.587</td>\n<td class=\"ltx_td ltx_align_right\">0.583</td>\n<td class=\"ltx_td ltx_align_right\">0.507</td>\n<td class=\"ltx_td ltx_align_right\">0.569</td>\n<td class=\"ltx_td ltx_align_right\">0.585</td>\n<td class=\"ltx_td ltx_align_right\">0.566</td>\n<td class=\"ltx_td ltx_align_right\">0.030</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Energy</th>\n<td class=\"ltx_td ltx_align_right\">0.584</td>\n<td class=\"ltx_td ltx_align_right\">0.564</td>\n<td class=\"ltx_td ltx_align_right\">0.494</td>\n<td class=\"ltx_td ltx_align_right\">0.563</td>\n<td class=\"ltx_td ltx_align_right\">0.566</td>\n<td class=\"ltx_td ltx_align_right\">0.554</td>\n<td class=\"ltx_td ltx_align_right\">0.031</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Environment</th>\n<td class=\"ltx_td ltx_align_right\">0.553</td>\n<td class=\"ltx_td ltx_align_right\">0.550</td>\n<td class=\"ltx_td ltx_align_right\">0.483</td>\n<td class=\"ltx_td ltx_align_right\">0.546</td>\n<td class=\"ltx_td ltx_align_right\">0.562</td>\n<td class=\"ltx_td ltx_align_right\">0.539</td>\n<td class=\"ltx_td ltx_align_right\">0.028</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">European Union</th>\n<td class=\"ltx_td ltx_align_right\">0.633</td>\n<td class=\"ltx_td ltx_align_right\">0.612</td>\n<td class=\"ltx_td ltx_align_right\">0.539</td>\n<td class=\"ltx_td ltx_align_right\">0.609</td>\n<td class=\"ltx_td ltx_align_right\">0.637</td>\n<td class=\"ltx_td ltx_align_right\">0.606</td>\n<td class=\"ltx_td ltx_align_right\">0.035</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Finance</th>\n<td class=\"ltx_td ltx_align_right\">0.614</td>\n<td class=\"ltx_td ltx_align_right\">0.601</td>\n<td class=\"ltx_td ltx_align_right\">0.546</td>\n<td class=\"ltx_td ltx_align_right\">0.608</td>\n<td class=\"ltx_td ltx_align_right\">0.599</td>\n<td class=\"ltx_td ltx_align_right\">0.594</td>\n<td class=\"ltx_td ltx_align_right\">0.024</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Geography</th>\n<td class=\"ltx_td ltx_align_right\">0.615</td>\n<td class=\"ltx_td ltx_align_right\">0.593</td>\n<td class=\"ltx_td ltx_align_right\">0.532</td>\n<td class=\"ltx_td ltx_align_right\">0.594</td>\n<td class=\"ltx_td ltx_align_right\">0.607</td>\n<td class=\"ltx_td ltx_align_right\">0.588</td>\n<td class=\"ltx_td ltx_align_right\">0.029</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Industry</th>\n<td class=\"ltx_td ltx_align_right\">0.591</td>\n<td class=\"ltx_td ltx_align_right\">0.564</td>\n<td class=\"ltx_td ltx_align_right\">0.481</td>\n<td class=\"ltx_td ltx_align_right\">0.565</td>\n<td class=\"ltx_td ltx_align_right\">0.580</td>\n<td class=\"ltx_td ltx_align_right\">0.556</td>\n<td class=\"ltx_td ltx_align_right\">0.039</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">International Organisations</th>\n<td class=\"ltx_td ltx_align_right\">0.583</td>\n<td class=\"ltx_td ltx_align_right\">0.577</td>\n<td class=\"ltx_td ltx_align_right\">0.490</td>\n<td class=\"ltx_td ltx_align_right\">0.575</td>\n<td class=\"ltx_td ltx_align_right\">0.586</td>\n<td class=\"ltx_td ltx_align_right\">0.562</td>\n<td class=\"ltx_td ltx_align_right\">0.036</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">International Relations</th>\n<td class=\"ltx_td ltx_align_right\">0.585</td>\n<td class=\"ltx_td ltx_align_right\">0.560</td>\n<td class=\"ltx_td ltx_align_right\">0.489</td>\n<td class=\"ltx_td ltx_align_right\">0.552</td>\n<td class=\"ltx_td ltx_align_right\">0.559</td>\n<td class=\"ltx_td ltx_align_right\">0.549</td>\n<td class=\"ltx_td ltx_align_right\">0.032</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Law</th>\n<td class=\"ltx_td ltx_align_right\">0.592</td>\n<td class=\"ltx_td ltx_align_right\">0.580</td>\n<td class=\"ltx_td ltx_align_right\">0.505</td>\n<td class=\"ltx_td ltx_align_right\">0.575</td>\n<td class=\"ltx_td ltx_align_right\">0.579</td>\n<td class=\"ltx_td ltx_align_right\">0.566</td>\n<td class=\"ltx_td ltx_align_right\">0.031</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Politics</th>\n<td class=\"ltx_td ltx_align_right\">0.617</td>\n<td class=\"ltx_td ltx_align_right\">0.605</td>\n<td class=\"ltx_td ltx_align_right\">0.549</td>\n<td class=\"ltx_td ltx_align_right\">0.597</td>\n<td class=\"ltx_td ltx_align_right\">0.592</td>\n<td class=\"ltx_td ltx_align_right\">0.592</td>\n<td class=\"ltx_td ltx_align_right\">0.023</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Production, Technology and Research</th>\n<td class=\"ltx_td ltx_align_right\">0.562</td>\n<td class=\"ltx_td ltx_align_right\">0.561</td>\n<td class=\"ltx_td ltx_align_right\">0.482</td>\n<td class=\"ltx_td ltx_align_right\">0.555</td>\n<td class=\"ltx_td ltx_align_right\">0.563</td>\n<td class=\"ltx_td ltx_align_right\">0.545</td>\n<td class=\"ltx_td ltx_align_right\">0.031</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Science</th>\n<td class=\"ltx_td ltx_align_right\">0.539</td>\n<td class=\"ltx_td ltx_align_right\">0.521</td>\n<td class=\"ltx_td ltx_align_right\">0.460</td>\n<td class=\"ltx_td ltx_align_right\">0.545</td>\n<td class=\"ltx_td ltx_align_right\">0.513</td>\n<td class=\"ltx_td ltx_align_right\">0.516</td>\n<td class=\"ltx_td ltx_align_right\">0.030</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Social Questions</th>\n<td class=\"ltx_td ltx_align_right\">0.592</td>\n<td class=\"ltx_td ltx_align_right\">0.583</td>\n<td class=\"ltx_td ltx_align_right\">0.511</td>\n<td class=\"ltx_td ltx_align_right\">0.589</td>\n<td class=\"ltx_td ltx_align_right\">0.600</td>\n<td class=\"ltx_td ltx_align_right\">0.575</td>\n<td class=\"ltx_td ltx_align_right\">0.032</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Trade</th>\n<td class=\"ltx_td ltx_align_right\">0.591</td>\n<td class=\"ltx_td ltx_align_right\">0.576</td>\n<td class=\"ltx_td ltx_align_right\">0.514</td>\n<td class=\"ltx_td ltx_align_right\">0.553</td>\n<td class=\"ltx_td ltx_align_right\">0.574</td>\n<td class=\"ltx_td ltx_align_right\">0.562</td>\n<td class=\"ltx_td ltx_align_right\">0.027</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Transport</th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">0.581</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">0.581</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">0.509</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">0.572</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">0.574</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">0.563</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">0.027</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "industry",
            "llama",
            "mistral",
            "employment",
            "std",
            "production",
            "topic",
            "questions",
            "v03",
            "business",
            "economics",
            "conditions",
            "relations",
            "education",
            "communications",
            "trade",
            "competition",
            "law",
            "model",
            "qwen2",
            "union",
            "international",
            "fisheries",
            "forestry",
            "alignment",
            "working",
            "average",
            "technology",
            "performance",
            "politics",
            "gemma",
            "domain",
            "transport",
            "energy",
            "finance",
            "agrifoodstuffs",
            "science",
            "scores",
            "social",
            "research",
            "organisations",
            "agriculture",
            "party",
            "environment",
            "geography",
            "european"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Topic Performance Patterns</span>. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F6\" title=\"Figure 6 &#8227; 6.3. Topic Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows model performance across topic domains. Science achieved lowest scores (avg 0.516), while Economics (0.610) and European Union (0.606) showed highest performance. Detailed scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T11\" title=\"Table 11 &#8227; 13.3. Topic-Specific Performance &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T11\" title=\"Table 11 &#8227; 13.3. Topic-Specific Performance &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> presents model performance across topic domains for fine-tuned models.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Parliamentary speech generation presents specific challenges for large language models beyond standard text generation tasks. Unlike general text generation, parliamentary speeches require not only linguistic quality but also political authenticity and ideological consistency. Current language models lack specialized training for parliamentary contexts, and existing evaluation methods focus on standard NLP metrics rather than political authenticity. To address this, we present ParliaBench, a benchmark for parliamentary speech generation. We constructed a dataset of speeches from UK Parliament to enable systematic model training. We introduce an evaluation framework combining computational metrics with LLM-as-a-judge assessments for measuring generation quality across three dimensions: linguistic quality, semantic coherence, and political authenticity. We propose two novel embedding-based metrics, Political Spectrum Alignment and Party Alignment, to quantify ideological positioning. We fine-tuned five large language models (LLMs), generated 28k speeches, and evaluated them\nusing our framework, comparing baseline and fine-tuned models. Results show that fine-tuning produces statistically significant improvements across the majority of metrics and our novel metrics demonstrate strong discriminative power for political dimensions.\n\n\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Parliamentary Speech Generation,LLM Evaluation,Political Authenticity,Benchmark Evaluation</p>\n\n",
                "matched_terms": [
                    "model",
                    "party",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Democracy thrives through debate. Democratic parliaments are open forums where elected representatives engage in arguments over policy <cite class=\"ltx_cite ltx_citemacro_cite\">Back et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib4\" title=\"\">2021</a>)</cite>. These debates provide unique insights into political reasoning and ideological positioning. Researchers in political science and computational linguistics increasingly seek to understand and model parliamentary debates.</p>\n\n",
                "matched_terms": [
                    "science",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in large language models (LLMs) have enabled a wide range of applications across political domains. <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib22\" title=\"\">2024b</a>)</cite> outlines several applications of LLM in political contexts, covering predictive, generative, and simulation-based approaches. The use of LLMs as substitutes for human experts in annotating political texts across multiple languages is explored in <cite class=\"ltx_cite ltx_citemacro_cite\">Heseltine and von Hohenberg (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib16\" title=\"\">2024</a>)</cite>, while <cite class=\"ltx_cite ltx_citemacro_cite\">Gunes and Florczak (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib14\" title=\"\">2023</a>)</cite> employ LLMs for classifying U.S. Congressional bills. <cite class=\"ltx_cite ltx_citemacro_cite\">Argyle et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib3\" title=\"\">2023</a>)</cite> investigate LLMs as proxies for specific human subpopulations in social science research and <cite class=\"ltx_cite ltx_citemacro_cite\">Bisbee et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib7\" title=\"\">2024</a>)</cite> raise concerns about the quality, reliability, and reproducibility of synthetic survey data generated by LLMs.\nAgent-based LLMs are utilized as coalition negotiators <cite class=\"ltx_cite ltx_citemacro_cite\">Moghimifar et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib25\" title=\"\">2024</a>)</cite> and as U.S. senators\nsimulating legislative processes <cite class=\"ltx_cite ltx_citemacro_cite\">Baker and Azher (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib5\" title=\"\">2024</a>)</cite>. However, these approaches predominantly emphasize analytical and simulation capabilities rather than authentic speech generation quality.</p>\n\n",
                "matched_terms": [
                    "science",
                    "research",
                    "social"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Domain-specific evaluation frameworks have emerged across professional fields, including FinBen <cite class=\"ltx_cite ltx_citemacro_cite\">Xie et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib36\" title=\"\">2024</a>)</cite> and LexEval <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib20\" title=\"\">2024a</a>)</cite>. Political science applications have developed specialized benchmarks for election prediction and legislative analysis, yet these focus primarily on classification and analysis tasks. Parliamentary speech generation has attracted recent computational interest, with work exploring European Parliament consensus building <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib38\" title=\"\">2025</a>)</cite> and political impersonation authenticity <cite class=\"ltx_cite ltx_citemacro_cite\">Herbold et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib15\" title=\"\">2024</a>)</cite>, but evaluation frameworks remain underdeveloped. Existing approaches focus on narrow aspects like style mimicry rather than systematic quality assessment across linguistic and political authenticity dimensions that parliamentary speech generation requires.</p>\n\n",
                "matched_terms": [
                    "science",
                    "european"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Step 4: Thematic Classification</span>. While ParlaMint uses CAP classification, we selected EuroVoc <cite class=\"ltx_cite ltx_citemacro_cite\">Publications Office of the European Union (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib28\" title=\"\">2025</a>)</cite> as the standard classification system for European parliamentary systems. For policy domains with clear semantic correspondence between CAP and EuroVoc taxonomies, we applied direct mapping rules. For semantically complex or ambiguous categories, we employed the methodology provided by <cite class=\"ltx_cite ltx_citemacro_cite\">Bocchi et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib8\" title=\"\">2024</a>)</cite>. We argue that this approach is particularly well-suited for our dataset because it was specifically designed for legal and governmental texts. For speeches yielding multiple concepts, we selected the highest individual concept score. (Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S9.SS2\" title=\"9.2. Hybrid Classification Strategy &#8227; 9. Dataset Details &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">9.2</span></a> documents the hybrid classification strategy)</p>\n\n",
                "matched_terms": [
                    "union",
                    "european"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S3.F2\" title=\"Figure 2 &#8227; 3.2. Statistics &#8227; 3. ParliaBench Dataset &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates key dataset characteristics. Panel (a) reveals a highly right-skewed distribution of speech lengths, indicating that the dataset is dominated by relatively short speeches while containing a smaller number of substantially longer ones. Panel (b) presents topic distribution showing that speeches most frequently address International Relations, Law, and Politics, which together account for over 45% of the corpus. Panel (c) demonstrates ideological balance across the political spectrum. While most parties by count are in the centre-left to left spectrum, the centre-right to right spectrum produces a larger amount of speeches by 110,000. Panel (d) displays temporal patterns and institutional differences. The House of Commons consistently produces 3-4x more speeches than the House of Lords across 2015-2022.</p>\n\n",
                "matched_terms": [
                    "law",
                    "international",
                    "relations",
                    "topic",
                    "politics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generated Dataset.</span> We release 27,560 speeches (model outputs) produced during evaluation (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S5\" title=\"5. Experimental Setup &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). The generated dataset follows the same format with additional fields: <span class=\"ltx_text ltx_font_italic\">Model</span> (architecture identifier), <span class=\"ltx_text ltx_font_italic\">Type</span> (baseline/fine-tuned), <span class=\"ltx_text ltx_font_italic\">Generated Speech</span> (model output), and <span class=\"ltx_text ltx_font_italic\">Evaluation Scores</span> (computed metrics). To ensure a fair comparison, we used the same input prompts across all models and model types, allowing evaluation on identical inputs.</p>\n\n",
                "matched_terms": [
                    "model",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our framework introduces two novel computational metrics (Political Spectrum Alignment and Party Alignment, Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.SS1.SSS1\" title=\"4.1.1. Novel Political Authenticity Metrics &#8227; 4.1. Speech Evaluation Metrics &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4.1.1</span></a>) alongside established metrics from both computational and LLM-judge traditions.</p>\n\n",
                "matched_terms": [
                    "party",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Perplexity</span> (PPL)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jelinek et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib17\" title=\"\">1977</a>)</cite> measures text naturalness. We compute perplexity using GPT-2 as a fixed reference model across all generated speeches, ensuring cross-model comparability. Lower scores indicate more natural-sounding text according to GPT-2&#8217;s language distribution. (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i1.I1.i1.I1.i1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math> better)</p>\n\n",
                "matched_terms": [
                    "model",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ two novel embedding-based metrics for political authenticity assessment: Political Spectrum Alignment (PSA) and Party Alignment (Party Align). Detailed calculation methodology provided in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.SS1.SSS1\" title=\"4.1.1. Novel Political Authenticity Metrics &#8227; 4.1. Speech Evaluation Metrics &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4.1.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "party",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Party Alignment</span> (Party Align) applies the same embedding methodology to party-specific alignment, using party affiliation rather than political orientation for centroid construction. The alignment score measures cosine similarity between generated speech and expected party centroid:</p>\n\n",
                "matched_terms": [
                    "party",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"c_{p}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p10.m1\" intent=\":literal\"><semantics><msub><mi>c</mi><mi>p</mi></msub><annotation encoding=\"application/x-tex\">c_{p}</annotation></semantics></math> is the party-specific centroid and <math alttext=\"\\text{sim}(s,c_{p})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p10.m2\" intent=\":literal\"><semantics><mrow><mtext>sim</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo>,</mo><msub><mi>c</mi><mi>p</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{sim}(s,c_{p})</annotation></semantics></math> the cosine similarity between speech <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.SSS1.p10.m3\" intent=\":literal\"><semantics><mi>s</mi><annotation encoding=\"application/x-tex\">s</annotation></semantics></math> and centroid (0-1 scale). Higher scores indicate stronger alignment with party-specific language.</p>\n\n",
                "matched_terms": [
                    "scores",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Context Stability</span> measures performance consistency using coefficient of variation, with higher scores indicating more consistent performance across political contexts. This meta-evaluation applies to all speech evaluation metrics, providing diagnostic insight into model reliability. For cross-metric comparison, all metrics, Computational &amp; LLM-Judge, are normalized to 0-1 scale.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We examine consistency across three dimensions: political parties, topic domains, and political orientations. The stability calculation quantifies performance variability:</p>\n\n",
                "matched_terms": [
                    "topic",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We selected five language models representing distinct architectural approaches to establish baseline performance for parliamentary speech generation:\n<span class=\"ltx_text ltx_font_bold\">Mistral 7B v0.3</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib18\" title=\"\">2023</a>)</cite> uses Grouped Query Attention and Sliding Window Attention for efficient long-context processing.\n<span class=\"ltx_text ltx_font_bold\">Llama 3.1 8B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Dubey et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib11\" title=\"\">2024</a>)</cite> features 128k-token context window and expanded vocabulary.\n<span class=\"ltx_text ltx_font_bold\">Gemma 2 9B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Team et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib32\" title=\"\">2024</a>)</cite> employs alternating local/global attention across 42 layers with logit soft-capping.\n<span class=\"ltx_text ltx_font_bold\">Qwen2 7B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Team (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib33\" title=\"\">2024</a>)</cite> is a multilingual model with enhanced reasoning capabilities.\n<span class=\"ltx_text ltx_font_bold\">YI 6B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">AI et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib2\" title=\"\">2025</a>)</cite> emphasizes strong reasoning and coding performance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "qwen2",
                    "llama",
                    "mistral",
                    "v03",
                    "performance",
                    "gemma"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed Quantized Low-Rank Adaptation (QLoRA) <cite class=\"ltx_cite ltx_citemacro_cite\">Dettmers et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib10\" title=\"\">2023</a>)</cite> for parameter-efficient fine-tuning. (<math alttext=\"r=16\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">r=16</annotation></semantics></math>, <math alttext=\"\\alpha=16\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=16</annotation></semantics></math>, <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math> epochs). Model-specific chat templates structure training inputs with political metadata (party affiliation, topic classification, orientation, section, and house). Training used SFTTrainer from TRL with 80%-20% train-test splits and automated checkpointing. Fine-tuned models were saved with adapter weights for subsequent evaluation, ensuring consistent model states across experiments.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "model",
                    "party"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We planned to generate 30,000 speeches (3,000 per model-type combination across 10 models) using stratified sampling from the held-out test set. To maintain consistency with training conditions, our prompt distribution matches the ParliaBench Dataset structure: 90% generic instruction prompts formatted with political context (party, topic, orientation, section, house) and 10% specific parliamentary questions from the test set.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "conditions",
                    "party",
                    "questions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated 27,560 generated speeches using our evaluation framework. This section presents fine-tuning effectiveness and performance patterns across political parties, topic domains, and ideological orientations. Representative examples of generated speeches are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S12\" title=\"12. Representative Generated Speeches &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.T2\" title=\"Table 2 &#8227; 6.1. Overview and Fine-Tuning Impact &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents metric results organized by our framework assessment categories. Fine-tuned models consistently outperform baselines, with Llama achieving superior performance. Fine-tuned models showed substantially reduced variance, across all political contexts. Extended context windows (128k tokens) and larger vocabularies contribute to architectural advantages.\n</p>\n\n",
                "matched_terms": [
                    "performance",
                    "llama"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Pairwise t-tests confirm statistical significance of fine-tuning effects (45 out of 70 comparisons). Model architectures exhibited differential responsiveness: <span class=\"ltx_text ltx_font_italic\">YI</span> and <span class=\"ltx_text ltx_font_italic\">Llama</span> achieved notable improvements (11/14 metrics, 79%), while others showed more selective gains (improvements marked with <sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.T2\" title=\"Table 2 &#8227; 6.1. Overview and Fine-Tuning Impact &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>).</p>\n\n",
                "matched_terms": [
                    "llama",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F3\" title=\"Figure 3 &#8227; 6.1. Overview and Fine-Tuning Impact &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows fine-tuning impact across evaluation categories. <span class=\"ltx_text ltx_font_italic\">YI</span> achieved the strongest improvements across all dimensions , while <span class=\"ltx_text ltx_font_italic\">Llama</span> had consistent gains. <span class=\"ltx_text ltx_font_italic\">Gemma2</span> and <span class=\"ltx_text ltx_font_italic\">Qwen2</span> exhibited quality trade-offs, with improvements in one category accompanied by declines in others, suggesting architectural differences in how models balance competing objectives during fine-tuning. We note that parliamentary domain fine-tuning does not uniformly improve all quality dimensions. Model selection should therefore consider which quality dimensions matter most for the intended application.</p>\n\n",
                "matched_terms": [
                    "domain",
                    "llama",
                    "model",
                    "qwen2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-context stability analysis (Eq.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.E4\" title=\"In 4.2. Consistency Evaluation Metrics &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) revealed fine-tuned models maintained consistent performance across political contexts (composite stability 91.4-96.2). Mistral achieved highest consistency (96.2) despite trade-offs in absolute performance, while Llama (95.1) balanced strong performance with stability.\nDetailed scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T12\" title=\"Table 12 &#8227; 13.4. Cross-Context Stability Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "llama",
                    "mistral",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Party alignment Patterns</span>. Party alignment performance varied substantially across models (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F4\" title=\"Figure 4 &#8227; 6.2. Political Context Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). Major parties (Conservative, Labour) achieved stable performance across models, benefiting from substantial training data (58.9%, 24.3%). Minor parties exhibited greater variability. <span class=\"ltx_text ltx_font_italic\">Mistral</span> struggled with heterogeneous groups (Non-Affiliated: 0.436), while <span class=\"ltx_text ltx_font_italic\">Qwen</span> excelled with ideologically coherent minorities (Bishops: 0.664). <span class=\"ltx_text ltx_font_italic\">YI</span> demonstrated robust cross-party performance (0.614-0.633).\nDetailed scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T8\" title=\"Table 8 &#8227; 13.1. Party-Specific Performance &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "mistral",
                    "scores",
                    "party",
                    "alignment",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both new political authenticity metrics (PSA and Party Align) successfully discriminate their target political dimensions. Party Align distinguishes parties while PSA distinguishes orientations (both <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>). Our analysis reveals that Party Align performance depends primarily on data abundance and ideological coherence rather than party size alone. Models successfully learn party-specific language patterns when training data provides clear stylistic signals, indicating targeted data collection for under-represented parties could improve coverage.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "party"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Party alignment Difficulty Analysis</span>. Applying cross-context stability analysis (Eq.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.E4\" title=\"In 4.2. Consistency Evaluation Metrics &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), party difficulty scores ranged narrowly (0.382-0.456), with no statistically significant differences. This suggests relatively consistent modeling challenges across parties regardless of size or ideological composition. Results are presented in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F5\" title=\"Figure 5 &#8227; 6.2. Political Context Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Detailed scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T9\" title=\"Table 9 &#8227; 13.2. Metric Validation: Political Discrimination Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "party",
                    "scores",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Topic Difficulty Analysis</span>. Different topics posed different challenges (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F7\" title=\"Figure 7 &#8227; 6.3. Topic Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>). Science and Geography ranked as most difficult while Finance, Business, and Economics ranked lowest. Technical and natural science domains display higher cross-model disagreement than economic and political topics, consistent with greater terminological specialization and rapidly evolving concepts. In contrast, economic and political discussions employs more stable conceptual frameworks aligned with core parliamentary functions.</p>\n\n",
                "matched_terms": [
                    "business",
                    "economics",
                    "science",
                    "finance",
                    "topic",
                    "geography"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Performance across political orientations showed expected patterns. Centrist positions (Centre-left: 0.607, Centre-right: 0.551) dominated the dataset (88%) and achieved higher scores. Model-specific strengths emerged as both <span class=\"ltx_text ltx_font_italic\">Gemma</span> and <span class=\"ltx_text ltx_font_italic\">Qwen</span> achieved highest scores on Right positions and <span class=\"ltx_text ltx_font_italic\">Mistral</span> underperformed consistently, indicating architectural rather than ideological limitations. As models are optimized for mainstream parliamentary speeches, extreme positions may require specialized training approaches. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F8\" title=\"Figure 8 &#8227; 6.4. Political Orientations Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> illustrates these patterns. Detailed orientation difficulty rankings are provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T10\" title=\"Table 10 &#8227; 13.2. Metric Validation: Political Discrimination Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a></p>\n\n",
                "matched_terms": [
                    "performance",
                    "gemma",
                    "mistral",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To the best of our knowledge, ParliaBench represents the first benchmark resource addressing the specialized challenges of parliamentary speech generation, comprising dataset, evaluation framework, novel metrics, and baseline benchmarks. ParliaBench provides standardized evaluation protocols and baseline performance results that\nsupport systematic comparison and reproducible research in the field.\nOur results demonstrate that domain specific fine-tuning produces significant quality improvements, while\nour novel political authenticity metrics successfully capture ideological dimensions absent from conventional evaluation approaches.</p>\n\n",
                "matched_terms": [
                    "domain",
                    "performance",
                    "research"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">AWS resources were provided by the National Infrastructures for Research and Technology GRNET and funded by the EU Recovery and Resiliency Facility.</p>\n\n",
                "matched_terms": [
                    "technology",
                    "research"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work establishes a benchmark resource for evaluating LLM-generated parliamentary speech in research and educational contexts. These resources should only be used as assistance to human experts with consideration of their limitations and biases. The parliamentary data is derived from publicly available UK parliamentary proceedings (ParlaMint corpus) licensed under Creative Commons Attribution 4.0 International, which our derived datasets maintain.\nGenerated parliamentary speeches must be clearly identified as AI-generated content and not misrepresented as authentic political speeches from actual parliamentarians.</p>\n\n",
                "matched_terms": [
                    "research",
                    "international"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Political Appropriateness (j_polapp)</span>: Alignment with party positions</p>\n\n",
                "matched_terms": [
                    "party",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Target modules encompass all linear transformation layers (q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj) to achieve performance comparable to full fine-tuning while requiring only a fraction of the computational resources. The consistent random state across all architectures ensures reproducible results essential for systematic model comparison.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_typewriter\">You are a seasoned UK parliamentary member.\nUse proper British parliamentary language appropriate for the specified House.\n<br class=\"ltx_break\"/>The speech should reflect the political orientation and typical positions of the specified party on the given topic.\n<br class=\"ltx_break\"/></span>\n</p>\n\n",
                "matched_terms": [
                    "topic",
                    "party"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">You are a seasoned UK parliamentary member. Generate a coherent speech of\nmin_words - max_words words in standard English (no Unicode artifacts, no special characters).</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_typewriter\">\nUse proper British parliamentary language appropriate for the specified House. \n<br class=\"ltx_break\"/>The speech should reflect the political orientation and typical positions of the specified party on the given topic. \n<br class=\"ltx_break\"/></span></p>\n\n",
                "matched_terms": [
                    "topic",
                    "party"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">TOPIC:</span> EuroVoc classification (e.g., TRADE)</p>\n\n",
                "matched_terms": [
                    "topic",
                    "trade"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T8\" title=\"Table 8 &#8227; 13.1. Party-Specific Performance &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> presents detailed performance scores for all UK parliamentary parties across fine-tuned models.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T12\" title=\"Table 12 &#8227; 13.4. Cross-Context Stability Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> presents cross-context stability scores. Fine-tuned models maintain high consistency across political contexts (91.4-96.2), with Mistral achieving highest overall stability (96.2).</p>\n\n",
                "matched_terms": [
                    "mistral",
                    "scores"
                ]
            }
        ]
    },
    "S13.T12": {
        "source_file": "ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech",
        "caption": "Table 12: Cross-Context Stability Analysis by Model and Dimension",
        "body": "Model\nAffiliation\nTopic\nOrientation\nOverall\n\n\n\n\nLlama 3.1 8B\n92.0\n97.6\n95.7\n95.1\n\n\nGemma 2 9B\n88.3\n95.0\n91.0\n91.4\n\n\nMistral 7B v0.3\n94.0\n97.2\n97.4\n96.2\n\n\nQwen2 7B\n92.7\n97.7\n96.4\n95.6\n\n\nYi 6B\n91.0\n97.9\n95.5\n94.8",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Affiliation</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Topic</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Orientation</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Overall</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Llama 3.1 8B</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">92.0</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">97.6</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">95.7</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">95.1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Gemma 2 9B</span></th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">88.3</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">95.0</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">91.0</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">91.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Mistral 7B v0.3</span></th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">94.0</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">97.2</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">97.4</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">96.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Qwen2 7B</span></th>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">92.7</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">97.7</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">96.4</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">95.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">Yi 6B</span></th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">91.0</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">97.9</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">95.5</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">94.8</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "stability",
            "overall",
            "model",
            "qwen2",
            "orientation",
            "llama",
            "mistral",
            "dimension",
            "topic",
            "analysis",
            "affiliation",
            "crosscontext",
            "v03",
            "gemma"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Cross-context stability analysis (Eq.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.E4\" title=\"In 4.2. Consistency Evaluation Metrics &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) revealed fine-tuned models maintained consistent performance across political contexts (composite stability 91.4-96.2). Mistral achieved highest consistency (96.2) despite trade-offs in absolute performance, while Llama (95.1) balanced strong performance with stability.\nDetailed scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T12\" title=\"Table 12 &#8227; 13.4. Cross-Context Stability Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T12\" title=\"Table 12 &#8227; 13.4. Cross-Context Stability Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> presents cross-context stability scores. Fine-tuned models maintain high consistency across political contexts (91.4-96.2), with Mistral achieving highest overall stability (96.2).</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Computational metrics provide deterministic assessment through established NLP measures, while LLM-judge metrics capture nuanced qualities requiring contextual understanding. For LLM-judge evaluation, we adapt the methodology from <cite class=\"ltx_cite ltx_citemacro_cite\">Zheng et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib40\" title=\"\">2023</a>)</cite>. We employed Flow-Judge-v0.1 as our LLM judge, an LLM specialized in system evaluation tasks. It inherits it&#8217;s architecture from Phi-3.5-mini instruct <cite class=\"ltx_cite ltx_citemacro_cite\">Abdin et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib1\" title=\"\">2024</a>)</cite>, thus ensuring complete architectural and training data independence from the evaluated models. Our judge rates each speech on a 1-10 scale with written explanations across six parliamentary specific dimensions. We acknowledge this introduces bias through single model judgment. We employ consistent prompt formatting and evaluate speeches sampled across diverse political contexts. Specific prompts for each dimension are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S10\" title=\"10. LLM-as-a-Judge Evaluation Prompts &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dimension"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Parties with unknown orientation are excluded from PSA analysis as their ideological position cannot be reliably mapped to the left-right spectrum.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "orientation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Party Alignment</span> (Party Align) applies the same embedding methodology to party-specific alignment, using party affiliation rather than political orientation for centroid construction. The alignment score measures cosine similarity between generated speech and expected party centroid:</p>\n\n",
                "matched_terms": [
                    "affiliation",
                    "orientation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cross-Context Stability</span> measures performance consistency using coefficient of variation, with higher scores indicating more consistent performance across political contexts. This meta-evaluation applies to all speech evaluation metrics, providing diagnostic insight into model reliability. For cross-metric comparison, all metrics, Computational &amp; LLM-Judge, are normalized to 0-1 scale.</p>\n\n",
                "matched_terms": [
                    "stability",
                    "model",
                    "crosscontext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We examine consistency across three dimensions: political parties, topic domains, and political orientations. The stability calculation quantifies performance variability:</p>\n\n",
                "matched_terms": [
                    "stability",
                    "topic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We selected five language models representing distinct architectural approaches to establish baseline performance for parliamentary speech generation:\n<span class=\"ltx_text ltx_font_bold\">Mistral 7B v0.3</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib18\" title=\"\">2023</a>)</cite> uses Grouped Query Attention and Sliding Window Attention for efficient long-context processing.\n<span class=\"ltx_text ltx_font_bold\">Llama 3.1 8B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Dubey et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib11\" title=\"\">2024</a>)</cite> features 128k-token context window and expanded vocabulary.\n<span class=\"ltx_text ltx_font_bold\">Gemma 2 9B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Team et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib32\" title=\"\">2024</a>)</cite> employs alternating local/global attention across 42 layers with logit soft-capping.\n<span class=\"ltx_text ltx_font_bold\">Qwen2 7B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Team (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib33\" title=\"\">2024</a>)</cite> is a multilingual model with enhanced reasoning capabilities.\n<span class=\"ltx_text ltx_font_bold\">YI 6B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">AI et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib2\" title=\"\">2025</a>)</cite> emphasizes strong reasoning and coding performance.</p>\n\n",
                "matched_terms": [
                    "model",
                    "qwen2",
                    "llama",
                    "mistral",
                    "v03",
                    "gemma"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed Quantized Low-Rank Adaptation (QLoRA) <cite class=\"ltx_cite ltx_citemacro_cite\">Dettmers et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib10\" title=\"\">2023</a>)</cite> for parameter-efficient fine-tuning. (<math alttext=\"r=16\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">r=16</annotation></semantics></math>, <math alttext=\"\\alpha=16\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=16</annotation></semantics></math>, <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math> epochs). Model-specific chat templates structure training inputs with political metadata (party affiliation, topic classification, orientation, section, and house). Training used SFTTrainer from TRL with 80%-20% train-test splits and automated checkpointing. Fine-tuned models were saved with adapter weights for subsequent evaluation, ensuring consistent model states across experiments.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "model",
                    "affiliation",
                    "orientation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We planned to generate 30,000 speeches (3,000 per model-type combination across 10 models) using stratified sampling from the held-out test set. To maintain consistency with training conditions, our prompt distribution matches the ParliaBench Dataset structure: 90% generic instruction prompts formatted with political context (party, topic, orientation, section, house) and 10% specific parliamentary questions from the test set.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "orientation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Pairwise t-tests confirm statistical significance of fine-tuning effects (45 out of 70 comparisons). Model architectures exhibited differential responsiveness: <span class=\"ltx_text ltx_font_italic\">YI</span> and <span class=\"ltx_text ltx_font_italic\">Llama</span> achieved notable improvements (11/14 metrics, 79%), while others showed more selective gains (improvements marked with <sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.T2\" title=\"Table 2 &#8227; 6.1. Overview and Fine-Tuning Impact &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>).</p>\n\n",
                "matched_terms": [
                    "llama",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F3\" title=\"Figure 3 &#8227; 6.1. Overview and Fine-Tuning Impact &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows fine-tuning impact across evaluation categories. <span class=\"ltx_text ltx_font_italic\">YI</span> achieved the strongest improvements across all dimensions , while <span class=\"ltx_text ltx_font_italic\">Llama</span> had consistent gains. <span class=\"ltx_text ltx_font_italic\">Gemma2</span> and <span class=\"ltx_text ltx_font_italic\">Qwen2</span> exhibited quality trade-offs, with improvements in one category accompanied by declines in others, suggesting architectural differences in how models balance competing objectives during fine-tuning. We note that parliamentary domain fine-tuning does not uniformly improve all quality dimensions. Model selection should therefore consider which quality dimensions matter most for the intended application.</p>\n\n",
                "matched_terms": [
                    "llama",
                    "model",
                    "qwen2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Party alignment Difficulty Analysis</span>. Applying cross-context stability analysis (Eq.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.E4\" title=\"In 4.2. Consistency Evaluation Metrics &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), party difficulty scores ranged narrowly (0.382-0.456), with no statistically significant differences. This suggests relatively consistent modeling challenges across parties regardless of size or ideological composition. Results are presented in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F5\" title=\"Figure 5 &#8227; 6.2. Political Context Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Detailed scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T9\" title=\"Table 9 &#8227; 13.2. Metric Validation: Political Discrimination Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "stability",
                    "analysis",
                    "crosscontext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Topic Performance Patterns</span>. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F6\" title=\"Figure 6 &#8227; 6.3. Topic Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> shows model performance across topic domains. Science achieved lowest scores (avg 0.516), while Economics (0.610) and European Union (0.606) showed highest performance. Detailed scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T11\" title=\"Table 11 &#8227; 13.3. Topic-Specific Performance &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Topic Difficulty Analysis</span>. Different topics posed different challenges (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F7\" title=\"Figure 7 &#8227; 6.3. Topic Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>). Science and Geography ranked as most difficult while Finance, Business, and Economics ranked lowest. Technical and natural science domains display higher cross-model disagreement than economic and political topics, consistent with greater terminological specialization and rapidly evolving concepts. In contrast, economic and political discussions employs more stable conceptual frameworks aligned with core parliamentary functions.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "analysis"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Performance across political orientations showed expected patterns. Centrist positions (Centre-left: 0.607, Centre-right: 0.551) dominated the dataset (88%) and achieved higher scores. Model-specific strengths emerged as both <span class=\"ltx_text ltx_font_italic\">Gemma</span> and <span class=\"ltx_text ltx_font_italic\">Qwen</span> achieved highest scores on Right positions and <span class=\"ltx_text ltx_font_italic\">Mistral</span> underperformed consistently, indicating architectural rather than ideological limitations. As models are optimized for mainstream parliamentary speeches, extreme positions may require specialized training approaches. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F8\" title=\"Figure 8 &#8227; 6.4. Political Orientations Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> illustrates these patterns. Detailed orientation difficulty rankings are provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T10\" title=\"Table 10 &#8227; 13.2. Metric Validation: Political Discrimination Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a></p>\n\n",
                "matched_terms": [
                    "gemma",
                    "mistral",
                    "orientation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_typewriter\">You are a seasoned UK parliamentary member.\nUse proper British parliamentary language appropriate for the specified House.\n<br class=\"ltx_break\"/>The speech should reflect the political orientation and typical positions of the specified party on the given topic.\n<br class=\"ltx_break\"/></span>\n</p>\n\n",
                "matched_terms": [
                    "topic",
                    "orientation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">You are a seasoned UK parliamentary member. Generate a coherent speech of\nmin_words - max_words words in standard English (no Unicode artifacts, no special characters).</span>\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_typewriter\">\nUse proper British parliamentary language appropriate for the specified House. \n<br class=\"ltx_break\"/>The speech should reflect the political orientation and typical positions of the specified party on the given topic. \n<br class=\"ltx_break\"/></span></p>\n\n",
                "matched_terms": [
                    "topic",
                    "orientation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T11\" title=\"Table 11 &#8227; 13.3. Topic-Specific Performance &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> presents model performance across topic domains for fine-tuned models.</p>\n\n",
                "matched_terms": [
                    "topic",
                    "model"
                ]
            }
        ]
    },
    "S13.T13": {
        "source_file": "ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech",
        "caption": "Table 13: Significant Pairwise T-Test Results (Bonferroni Corrected, a=0.05)",
        "body": "Test Type\nComparison\nMetric\nt-statistic\np-value\nCohen’s d\nMean Diff\n\n\n\n\nFine-Tuning Impact\nllama: Baseline vs Fine-tuned\nPPL\n-42.093\n0.0000\n-1.134\n-29.5273\n\n\nFine-Tuning Impact\nllama: Baseline vs Fine-tuned\nDist-N\n-28.085\n0.0000\n-0.757\n-0.0140\n\n\nFine-Tuning Impact\nllama: Baseline vs Fine-tuned\nSelf-BLEU\n54.928\n0.0000\n1.499\n0.0119\n\n\nFine-Tuning Impact\nllama: Baseline vs Fine-tuned\nGRUEN\n-16.938\n0.0000\n-0.456\n-0.0546\n\n\nFine-Tuning Impact\nllama: Baseline vs Fine-tuned\nBERTScore\n45.713\n0.0000\n1.231\n0.0150\n\n\nFine-Tuning Impact\nllama: Baseline vs Fine-tuned\nMoverScore\n27.713\n0.0000\n0.747\n0.0048\n\n\nFine-Tuning Impact\nllama: Baseline vs Fine-tuned\nPSA\n23.746\n0.0000\n0.648\n0.0907\n\n\nFine-Tuning Impact\nllama: Baseline vs Fine-tuned\nParty Align\n22.698\n0.0000\n0.611\n0.0739\n\n\nFine-Tuning Impact\nllama: Baseline vs Fine-tuned\nJ_Coh\n16.528\n0.0000\n0.445\n0.8599\n\n\nFine-Tuning Impact\nllama: Baseline vs Fine-tuned\nJ_Conc\n16.016\n0.0000\n0.431\n1.2362\n\n\nFine-Tuning Impact\nllama: Baseline vs Fine-tuned\nJ_Rel\n9.013\n0.0000\n0.243\n0.6684\n\n\nFine-Tuning Impact\nllama: Baseline vs Fine-tuned\nJ_Auth\n23.204\n0.0000\n0.625\n1.7689\n\n\nFine-Tuning Impact\nllama: Baseline vs Fine-tuned\nJ_PolApp\n18.941\n0.0000\n0.510\n1.0925\n\n\nFine-Tuning Impact\nllama: Baseline vs Fine-tuned\nJ_Qual\n9.903\n0.0000\n0.267\n0.6190\n\n\nFine-Tuning Impact\ngemma: Baseline vs Fine-tuned\nPPL\n8.583\n0.0000\n0.231\n12.5989\n\n\nFine-Tuning Impact\ngemma: Baseline vs Fine-tuned\nDist-N\n-4.927\n0.0000\n-0.133\n-0.0021\n\n\nFine-Tuning Impact\ngemma: Baseline vs Fine-tuned\nSelf-BLEU\n18.335\n0.0000\n0.501\n0.0026\n\n\nFine-Tuning Impact\ngemma: Baseline vs Fine-tuned\nGRUEN\n-22.653\n0.0000\n-0.610\n-0.0464\n\n\nFine-Tuning Impact\ngemma: Baseline vs Fine-tuned\nPSA\n14.218\n0.0000\n0.388\n0.0543\n\n\nFine-Tuning Impact\ngemma: Baseline vs Fine-tuned\nParty Align\n15.775\n0.0000\n0.425\n0.0469\n\n\nFine-Tuning Impact\ngemma: Baseline vs Fine-tuned\nJ_Coh\n-7.397\n0.0000\n-0.199\n-0.2794\n\n\nFine-Tuning Impact\ngemma: Baseline vs Fine-tuned\nJ_Auth\n5.658\n0.0000\n0.152\n0.3726\n\n\nFine-Tuning Impact\ngemma: Baseline vs Fine-tuned\nJ_PolApp\n16.112\n0.0000\n0.434\n0.8001\n\n\nFine-Tuning Impact\ngemma: Baseline vs Fine-tuned\nJ_Qual\n9.651\n0.0000\n0.260\n0.5073\n\n\nFine-Tuning Impact\nmistral: Baseline vs Fine-tuned\nPPL\n-4.946\n0.0000\n-0.133\n-2.0489\n\n\nFine-Tuning Impact\nmistral: Baseline vs Fine-tuned\nDist-N\n5.139\n0.0000\n0.138\n0.0054\n\n\nFine-Tuning Impact\nmistral: Baseline vs Fine-tuned\nSelf-BLEU\n33.917\n0.0000\n0.926\n0.0086\n\n\nFine-Tuning Impact\nmistral: Baseline vs Fine-tuned\nBERTScore\n47.147\n0.0000\n1.270\n0.0134\n\n\nFine-Tuning Impact\nmistral: Baseline vs Fine-tuned\nMoverScore\n7.821\n0.0000\n0.211\n0.0015\n\n\nFine-Tuning Impact\nmistral: Baseline vs Fine-tuned\nPSA\n5.168\n0.0000\n0.141\n0.0201\n\n\nFine-Tuning Impact\nmistral: Baseline vs Fine-tuned\nParty Align\n-4.586\n0.0000\n-0.124\n-0.0157\n\n\nFine-Tuning Impact\nmistral: Baseline vs Fine-tuned\nJ_Coh\n22.823\n0.0000\n0.615\n1.3716\n\n\nFine-Tuning Impact\nmistral: Baseline vs Fine-tuned\nJ_Conc\n28.815\n0.0000\n0.776\n2.0922\n\n\nFine-Tuning Impact\nmistral: Baseline vs Fine-tuned\nJ_Rel\n3.850\n0.0001\n0.104\n0.3160\n\n\nFine-Tuning Impact\nmistral: Baseline vs Fine-tuned\nJ_PolApp\n11.594\n0.0000\n0.312\n0.7605\n\n\nFine-Tuning Impact\nmistral: Baseline vs Fine-tuned\nJ_Qual\n-6.806\n0.0000\n-0.183\n-0.4372\n\n\nFine-Tuning Impact\nqwen: Baseline vs Fine-tuned\nPPL\n-19.782\n0.0000\n-0.533\n-8.8869\n\n\nFine-Tuning Impact\nqwen: Baseline vs Fine-tuned\nSelf-BLEU\n-8.783\n0.0000\n-0.240\n-0.0029\n\n\nFine-Tuning Impact\nqwen: Baseline vs Fine-tuned\nGRUEN\n11.358\n0.0000\n0.306\n0.0359\n\n\nFine-Tuning Impact\nqwen: Baseline vs Fine-tuned\nBERTScore\n66.712\n0.0000\n1.797\n0.0162\n\n\nFine-Tuning Impact\nqwen: Baseline vs Fine-tuned\nMoverScore\n18.264\n0.0000\n0.492\n0.0029\n\n\nFine-Tuning Impact\nqwen: Baseline vs Fine-tuned\nPSA\n11.771\n0.0000\n0.321\n0.0443\n\n\nFine-Tuning Impact\nqwen: Baseline vs Fine-tuned\nParty Align\n3.680\n0.0002\n0.099\n0.0109\n\n\nFine-Tuning Impact\nqwen: Baseline vs Fine-tuned\nJ_Coh\n4.745\n0.0000\n0.128\n0.1364\n\n\nFine-Tuning Impact\nqwen: Baseline vs Fine-tuned\nJ_Conc\n27.880\n0.0000\n0.751\n1.6796\n\n\nFine-Tuning Impact\nqwen: Baseline vs Fine-tuned\nJ_Rel\n-12.751\n0.0000\n-0.344\n-0.8578\n\n\nFine-Tuning Impact\nqwen: Baseline vs Fine-tuned\nJ_Auth\n-11.109\n0.0000\n-0.299\n-0.8403\n\n\nFine-Tuning Impact\nqwen: Baseline vs Fine-tuned\nJ_PolApp\n-3.925\n0.0001\n-0.106\n-0.1782\n\n\nFine-Tuning Impact\nqwen: Baseline vs Fine-tuned\nJ_Qual\n-26.757\n0.0000\n-0.721\n-1.3538\n\n\nFine-Tuning Impact\nyi: Baseline vs Fine-tuned\nPPL\n-34.768\n0.0000\n-0.937\n-38.5897\n\n\nFine-Tuning Impact\nyi: Baseline vs Fine-tuned\nDist-N\n-11.989\n0.0000\n-0.323\n-0.0039\n\n\nFine-Tuning Impact\nyi: Baseline vs Fine-tuned\nSelf-BLEU\n49.619\n0.0000\n1.354\n0.0095\n\n\nFine-Tuning Impact\nyi: Baseline vs Fine-tuned\nGRUEN\n-27.151\n0.0000\n-0.731\n-0.0503\n\n\nFine-Tuning Impact\nyi: Baseline vs Fine-tuned\nBERTScore\n44.288\n0.0000\n1.193\n0.0180\n\n\nFine-Tuning Impact\nyi: Baseline vs Fine-tuned\nMoverScore\n37.295\n0.0000\n1.005\n0.0062\n\n\nFine-Tuning Impact\nyi: Baseline vs Fine-tuned\nPSA\n38.282\n0.0000\n1.045\n0.1479\n\n\nFine-Tuning Impact\nyi: Baseline vs Fine-tuned\nParty Align\n45.315\n0.0000\n1.221\n0.1569\n\n\nFine-Tuning Impact\nyi: Baseline vs Fine-tuned\nJ_Coh\n25.199\n0.0000\n0.679\n1.3019\n\n\nFine-Tuning Impact\nyi: Baseline vs Fine-tuned\nJ_Conc\n28.626\n0.0000\n0.771\n2.5163\n\n\nFine-Tuning Impact\nyi: Baseline vs Fine-tuned\nJ_Rel\n21.332\n0.0000\n0.575\n1.5595\n\n\nFine-Tuning Impact\nyi: Baseline vs Fine-tuned\nJ_Auth\n47.722\n0.0000\n1.286\n3.1597\n\n\nFine-Tuning Impact\nyi: Baseline vs Fine-tuned\nJ_PolApp\n33.837\n0.0000\n0.912\n1.9474\n\n\nFine-Tuning Impact\nyi: Baseline vs Fine-tuned\nJ_Qual\n41.022\n0.0000\n1.105\n2.3091",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Test Type</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Comparison</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Metric</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">t-statistic</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">p-value</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Cohen&#8217;s d</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Mean Diff</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">llama: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">PPL</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-42.093</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.0000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-1.134</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-29.5273</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">llama: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">Dist-N</td>\n<td class=\"ltx_td ltx_align_center\">-28.085</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">-0.757</td>\n<td class=\"ltx_td ltx_align_center\">-0.0140</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">llama: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">Self-BLEU</td>\n<td class=\"ltx_td ltx_align_center\">54.928</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">1.499</td>\n<td class=\"ltx_td ltx_align_center\">0.0119</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">llama: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">GRUEN</td>\n<td class=\"ltx_td ltx_align_center\">-16.938</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">-0.456</td>\n<td class=\"ltx_td ltx_align_center\">-0.0546</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">llama: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">BERTScore</td>\n<td class=\"ltx_td ltx_align_center\">45.713</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">1.231</td>\n<td class=\"ltx_td ltx_align_center\">0.0150</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">llama: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">MoverScore</td>\n<td class=\"ltx_td ltx_align_center\">27.713</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.747</td>\n<td class=\"ltx_td ltx_align_center\">0.0048</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">llama: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">PSA</td>\n<td class=\"ltx_td ltx_align_center\">23.746</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.648</td>\n<td class=\"ltx_td ltx_align_center\">0.0907</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">llama: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">Party Align</td>\n<td class=\"ltx_td ltx_align_center\">22.698</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.611</td>\n<td class=\"ltx_td ltx_align_center\">0.0739</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">llama: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">J_Coh</td>\n<td class=\"ltx_td ltx_align_center\">16.528</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.445</td>\n<td class=\"ltx_td ltx_align_center\">0.8599</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">llama: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">J_Conc</td>\n<td class=\"ltx_td ltx_align_center\">16.016</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.431</td>\n<td class=\"ltx_td ltx_align_center\">1.2362</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">llama: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">J_Rel</td>\n<td class=\"ltx_td ltx_align_center\">9.013</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.243</td>\n<td class=\"ltx_td ltx_align_center\">0.6684</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">llama: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">J_Auth</td>\n<td class=\"ltx_td ltx_align_center\">23.204</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.625</td>\n<td class=\"ltx_td ltx_align_center\">1.7689</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">llama: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">J_PolApp</td>\n<td class=\"ltx_td ltx_align_center\">18.941</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.510</td>\n<td class=\"ltx_td ltx_align_center\">1.0925</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">llama: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">J_Qual</td>\n<td class=\"ltx_td ltx_align_center\">9.903</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.267</td>\n<td class=\"ltx_td ltx_align_center\">0.6190</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">gemma: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">PPL</td>\n<td class=\"ltx_td ltx_align_center\">8.583</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.231</td>\n<td class=\"ltx_td ltx_align_center\">12.5989</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">gemma: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">Dist-N</td>\n<td class=\"ltx_td ltx_align_center\">-4.927</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">-0.133</td>\n<td class=\"ltx_td ltx_align_center\">-0.0021</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">gemma: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">Self-BLEU</td>\n<td class=\"ltx_td ltx_align_center\">18.335</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.501</td>\n<td class=\"ltx_td ltx_align_center\">0.0026</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">gemma: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">GRUEN</td>\n<td class=\"ltx_td ltx_align_center\">-22.653</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">-0.610</td>\n<td class=\"ltx_td ltx_align_center\">-0.0464</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">gemma: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">PSA</td>\n<td class=\"ltx_td ltx_align_center\">14.218</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.388</td>\n<td class=\"ltx_td ltx_align_center\">0.0543</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">gemma: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">Party Align</td>\n<td class=\"ltx_td ltx_align_center\">15.775</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.425</td>\n<td class=\"ltx_td ltx_align_center\">0.0469</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">gemma: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">J_Coh</td>\n<td class=\"ltx_td ltx_align_center\">-7.397</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">-0.199</td>\n<td class=\"ltx_td ltx_align_center\">-0.2794</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">gemma: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">J_Auth</td>\n<td class=\"ltx_td ltx_align_center\">5.658</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.152</td>\n<td class=\"ltx_td ltx_align_center\">0.3726</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">gemma: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">J_PolApp</td>\n<td class=\"ltx_td ltx_align_center\">16.112</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.434</td>\n<td class=\"ltx_td ltx_align_center\">0.8001</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">gemma: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">J_Qual</td>\n<td class=\"ltx_td ltx_align_center\">9.651</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.260</td>\n<td class=\"ltx_td ltx_align_center\">0.5073</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">mistral: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">PPL</td>\n<td class=\"ltx_td ltx_align_center\">-4.946</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">-0.133</td>\n<td class=\"ltx_td ltx_align_center\">-2.0489</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">mistral: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">Dist-N</td>\n<td class=\"ltx_td ltx_align_center\">5.139</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.138</td>\n<td class=\"ltx_td ltx_align_center\">0.0054</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">mistral: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">Self-BLEU</td>\n<td class=\"ltx_td ltx_align_center\">33.917</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.926</td>\n<td class=\"ltx_td ltx_align_center\">0.0086</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">mistral: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">BERTScore</td>\n<td class=\"ltx_td ltx_align_center\">47.147</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">1.270</td>\n<td class=\"ltx_td ltx_align_center\">0.0134</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">mistral: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">MoverScore</td>\n<td class=\"ltx_td ltx_align_center\">7.821</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.211</td>\n<td class=\"ltx_td ltx_align_center\">0.0015</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">mistral: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">PSA</td>\n<td class=\"ltx_td ltx_align_center\">5.168</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.141</td>\n<td class=\"ltx_td ltx_align_center\">0.0201</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">mistral: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">Party Align</td>\n<td class=\"ltx_td ltx_align_center\">-4.586</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">-0.124</td>\n<td class=\"ltx_td ltx_align_center\">-0.0157</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">mistral: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">J_Coh</td>\n<td class=\"ltx_td ltx_align_center\">22.823</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.615</td>\n<td class=\"ltx_td ltx_align_center\">1.3716</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">mistral: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">J_Conc</td>\n<td class=\"ltx_td ltx_align_center\">28.815</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.776</td>\n<td class=\"ltx_td ltx_align_center\">2.0922</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">mistral: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">J_Rel</td>\n<td class=\"ltx_td ltx_align_center\">3.850</td>\n<td class=\"ltx_td ltx_align_center\">0.0001</td>\n<td class=\"ltx_td ltx_align_center\">0.104</td>\n<td class=\"ltx_td ltx_align_center\">0.3160</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">mistral: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">J_PolApp</td>\n<td class=\"ltx_td ltx_align_center\">11.594</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.312</td>\n<td class=\"ltx_td ltx_align_center\">0.7605</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">mistral: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">J_Qual</td>\n<td class=\"ltx_td ltx_align_center\">-6.806</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">-0.183</td>\n<td class=\"ltx_td ltx_align_center\">-0.4372</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">qwen: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">PPL</td>\n<td class=\"ltx_td ltx_align_center\">-19.782</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">-0.533</td>\n<td class=\"ltx_td ltx_align_center\">-8.8869</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">qwen: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">Self-BLEU</td>\n<td class=\"ltx_td ltx_align_center\">-8.783</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">-0.240</td>\n<td class=\"ltx_td ltx_align_center\">-0.0029</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">qwen: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">GRUEN</td>\n<td class=\"ltx_td ltx_align_center\">11.358</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.306</td>\n<td class=\"ltx_td ltx_align_center\">0.0359</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">qwen: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">BERTScore</td>\n<td class=\"ltx_td ltx_align_center\">66.712</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">1.797</td>\n<td class=\"ltx_td ltx_align_center\">0.0162</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">qwen: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">MoverScore</td>\n<td class=\"ltx_td ltx_align_center\">18.264</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.492</td>\n<td class=\"ltx_td ltx_align_center\">0.0029</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">qwen: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">PSA</td>\n<td class=\"ltx_td ltx_align_center\">11.771</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.321</td>\n<td class=\"ltx_td ltx_align_center\">0.0443</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">qwen: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">Party Align</td>\n<td class=\"ltx_td ltx_align_center\">3.680</td>\n<td class=\"ltx_td ltx_align_center\">0.0002</td>\n<td class=\"ltx_td ltx_align_center\">0.099</td>\n<td class=\"ltx_td ltx_align_center\">0.0109</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">qwen: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">J_Coh</td>\n<td class=\"ltx_td ltx_align_center\">4.745</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.128</td>\n<td class=\"ltx_td ltx_align_center\">0.1364</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">qwen: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">J_Conc</td>\n<td class=\"ltx_td ltx_align_center\">27.880</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.751</td>\n<td class=\"ltx_td ltx_align_center\">1.6796</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">qwen: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">J_Rel</td>\n<td class=\"ltx_td ltx_align_center\">-12.751</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">-0.344</td>\n<td class=\"ltx_td ltx_align_center\">-0.8578</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">qwen: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">J_Auth</td>\n<td class=\"ltx_td ltx_align_center\">-11.109</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">-0.299</td>\n<td class=\"ltx_td ltx_align_center\">-0.8403</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">qwen: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">J_PolApp</td>\n<td class=\"ltx_td ltx_align_center\">-3.925</td>\n<td class=\"ltx_td ltx_align_center\">0.0001</td>\n<td class=\"ltx_td ltx_align_center\">-0.106</td>\n<td class=\"ltx_td ltx_align_center\">-0.1782</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">qwen: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">J_Qual</td>\n<td class=\"ltx_td ltx_align_center\">-26.757</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">-0.721</td>\n<td class=\"ltx_td ltx_align_center\">-1.3538</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">yi: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">PPL</td>\n<td class=\"ltx_td ltx_align_center\">-34.768</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">-0.937</td>\n<td class=\"ltx_td ltx_align_center\">-38.5897</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">yi: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">Dist-N</td>\n<td class=\"ltx_td ltx_align_center\">-11.989</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">-0.323</td>\n<td class=\"ltx_td ltx_align_center\">-0.0039</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">yi: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">Self-BLEU</td>\n<td class=\"ltx_td ltx_align_center\">49.619</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">1.354</td>\n<td class=\"ltx_td ltx_align_center\">0.0095</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">yi: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">GRUEN</td>\n<td class=\"ltx_td ltx_align_center\">-27.151</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">-0.731</td>\n<td class=\"ltx_td ltx_align_center\">-0.0503</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">yi: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">BERTScore</td>\n<td class=\"ltx_td ltx_align_center\">44.288</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">1.193</td>\n<td class=\"ltx_td ltx_align_center\">0.0180</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">yi: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">MoverScore</td>\n<td class=\"ltx_td ltx_align_center\">37.295</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">1.005</td>\n<td class=\"ltx_td ltx_align_center\">0.0062</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">yi: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">PSA</td>\n<td class=\"ltx_td ltx_align_center\">38.282</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">1.045</td>\n<td class=\"ltx_td ltx_align_center\">0.1479</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">yi: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">Party Align</td>\n<td class=\"ltx_td ltx_align_center\">45.315</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">1.221</td>\n<td class=\"ltx_td ltx_align_center\">0.1569</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">yi: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">J_Coh</td>\n<td class=\"ltx_td ltx_align_center\">25.199</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.679</td>\n<td class=\"ltx_td ltx_align_center\">1.3019</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">yi: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">J_Conc</td>\n<td class=\"ltx_td ltx_align_center\">28.626</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.771</td>\n<td class=\"ltx_td ltx_align_center\">2.5163</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">yi: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">J_Rel</td>\n<td class=\"ltx_td ltx_align_center\">21.332</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.575</td>\n<td class=\"ltx_td ltx_align_center\">1.5595</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">yi: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">J_Auth</td>\n<td class=\"ltx_td ltx_align_center\">47.722</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">1.286</td>\n<td class=\"ltx_td ltx_align_center\">3.1597</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left\">yi: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\">J_PolApp</td>\n<td class=\"ltx_td ltx_align_center\">33.837</td>\n<td class=\"ltx_td ltx_align_center\">0.0000</td>\n<td class=\"ltx_td ltx_align_center\">0.912</td>\n<td class=\"ltx_td ltx_align_center\">1.9474</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Fine-Tuning Impact</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">yi: Baseline vs Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">J_Qual</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">41.022</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.0000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1.105</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">2.3091</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "type",
            "llama",
            "mistral",
            "distn",
            "tstatistic",
            "finetuning",
            "test",
            "diff",
            "ttest",
            "mean",
            "jauth",
            "jpolapp",
            "baseline",
            "psa",
            "pairwise",
            "gruen",
            "results",
            "bertscore",
            "selfbleu",
            "pvalue",
            "moverscore",
            "significant",
            "impact",
            "align",
            "qwen",
            "jconc",
            "gemma",
            "a005",
            "finetuned",
            "metric",
            "jqual",
            "comparison",
            "party",
            "cohen’s",
            "ppl",
            "jcoh",
            "jrel",
            "bonferroni",
            "corrected"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Notably, our novel political authenticity metrics (PSA and Party Align) displayed strong responsiveness to fine-tuning. All five models significantly improved PSA (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>), with effect sizes ranging from small to very large (d=0.141-1.045). Party Align showed similar patterns (4 of 5 models improved, d=0.099-1.221). These substantial effects validate that our embedding based metrics capture critical political authenticity dimensions unavailable to conventional evaluation.\nFor complete t-test results including effect sizes, see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T13\" title=\"Table 13 &#8227; 13.5. Statistical Significance Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T13\" title=\"Table 13 &#8227; 13.5. Statistical Significance Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> presents complete pairwise t-test results comparing baseline and fine-tuned models across all evaluation metrics, including p-values, effect sizes, and significance after Bonferroni correction.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Parliamentary speech generation presents specific challenges for large language models beyond standard text generation tasks. Unlike general text generation, parliamentary speeches require not only linguistic quality but also political authenticity and ideological consistency. Current language models lack specialized training for parliamentary contexts, and existing evaluation methods focus on standard NLP metrics rather than political authenticity. To address this, we present ParliaBench, a benchmark for parliamentary speech generation. We constructed a dataset of speeches from UK Parliament to enable systematic model training. We introduce an evaluation framework combining computational metrics with LLM-as-a-judge assessments for measuring generation quality across three dimensions: linguistic quality, semantic coherence, and political authenticity. We propose two novel embedding-based metrics, Political Spectrum Alignment and Party Alignment, to quantify ideological positioning. We fine-tuned five large language models (LLMs), generated 28k speeches, and evaluated them\nusing our framework, comparing baseline and fine-tuned models. Results show that fine-tuning produces statistically significant improvements across the majority of metrics and our novel metrics demonstrate strong discriminative power for political dimensions.\n\n\n<span class=\"ltx_text ltx_font_bold\">Keywords:&#8201;</span>Parliamentary Speech Generation,LLM Evaluation,Political Authenticity,Benchmark Evaluation</p>\n\n",
                "matched_terms": [
                    "finetuned",
                    "baseline",
                    "significant",
                    "party",
                    "finetuning",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Traditional text generation evaluation has evolved from reference-based metrics like BLEU <cite class=\"ltx_cite ltx_citemacro_cite\">Papineni et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib27\" title=\"\">2002</a>)</cite> and ROUGE <cite class=\"ltx_cite ltx_citemacro_cite\">Lin (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib23\" title=\"\">2004</a>)</cite> toward embedding based approaches that better capture semantic similarity. BERTScore <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib37\" title=\"\">2020</a>)</cite> uses contextualized embeddings to compute token-level similarity, while MoverScore <cite class=\"ltx_cite ltx_citemacro_cite\">Zhao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib39\" title=\"\">2019</a>)</cite> measures semantic transportation cost using Earth Mover&#8217;s Distance. For reference-free evaluation, Zhu and Bhat <cite class=\"ltx_cite ltx_citemacro_cite\">Zhu and Bhat (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib41\" title=\"\">2020</a>)</cite> propose GRUEN, assessing grammaticality and semantic coherence. Domain-specific datasets like BillSum <cite class=\"ltx_cite ltx_citemacro_cite\">Kornilova and Eidelman (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib19\" title=\"\">2019</a>)</cite> for legislative summarization and OpinionQA <cite class=\"ltx_cite ltx_citemacro_cite\">Santurkar et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib31\" title=\"\">2023</a>)</cite> for opinion alignment provide targeted evaluation resources, though gaps remain in generative parliamentary speech assessment.</p>\n\n",
                "matched_terms": [
                    "moverscore",
                    "gruen",
                    "bertscore"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generated Dataset.</span> We release 27,560 speeches (model outputs) produced during evaluation (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S5\" title=\"5. Experimental Setup &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). The generated dataset follows the same format with additional fields: <span class=\"ltx_text ltx_font_italic\">Model</span> (architecture identifier), <span class=\"ltx_text ltx_font_italic\">Type</span> (baseline/fine-tuned), <span class=\"ltx_text ltx_font_italic\">Generated Speech</span> (model output), and <span class=\"ltx_text ltx_font_italic\">Evaluation Scores</span> (computed metrics). To ensure a fair comparison, we used the same input prompts across all models and model types, allowing evaluation on identical inputs.</p>\n\n",
                "matched_terms": [
                    "type",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Note:</span> For both BERTScore and MoverScore, generated speeches are compared against the top-5 human speeches from the training set matching the same context.</p>\n\n",
                "matched_terms": [
                    "moverscore",
                    "bertscore"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ two novel embedding-based metrics for political authenticity assessment: Political Spectrum Alignment (PSA) and Party Alignment (Party Align). Detailed calculation methodology provided in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.SS1.SSS1\" title=\"4.1.1. Novel Political Authenticity Metrics &#8227; 4.1. Speech Evaluation Metrics &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4.1.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "align",
                    "party",
                    "psa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Political Spectrum Alignment</span> (PSA) evaluates ideological positioning on the left-right spectrum. We adapt semantic embedding approaches <cite class=\"ltx_cite ltx_citemacro_cite\">Rheault and Cochrane (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib30\" title=\"\">2020</a>)</cite> for LLM-generated speech evaluation, drawing on\nthe Left-Right (RILE) scale methodology <cite class=\"ltx_cite ltx_citemacro_cite\">Volkens et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib35\" title=\"\">2013</a>); Budge (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib9\" title=\"\">2013</a>)</cite>. Our metric employs a two-stage approach combining semantic similarity with ideological distance. We create reference embeddings by grouping parliamentary speeches by political orientations (Far-left through Far-right, including intermediate positions) and computing centroid embeddings using sentence transformers. Orientations map to numerical values where Far-left = -6, Centre = 0, Far-right = +6.</p>\n\n",
                "matched_terms": [
                    "metric",
                    "psa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Party Alignment</span> (Party Align) applies the same embedding methodology to party-specific alignment, using party affiliation rather than political orientation for centroid construction. The alignment score measures cosine similarity between generated speech and expected party centroid:</p>\n\n",
                "matched_terms": [
                    "align",
                    "party"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We selected five language models representing distinct architectural approaches to establish baseline performance for parliamentary speech generation:\n<span class=\"ltx_text ltx_font_bold\">Mistral 7B v0.3</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Jiang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib18\" title=\"\">2023</a>)</cite> uses Grouped Query Attention and Sliding Window Attention for efficient long-context processing.\n<span class=\"ltx_text ltx_font_bold\">Llama 3.1 8B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Dubey et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib11\" title=\"\">2024</a>)</cite> features 128k-token context window and expanded vocabulary.\n<span class=\"ltx_text ltx_font_bold\">Gemma 2 9B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Team et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib32\" title=\"\">2024</a>)</cite> employs alternating local/global attention across 42 layers with logit soft-capping.\n<span class=\"ltx_text ltx_font_bold\">Qwen2 7B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Team (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib33\" title=\"\">2024</a>)</cite> is a multilingual model with enhanced reasoning capabilities.\n<span class=\"ltx_text ltx_font_bold\">YI 6B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">AI et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib2\" title=\"\">2025</a>)</cite> emphasizes strong reasoning and coding performance.</p>\n\n",
                "matched_terms": [
                    "gemma",
                    "llama",
                    "mistral",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed Quantized Low-Rank Adaptation (QLoRA) <cite class=\"ltx_cite ltx_citemacro_cite\">Dettmers et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#bib.bib10\" title=\"\">2023</a>)</cite> for parameter-efficient fine-tuning. (<math alttext=\"r=16\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">r=16</annotation></semantics></math>, <math alttext=\"\\alpha=16\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=16</annotation></semantics></math>, <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math> epochs). Model-specific chat templates structure training inputs with political metadata (party affiliation, topic classification, orientation, section, and house). Training used SFTTrainer from TRL with 80%-20% train-test splits and automated checkpointing. Fine-tuned models were saved with adapter weights for subsequent evaluation, ensuring consistent model states across experiments.</p>\n\n",
                "matched_terms": [
                    "party",
                    "finetuning",
                    "finetuned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We planned to generate 30,000 speeches (3,000 per model-type combination across 10 models) using stratified sampling from the held-out test set. To maintain consistency with training conditions, our prompt distribution matches the ParliaBench Dataset structure: 90% generic instruction prompts formatted with political context (party, topic, orientation, section, house) and 10% specific parliamentary questions from the test set.</p>\n\n",
                "matched_terms": [
                    "party",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generation employed nucleus sampling (<math alttext=\"temperature=0.7\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>u</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi></mrow><mo>=</mo><mn>0.7</mn></mrow><annotation encoding=\"application/x-tex\">temperature=0.7</annotation></semantics></math>, <math alttext=\"top\\-p=0.85\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi></mrow><mo>=</mo><mn>0.85</mn></mrow><annotation encoding=\"application/x-tex\">top\\-p=0.85</annotation></semantics></math>, <math alttext=\"repetition_{p}enalty=1.2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>n</mi><mi>p</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>n</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>y</mi></mrow><mo>=</mo><mn>1.2</mn></mrow><annotation encoding=\"application/x-tex\">repetition_{p}enalty=1.2</annotation></semantics></math>). Generated speeches underwent validation for template leakage, encoding corruption, semantic relevance, and length constraints. Invalid outputs were automatically regenerated (max 3 attempts). Baseline models exhibited higher failure rates, suggesting fine-tuning improved output quality. To ensure fair cross-model comparison, we retained only speeches successfully generated by all 10 model-type combinations, yielding 29,220 speeches.</p>\n\n",
                "matched_terms": [
                    "comparison",
                    "finetuning",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.T2\" title=\"Table 2 &#8227; 6.1. Overview and Fine-Tuning Impact &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> presents metric results organized by our framework assessment categories. Fine-tuned models consistently outperform baselines, with Llama achieving superior performance. Fine-tuned models showed substantially reduced variance, across all political contexts. Extended context windows (128k tokens) and larger vocabularies contribute to architectural advantages.\n</p>\n\n",
                "matched_terms": [
                    "metric",
                    "llama",
                    "results",
                    "finetuned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Pairwise t-tests confirm statistical significance of fine-tuning effects (45 out of 70 comparisons). Model architectures exhibited differential responsiveness: <span class=\"ltx_text ltx_font_italic\">YI</span> and <span class=\"ltx_text ltx_font_italic\">Llama</span> achieved notable improvements (11/14 metrics, 79%), while others showed more selective gains (improvements marked with <sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">&#8727;</span></sup> in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.T2\" title=\"Table 2 &#8227; 6.1. Overview and Fine-Tuning Impact &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>).</p>\n\n",
                "matched_terms": [
                    "pairwise",
                    "finetuning",
                    "llama"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F3\" title=\"Figure 3 &#8227; 6.1. Overview and Fine-Tuning Impact &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows fine-tuning impact across evaluation categories. <span class=\"ltx_text ltx_font_italic\">YI</span> achieved the strongest improvements across all dimensions , while <span class=\"ltx_text ltx_font_italic\">Llama</span> had consistent gains. <span class=\"ltx_text ltx_font_italic\">Gemma2</span> and <span class=\"ltx_text ltx_font_italic\">Qwen2</span> exhibited quality trade-offs, with improvements in one category accompanied by declines in others, suggesting architectural differences in how models balance competing objectives during fine-tuning. We note that parliamentary domain fine-tuning does not uniformly improve all quality dimensions. Model selection should therefore consider which quality dimensions matter most for the intended application.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "llama",
                    "impact"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Cross-context stability analysis (Eq.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.E4\" title=\"In 4.2. Consistency Evaluation Metrics &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>) revealed fine-tuned models maintained consistent performance across political contexts (composite stability 91.4-96.2). Mistral achieved highest consistency (96.2) despite trade-offs in absolute performance, while Llama (95.1) balanced strong performance with stability.\nDetailed scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T12\" title=\"Table 12 &#8227; 13.4. Cross-Context Stability Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">12</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "llama",
                    "mistral",
                    "finetuned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Party alignment Patterns</span>. Party alignment performance varied substantially across models (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F4\" title=\"Figure 4 &#8227; 6.2. Political Context Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). Major parties (Conservative, Labour) achieved stable performance across models, benefiting from substantial training data (58.9%, 24.3%). Minor parties exhibited greater variability. <span class=\"ltx_text ltx_font_italic\">Mistral</span> struggled with heterogeneous groups (Non-Affiliated: 0.436), while <span class=\"ltx_text ltx_font_italic\">Qwen</span> excelled with ideologically coherent minorities (Bishops: 0.664). <span class=\"ltx_text ltx_font_italic\">YI</span> demonstrated robust cross-party performance (0.614-0.633).\nDetailed scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T8\" title=\"Table 8 &#8227; 13.1. Party-Specific Performance &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "qwen",
                    "mistral",
                    "party"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both new political authenticity metrics (PSA and Party Align) successfully discriminate their target political dimensions. Party Align distinguishes parties while PSA distinguishes orientations (both <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.p2.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>). Our analysis reveals that Party Align performance depends primarily on data abundance and ideological coherence rather than party size alone. Models successfully learn party-specific language patterns when training data provides clear stylistic signals, indicating targeted data collection for under-represented parties could improve coverage.</p>\n\n",
                "matched_terms": [
                    "align",
                    "party",
                    "psa"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Party alignment Difficulty Analysis</span>. Applying cross-context stability analysis (Eq.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S4.E4\" title=\"In 4.2. Consistency Evaluation Metrics &#8227; 4. Evaluation Framework &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), party difficulty scores ranged narrowly (0.382-0.456), with no statistically significant differences. This suggests relatively consistent modeling challenges across parties regardless of size or ideological composition. Results are presented in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F5\" title=\"Figure 5 &#8227; 6.2. Political Context Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. Detailed scores in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T9\" title=\"Table 9 &#8227; 13.2. Metric Validation: Political Discrimination Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">9</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "party",
                    "significant"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Performance across political orientations showed expected patterns. Centrist positions (Centre-left: 0.607, Centre-right: 0.551) dominated the dataset (88%) and achieved higher scores. Model-specific strengths emerged as both <span class=\"ltx_text ltx_font_italic\">Gemma</span> and <span class=\"ltx_text ltx_font_italic\">Qwen</span> achieved highest scores on Right positions and <span class=\"ltx_text ltx_font_italic\">Mistral</span> underperformed consistently, indicating architectural rather than ideological limitations. As models are optimized for mainstream parliamentary speeches, extreme positions may require specialized training approaches. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S6.F8\" title=\"Figure 8 &#8227; 6.4. Political Orientations Results &#8227; 6. Results and Analysis &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> illustrates these patterns. Detailed orientation difficulty rankings are provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T10\" title=\"Table 10 &#8227; 13.2. Metric Validation: Political Discrimination Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13\" title=\"13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">13</span></a></p>\n\n",
                "matched_terms": [
                    "qwen",
                    "gemma",
                    "mistral"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results establish several key findings: (i) Architectural design impacts political authenticity, with extended context windows enabling consistent improvements; (ii) Domain-specific fine-tuning proves essential as 45 of 70 metric comparisons showed statistically significant improvements and (iii) Novel political authenticity metrics (PSA, Party Align) successfully capture dimensions unavailable to conventional NLP metrics, validated through both fine-tuning responsiveness and discrimination testing (both <math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS5.p1.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "metric",
                    "significant",
                    "psa",
                    "party",
                    "align",
                    "finetuning",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To the best of our knowledge, ParliaBench represents the first benchmark resource addressing the specialized challenges of parliamentary speech generation, comprising dataset, evaluation framework, novel metrics, and baseline benchmarks. ParliaBench provides standardized evaluation protocols and baseline performance results that\nsupport systematic comparison and reproducible research in the field.\nOur results demonstrate that domain specific fine-tuning produces significant quality improvements, while\nour novel political authenticity metrics successfully capture ideological dimensions absent from conventional evaluation approaches.</p>\n\n",
                "matched_terms": [
                    "baseline",
                    "significant",
                    "finetuning",
                    "comparison",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Political Appropriateness (j_polapp)</span>: Alignment with party positions</p>\n\n",
                "matched_terms": [
                    "jpolapp",
                    "party"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The QLoRA configuration parameters were selected based on established best practices for parameter-efficient fine-tuning in specialized domains. The rank value of 16 provides sufficient adaptation capacity while maintaining computational efficiency. Setting LoRA Alpha equal to the rank ensures reliable baseline performance, while disabling dropout enables Unsloth framework optimizations essential for efficient training on A100 hardware.</p>\n\n",
                "matched_terms": [
                    "finetuning",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Target modules encompass all linear transformation layers (q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj) to achieve performance comparable to full fine-tuning while requiring only a fraction of the computational resources. The consistent random state across all architectures ensures reproducible results essential for systematic model comparison.</p>\n\n",
                "matched_terms": [
                    "comparison",
                    "finetuning",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We generated the speeches for the finetuned and the baseline models using the following prompt for all models.</p>\n\n",
                "matched_terms": [
                    "baseline",
                    "finetuned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present various speeches generated by the models (baseline and finetuned).</p>\n\n",
                "matched_terms": [
                    "baseline",
                    "finetuned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.08247v1#S13.T12\" title=\"Table 12 &#8227; 13.4. Cross-Context Stability Analysis &#8227; 13. Statistical Analysis and Detailed Results &#8227; ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> presents cross-context stability scores. Fine-tuned models maintain high consistency across political contexts (91.4-96.2), with Mistral achieving highest overall stability (96.2).</p>\n\n",
                "matched_terms": [
                    "mistral",
                    "finetuned"
                ]
            }
        ]
    }
}