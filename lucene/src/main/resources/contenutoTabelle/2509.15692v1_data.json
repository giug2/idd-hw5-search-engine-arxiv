{
    "S3.T1": {
        "caption": "Table 1: BLEU and xCOMET results for different models, various chunk sizes kk, and rollback sizes bb. SFT size refers to the amount of training data used during the SFT phase, and k=∞k=\\infty refers to the offline translation.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"16\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Rollback <math alttext=\"b=0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m7\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">b=0</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SFT Size</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"7\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Chunk Size <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m8\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> (BLEU)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"7\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Chunk Size <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m9\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> (xCOMET)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"\\infty\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m10\" intent=\":literal\"><semantics><mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8734;</mi><annotation encoding=\"application/x-tex\">\\infty</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"500\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m11\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">500</mn><annotation encoding=\"application/x-tex\">500</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"1000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m12\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">1000</mn><annotation encoding=\"application/x-tex\">1000</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"1500\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m13\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">1500</mn><annotation encoding=\"application/x-tex\">1500</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"2000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m14\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">2000</mn><annotation encoding=\"application/x-tex\">2000</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"3000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m15\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">3000</mn><annotation encoding=\"application/x-tex\">3000</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"4000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m16\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">4000</mn><annotation encoding=\"application/x-tex\">4000</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"\\infty\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m17\" intent=\":literal\"><semantics><mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8734;</mi><annotation encoding=\"application/x-tex\">\\infty</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"500\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m18\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">500</mn><annotation encoding=\"application/x-tex\">500</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"1000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m19\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">1000</mn><annotation encoding=\"application/x-tex\">1000</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"1500\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m20\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">1500</mn><annotation encoding=\"application/x-tex\">1500</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"2000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m21\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">2000</mn><annotation encoding=\"application/x-tex\">2000</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"3000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m22\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">3000</mn><annotation encoding=\"application/x-tex\">3000</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"4000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m23\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">4000</mn><annotation encoding=\"application/x-tex\">4000</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Qwen2-Audio-Base</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">44.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">14.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">24.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">30.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">85.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">28.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">38.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">46.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">53.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">63.7</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">+ SFT</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">232k</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">46.1</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.7</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.6</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">18.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">24.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">87.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">30.1</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">33.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">42.7</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">51.1</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">61.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">69.4</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F0F0F0;\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">+ <span class=\"ltx_text ltx_font_bold\">SFT &amp; SimulSA (Ours)</span></span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">235k</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">46.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">7.9</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">13.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">20.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">24.3</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">29.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">33.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">88.1</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">35.7</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">42.9</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">50.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">57.1</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">63.7</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">68.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"16\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Rollback <math alttext=\"b=3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m24\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">b=3</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SFT Size</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"7\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Chunk Size <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m25\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> (BLEU)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"7\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Chunk Size <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m26\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> (xCOMET)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"\\infty\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m27\" intent=\":literal\"><semantics><mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8734;</mi><annotation encoding=\"application/x-tex\">\\infty</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"500\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m28\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">500</mn><annotation encoding=\"application/x-tex\">500</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"1000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m29\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">1000</mn><annotation encoding=\"application/x-tex\">1000</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"1500\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m30\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">1500</mn><annotation encoding=\"application/x-tex\">1500</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"2000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m31\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">2000</mn><annotation encoding=\"application/x-tex\">2000</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"3000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m32\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">3000</mn><annotation encoding=\"application/x-tex\">3000</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"4000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m33\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">4000</mn><annotation encoding=\"application/x-tex\">4000</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"\\infty\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m34\" intent=\":literal\"><semantics><mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8734;</mi><annotation encoding=\"application/x-tex\">\\infty</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"500\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m35\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">500</mn><annotation encoding=\"application/x-tex\">500</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"1000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m36\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">1000</mn><annotation encoding=\"application/x-tex\">1000</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"1500\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m37\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">1500</mn><annotation encoding=\"application/x-tex\">1500</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"2000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m38\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">2000</mn><annotation encoding=\"application/x-tex\">2000</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"3000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m39\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">3000</mn><annotation encoding=\"application/x-tex\">3000</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"4000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m40\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">4000</mn><annotation encoding=\"application/x-tex\">4000</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Qwen2-Audio-Base</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">44.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">22.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">29.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">34.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">36.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">85.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">52.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">57.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">60.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">64.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">71.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">72.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">+ SFT</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">232k</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">46.1</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">29.1</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">33.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">35.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">37.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">38.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">39.3</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">87.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">58.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">66.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">68.9</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">70.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">72.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">74.0</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F0F0F0;\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">+ <span class=\"ltx_text ltx_font_bold\">SFT &amp; SimulSA (Ours)</span></span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">235k</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">46.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">34.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">36.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">37.1</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">38.4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">39.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">40.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">88.1</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">67.3</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">69.9</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">72.9</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">73.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">74.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">75.2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"16\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Rollback <math alttext=\"b=5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m41\" intent=\":literal\"><semantics><mrow><mi>b</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">b=5</annotation></semantics></math></span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SFT Size</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"7\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Chunk Size <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m42\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> (BLEU)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"7\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Chunk Size <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m43\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> (xCOMET)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"\\infty\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m44\" intent=\":literal\"><semantics><mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8734;</mi><annotation encoding=\"application/x-tex\">\\infty</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"500\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m45\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">500</mn><annotation encoding=\"application/x-tex\">500</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"1000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m46\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">1000</mn><annotation encoding=\"application/x-tex\">1000</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"1500\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m47\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">1500</mn><annotation encoding=\"application/x-tex\">1500</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"2000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m48\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">2000</mn><annotation encoding=\"application/x-tex\">2000</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"3000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m49\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">3000</mn><annotation encoding=\"application/x-tex\">3000</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"4000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m50\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">4000</mn><annotation encoding=\"application/x-tex\">4000</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"\\infty\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m51\" intent=\":literal\"><semantics><mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8734;</mi><annotation encoding=\"application/x-tex\">\\infty</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"500\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m52\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">500</mn><annotation encoding=\"application/x-tex\">500</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"1000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m53\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">1000</mn><annotation encoding=\"application/x-tex\">1000</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"1500\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m54\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">1500</mn><annotation encoding=\"application/x-tex\">1500</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"2000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m55\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">2000</mn><annotation encoding=\"application/x-tex\">2000</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"3000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m56\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">3000</mn><annotation encoding=\"application/x-tex\">3000</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><math alttext=\"4000\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m57\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">4000</mn><annotation encoding=\"application/x-tex\">4000</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Qwen2-Audio-Base</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">44.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">29.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">32.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">34.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">36.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">38.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">85.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">56.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">62.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">66.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">68.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">71.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">73.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">+ SFT</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">232k</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">46.1</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">37.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">38.6</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">39.6</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">40.3</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">41.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">41.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">87.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">64.9</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">68.7</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">69.6</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">71.2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">74.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">74.7</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F0F0F0;\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">+ <span class=\"ltx_text ltx_font_bold\">SFT &amp; SimulSA (Ours)</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">235k</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">46.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">38.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">39.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">40.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">40.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">41.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">41.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">88.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">69.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">71.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">73.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">74.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">76.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:-0.25pt 8.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#F0F0F0;\">77.8</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "size",
            "amount",
            "various",
            "b3b3",
            "simulsa",
            "translation",
            "b0b0",
            "sizes",
            "training",
            "used",
            "b5b5",
            "chunk",
            "phase",
            "232k",
            "xcomet",
            "sft",
            "235k",
            "results",
            "offline",
            "rollback",
            "ours",
            "k∞kinfty",
            "during",
            "model",
            "data",
            "qwen2audiobase",
            "refers",
            "different",
            "∞infty",
            "bleu"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3.1 Speech Truncation &#8227; 3 Method &#8227; Direct Simultaneous Translation Activation for Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> presents the main results of each model under varying inference settings. Our analysis yields the following findings:</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Simultaneous speech-to-text translation (Simul-S2TT) aims to translate speech into target text in real time, outputting translations while receiving source speech input, rather than waiting for the entire utterance to be spoken. Simul-S2TT research often modifies model architectures to implement read-write strategies. However, with the rise of large audio-language models (LALMs), a key challenge is how to directly activate Simul-S2TT capabilities in base models without additional architectural changes.\nIn this paper, we introduce <span class=\"ltx_text ltx_font_bold\">Simul</span>taneous <span class=\"ltx_text ltx_font_bold\">S</span>elf-<span class=\"ltx_text ltx_font_bold\">A</span>ugmentation (<span class=\"ltx_text ltx_font_bold\">SimulSA</span>), a strategy that utilizes LALMs&#8217; inherent capabilities to obtain simultaneous data by randomly truncating speech and constructing partially aligned translation. By incorporating them into offline SFT data, SimulSA effectively bridges the distribution gap between offline translation during pretraining and simultaneous translation during inference.\nExperimental results demonstrate that augmenting only about <span class=\"ltx_text ltx_font_bold\">1%</span> of the simultaneous data, compared to the full offline SFT data, can significantly activate LALMs&#8217; Simul-S2TT capabilities without modifications to model architecture or decoding strategy.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "simulsa",
                    "translation",
                    "during",
                    "model",
                    "sft",
                    "data",
                    "results",
                    "offline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;<span class=\"ltx_text ltx_font_medium\">\nLarge Audio-Language Models, Simultaneous Speech-to-Text Translation, Data Augmentation</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "data",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">However, Simul-S2TT faces challenges distinct from offline S2TT. Its main difficulty lies in handling streaming input: the system receives only partial audio segments rather than the full utterance. It must therefore generate translations from incomplete fragments, which may not align with full semantic units or clear linguistic boundaries. This raises the risk of early mistranslations that can propagate and compound, ultimately degrading translation quality.\nExisting research primarily tackles these issues through speech segmentation and &#8220;Read-Write (RW)&#8221; strategies </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#bib.bib7\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">7</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Some methods segment speech into complete semantic units for improved recognition </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#bib.bib8\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">8</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#bib.bib9\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">9</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, while others introduce new decoder-encoder architectures </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#bib.bib10\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">10</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#bib.bib11\" title=\"\"><span class=\"ltx_text\" style=\"font-size:90%;\">11</span></a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, incremental decoding </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#bib.bib12\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">12</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, or attention constraints </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#bib.bib13\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">13</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to adapt offline models to precise RW timing.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "translation",
                    "offline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We argue that the fundamental difficulty lies in constructing effective training data for truncated speech input paired with its corresponding ground-truth translation output.\nTo elaborate, the main gap between Simul-MT and Offline-MT (</span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">i.e.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, complete end-to-end MT) is </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">the completeness of information alignment</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, as shown in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Direct Simultaneous Translation Activation for Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nIn Offline-MT, the model fully aligns complete source-side information with the target, ensuring no omissions.\nIn contrast, when processing truncated speech, Simul-MT often needs to &#8220;wait&#8221; for some speech information to be translated later for handling &#8220;word reordering&#8221; across languages, resulting in incomplete source-to-target alignment.\nThis alignment difference causes distribution gaps between training and inference. Moreover, truncated speech may contain incomplete semantic units that are also absent during training, further complicating inference for Simul-S2TT tasks.</span>\n</p>\n\n",
                "matched_terms": [
                    "during",
                    "translation",
                    "model",
                    "data",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Given these challenges, we aim to augment simultaneous data, enabling LALMs to directly learn features relevant to simultaneous translation during SFT and thereby bridge the gap between Simul-MT and Offline-MT.\nTo address translation errors stemming from incomplete early speech segments, we propose the Beta Decay distribution for audio truncation when constructing streaming speech data, ensuring more realistic simulation of streaming segment boundaries. We further utilize the ground-truth translation and the probability distribution from the LALM output to generate high-quality truncated speech-translation pairs.\nWe term this method </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Simul</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">taneous </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">S</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">elf-</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">A</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">ugmentation (</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SimulSA</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) strategy, with the overall pipeline shown in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; Direct Simultaneous Translation Activation for Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nSpecifically, we first select a subset from the offline speech-text SFT pairs and segment the audio using the Beta Decay distribution. For each truncated speech segment, we use the probability distribution of the full ground-truth translation generated from a pre-trained LALM to speculate the most probable translation.\nFinally, the LALM is mixed fine-tuned on the original SFT data and truncated speech-text pairs, allowing it to learn both offline and simultaneous translation capabilities.</span>\n</p>\n\n",
                "matched_terms": [
                    "simulsa",
                    "translation",
                    "during",
                    "sft",
                    "data",
                    "offline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We conducted extensive experiments to validate the effectiveness of our SimulSA. Results show that augmenting just </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1%</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> of the SFT data boosts Simul-S2TT performance by </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.1 BLEU</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in the 500ms-latency scenario, effectively activating the simultaneous capabilities of base LALMs at minimal cost, without compromising offline speech translation performance. Detailed ablation study and analysis further demonstrate that employing a decay truncation distribution for audio segmentation and utilizing mix-SFT during training are crucial for performance gains. Moreover, we observe that continually scaling up augmented data yields substantial benefits under low-latency conditions. Notably, our approach requires no modifications to the model architecture and remains fully compatible with other tasks, highlighting its scalability and practical applicability.</span>\n</p>\n\n",
                "matched_terms": [
                    "simulsa",
                    "translation",
                    "during",
                    "model",
                    "sft",
                    "data",
                    "training",
                    "results",
                    "offline",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">LALMs integrate LLM generation with audio understanding to unify the two modalities. A representative architecture, such as Qwen2-Audio </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#bib.bib17\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">17</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, consists of three components: an </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Audio Encoder (AE)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Large Language Model (LLM)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and an </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Adapter (ADA)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> connecting them.\nGiven an input triplet </span>\n  <math alttext=\"(\\bm{u},\\bm{x},\\bm{y})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n        <mi mathsize=\"0.900em\">&#119958;</mi>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mi mathsize=\"0.900em\">&#119961;</mi>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mi mathsize=\"0.900em\">&#119962;</mi>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">(\\bm{u},\\bm{x},\\bm{y})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">&#8212;where </span>\n  <math alttext=\"\\bm{u}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119958;</mi>\n      <annotation encoding=\"application/x-tex\">\\bm{u}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a textual prompt, </span>\n  <math alttext=\"\\bm{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119961;</mi>\n      <annotation encoding=\"application/x-tex\">\\bm{x}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> the source-language audio, and </span>\n  <math alttext=\"\\bm{y}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119962;</mi>\n      <annotation encoding=\"application/x-tex\">\\bm{y}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> the target-language translation&#8212;the AE encodes the audio, the ADA maps it into the LLM space, and the output is concatenated with prompt embeddings before being processed by the LLM.\nThe training objective is:</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Mixed Supervised Fine-Tuning (SFT)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Mix the full and truncated speech-text pairs obtained above, and perform a single-stage SFT on the base LALM. We also tested a two-stage SFT&#8212;first training on </span>\n  <math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119967;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for full translation, then on </span>\n  <math alttext=\"\\mathcal{D^{\\prime\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i3.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119967;</mi>\n        <mo mathsize=\"0.900em\">&#8242;&#8242;</mo>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\mathcal{D^{\\prime\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for streaming capabilities, following </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#bib.bib18\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">18</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. However, this two-stage approach led to a significant performance drop.</span>\n</p>\n\n",
                "matched_terms": [
                    "sft",
                    "translation",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">However, considering truncated speech of all lengths uniformly presents several issues. For nearly complete speech, translation styles are similar to full translations, making extra training unnecessary. Conversely, very short truncated speech lacks enough information, preventing the model from learning effective &#8220;wait&#8221; signals. Additionally, accuracy in early decoding is crucial because early mistakes can accumulate and lead to irreversible consequences.</span>\n</p>\n\n",
                "matched_terms": [
                    "model",
                    "translation",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For the base model, we use Qwen2-Audio-7B </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#bib.bib17\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">17</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and apply SimulSA and SFT to its base version</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">1</sup>\n        <span class=\"ltx_tag ltx_tag_note\">1</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/Qwen/Qwen2-Audio-7B\" title=\"\">https://huggingface.co/Qwen/Qwen2-Audio-7B</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For SFT, we utilize LoRA </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#bib.bib22\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">22</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with a rank of 8 and an alpha of 32. During training, we set the batch size to 128, the learning rate to 1e-4, and the weight decay to 0.1. We train the model using the </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">ms-swift</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> framework</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\">\n    <sup class=\"ltx_note_mark\">2</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\">\n        <sup class=\"ltx_note_mark\">2</sup>\n        <span class=\"ltx_tag ltx_tag_note\">2</span>\n        <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/modelscope/ms-swift\" title=\"\">https://github.com/modelscope/ms-swift</a>\n      </span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and use the checkpoint from the second training epoch for evaluation. We follow the template similar to Qwen2-Audio:</span>\n</p>\n\n",
                "matched_terms": [
                    "simulsa",
                    "during",
                    "model",
                    "size",
                    "sft",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For evaluation, we test Simul-S2TT with chunk sizes </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> of 500, 1000, 1500, 2000, 3000, and 4000 ms. We also perform offline translation by setting </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to infinity, translating after all speech is received for the best performance. In practice, we use a rollback strategy by removing the last </span>\n  <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">b</mi>\n      <annotation encoding=\"application/x-tex\">b</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> tokens after each step, testing </span>\n  <math alttext=\"b=0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">b</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">b=0</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m5\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">3</mn>\n      <annotation encoding=\"application/x-tex\">3</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m6\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">5</mn>\n      <annotation encoding=\"application/x-tex\">5</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (no rollback). We measure BLEU scores </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#bib.bib23\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">23</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> using </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">SacreBLEU<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\" style=\"font-size:111%;\">3</span></span><a class=\"ltx_ref ltx_url\" href=\"https://github.com/mjpost/sacrebleu\" style=\"font-size:111%;\" title=\"\">https://github.com/mjpost/sacrebleu</a></span></span></span></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and xCOMET scores </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#bib.bib24\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">24</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> using </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">XCOMET-XXL<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\" style=\"font-size:111%;\">4</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/Unbabel/XCOMET-XXL\" style=\"font-size:111%;\" title=\"\">https://huggingface.co/Unbabel/XCOMET-XXL</a></span></span></span></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "rollback",
                    "chunk",
                    "translation",
                    "xcomet",
                    "b0b0",
                    "sizes",
                    "offline",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For hyper-parameters, the sampling interval </span>\n  <math alttext=\"[l,r]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n        <mi mathsize=\"0.900em\">l</mi>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mi mathsize=\"0.900em\">r</mi>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">[l,r]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in speech truncation is </span>\n  <math alttext=\"l=500ms\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">l</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">500</mn>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">m</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">s</mi>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">l=500ms</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"r=5000ms\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">r</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">5000</mn>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">m</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">s</mi>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">r=5000ms</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, or the maximum feature count if needed.\nThe position threshold </span>\n  <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#964;</mi>\n      <annotation encoding=\"application/x-tex\">\\tau</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is </span>\n  <math alttext=\"\\tau=100/v\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#964;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">100</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\" symmetric=\"true\">/</mo>\n          <mi mathsize=\"0.900em\">v</mi>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\tau=100/v</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"v=151{,}646\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">v</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">151</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">646</mn>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">v=151{,}646</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, so </span>\n  <math alttext=\"\\tau=6.6\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m7\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#964;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">6.6</mn>\n          <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n          <msup>\n            <mn mathsize=\"0.900em\">10</mn>\n            <mrow>\n              <mo mathsize=\"0.900em\">&#8722;</mo>\n              <mn mathsize=\"0.900em\">4</mn>\n            </mrow>\n          </msup>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\tau=6.6\\times 10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe augmented data size </span>\n  <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m8\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">m</mi>\n      <annotation encoding=\"application/x-tex\">m</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is set to 3,000. An ablation study on this is in Section </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#S5.SS1\" style=\"font-size:90%;\" title=\"5.1 Self-Augmentation Data Size &#8227; 5 Ablation and Analysis &#8227; Direct Simultaneous Translation Activation for Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">5.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "size",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Low-Cost for High-Improvement.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nAugmenting SFT with SimulSA increases the corpus only slightly (232k &#8594; 235k) yet yields substantial BLEU gains. For a fixed chunk size, improvements are largest at smaller rollback sizes, which use fewer inference tokens. For example, with chunk size 1500, BLEU improves over SFT by +7.4 (rollback 0), +1.7 (rollback 3), and +0.8 (rollback 5). This trend holds across chunk sizes, showing that </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">a small amount of SimulSA data plus our training strategy markedly improves simultaneous translation with minimal additional cost.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</p>\n\n",
                "matched_terms": [
                    "rollback",
                    "chunk",
                    "simulsa",
                    "translation",
                    "232k",
                    "size",
                    "sft",
                    "235k",
                    "sizes",
                    "training",
                    "data",
                    "amount",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Low-Latency Benefit Most.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nAs chunk size decreases (lower latency), gains grow. At rollback 0, SimulSA+SFT improves BLEU by +7.2, +8.6, and +7.4 for chunk sizes 500, 1000, and 1500, versus +5.0 and +2.3 for chunk sizes 3000 and 4000. This underscores the effectiveness of our speech-truncation strategy (Sec.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#S3.SS1\" style=\"font-size:90%;\" title=\"3.1 Speech Truncation &#8227; 3 Method &#8227; Direct Simultaneous Translation Activation for Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">3.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) in low-latency regimes: </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">gains are modest at large chunks (high latency) but substantial at small chunks.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</p>\n\n",
                "matched_terms": [
                    "rollback",
                    "chunk",
                    "size",
                    "sizes",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Robust Offline MT.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nAlthough simultaneous and offline MT distributions differ (Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Direct Simultaneous Translation Activation for Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), adding SimulSA does not harm offline MT: at </span>\n  <math alttext=\"k=\\infty\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">k</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8734;</mi>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">k=\\infty</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, BLEU changes within random variation. Hence, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SimulSA improves simultaneous MT without compromising offline MT, ensuring robustness and reliability for real-world applications.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</p>\n\n",
                "matched_terms": [
                    "k∞kinfty",
                    "simulsa",
                    "offline",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To thoroughly investigate how the size of SimulSA data (</span>\n  <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">m</mi>\n      <annotation encoding=\"application/x-tex\">m</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) affects performance, we conduct an ablation study using </span>\n  <math alttext=\"m\\in\\{1000,2000,3000\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">m</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <mn mathsize=\"0.900em\">1000</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">2000</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">3000</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">m\\in\\{1000,2000,3000\\}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, performing SFT for each case. BLEU scores were measured for each </span>\n  <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">m</mi>\n      <annotation encoding=\"application/x-tex\">m</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> under the settings of</span>\n</p>\n\n",
                "matched_terms": [
                    "simulsa",
                    "size",
                    "data",
                    "sft",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As shown in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#S5.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 5 Ablation and Analysis &#8227; Direct Simultaneous Translation Activation for Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, increasing the amount of augmented data consistently boosts BLEU scores across all </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">b</mi>\n      <annotation encoding=\"application/x-tex\">b</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> values. When </span>\n  <math alttext=\"b=0\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">b</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">b=0</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (no rollback), BLEU improvements scale almost linearly with additional data, suggesting further gains as more augmented data is added. For </span>\n  <math alttext=\"b\\geq 3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m7\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">b</mi>\n        <mo mathsize=\"0.900em\">&#8805;</mo>\n        <mn mathsize=\"0.900em\">3</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">b\\geq 3</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (higher </span>\n  <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m8\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">b</mi>\n      <annotation encoding=\"application/x-tex\">b</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> means greater inference latency), BLEU improvements plateau once the augmented data reaches about 1.3% of the original dataset. This saturation indicates that only a modest increase in augmented data is needed to meaningfully enhance the performance of these stronger baselines.</span>\n</p>\n\n",
                "matched_terms": [
                    "rollback",
                    "b0b0",
                    "data",
                    "amount",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In Section </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#S3.SS1\" style=\"font-size:90%;\" title=\"3.1 Speech Truncation &#8227; 3 Method &#8227; Direct Simultaneous Translation Activation for Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">3.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we proposed a decaying sampling distribution over speech truncation points to avoid extremely short or long segments and to emphasize learning from early speech&#8211;text prefixes. We evaluate this design via an ablation on the truncation distribution.\nWe compare three variants of the sampling distribution (Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#S5.F4\" style=\"font-size:90%;\" title=\"Figure 4 &#8227; 5.1 Self-Augmentation Data Size &#8227; 5 Ablation and Analysis &#8227; Direct Simultaneous Translation Activation for Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) and report BLEU in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#S5.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 5.1 Self-Augmentation Data Size &#8227; 5 Ablation and Analysis &#8227; Direct Simultaneous Translation Activation for Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The results show:\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Variant 2</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> performs worst, likely because it oversamples short, uninformative or noisy prefixes and also yields very long segments that approximate offline translation;\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Variant 3</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> underperforms due to reduced sampling diversity, which weakens robustness and degrades translation quality;\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Variant 1</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> improves by avoiding extreme segment lengths and increasing truncation diversity, but still fails to sufficiently strengthen learning on early simultaneous-translation segments.\nBy contrast, our beta-decay distribution yields the best BLEU. It preserves diversity within a bounded interval while biasing toward early truncation points. The gain is especially pronounced when rollback is disabled, indicating reduced error accumulation from early mistranslations.</span>\n</p>\n\n",
                "matched_terms": [
                    "rollback",
                    "translation",
                    "results",
                    "offline",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We address the challenge of equipping large audio&#8211;language models (LALMs) for real-time speech translation. We present SimulSA, a lightweight augmentation that derives truncated audio-text segments from full pairs. Adding only 1% augmented data during training enables a single model to support both streaming and offline translation without architectural changes. SimulSA yields substantial gains under low-latency constraints and works well with other techniques, enhancing the practicality of LALMs for real-time deployment.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "simulsa",
                    "translation",
                    "during",
                    "model",
                    "data",
                    "training",
                    "offline"
                ]
            }
        ]
    },
    "S5.T2": {
        "caption": "Table 2: BLEU scores for Original and Variant 1–3 truncation distributions across speech chunk sizes (k=500,1000,1500,2000k=500,1000,1500,2000) and rollback values (0, 3, 5).",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\" style=\"padding:0.9pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Truncation</span></span>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Distribution</span></span>\n</span></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\" style=\"padding:0.9pt 2.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Chunk Size <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m3\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math></span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.9pt 2.8pt;\"><math alttext=\"500\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m4\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">500</mn><annotation encoding=\"application/x-tex\">500</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.9pt 2.8pt;\"><math alttext=\"1000\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m5\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">1000</mn><annotation encoding=\"application/x-tex\">1000</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.9pt 2.8pt;\"><math alttext=\"1500\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m6\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">1500</mn><annotation encoding=\"application/x-tex\">1500</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding:0.9pt 2.8pt;\"><math alttext=\"2000\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m7\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">2000</mn><annotation encoding=\"application/x-tex\">2000</annotation></semantics></math></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding:0.9pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Original</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.9pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.9 / 34.2 / 38.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.9pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.4 / 36.4 / 39.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.9pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.0 / 37.1 / 40.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.9pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">24.3 / 38.4 / 40.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:0.9pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Variant 1</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.9pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.0 / 33.5 / 38.1</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.9pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.2 / 36.6 / 39.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.9pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">19.2 / 37.0 / 40.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.9pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">23.4 / 38.2 / 40.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding:0.9pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Variant 2</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.9pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.3 / 31.5 / 37.6</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.9pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.2 / 34.9 / 38.9</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.9pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">17.3 / 36.7 / 39.8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.9pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">21.8 / 37.6 / 40.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding:0.9pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Variant 3</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.9pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.5 / 33.1 / 37.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.9pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.9 / 35.7 / 39.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.9pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">18.8 / 36.9 / 40.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.9pt 2.8pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">22.6 / 38.1 / 40.5</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "rollback",
            "across",
            "truncation",
            "distributions",
            "original",
            "chunk",
            "variant",
            "values",
            "distribution",
            "1–3",
            "size",
            "sizes",
            "k500100015002000k500100015002000",
            "speech",
            "bleu",
            "scores"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In Section </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#S3.SS1\" style=\"font-size:90%;\" title=\"3.1 Speech Truncation &#8227; 3 Method &#8227; Direct Simultaneous Translation Activation for Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">3.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we proposed a decaying sampling distribution over speech truncation points to avoid extremely short or long segments and to emphasize learning from early speech&#8211;text prefixes. We evaluate this design via an ablation on the truncation distribution.\nWe compare three variants of the sampling distribution (Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#S5.F4\" style=\"font-size:90%;\" title=\"Figure 4 &#8227; 5.1 Self-Augmentation Data Size &#8227; 5 Ablation and Analysis &#8227; Direct Simultaneous Translation Activation for Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) and report BLEU in Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#S5.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 5.1 Self-Augmentation Data Size &#8227; 5 Ablation and Analysis &#8227; Direct Simultaneous Translation Activation for Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. The results show:\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Variant 2</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> performs worst, likely because it oversamples short, uninformative or noisy prefixes and also yields very long segments that approximate offline translation;\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Variant 3</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> underperforms due to reduced sampling diversity, which weakens robustness and degrades translation quality;\n</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Variant 1</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> improves by avoiding extreme segment lengths and increasing truncation diversity, but still fails to sufficiently strengthen learning on early simultaneous-translation segments.\nBy contrast, our beta-decay distribution yields the best BLEU. It preserves diversity within a bounded interval while biasing toward early truncation points. The gain is especially pronounced when rollback is disabled, indicating reduced error accumulation from early mistranslations.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Simultaneous speech-to-text translation (Simul-S2TT) aims to translate speech into target text in real time, outputting translations while receiving source speech input, rather than waiting for the entire utterance to be spoken. Simul-S2TT research often modifies model architectures to implement read-write strategies. However, with the rise of large audio-language models (LALMs), a key challenge is how to directly activate Simul-S2TT capabilities in base models without additional architectural changes.\nIn this paper, we introduce <span class=\"ltx_text ltx_font_bold\">Simul</span>taneous <span class=\"ltx_text ltx_font_bold\">S</span>elf-<span class=\"ltx_text ltx_font_bold\">A</span>ugmentation (<span class=\"ltx_text ltx_font_bold\">SimulSA</span>), a strategy that utilizes LALMs&#8217; inherent capabilities to obtain simultaneous data by randomly truncating speech and constructing partially aligned translation. By incorporating them into offline SFT data, SimulSA effectively bridges the distribution gap between offline translation during pretraining and simultaneous translation during inference.\nExperimental results demonstrate that augmenting only about <span class=\"ltx_text ltx_font_bold\">1%</span> of the simultaneous data, compared to the full offline SFT data, can significantly activate LALMs&#8217; Simul-S2TT capabilities without modifications to model architecture or decoding strategy.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "distribution"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We argue that the fundamental difficulty lies in constructing effective training data for truncated speech input paired with its corresponding ground-truth translation output.\nTo elaborate, the main gap between Simul-MT and Offline-MT (</span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">i.e.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, complete end-to-end MT) is </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">the completeness of information alignment</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, as shown in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Direct Simultaneous Translation Activation for Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nIn Offline-MT, the model fully aligns complete source-side information with the target, ensuring no omissions.\nIn contrast, when processing truncated speech, Simul-MT often needs to &#8220;wait&#8221; for some speech information to be translated later for handling &#8220;word reordering&#8221; across languages, resulting in incomplete source-to-target alignment.\nThis alignment difference causes distribution gaps between training and inference. Moreover, truncated speech may contain incomplete semantic units that are also absent during training, further complicating inference for Simul-S2TT tasks.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "across",
                    "distribution"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Given these challenges, we aim to augment simultaneous data, enabling LALMs to directly learn features relevant to simultaneous translation during SFT and thereby bridge the gap between Simul-MT and Offline-MT.\nTo address translation errors stemming from incomplete early speech segments, we propose the Beta Decay distribution for audio truncation when constructing streaming speech data, ensuring more realistic simulation of streaming segment boundaries. We further utilize the ground-truth translation and the probability distribution from the LALM output to generate high-quality truncated speech-translation pairs.\nWe term this method </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Simul</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">taneous </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">S</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">elf-</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">A</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">ugmentation (</span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SimulSA</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) strategy, with the overall pipeline shown in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#S1.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 1 Introduction &#8227; Direct Simultaneous Translation Activation for Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nSpecifically, we first select a subset from the offline speech-text SFT pairs and segment the audio using the Beta Decay distribution. For each truncated speech segment, we use the probability distribution of the full ground-truth translation generated from a pre-trained LALM to speculate the most probable translation.\nFinally, the LALM is mixed fine-tuned on the original SFT data and truncated speech-text pairs, allowing it to learn both offline and simultaneous translation capabilities.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "distribution",
                    "truncation",
                    "original"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We conducted extensive experiments to validate the effectiveness of our SimulSA. Results show that augmenting just </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1%</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> of the SFT data boosts Simul-S2TT performance by </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">5.1 BLEU</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in the 500ms-latency scenario, effectively activating the simultaneous capabilities of base LALMs at minimal cost, without compromising offline speech translation performance. Detailed ablation study and analysis further demonstrate that employing a decay truncation distribution for audio segmentation and utilizing mix-SFT during training are crucial for performance gains. Moreover, we observe that continually scaling up augmented data yields substantial benefits under low-latency conditions. Notably, our approach requires no modifications to the model architecture and remains fully compatible with other tasks, highlighting its scalability and practical applicability.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "distribution",
                    "truncation",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Speech Truncation (Sec.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#S3.SS1\" title=\"3.1 Speech Truncation &#8227; 3 Method &#8227; Direct Simultaneous Translation Activation for Large Audio-Language Models\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>)</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Sample a small number of speech-text pairs </span>\n  <math alttext=\"(\\bm{x}_{1:s},\\bm{y}_{1:t})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119961;</mi>\n          <mrow>\n            <mn mathsize=\"0.900em\">1</mn>\n            <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n            <mi mathsize=\"0.900em\">s</mi>\n          </mrow>\n        </msub>\n        <mo mathsize=\"0.900em\">,</mo>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119962;</mi>\n          <mrow>\n            <mn mathsize=\"0.900em\">1</mn>\n            <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n            <mi mathsize=\"0.900em\">t</mi>\n          </mrow>\n        </msub>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">(\\bm{x}_{1:s},\\bm{y}_{1:t})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> from the original offline S2TT dataset and randomly truncate the full speech </span>\n  <math alttext=\"\\bm{x}_{1:s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119961;</mi>\n        <mrow>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n          <mi mathsize=\"0.900em\">s</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\bm{x}_{1:s}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to obtain </span>\n  <math alttext=\"\\bm{x}_{1:s^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119961;</mi>\n        <mrow>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n          <msup>\n            <mi mathsize=\"0.900em\">s</mi>\n            <mo mathsize=\"0.900em\">&#8242;</mo>\n          </msup>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\bm{x}_{1:s^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">;</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "truncation",
                    "original"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Assume the SFT dataset is </span>\n  <math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119967;</mi>\n      <annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, containing </span>\n  <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">N</mi>\n      <annotation encoding=\"application/x-tex\">N</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> speech-text pairs </span>\n  <math alttext=\"\\{(\\bm{x}_{1:s},\\bm{y}_{1:t})\\}_{i=1}^{N}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <msub>\n              <mi mathsize=\"0.900em\">&#119961;</mi>\n              <mrow>\n                <mn mathsize=\"0.900em\">1</mn>\n                <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n                <mi mathsize=\"0.900em\">s</mi>\n              </mrow>\n            </msub>\n            <mo mathsize=\"0.900em\">,</mo>\n            <msub>\n              <mi mathsize=\"0.900em\">&#119962;</mi>\n              <mrow>\n                <mn mathsize=\"0.900em\">1</mn>\n                <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n                <mi mathsize=\"0.900em\">t</mi>\n              </mrow>\n            </msub>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">i</mi>\n          <mo mathsize=\"0.900em\">=</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n        </mrow>\n        <mi mathsize=\"0.900em\">N</mi>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\{(\\bm{x}_{1:s},\\bm{y}_{1:t})\\}_{i=1}^{N}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we select </span>\n  <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">M</mi>\n      <annotation encoding=\"application/x-tex\">M</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (</span>\n  <math alttext=\"M\\ll N\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">M</mi>\n        <mo mathsize=\"0.900em\">&#8810;</mo>\n        <mi mathsize=\"0.900em\">N</mi>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">M\\ll N</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) samples to form a subset </span>\n  <math alttext=\"\\mathcal{D^{\\prime}}\\subset\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msup>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119967;</mi>\n          <mo mathsize=\"0.900em\">&#8242;</mo>\n        </msup>\n        <mo mathsize=\"0.900em\">&#8834;</mo>\n        <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119967;</mi>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{D^{\\prime}}\\subset\\mathcal{D}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThen, we truncate </span>\n  <math alttext=\"\\bm{x}_{1:s}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m7\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119961;</mi>\n        <mrow>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n          <mi mathsize=\"0.900em\">s</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\bm{x}_{1:s}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in each </span>\n  <math alttext=\"(\\bm{x}_{1:s},\\bm{y}_{1:t})\\in\\mathcal{D^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m8\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119961;</mi>\n            <mrow>\n              <mn mathsize=\"0.900em\">1</mn>\n              <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n              <mi mathsize=\"0.900em\">s</mi>\n            </mrow>\n          </msub>\n          <mo mathsize=\"0.900em\">,</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">&#119962;</mi>\n            <mrow>\n              <mn mathsize=\"0.900em\">1</mn>\n              <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n              <mi mathsize=\"0.900em\">t</mi>\n            </mrow>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <msup>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#119967;</mi>\n          <mo mathsize=\"0.900em\">&#8242;</mo>\n        </msup>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">(\\bm{x}_{1:s},\\bm{y}_{1:t})\\in\\mathcal{D^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to obtain truncated speech. One of the simplest ways is uniform random truncation. Assuming </span>\n  <math alttext=\"\\bm{x}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m9\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#119961;</mi>\n      <annotation encoding=\"application/x-tex\">\\bm{x}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> has </span>\n  <math alttext=\"s\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m10\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">s</mi>\n      <annotation encoding=\"application/x-tex\">s</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> features, we randomly select an index </span>\n  <math alttext=\"s^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m11\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo mathsize=\"0.900em\">&#8242;</mo>\n      </msup>\n      <annotation encoding=\"application/x-tex\">s^{\\prime}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in the interval </span>\n  <math alttext=\"[1,s]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m12\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">[1,s]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to obtain the truncated speech </span>\n  <math alttext=\"\\bm{x}_{1:s^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m13\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119961;</mi>\n        <mrow>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n          <msup>\n            <mi mathsize=\"0.900em\">s</mi>\n            <mo mathsize=\"0.900em\">&#8242;</mo>\n          </msup>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\bm{x}_{1:s^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "truncation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To address these issues, we first define a sub-interval </span>\n  <math alttext=\"[l,r]\\subset[1,s]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n          <mi mathsize=\"0.900em\">l</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">r</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n        </mrow>\n        <mo mathsize=\"0.900em\">&#8834;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">s</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">[l,r]\\subset[1,s]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This means we only sample truncation points from the interval </span>\n  <math alttext=\"[l,r]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n        <mi mathsize=\"0.900em\">l</mi>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mi mathsize=\"0.900em\">r</mi>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">[l,r]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, effectively </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">filtering out excessively short or long truncated speech</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nNext, we consider a Beta distribution </span>\n  <math alttext=\"f(X;\\alpha,\\beta)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">f</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mi mathsize=\"0.900em\">X</mi>\n          <mo mathsize=\"0.900em\">;</mo>\n          <mi mathsize=\"0.900em\">&#945;</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">&#946;</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">f(X;\\alpha,\\beta)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#bib.bib19\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">19</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with the probability density function given by:</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "distribution",
                    "truncation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We set </span>\n  <math alttext=\"\\alpha=1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#945;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">1</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\alpha=1</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"\\beta=3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#946;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">3</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\beta=3</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, resulting in a monotonically decreasing function for </span>\n  <math alttext=\"f(X)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">f</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mi mathsize=\"0.900em\">X</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">f(X)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. As </span>\n  <math alttext=\"X\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m7\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">X</mi>\n      <annotation encoding=\"application/x-tex\">X</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> increases, the gradient of </span>\n  <math alttext=\"f(X)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m8\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">f</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mi mathsize=\"0.900em\">X</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">f(X)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> diminishes, which meets our objective to </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">prioritize early-stage translation decoding</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFinally, since the domain of </span>\n  <math alttext=\"f(X)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m9\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">f</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mi mathsize=\"0.900em\">X</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">f(X)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is </span>\n  <math alttext=\"X\\in(0,1)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m10\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">X</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mn mathsize=\"0.900em\">0</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">X\\in(0,1)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we construct a linear mapping from this interval to </span>\n  <math alttext=\"[l,r]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m11\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n        <mi mathsize=\"0.900em\">l</mi>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mi mathsize=\"0.900em\">r</mi>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">[l,r]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This enables us to transform </span>\n  <math alttext=\"X\\sim f(X)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m12\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">X</mi>\n        <mo mathsize=\"0.900em\">&#8764;</mo>\n        <mrow>\n          <mi mathsize=\"0.900em\">f</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <mi mathsize=\"0.900em\">X</mi>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">X\\sim f(X)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> into specific truncation points </span>\n  <math alttext=\"s^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m13\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo mathsize=\"0.900em\">&#8242;</mo>\n      </msup>\n      <annotation encoding=\"application/x-tex\">s^{\\prime}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Thus, the speech truncation point </span>\n  <math alttext=\"s^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m14\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo mathsize=\"0.900em\">&#8242;</mo>\n      </msup>\n      <annotation encoding=\"application/x-tex\">s^{\\prime}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> can be expressed as:</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "truncation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"f(s^{\\prime})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m15\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">f</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <msup>\n            <mi mathsize=\"0.900em\">s</mi>\n            <mo mathsize=\"0.900em\">&#8242;</mo>\n          </msup>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">f(s^{\\prime})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the speech truncation distribution we define.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "distribution",
                    "truncation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Since LALMs generate translated text in an autoregressive manner, the translation corresponding to </span>\n  <math alttext=\"\\bm{x}_{1:s^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119961;</mi>\n        <mrow>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n          <msup>\n            <mi mathsize=\"0.900em\">s</mi>\n            <mo mathsize=\"0.900em\">&#8242;</mo>\n          </msup>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\bm{x}_{1:s^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> must be in the form </span>\n  <math alttext=\"\\bm{y}_{1:t^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119962;</mi>\n        <mrow>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n          <msup>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mo mathsize=\"0.900em\">&#8242;</mo>\n          </msup>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\bm{y}_{1:t^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"t^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m3\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mo mathsize=\"0.900em\">&#8242;</mo>\n      </msup>\n      <annotation encoding=\"application/x-tex\">t^{\\prime}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> indicates the text truncation point, with </span>\n  <math alttext=\"1\\leq t^{\\prime}\\leq t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo mathsize=\"0.900em\">&#8804;</mo>\n        <msup>\n          <mi mathsize=\"0.900em\">t</mi>\n          <mo mathsize=\"0.900em\">&#8242;</mo>\n        </msup>\n        <mo mathsize=\"0.900em\">&#8804;</mo>\n        <mi mathsize=\"0.900em\">t</mi>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1\\leq t^{\\prime}\\leq t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Therefore, determining </span>\n  <math alttext=\"\\bm{y}_{1:t^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119962;</mi>\n        <mrow>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n          <msup>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mo mathsize=\"0.900em\">&#8242;</mo>\n          </msup>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\bm{y}_{1:t^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> based on </span>\n  <math alttext=\"\\bm{x}_{1:s^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m6\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119961;</mi>\n        <mrow>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n          <msup>\n            <mi mathsize=\"0.900em\">s</mi>\n            <mo mathsize=\"0.900em\">&#8242;</mo>\n          </msup>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\bm{x}_{1:s^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is essential.\nFrom the perspective of the target text, all information in </span>\n  <math alttext=\"\\bm{y}_{1:t^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m7\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119962;</mi>\n        <mrow>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n          <msup>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mo mathsize=\"0.900em\">&#8242;</mo>\n          </msup>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\bm{y}_{1:t^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> must be presented in </span>\n  <math alttext=\"\\bm{x}_{1:s^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m8\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119961;</mi>\n        <mrow>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n          <msup>\n            <mi mathsize=\"0.900em\">s</mi>\n            <mo mathsize=\"0.900em\">&#8242;</mo>\n          </msup>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\bm{x}_{1:s^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, while maximizing the retention of translatable content. Thus, </span>\n  <math alttext=\"t^{\\prime}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m9\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">t</mi>\n        <mo mathsize=\"0.900em\">&#8242;</mo>\n      </msup>\n      <annotation encoding=\"application/x-tex\">t^{\\prime}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> must meet the following conditions: (1) For each </span>\n  <math alttext=\"\\bm{y}_{i}(1\\leq i\\leq t^{\\prime})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m10\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">&#119962;</mi>\n          <mi mathsize=\"0.900em\">i</mi>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mrow>\n            <mn mathsize=\"0.900em\">1</mn>\n            <mo mathsize=\"0.900em\">&#8804;</mo>\n            <mi mathsize=\"0.900em\">i</mi>\n            <mo mathsize=\"0.900em\">&#8804;</mo>\n            <msup>\n              <mi mathsize=\"0.900em\">t</mi>\n              <mo mathsize=\"0.900em\">&#8242;</mo>\n            </msup>\n          </mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\bm{y}_{i}(1\\leq i\\leq t^{\\prime})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, there must be corresponding information in </span>\n  <math alttext=\"\\bm{x}_{1:s^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m11\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119961;</mi>\n        <mrow>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n          <msup>\n            <mi mathsize=\"0.900em\">s</mi>\n            <mo mathsize=\"0.900em\">&#8242;</mo>\n          </msup>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\bm{x}_{1:s^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">; (2) For </span>\n  <math alttext=\"\\bm{y}_{t^{\\prime}+1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m12\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119962;</mi>\n        <mrow>\n          <msup>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mo mathsize=\"0.900em\">&#8242;</mo>\n          </msup>\n          <mo mathsize=\"0.900em\">+</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\bm{y}_{t^{\\prime}+1}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, there should be no corresponding information in </span>\n  <math alttext=\"\\bm{x}_{1:s^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m13\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119961;</mi>\n        <mrow>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n          <msup>\n            <mi mathsize=\"0.900em\">s</mi>\n            <mo mathsize=\"0.900em\">&#8242;</mo>\n          </msup>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\bm{x}_{1:s^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nExcluding the content already translated in </span>\n  <math alttext=\"\\bm{y}_{1:t^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m14\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119962;</mi>\n        <mrow>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n          <msup>\n            <mi mathsize=\"0.900em\">t</mi>\n            <mo mathsize=\"0.900em\">&#8242;</mo>\n          </msup>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\bm{y}_{1:t^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> from </span>\n  <math alttext=\"\\bm{x}_{1:s^{\\prime}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m15\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">&#119961;</mi>\n        <mrow>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n          <msup>\n            <mi mathsize=\"0.900em\">s</mi>\n            <mo mathsize=\"0.900em\">&#8242;</mo>\n          </msup>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\bm{x}_{1:s^{\\prime}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> leaves the remaining speech information, indicating what still needs to be translated later. This forms a speech-text pair that captures the &#8220;wait&#8221; signal.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "truncation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For evaluation, we test Simul-S2TT with chunk sizes </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> of 500, 1000, 1500, 2000, 3000, and 4000 ms. We also perform offline translation by setting </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m2\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to infinity, translating after all speech is received for the best performance. In practice, we use a rollback strategy by removing the last </span>\n  <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">b</mi>\n      <annotation encoding=\"application/x-tex\">b</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> tokens after each step, testing </span>\n  <math alttext=\"b=0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m4\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">b</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">b=0</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, </span>\n  <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m5\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">3</mn>\n      <annotation encoding=\"application/x-tex\">3</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and </span>\n  <math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p4.m6\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">5</mn>\n      <annotation encoding=\"application/x-tex\">5</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (no rollback). We measure BLEU scores </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#bib.bib23\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">23</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> using </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">SacreBLEU<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\" style=\"font-size:111%;\">3</span></span><a class=\"ltx_ref ltx_url\" href=\"https://github.com/mjpost/sacrebleu\" style=\"font-size:111%;\" title=\"\">https://github.com/mjpost/sacrebleu</a></span></span></span></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and xCOMET scores </span>\n  <cite class=\"ltx_cite ltx_citemacro_citep\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#bib.bib24\" title=\"\">\n      <span class=\"ltx_text\" style=\"font-size:90%;\">24</span>\n    </a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> using </span>\n  <span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">XCOMET-XXL<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\" style=\"font-size:111%;\">4</span></span><a class=\"ltx_ref ltx_url\" href=\"https://huggingface.co/Unbabel/XCOMET-XXL\" style=\"font-size:111%;\" title=\"\">https://huggingface.co/Unbabel/XCOMET-XXL</a></span></span></span></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "rollback",
                    "chunk",
                    "scores",
                    "sizes",
                    "speech",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For hyper-parameters, the sampling interval </span>\n  <math alttext=\"[l,r]\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">[</mo>\n        <mi mathsize=\"0.900em\">l</mi>\n        <mo mathsize=\"0.900em\">,</mo>\n        <mi mathsize=\"0.900em\">r</mi>\n        <mo maxsize=\"0.900em\" minsize=\"0.900em\">]</mo>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">[l,r]</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in speech truncation is </span>\n  <math alttext=\"l=500ms\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">l</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">500</mn>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">m</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">s</mi>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">l=500ms</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"r=5000ms\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">r</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">5000</mn>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">m</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">s</mi>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">r=5000ms</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, or the maximum feature count if needed.\nThe position threshold </span>\n  <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#964;</mi>\n      <annotation encoding=\"application/x-tex\">\\tau</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is </span>\n  <math alttext=\"\\tau=100/v\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m5\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#964;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">100</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\" stretchy=\"true\" symmetric=\"true\">/</mo>\n          <mi mathsize=\"0.900em\">v</mi>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\tau=100/v</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"v=151{,}646\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">v</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">151</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">646</mn>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">v=151{,}646</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, so </span>\n  <math alttext=\"\\tau=6.6\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m7\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">&#964;</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">6.6</mn>\n          <mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo>\n          <msup>\n            <mn mathsize=\"0.900em\">10</mn>\n            <mrow>\n              <mo mathsize=\"0.900em\">&#8722;</mo>\n              <mn mathsize=\"0.900em\">4</mn>\n            </mrow>\n          </msup>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\tau=6.6\\times 10^{-4}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe augmented data size </span>\n  <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m8\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">m</mi>\n      <annotation encoding=\"application/x-tex\">m</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is set to 3,000. An ablation study on this is in Section </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#S5.SS1\" style=\"font-size:90%;\" title=\"5.1 Self-Augmentation Data Size &#8227; 5 Ablation and Analysis &#8227; Direct Simultaneous Translation Activation for Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">5.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "size",
                    "truncation",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Low-Cost for High-Improvement.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nAugmenting SFT with SimulSA increases the corpus only slightly (232k &#8594; 235k) yet yields substantial BLEU gains. For a fixed chunk size, improvements are largest at smaller rollback sizes, which use fewer inference tokens. For example, with chunk size 1500, BLEU improves over SFT by +7.4 (rollback 0), +1.7 (rollback 3), and +0.8 (rollback 5). This trend holds across chunk sizes, showing that </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">a small amount of SimulSA data plus our training strategy markedly improves simultaneous translation with minimal additional cost.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</p>\n\n",
                "matched_terms": [
                    "rollback",
                    "across",
                    "chunk",
                    "size",
                    "sizes",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Low-Latency Benefit Most.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nAs chunk size decreases (lower latency), gains grow. At rollback 0, SimulSA+SFT improves BLEU by +7.2, +8.6, and +7.4 for chunk sizes 500, 1000, and 1500, versus +5.0 and +2.3 for chunk sizes 3000 and 4000. This underscores the effectiveness of our speech-truncation strategy (Sec.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#S3.SS1\" style=\"font-size:90%;\" title=\"3.1 Speech Truncation &#8227; 3 Method &#8227; Direct Simultaneous Translation Activation for Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">3.1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) in low-latency regimes: </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">gains are modest at large chunks (high latency) but substantial at small chunks.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</p>\n\n",
                "matched_terms": [
                    "rollback",
                    "chunk",
                    "size",
                    "sizes",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Robust Offline MT.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nAlthough simultaneous and offline MT distributions differ (Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Direct Simultaneous Translation Activation for Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">), adding SimulSA does not harm offline MT: at </span>\n  <math alttext=\"k=\\infty\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p4.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">k</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mi mathsize=\"0.900em\" mathvariant=\"normal\">&#8734;</mi>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">k=\\infty</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, BLEU changes within random variation. Hence, </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">SimulSA improves simultaneous MT without compromising offline MT, ensuring robustness and reliability for real-world applications.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</p>\n\n",
                "matched_terms": [
                    "distributions",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To thoroughly investigate how the size of SimulSA data (</span>\n  <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">m</mi>\n      <annotation encoding=\"application/x-tex\">m</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) affects performance, we conduct an ablation study using </span>\n  <math alttext=\"m\\in\\{1000,2000,3000\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">m</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <mn mathsize=\"0.900em\">1000</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">2000</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">3000</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">m\\in\\{1000,2000,3000\\}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, performing SFT for each case. BLEU scores were measured for each </span>\n  <math alttext=\"m\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">m</mi>\n      <annotation encoding=\"application/x-tex\">m</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> under the settings of</span>\n</p>\n\n",
                "matched_terms": [
                    "size",
                    "scores",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As shown in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15692v1#S5.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 5 Ablation and Analysis &#8227; Direct Simultaneous Translation Activation for Large Audio-Language Models\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, increasing the amount of augmented data consistently boosts BLEU scores across all </span>\n  <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">k</mi>\n      <annotation encoding=\"application/x-tex\">k</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">b</mi>\n      <annotation encoding=\"application/x-tex\">b</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> values. When </span>\n  <math alttext=\"b=0\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m6\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">b</mi>\n        <mo mathsize=\"0.900em\">=</mo>\n        <mn mathsize=\"0.900em\">0</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">b=0</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (no rollback), BLEU improvements scale almost linearly with additional data, suggesting further gains as more augmented data is added. For </span>\n  <math alttext=\"b\\geq 3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m7\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">b</mi>\n        <mo mathsize=\"0.900em\">&#8805;</mo>\n        <mn mathsize=\"0.900em\">3</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">b\\geq 3</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (higher </span>\n  <math alttext=\"b\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m8\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">b</mi>\n      <annotation encoding=\"application/x-tex\">b</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> means greater inference latency), BLEU improvements plateau once the augmented data reaches about 1.3% of the original dataset. This saturation indicates that only a modest increase in augmented data is needed to meaningfully enhance the performance of these stronger baselines.</span>\n</p>\n\n",
                "matched_terms": [
                    "rollback",
                    "across",
                    "original",
                    "values",
                    "scores",
                    "bleu"
                ]
            }
        ]
    }
}