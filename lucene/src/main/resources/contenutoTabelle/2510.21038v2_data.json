{
    "S4.T1": {
        "caption": "Table 1: Performance compared to random baselines derived from permutation nulls. Thresholded metrics use threshold τ=0.5\\tau=0.5. Standard errors are approximated from the 95% bootstrap CIs via normality (SE≈(CIhi−CIlo)/3.92\\mathrm{SE}\\approx(\\mathrm{CI}_{\\mathrm{hi}}-\\mathrm{CI}_{\\mathrm{lo}})/3.92).",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Metric</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Baseline</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">Model</span> (<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> SE)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">% improvement</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">p-value</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">F1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.010</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\">0.107</span> <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m2\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.038</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">+970%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"\\approx 1.00\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mrow><mn>1.00</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\approx 1.00\\times 10^{-5}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">F1-Macro</td>\n<td class=\"ltx_td ltx_align_center\">0.431</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\">0.542</span> <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m4\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.028</td>\n<td class=\"ltx_td ltx_align_center\">+25.8%</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\approx 1.00\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mrow><mn>1.00</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\approx 1.00\\times 10^{-5}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Accuracy</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.995</span></td>\n<td class=\"ltx_td ltx_align_center\">0.955 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m6\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.033</td>\n<td class=\"ltx_td ltx_align_center\">-4.0%</td>\n<td class=\"ltx_td ltx_align_center\">n.s.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MCC</td>\n<td class=\"ltx_td ltx_align_center\">0.000</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\">0.119</span> <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m7\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.027</td>\n<td class=\"ltx_td ltx_align_center\">n/a</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\approx 1.00\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m8\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mrow><mn>1.00</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\approx 1.00\\times 10^{-5}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">AUROC</td>\n<td class=\"ltx_td ltx_align_center\">0.500</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\">0.804</span> <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m9\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.017</td>\n<td class=\"ltx_td ltx_align_center\">+60.8%</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\approx 2.00\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m10\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mrow><mn>2.00</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\approx 2.00\\times 10^{-5}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\">AUPRC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">0.007</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">\n<span class=\"ltx_text ltx_font_bold\">0.094</span> <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m11\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.032</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">+1243%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><math alttext=\"\\approx 2.00\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m12\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mrow><mn>2.00</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\approx 2.00\\times 10^{-5}</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "pvalue",
            "nulls",
            "≈200×10−5approx",
            "baselines",
            "bootstrap",
            "baseline",
            "f1macro",
            "cis",
            "from",
            "100times",
            "accuracy",
            "τ05tau05",
            "normality",
            "performance",
            "se≈cihi−cilo392mathrmseapproxmathrmcimathrmhimathrmcimathrmlo392",
            "improvement",
            "metrics",
            "auroc",
            "metric",
            "approximated",
            "errors",
            "via",
            "random",
            "derived",
            "auprc",
            "standard",
            "use",
            "compared",
            "model",
            "permutation",
            "200times",
            "≈100×10−5approx",
            "±pm",
            "mcc",
            "thresholded",
            "threshold"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Where possible, all experiments use the standard train/validation/test splits provided by the <span class=\"ltx_text ltx_font_typewriter\">pnpl</span> dataset (using the logic described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S3.SS4\" title=\"3.4 Reference Model &#8227; 3 Methods &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a>) and are fully reproducible (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#A1\" title=\"Appendix A Open Resources: Code, Tutorial, and Leaderboard &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>). Unless noted, values are seed-averages over three runs. Error bars are standard errors across seeds. For Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Model Performance &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> we report standard errors approximated from 95% bootstrap CIs (4,000 resamples).</p>\n\n",
            "<p class=\"ltx_p\">We first establish that the dataset carries usable signal for keyword detection by evaluating a single model on the held-out test set (<math alttext=\"n=4660\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>4660</mn></mrow><annotation encoding=\"application/x-tex\">n=4660</annotation></semantics></math>, positives <math alttext=\"=24\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>=</mo><mn>24</mn></mrow><annotation encoding=\"application/x-tex\">=24</annotation></semantics></math>; base rate <math alttext=\"=0.00515\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>=</mo><mn>0.00515</mn></mrow><annotation encoding=\"application/x-tex\">=0.00515</annotation></semantics></math>). Given the absence of prior publicly reproducible MEG keyword spotting benchmarks, we evaluate against permutation-derived random baselines to demonstrate the presence of meaningful signal rather than competitive performance. While overall performance is modest, threshold-free metrics indicate clear signal (AUPRC <math alttext=\"\\approx 13.4\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mo>&#8776;</mo><mn>13.4</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">\\approx 13.4\\times</annotation></semantics></math> the permutation baseline; AUROC <math alttext=\"\\approx 0.80\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.80</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.80</annotation></semantics></math>). Full results are provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Model Performance &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nBeyond threshold-free metrics, we include an operational snapshot. At a target recall of <math alttext=\"\\sim\\!0.10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>0.10</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!0.10</annotation></semantics></math>, the scenario-translated FA/h for the assistive case (<math alttext=\"\\lambda=2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=2</annotation></semantics></math>/h) is <math alttext=\"\\sim\\!2.19\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>2.19</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!2.19</annotation></semantics></math> (SE <math alttext=\"\\approx\\!1.63\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m9\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8776;</mo><mn>1.63</mn></mrow><annotation encoding=\"application/x-tex\">\\approx\\!1.63</annotation></semantics></math>), corresponding to <math alttext=\"\\sim\\!13\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m10\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>13</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!13</annotation></semantics></math> alerts per correct detection. For reference, directly counting false positives per hour under the labelled test coverage yields <math alttext=\"\\sim\\!16.3\\,\\mathrm{FA/h}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m11\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mrow><mrow><mn>16.3</mn><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><mi>FA</mi></mrow><mo>/</mo><mi mathvariant=\"normal\">h</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\sim\\!16.3\\,\\mathrm{FA/h}</annotation></semantics></math> (seed-avg; SE <math alttext=\"\\approx\\!12.1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m12\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8776;</mo><mn>12.1</mn></mrow><annotation encoding=\"application/x-tex\">\\approx\\!12.1</annotation></semantics></math>). Under FA/h budgets (scenario scale, <math alttext=\"\\lambda=2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m13\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=2</annotation></semantics></math>/h), the model achieves recall <math alttext=\"\\sim\\!0.14\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m14\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>0.14</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!0.14</annotation></semantics></math> at 2.0 FA/h and <math alttext=\"\\sim\\!0.08\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m15\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>0.08</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!0.08</annotation></semantics></math> at 0.5 FA/h (seed-averaged). These numbers contextualise the ranking metrics and set a baseline for future improvements. The right panel of Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.F4\" title=\"Figure 4 &#8227; 4.2 Keyword Choice &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the mean recall&#8211;FA/h operating curve with per-seed traces. For this snapshot, operating points are chosen on the test PR curves for presentation; in deployment we would select thresholds on validation and freeze them before testing.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Non-invasive brain-computer interfaces (BCIs) are beginning to benefit from large, public benchmarks. However, current benchmarks target relatively simple, foundational tasks like Speech Detection and Phoneme Classification, while application-ready results on tasks like Brain-to-Text remain elusive. We propose Keyword Spotting (KWS) as a practically applicable, privacy-aware intermediate task. Using the deep 52-hour, within-subject LibriBrain corpus, we provide standardized train/validation/test splits for reproducible benchmarking, and adopt an evaluation protocol tailored to extreme class imbalance. Concretely, we use area under the precision-recall curve (AUPRC) as a robust evaluation metric, complemented by false alarms per hour (FA/h) at fixed recall to capture user-facing trade-offs. To simplify deployment and further experimentation within the research community, we are releasing an updated version of the <span class=\"ltx_text ltx_font_typewriter\">pnpl</span> library with word-level dataloaders and Colab-ready tutorials. As an initial reference model, we present a compact 1-D Conv/ResNet baseline with focal loss and top-<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> pooling that is trainable on a single consumer-class GPU. The reference model achieves <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math><math alttext=\"13\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"m3\" intent=\":literal\"><semantics><mrow><mn>13</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">13\\times</annotation></semantics></math> the permutation-baseline AUPRC on held-out sessions, demonstrating the viability of the task. Exploratory analyses reveal: (i) predictable within-subject scaling&#8212;performance improves log-linearly with more training hours&#8212;and (ii) the existence of word-level factors (frequency and duration) that systematically modulate detectability.</p>\n\n",
                "matched_terms": [
                    "auprc",
                    "use",
                    "model",
                    "from",
                    "metric",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The arrival of large and readily available datasets has begun to supply non-invasive brain-computer-interface (BCI) research with the kind of &#8220;common yard-stick&#8221; that ImageNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Russakovsky et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib45\" title=\"\">2015</a>)</cite> provided for computer vision. Among current non-invasive datasets for decoding speech, LibriBrain <cite class=\"ltx_cite ltx_citemacro_citep\">(&#214;zdogan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib42\" title=\"\">2025</a>)</cite> is the <span class=\"ltx_text ltx_font_italic\">deepest</span> (i.e., <span class=\"ltx_text ltx_font_italic\">largest within-subject</span>) with 52 hours of magnetoencephalography (MEG) recorded from a single participant. This dataset forms the foundation for the 2025 PNPL Competition <cite class=\"ltx_cite ltx_citemacro_citep\">(Landau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib30\" title=\"\">2025</a>)</cite>, an open machine-learning competition that has catalyzed progress on two foundational decoding tasks: Speech Detection and Phoneme Classification. Progress on these tasks can be seen by looking at the online leaderboards (<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://libribrain.com/\" title=\"\">https://libribrain.com/</a>). For example, in just two months F1 macro scores on the Speech Detection task advanced rapidly from about 68% to a new state-of-the-art on the (extended) public leaderboard of 96%. Our aim in this paper is to build on this success by introducing a new standardized task: Word Detection (a.k.a.&#160;Keyword Spotting). Substantively, we provide the same kinds of supporting infrastructure for this task (e.g., data loader, reference model, reproducible metrics, public leaderboard) which led directly to accelerated improvements in Speech Detection and Phoneme Classification.</p>\n\n",
                "matched_terms": [
                    "model",
                    "metrics",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As <cite class=\"ltx_cite ltx_citemacro_citet\">Landau et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib30\" title=\"\">2025</a>)</cite> explain, the two tasks in the 2025 PNPL Competition were selected for their simplicity. Over time, the idea was to increase the complexity, and utility, of benchmark decoding tasks. Keyword Spotting (KWS) is an exciting landmark as it represents the first decoding task on this curriculum with practical utility for BCIs. In the established domain of voice computing, Keyword Spotting is commonly used to detect &#8220;wake words&#8221; (e.g., &#8220;Hey Siri&#8221;, &#8220;Alexa&#8221;, &#8220;OK Google&#8221;). Wake words like these can be used to indicate that subsequent speech should be interpreted as a command, or they can be used themselves as commands. In the emerging domain of <span class=\"ltx_text ltx_font_italic\">brain</span> computing, even a single wake word (e.g., &#8220;help&#8221;) could be profoundly meaningful to someone with severe paralysis. Only slightly further along the curriculum, a small set of working keywords (e.g., &#8220;hungry&#8221;, &#8220;tired&#8221;, &#8220;thirsty&#8221;, &#8220;toilet&#8221;, &#8220;pain&#8221;) would transform their quality of life. The benchmark task established in this paper is intended, ultimately, to lead to such an outcome. For clarity, and to contrast the use of acoustic inputs, we use the term <span class=\"ltx_text ltx_font_italic\">Neural Keyword Spotting</span> to denote Keyword Spotting from continuous brain data, a task represented schematically in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S0.F1\" title=\"Figure 1 &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "use",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior to the release of LibriBrain <cite class=\"ltx_cite ltx_citemacro_citep\">(&#214;zdogan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib42\" title=\"\">2025</a>)</cite>, together with a standard Python library (<span class=\"ltx_text ltx_font_typewriter\">pnpl</span>) for loading predefined data splits <cite class=\"ltx_cite ltx_citemacro_citep\">(Landau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib30\" title=\"\">2025</a>)</cite>, a number of open datasets <cite class=\"ltx_cite ltx_citemacro_citep\">(e.g., Schoffelen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib51\" title=\"\">2019</a>; Nastase et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib40\" title=\"\">2022</a>; Gwilliams et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib19\" title=\"\">2023</a>)</cite> were starting to reappear across large-scale studies <cite class=\"ltx_cite ltx_citemacro_citep\">(e.g., D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib16\" title=\"\">2023</a>; d&#8217;Ascoli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib11\" title=\"\">2024</a>; Ridge and Parker&#160;Jones, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib43\" title=\"\">2024</a>; Jayalath et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib26\" title=\"\">2025b</a>)</cite>.\nHowever, data splits were not generally replicated making it difficult to compare methods. The same model architectures and weights were neither generally shared nor used as baselines and there were no public leaderboards.</p>\n\n",
                "matched_terms": [
                    "baselines",
                    "standard",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use area under the precision-recall curve (AUPRC) as the primary metric. This is well suited to keyword spotting because its base rate equals the empirical prevalence of positives, so gains are easy to interpret. It also summarises the trade-off we actually care about under heavy imbalance&#8212;how many of the system&#8217;s alarms are correct (precision) as we demand more coverage (recall). Finally, unlike alternative metrics (e.g., AUROC), AUPRC is not overly optimistic when precision is too low to be usable. We supplement AUPRC with additional metrics like AUROC to provide a comprehensive view.\nFor scenario-grounded reporting, we translate any validation-selected threshold (precision <math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math>, recall <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math>) into hourly rates under an assumed event frequency <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> (keywords/hour):</p>\n\n",
                "matched_terms": [
                    "auprc",
                    "use",
                    "metrics",
                    "auroc",
                    "metric",
                    "threshold"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We consider two illustrative use cases: assistive access (<math alttext=\"\\lambda\\!\\approx\\!2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo lspace=\"0.108em\" rspace=\"0.108em\">&#8776;</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda\\!\\approx\\!2</annotation></semantics></math>/h) and hands-free control (<math alttext=\"\\lambda\\!\\approx\\!10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo lspace=\"0.108em\" rspace=\"0.108em\">&#8776;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda\\!\\approx\\!10</annotation></semantics></math>/h). Thresholds <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m6\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> are chosen on validation either (a) by maximising recall under a false-alarm budget, or (b) by minimising FA/h subject to a target recall; selected <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m7\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> are then frozen for testing. For clarity, FA/h can also be computed directly from test-set coverage (false positives per hour of labelled windows); unless otherwise noted, we report the scenario-translated FA/h using <math alttext=\"(P,R,\\lambda)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m8\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>P</mi><mo>,</mo><mi>R</mi><mo>,</mo><mi>&#955;</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(P,R,\\lambda)</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "use",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our reference system ingests 306-channel MEG windows of length <span class=\"ltx_text ltx_font_italic\">T</span> and processes them with a compact temporal convolutional trunk that includes a residual block and a time-downsampling layer, yielding a\n<math alttext=\"128\\times\\textit{T'}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>128</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mtext class=\"ltx_mathvariant_italic\">T&#8217;</mtext></mrow><annotation encoding=\"application/x-tex\">128\\times\\textit{T'}</annotation></semantics></math> representation (temporal CNNs are strong sequence/biosignal decoders; residual connections stabilize deeper stacks and enlarge receptive fields efficiently <cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib4\" title=\"\">2018</a>; Schirrmeister et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib50\" title=\"\">2017</a>; Lawhern et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib32\" title=\"\">2018</a>; He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib23\" title=\"\">2016</a>)</cite>). A projection stage produces a 512-channel sequence, from which two <math alttext=\"1\\times 1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1\\times 1</annotation></semantics></math> temporal heads compute (i) per-time logits and (ii) attention scores normalised along time. The final output is a scalar logit obtained by attention-weighted summation of the per-time logits (a learned MIL-style pooling well-suited to brief events within longer windows <cite class=\"ltx_cite ltx_citemacro_citep\">(Ilse et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib24\" title=\"\">2018</a>; Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib28\" title=\"\">2020</a>; McFee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib37\" title=\"\">2018</a>)</cite>; in MEG, this lets the model emphasise time-locked acoustic/lexical responses such as M100/N400 components <cite class=\"ltx_cite ltx_citemacro_citep\">(Gage et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib17\" title=\"\">1998</a>; Halgren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib20\" title=\"\">2002</a>; Hari and Salmelin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib21\" title=\"\">2012</a>)</cite>). Training uses focal loss with a small pairwise ranking term: focal down-weights abundant easy negatives and focuses gradient on rare, hard positives under extreme imbalance <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib34\" title=\"\">2017</a>)</cite>, while the pairwise (logistic) ranking aux loss encourages correct ordering of positives above negatives, supporting PR/Average-Precision-aligned selection <cite class=\"ltx_cite ltx_citemacro_citep\">(Burges, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib7\" title=\"\">2010</a>; Yue et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib61\" title=\"\">2007</a>; Davis and Goadrich, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib13\" title=\"\">2006</a>; Saito and Rehmsmeier, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib47\" title=\"\">2015</a>)</cite>. Batches are class-balanced by oversampling positives, and we apply light temporal jitter and additive noise (both standard, effective regularizers for EEG/MEG time-series <cite class=\"ltx_cite ltx_citemacro_citep\">(Buda et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib6\" title=\"\">2018</a>; Lashgari et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib31\" title=\"\">2020</a>; He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib22\" title=\"\">2021</a>; Rommel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib44\" title=\"\">2022</a>)</cite>). We optimise with AdamW <cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov and Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib35\" title=\"\">2019</a>)</cite> and select checkpoints by validation AUPRC (preferred under heavy class imbalance <cite class=\"ltx_cite ltx_citemacro_citep\">(Saito and Rehmsmeier, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib47\" title=\"\">2015</a>; Davis and Goadrich, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib13\" title=\"\">2006</a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "model",
                    "standard",
                    "from",
                    "auprc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.F5\" title=\"Figure 5 &#8227; 4.3 Data Scaling &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, AUPRC improves approximately log-linearly as we increase the training fraction from 10% to 100%, consistent with established within-subject scaling laws in neural decoding <cite class=\"ltx_cite ltx_citemacro_citep\">(d&#8217;Ascoli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib11\" title=\"\">2024</a>; Sato et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib49\" title=\"\">2024</a>)</cite>. Notably, even with just 10% of the training data (<math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>14&#8201;h windowed; <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m2\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>5.2&#8201;h unique), the model achieves meaningful performance above chance, suggesting that keyword detection remains feasible even in scenarios with limited recording time. Permutation tests confirm that AUPRC is not above chance at 5% (<math alttext=\"p=0.108\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.108</mn></mrow><annotation encoding=\"application/x-tex\">p=0.108</annotation></semantics></math>), but is already significant at 10% (<math alttext=\"p=0.0156\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.0156</mn></mrow><annotation encoding=\"application/x-tex\">p=0.0156</annotation></semantics></math>; one-sided, 10,000 draws), and remains strongly significant thereafter (20% <math alttext=\"p=6.0\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mrow><mn>6.0</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">p=6.0\\times 10^{-4}</annotation></semantics></math>; 40&#8211;100% <math alttext=\"p\\leq 2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m6\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&#8804;</mo><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">p\\leq 2\\times 10^{-4}</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "auprc",
                    "model",
                    "permutation",
                    "from",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Adding pre- and post-onset offsets modestly improves detection. Averaged over all non-zero offsets, AUPRC increases by <math alttext=\"\\sim\\!25\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mrow><mn>25</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\sim\\!25\\%</annotation></semantics></math> relative to the 0/0 baseline (absolute +0.0097). This improvement is statistically significant (paired per-seed mean +0.0099 AUPRC, SE 0.0028; 95% CI [0.0045, 0.0154]; one-sided <math alttext=\"p&lt;10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">p&lt;10^{-3}</annotation></semantics></math>). We observe the best AUPRC near (neg=0.1s, pos=0.3s), but no clear monotonic trend across offsets; performance is relatively flat in a small neighbourhood around this setting and declines for very short or overly long windows, suggesting a bias&#8211;variance trade-off between providing sufficient context and diluting signal with unrelated activity.</p>\n\n",
                "matched_terms": [
                    "improvement",
                    "baseline",
                    "auprc",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a reproducible MEG keyword-spotting task on LibriBrain, demonstrate meaningful signal, and release task specifications, a modified <span class=\"ltx_text ltx_font_typewriter\">pnpl</span> library, baseline model, and tutorial materials.</p>\n\n",
                "matched_terms": [
                    "baseline",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the achieved improvement in metrics, performance is not yet sufficient for reliable hands-free use. In an assistive scenario (<math alttext=\"\\lambda{=}2\\,\\mathrm{h}^{-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mrow><mn>2</mn><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><msup><mi mathvariant=\"normal\">h</mi><mrow><mo>&#8722;</mo><mn>1</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\lambda{=}2\\,\\mathrm{h}^{-1}</annotation></semantics></math>), at recall <math alttext=\"\\approx\\!0.10\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8776;</mo><mn>0.10</mn></mrow><annotation encoding=\"application/x-tex\">\\approx\\!0.10</annotation></semantics></math> the system yields <math alttext=\"\\approx\\!2.2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8776;</mo><mn>2.2</mn></mrow><annotation encoding=\"application/x-tex\">\\approx\\!2.2</annotation></semantics></math> false alarms per hour (about 13 alerts per correct detection). Priorities for future work thus include: (i) stronger ranking, (ii) calibration and principled threshold selection, and (iii) deployment strategies that suppress false alarms (multi-confirmation, small ensembles, cascaded detectors with context-aware priors).</p>\n\n",
                "matched_terms": [
                    "improvement",
                    "use",
                    "metrics",
                    "threshold",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study reports results for a single participant on a single corpus. Generalisability across participants remains an open question for Neural Keyword Spotting, though generalization is becoming less of a problem in decoding than it was <cite class=\"ltx_cite ltx_citemacro_citep\">(Csaky et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib10\" title=\"\">2023</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib16\" title=\"\">2023</a>; d&#8217;Ascoli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib11\" title=\"\">2024</a>; Jayalath et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib26\" title=\"\">2025b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib25\" title=\"\">a</a>)</cite>. Validation and test sessions were selected to maximise positives, stabilising metrics at the expense of a mild base-rate bias. Test sets contain few positives, so thresholded metrics carry substantial uncertainty. We evaluate a compact model only, without exploring richer encoders, self-supervised pretraining, streaming inference, or multi-keyword training. Finally, we use event-referenced windows; continuous-stream detection with latency and explicit false-alarm accounting remains open.</p>\n\n",
                "matched_terms": [
                    "model",
                    "thresholded",
                    "use",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To encourage further exploration within the community, we also release a tutorial in the format of a Jupyter Notebook. Within the compute limits of the Colab Free Tier (<span class=\"ltx_text ltx_font_typewriter\">T4</span> GPU), the notebook allows for training a model around 10% of the LibriBrain dataset, reaching significantly above chance performance in under 30 minutes.\nThe notebook is available in the <span class=\"ltx_text ltx_font_typewriter\">tutorial</span> folder of the <span class=\"ltx_text ltx_font_typewriter\">keyword-experiments</span> repository<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>http://github.com/neural-processing-lab/libribrain-keyword-experiments</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            }
        ]
    },
    "A3.T2": {
        "caption": "Table 2: Detailed scaling results for keyword detection across training fractions (seed-averaged over three runs). Standard errors are approximated from 95% bootstrap CIs as SE≈(CIhi−CIlo)/3.92\\mathrm{SE}\\approx(\\mathrm{CI}_{\\mathrm{hi}}-\\mathrm{CI}_{\\mathrm{lo}})/3.92. P-values are from one-sided permutation tests of the seed-average AUPRC against the null. The base rate for the fixed test set is 0.00515.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training fraction</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">AUPRC</span><span class=\"ltx_text\" style=\"font-size:90%;\"> (</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T2.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> SE)</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">AUROC</span><span class=\"ltx_text\" style=\"font-size:90%;\"> (</span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T2.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> SE)</span>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">AUPRC p-value</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">AUPRC/base rate (<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T2.m3\" intent=\":literal\"><semantics><mo>&#215;</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">5%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.009 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T2.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.003</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.626 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T2.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.058</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.108</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.69</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">10%</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.019 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T2.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.009</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.746 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T2.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.049</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.0156</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.67</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">20%</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.027 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T2.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.020</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.733 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T2.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.050</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"6.0\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T2.m10\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">6.0</mn><mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo><msup><mn mathsize=\"0.900em\">10</mn><mrow><mo mathsize=\"0.900em\">&#8722;</mo><mn mathsize=\"0.900em\">4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">6.0\\times 10^{-4}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.18</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">40%</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.044 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T2.m11\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.027</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.782 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T2.m12\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.046</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"&lt;5\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T2.m13\" intent=\":literal\"><semantics><mrow><mi/><mo mathsize=\"0.900em\">&lt;</mo><mrow><mn mathsize=\"0.900em\">5</mn><mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo><msup><mn mathsize=\"0.900em\">10</mn><mrow><mo mathsize=\"0.900em\">&#8722;</mo><mn mathsize=\"0.900em\">5</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">&lt;5\\times 10^{-5}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.59</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">60%</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.032 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T2.m14\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.016</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.784 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T2.m15\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.046</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"2.0\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T2.m16\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">2.0</mn><mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo><msup><mn mathsize=\"0.900em\">10</mn><mrow><mo mathsize=\"0.900em\">&#8722;</mo><mn mathsize=\"0.900em\">4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">2.0\\times 10^{-4}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.28</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">80%</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.048 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T2.m17\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.029</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.796 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T2.m18\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.047</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"&lt;5\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T2.m19\" intent=\":literal\"><semantics><mrow><mi/><mo mathsize=\"0.900em\">&lt;</mo><mrow><mn mathsize=\"0.900em\">5</mn><mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo><msup><mn mathsize=\"0.900em\">10</mn><mrow><mo mathsize=\"0.900em\">&#8722;</mo><mn mathsize=\"0.900em\">5</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">&lt;5\\times 10^{-5}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.37</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">100%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.045 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T2.m20\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.021</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.834 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T2.m21\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.037</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><math alttext=\"&lt;5\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T2.m22\" intent=\":literal\"><semantics><mrow><mi/><mo mathsize=\"0.900em\">&lt;</mo><mrow><mn mathsize=\"0.900em\">5</mn><mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">&#215;</mo><msup><mn mathsize=\"0.900em\">10</mn><mrow><mo mathsize=\"0.900em\">&#8722;</mo><mn mathsize=\"0.900em\">5</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">&lt;5\\times 10^{-5}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.66</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "pvalue",
            "5×10−55times",
            "null",
            "scaling",
            "detection",
            "auprcbase",
            "bootstrap",
            "base",
            "against",
            "rate",
            "seedaveraged",
            "20×10−420times",
            "cis",
            "runs",
            "from",
            "training",
            "seedaverage",
            "test",
            "fraction",
            "fractions",
            "over",
            "across",
            "se≈cihi−cilo392mathrmseapproxmathrmcimathrmhimathrmcimathrmlo392",
            "×times",
            "tests",
            "auroc",
            "results",
            "onesided",
            "approximated",
            "errors",
            "pvalues",
            "set",
            "fixed",
            "auprc",
            "standard",
            "three",
            "60×10−460times",
            "permutation",
            "±pm",
            "keyword",
            "detailed"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Non-invasive brain-computer interfaces (BCIs) are beginning to benefit from large, public benchmarks. However, current benchmarks target relatively simple, foundational tasks like Speech Detection and Phoneme Classification, while application-ready results on tasks like Brain-to-Text remain elusive. We propose Keyword Spotting (KWS) as a practically applicable, privacy-aware intermediate task. Using the deep 52-hour, within-subject LibriBrain corpus, we provide standardized train/validation/test splits for reproducible benchmarking, and adopt an evaluation protocol tailored to extreme class imbalance. Concretely, we use area under the precision-recall curve (AUPRC) as a robust evaluation metric, complemented by false alarms per hour (FA/h) at fixed recall to capture user-facing trade-offs. To simplify deployment and further experimentation within the research community, we are releasing an updated version of the <span class=\"ltx_text ltx_font_typewriter\">pnpl</span> library with word-level dataloaders and Colab-ready tutorials. As an initial reference model, we present a compact 1-D Conv/ResNet baseline with focal loss and top-<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> pooling that is trainable on a single consumer-class GPU. The reference model achieves <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math><math alttext=\"13\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"m3\" intent=\":literal\"><semantics><mrow><mn>13</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">13\\times</annotation></semantics></math> the permutation-baseline AUPRC on held-out sessions, demonstrating the viability of the task. Exploratory analyses reveal: (i) predictable within-subject scaling&#8212;performance improves log-linearly with more training hours&#8212;and (ii) the existence of word-level factors (frequency and duration) that systematically modulate detectability.</p>\n\n",
                "matched_terms": [
                    "fixed",
                    "auprc",
                    "detection",
                    "from",
                    "training",
                    "results",
                    "keyword"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The arrival of large and readily available datasets has begun to supply non-invasive brain-computer-interface (BCI) research with the kind of &#8220;common yard-stick&#8221; that ImageNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Russakovsky et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib45\" title=\"\">2015</a>)</cite> provided for computer vision. Among current non-invasive datasets for decoding speech, LibriBrain <cite class=\"ltx_cite ltx_citemacro_citep\">(&#214;zdogan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib42\" title=\"\">2025</a>)</cite> is the <span class=\"ltx_text ltx_font_italic\">deepest</span> (i.e., <span class=\"ltx_text ltx_font_italic\">largest within-subject</span>) with 52 hours of magnetoencephalography (MEG) recorded from a single participant. This dataset forms the foundation for the 2025 PNPL Competition <cite class=\"ltx_cite ltx_citemacro_citep\">(Landau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib30\" title=\"\">2025</a>)</cite>, an open machine-learning competition that has catalyzed progress on two foundational decoding tasks: Speech Detection and Phoneme Classification. Progress on these tasks can be seen by looking at the online leaderboards (<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://libribrain.com/\" title=\"\">https://libribrain.com/</a>). For example, in just two months F1 macro scores on the Speech Detection task advanced rapidly from about 68% to a new state-of-the-art on the (extended) public leaderboard of 96%. Our aim in this paper is to build on this success by introducing a new standardized task: Word Detection (a.k.a.&#160;Keyword Spotting). Substantively, we provide the same kinds of supporting infrastructure for this task (e.g., data loader, reference model, reproducible metrics, public leaderboard) which led directly to accelerated improvements in Speech Detection and Phoneme Classification.</p>\n\n",
                "matched_terms": [
                    "from",
                    "keyword",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As <cite class=\"ltx_cite ltx_citemacro_citet\">Landau et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib30\" title=\"\">2025</a>)</cite> explain, the two tasks in the 2025 PNPL Competition were selected for their simplicity. Over time, the idea was to increase the complexity, and utility, of benchmark decoding tasks. Keyword Spotting (KWS) is an exciting landmark as it represents the first decoding task on this curriculum with practical utility for BCIs. In the established domain of voice computing, Keyword Spotting is commonly used to detect &#8220;wake words&#8221; (e.g., &#8220;Hey Siri&#8221;, &#8220;Alexa&#8221;, &#8220;OK Google&#8221;). Wake words like these can be used to indicate that subsequent speech should be interpreted as a command, or they can be used themselves as commands. In the emerging domain of <span class=\"ltx_text ltx_font_italic\">brain</span> computing, even a single wake word (e.g., &#8220;help&#8221;) could be profoundly meaningful to someone with severe paralysis. Only slightly further along the curriculum, a small set of working keywords (e.g., &#8220;hungry&#8221;, &#8220;tired&#8221;, &#8220;thirsty&#8221;, &#8220;toilet&#8221;, &#8220;pain&#8221;) would transform their quality of life. The benchmark task established in this paper is intended, ultimately, to lead to such an outcome. For clarity, and to contrast the use of acoustic inputs, we use the term <span class=\"ltx_text ltx_font_italic\">Neural Keyword Spotting</span> to denote Keyword Spotting from continuous brain data, a task represented schematically in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S0.F1\" title=\"Figure 1 &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "over",
                    "from",
                    "keyword"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior to the release of LibriBrain <cite class=\"ltx_cite ltx_citemacro_citep\">(&#214;zdogan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib42\" title=\"\">2025</a>)</cite>, together with a standard Python library (<span class=\"ltx_text ltx_font_typewriter\">pnpl</span>) for loading predefined data splits <cite class=\"ltx_cite ltx_citemacro_citep\">(Landau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib30\" title=\"\">2025</a>)</cite>, a number of open datasets <cite class=\"ltx_cite ltx_citemacro_citep\">(e.g., Schoffelen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib51\" title=\"\">2019</a>; Nastase et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib40\" title=\"\">2022</a>; Gwilliams et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib19\" title=\"\">2023</a>)</cite> were starting to reappear across large-scale studies <cite class=\"ltx_cite ltx_citemacro_citep\">(e.g., D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib16\" title=\"\">2023</a>; d&#8217;Ascoli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib11\" title=\"\">2024</a>; Ridge and Parker&#160;Jones, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib43\" title=\"\">2024</a>; Jayalath et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib26\" title=\"\">2025b</a>)</cite>.\nHowever, data splits were not generally replicated making it difficult to compare methods. The same model architectures and weights were neither generally shared nor used as baselines and there were no public leaderboards.</p>\n\n",
                "matched_terms": [
                    "across",
                    "standard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Brain-to-Text (B2T)</span> takes variable-length neural sequences (e.g., EEG, MEG, fMRI) as input and outputs text transcripts, typically evaluated with word error rate (WER) or semantic similarity metrics such as the BERTScore <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib62\" title=\"\">2020</a>)</cite>. B2T is the analogue of ASR and represents a long-term goal, though its difficulty has motivated the development of what we call intermediate tasks, which lie between more foundational tasks like Speech Detection and full B2T. Non-invasive B2T has been explored with EEG and MEG <cite class=\"ltx_cite ltx_citemacro_citep\">(Duan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib15\" title=\"\">2023</a>; Jo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib27\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib59\" title=\"\">2024b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib60\" title=\"\">c</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib58\" title=\"\">a</a>)</cite>, with semantic metrics often reported in place of WER, although recent work shows that competitive WERs are beginning to be achievable non-invasively <cite class=\"ltx_cite ltx_citemacro_citep\">(Jayalath et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib25\" title=\"\">2025a</a>)</cite>. In fMRI, the coarse temporal resolution makes word-level alignment unlikely, though remarkable paraphrases have been produced which retain some semantic similarities to the ground truth speech <cite class=\"ltx_cite ltx_citemacro_citep\">(e.g., Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib52\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Keyword spotting in the traditional audio domain (also referred to as wake-word detection) is a mature, highly-imbalanced detection problem optimizsed for very low false-alarm (FA) rates at fixed recall. Early small-footprint CNN and CRNN systems established the modern operating regime (e.g., 0.5 FA/h at acceptable FRR) under tight on-device constraints\n<cite class=\"ltx_cite ltx_citemacro_citep\">(e.g.,  Sainath and Parada, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib46\" title=\"\">2015</a>; Ar&#237;k et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib3\" title=\"\">2017</a>)</cite>. Large benchmarks like Speech Commands <cite class=\"ltx_cite ltx_citemacro_citep\">(Warden, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib55\" title=\"\">2018</a>)</cite>) and efficient architectures like MatchboxNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Maidina et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib36\" title=\"\">2020</a>)</cite> and Keyword Transformer <cite class=\"ltx_cite ltx_citemacro_citep\">(Berg et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib5\" title=\"\">2021</a>)</cite> further drove accuracy/latency trade-offs for embedded devices, while industrial deployments (e.g., Apple&#8217;s &#8220;Hey Siri&#8221;) codified evaluation practices around FA/h and user-centric thresholds <cite class=\"ltx_cite ltx_citemacro_citep\">(Apple Siri Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib2\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "fixed",
                    "keyword",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the invasive brain side, <cite class=\"ltx_cite ltx_citemacro_citet\">Milsap et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib39\" title=\"\">2019</a>)</cite> introduced neural KWS with ECoG, showing low-latency, high-specificity detection using matched-filter templates spanning motor and auditory speech representations.\nRecent intracortical studies push to large-vocabulary online decoding and inner-speech control, but their goals (continuous B2T, WER/CER) and signal quality differ materially from non-invasive KWS <cite class=\"ltx_cite ltx_citemacro_citep\">(Willett et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib57\" title=\"\">2023</a>; Metzger et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib38\" title=\"\">2023</a>; Kunz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib29\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "from",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Non-invasive technologies (EEG/MEG) have dramatic benefits over surgical implants in terms of safety and scalability. The application of keyword spotting is motivated by two converging strands. First, segment identification decoders trained to predict self-supervised speech representations from brain signals reliably retrieve the matching few-second stimulus among large candidate sets and generalise across participants - evidence that non-invasive signals carry phonetic/lexical detail at the granularity needed for lexical identification <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib16\" title=\"\">2023</a>; d&#8217;Ascoli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib11\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "over",
                    "across",
                    "from",
                    "keyword"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, converging MEG/EEG results show sensitivity to phoneme sequence structure and higher-level linguistic content, and recent deep models capture meaningful portions of the speech-to-language transform in these signals <cite class=\"ltx_cite ltx_citemacro_citep\">(Gwilliams et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib18\" title=\"\">2022</a>; Tezcan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib53\" title=\"\">2023</a>; Desai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib14\" title=\"\">2021</a>)</cite>.\nAgainst this recent progress, LibriBrain allows testing whether the same brain-speech representations that enable segment retrieval also support lexical selectivity for pre-specified words in its long-form, naturalistic stories. Due to its larger scale, it also allows building on prior EEG-based KWS pilots, which have largely remained at small-lexicon trialwise classification/onset detection <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakthi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib48\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "against",
                    "results",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The release spans 93 sessions (3,139&#8201;min; 52.32&#8201;h) with 466,230 word tokens (16,892 unique) and 1,511,732 phoneme tokens. See Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#A2.F6\" title=\"Figure 6 &#8227; Appendix B Dataset Figures &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#A2.F7\" title=\"Figure 7 &#8227; Appendix B Dataset Figures &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> for an overview of the dataset. Word frequencies are Zipfian, providing keywords across a wide base-rate spectrum (short, frequent function words vs. longer, rarer content/proper names). These properties suit event-referenced keyword detection with extreme class imbalance.</p>\n\n",
                "matched_terms": [
                    "across",
                    "keyword",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We cast neural keyword spotting (KWS) from MEG as an event-referenced detection task using LibriBrain word onsets <cite class=\"ltx_cite ltx_citemacro_citep\">(&#214;zdogan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib42\" title=\"\">2025</a>)</cite>. This can be formalized as follows: First, we fix a small keyword set <math alttext=\"\\mathcal{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><annotation encoding=\"application/x-tex\">\\mathcal{V}</annotation></semantics></math> (minimally <math alttext=\"|\\mathcal{V}|=1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">|\\mathcal{V}|=1</annotation></semantics></math>). For each keyword <math alttext=\"k\\in\\mathcal{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi></mrow><annotation encoding=\"application/x-tex\">k\\in\\mathcal{V}</annotation></semantics></math>, let <math alttext=\"d_{\\max}(k)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>d</mi><mi>max</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">d_{\\max}(k)</annotation></semantics></math> be the maximum duration of any instance of <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> in the corpus. Given that we may want to extract brain recordings that start and end before and after audio event boundaries (e.g., because neural processing continues after the presentation of a stimulus), we can select fixed pre/post buffers <math alttext=\"\\beta^{-}\\!\\geq 0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m6\" intent=\":literal\"><semantics><mrow><msup><mi>&#946;</mi><mo>&#8722;</mo></msup><mo lspace=\"0.108em\">&#8805;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\beta^{-}\\!\\geq 0</annotation></semantics></math> and <math alttext=\"\\beta^{+}\\!\\geq 0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m7\" intent=\":literal\"><semantics><mrow><msup><mi>&#946;</mi><mo>+</mo></msup><mo lspace=\"0.108em\">&#8805;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\beta^{+}\\!\\geq 0</annotation></semantics></math>.\nThese offsets can then be used to define <math alttext=\"D(\\mathcal{V})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m8\" intent=\":literal\"><semantics><mrow><mi>D</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">D(\\mathcal{V})</annotation></semantics></math>, which is the total window duration (in seconds) of neural data to extract around any keyword in <math alttext=\"\\mathcal{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m9\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><annotation encoding=\"application/x-tex\">\\mathcal{V}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "set",
                    "fixed",
                    "detection",
                    "from",
                    "keyword"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to KWS, full-vocabulary Brain-to-Text aims to identify <math alttext=\"w\\in\\mathcal{W}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><mi>w</mi><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119986;</mi></mrow><annotation encoding=\"application/x-tex\">w\\in\\mathcal{W}</annotation></semantics></math> (hundreds of thousands of words), which introduces severe long-tail sparsity and requires calibrating thresholds across many classes. KWS is a practical, fixed-lexicon task: it asks only whether any member of a small, predefined set <math alttext=\"\\mathcal{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><annotation encoding=\"application/x-tex\">\\mathcal{V}</annotation></semantics></math> occurred. This offers a simple, reliable trigger with clean control over latency and false alarms&#8212;useful both on its own (assistive or hands-free commands) and as a stepping stone towards richer decoders.</p>\n\n",
                "matched_terms": [
                    "set",
                    "over",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By default, we adopt LibriBrain&#8217;s session-level train/val/test split <cite class=\"ltx_cite ltx_citemacro_citep\">(&#214;zdogan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib42\" title=\"\">2025</a>)</cite>. For a chosen set of keywords <math alttext=\"\\mathcal{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><annotation encoding=\"application/x-tex\">\\mathcal{V}</annotation></semantics></math>, we verify that positives occur in both validation and test. If not, we replace them with the two sessions containing the most positives for <math alttext=\"\\mathcal{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><annotation encoding=\"application/x-tex\">\\mathcal{V}</annotation></semantics></math>.\nThis ensures sufficient positive examples for reliable metric computation, particularly important given the extreme class imbalance in keyword detection. For a session <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> containing word tokens with strings <math alttext=\"\\{s_{i}\\}_{i=1}^{n_{S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m4\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>n</mi><mi>S</mi></msub></msubsup><annotation encoding=\"application/x-tex\">\\{s_{i}\\}_{i=1}^{n_{S}}</annotation></semantics></math>, let</p>\n\n",
                "matched_terms": [
                    "set",
                    "keyword",
                    "detection",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">be the count of keyword instances in session <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m5\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math>. Validation and test are set to the two sessions maximizing <math alttext=\"c_{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m6\" intent=\":literal\"><semantics><msub><mi>c</mi><mi>S</mi></msub><annotation encoding=\"application/x-tex\">c_{S}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "keyword",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use area under the precision-recall curve (AUPRC) as the primary metric. This is well suited to keyword spotting because its base rate equals the empirical prevalence of positives, so gains are easy to interpret. It also summarises the trade-off we actually care about under heavy imbalance&#8212;how many of the system&#8217;s alarms are correct (precision) as we demand more coverage (recall). Finally, unlike alternative metrics (e.g., AUROC), AUPRC is not overly optimistic when precision is too low to be usable. We supplement AUPRC with additional metrics like AUROC to provide a comprehensive view.\nFor scenario-grounded reporting, we translate any validation-selected threshold (precision <math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math>, recall <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math>) into hourly rates under an assumed event frequency <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> (keywords/hour):</p>\n\n",
                "matched_terms": [
                    "auprc",
                    "base",
                    "rate",
                    "auroc",
                    "keyword"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our reference system ingests 306-channel MEG windows of length <span class=\"ltx_text ltx_font_italic\">T</span> and processes them with a compact temporal convolutional trunk that includes a residual block and a time-downsampling layer, yielding a\n<math alttext=\"128\\times\\textit{T'}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>128</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mtext class=\"ltx_mathvariant_italic\">T&#8217;</mtext></mrow><annotation encoding=\"application/x-tex\">128\\times\\textit{T'}</annotation></semantics></math> representation (temporal CNNs are strong sequence/biosignal decoders; residual connections stabilize deeper stacks and enlarge receptive fields efficiently <cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib4\" title=\"\">2018</a>; Schirrmeister et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib50\" title=\"\">2017</a>; Lawhern et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib32\" title=\"\">2018</a>; He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib23\" title=\"\">2016</a>)</cite>). A projection stage produces a 512-channel sequence, from which two <math alttext=\"1\\times 1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1\\times 1</annotation></semantics></math> temporal heads compute (i) per-time logits and (ii) attention scores normalised along time. The final output is a scalar logit obtained by attention-weighted summation of the per-time logits (a learned MIL-style pooling well-suited to brief events within longer windows <cite class=\"ltx_cite ltx_citemacro_citep\">(Ilse et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib24\" title=\"\">2018</a>; Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib28\" title=\"\">2020</a>; McFee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib37\" title=\"\">2018</a>)</cite>; in MEG, this lets the model emphasise time-locked acoustic/lexical responses such as M100/N400 components <cite class=\"ltx_cite ltx_citemacro_citep\">(Gage et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib17\" title=\"\">1998</a>; Halgren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib20\" title=\"\">2002</a>; Hari and Salmelin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib21\" title=\"\">2012</a>)</cite>). Training uses focal loss with a small pairwise ranking term: focal down-weights abundant easy negatives and focuses gradient on rare, hard positives under extreme imbalance <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib34\" title=\"\">2017</a>)</cite>, while the pairwise (logistic) ranking aux loss encourages correct ordering of positives above negatives, supporting PR/Average-Precision-aligned selection <cite class=\"ltx_cite ltx_citemacro_citep\">(Burges, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib7\" title=\"\">2010</a>; Yue et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib61\" title=\"\">2007</a>; Davis and Goadrich, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib13\" title=\"\">2006</a>; Saito and Rehmsmeier, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib47\" title=\"\">2015</a>)</cite>. Batches are class-balanced by oversampling positives, and we apply light temporal jitter and additive noise (both standard, effective regularizers for EEG/MEG time-series <cite class=\"ltx_cite ltx_citemacro_citep\">(Buda et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib6\" title=\"\">2018</a>; Lashgari et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib31\" title=\"\">2020</a>; He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib22\" title=\"\">2021</a>; Rommel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib44\" title=\"\">2022</a>)</cite>). We optimise with AdamW <cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov and Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib35\" title=\"\">2019</a>)</cite> and select checkpoints by validation AUPRC (preferred under heavy class imbalance <cite class=\"ltx_cite ltx_citemacro_citep\">(Saito and Rehmsmeier, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib47\" title=\"\">2015</a>; Davis and Goadrich, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib13\" title=\"\">2006</a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "auprc",
                    "standard",
                    "from",
                    "training"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Where possible, all experiments use the standard train/validation/test splits provided by the <span class=\"ltx_text ltx_font_typewriter\">pnpl</span> dataset (using the logic described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S3.SS4\" title=\"3.4 Reference Model &#8227; 3 Methods &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a>) and are fully reproducible (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#A1\" title=\"Appendix A Open Resources: Code, Tutorial, and Leaderboard &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>). Unless noted, values are seed-averages over three runs. Error bars are standard errors across seeds. For Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Model Performance &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> we report standard errors approximated from 95% bootstrap CIs (4,000 resamples).</p>\n\n",
                "matched_terms": [
                    "over",
                    "across",
                    "standard",
                    "three",
                    "cis",
                    "runs",
                    "from",
                    "approximated",
                    "errors",
                    "bootstrap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first establish that the dataset carries usable signal for keyword detection by evaluating a single model on the held-out test set (<math alttext=\"n=4660\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>4660</mn></mrow><annotation encoding=\"application/x-tex\">n=4660</annotation></semantics></math>, positives <math alttext=\"=24\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>=</mo><mn>24</mn></mrow><annotation encoding=\"application/x-tex\">=24</annotation></semantics></math>; base rate <math alttext=\"=0.00515\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>=</mo><mn>0.00515</mn></mrow><annotation encoding=\"application/x-tex\">=0.00515</annotation></semantics></math>). Given the absence of prior publicly reproducible MEG keyword spotting benchmarks, we evaluate against permutation-derived random baselines to demonstrate the presence of meaningful signal rather than competitive performance. While overall performance is modest, threshold-free metrics indicate clear signal (AUPRC <math alttext=\"\\approx 13.4\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mo>&#8776;</mo><mn>13.4</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">\\approx 13.4\\times</annotation></semantics></math> the permutation baseline; AUROC <math alttext=\"\\approx 0.80\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.80</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.80</annotation></semantics></math>). Full results are provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Model Performance &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nBeyond threshold-free metrics, we include an operational snapshot. At a target recall of <math alttext=\"\\sim\\!0.10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>0.10</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!0.10</annotation></semantics></math>, the scenario-translated FA/h for the assistive case (<math alttext=\"\\lambda=2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=2</annotation></semantics></math>/h) is <math alttext=\"\\sim\\!2.19\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>2.19</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!2.19</annotation></semantics></math> (SE <math alttext=\"\\approx\\!1.63\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m9\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8776;</mo><mn>1.63</mn></mrow><annotation encoding=\"application/x-tex\">\\approx\\!1.63</annotation></semantics></math>), corresponding to <math alttext=\"\\sim\\!13\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m10\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>13</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!13</annotation></semantics></math> alerts per correct detection. For reference, directly counting false positives per hour under the labelled test coverage yields <math alttext=\"\\sim\\!16.3\\,\\mathrm{FA/h}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m11\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mrow><mrow><mn>16.3</mn><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><mi>FA</mi></mrow><mo>/</mo><mi mathvariant=\"normal\">h</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\sim\\!16.3\\,\\mathrm{FA/h}</annotation></semantics></math> (seed-avg; SE <math alttext=\"\\approx\\!12.1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m12\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8776;</mo><mn>12.1</mn></mrow><annotation encoding=\"application/x-tex\">\\approx\\!12.1</annotation></semantics></math>). Under FA/h budgets (scenario scale, <math alttext=\"\\lambda=2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m13\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=2</annotation></semantics></math>/h), the model achieves recall <math alttext=\"\\sim\\!0.14\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m14\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>0.14</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!0.14</annotation></semantics></math> at 2.0 FA/h and <math alttext=\"\\sim\\!0.08\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m15\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>0.08</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!0.08</annotation></semantics></math> at 0.5 FA/h (seed-averaged). These numbers contextualise the ranking metrics and set a baseline for future improvements. The right panel of Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.F4\" title=\"Figure 4 &#8227; 4.2 Keyword Choice &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the mean recall&#8211;FA/h operating curve with per-seed traces. For this snapshot, operating points are chosen on the test PR curves for presentation; in deployment we would select thresholds on validation and freeze them before testing.</p>\n\n",
                "matched_terms": [
                    "set",
                    "auprc",
                    "base",
                    "against",
                    "rate",
                    "detection",
                    "permutation",
                    "seedaveraged",
                    "auroc",
                    "keyword",
                    "results",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An important consideration in KWS is the choise of keyword(s). In LibriBrain, as in many real-world corpora, longer words are rarer: word length in phonemes is negatively correlated with token frequency (Spearman <math alttext=\"r=-0.28\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mrow><mo>&#8722;</mo><mn>0.28</mn></mrow></mrow><annotation encoding=\"application/x-tex\">r=-0.28</annotation></semantics></math> with <math alttext=\"\\log\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\"><semantics><mi>log</mi><annotation encoding=\"application/x-tex\">\\log</annotation></semantics></math> frequency; <math alttext=\"p=2.7\\times 10^{-185}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mrow><mn>2.7</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>185</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">p=2.7\\times 10^{-185}</annotation></semantics></math>; Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.F3\" title=\"Figure 3 &#8227; 4.2 Keyword Choice &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> left). This matters because length can reduce false alarms while frequency controls how many positives we can realistically train on. To navigate this trade-off, we selected the most frequent word at each phoneme length and measured %<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math>AUPRC over the empirical base rate. The length-%<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m5\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math>AUPRC relation is non-monotonic (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.F3\" title=\"Figure 3 &#8227; 4.2 Keyword Choice &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> right): among a 12-item shortlist spanning 1-12 phonemes, the 5-phoneme <em class=\"ltx_emph ltx_font_italic\">watson</em> yields the largest %<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m6\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math>AUPRC, whereas several longer items (e.g., 9-12 phonemes) underperform despite greater duration.</p>\n\n",
                "matched_terms": [
                    "base",
                    "over",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A controlled comparison across three similarly frequent keywords of different lengths (<em class=\"ltx_emph ltx_font_italic\">walk</em>, <em class=\"ltx_emph ltx_font_italic\">surely</em>, <em class=\"ltx_emph ltx_font_italic\">excellent</em>; 3/5/8 phonemes) shows no detectable difference in %<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math>AUPRC within our precision (overlapping SEMs; Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.F4\" title=\"Figure 4 &#8227; 4.2 Keyword Choice &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). This indicates that, once frequency is matched, mere length is not the primary driver of detectability in MEG KWS. The pattern is consistent with established constraints on neural speech processing and KWS: benefits accrue less from duration per se and more from properties that improve time-locking and reduce lexical competition&#8212;salient acoustic onsets and early stress (stronger M100/M200), an early uniqueness point (UP)&#8212;i.e., the keyword becomes lexically unique after only a few initial phonemes (a small UP index relative to its length)&#8212;a sparse phonological neighborhood, and moderate frequency with lower contextual predictability <cite class=\"ltx_cite ltx_citemacro_citep\">(Gage et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib17\" title=\"\">1998</a>; Leminen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib33\" title=\"\">2011</a>; Vitevitch and Luce, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib54\" title=\"\">1999</a>; Halgren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib20\" title=\"\">2002</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib9\" title=\"\">2014</a>)</cite>. Empirically, <em class=\"ltx_emph ltx_font_italic\">watson</em> may profit from prosodic prominence in narrative speech and an early uniqueness point, outweighing any gains attributable to length alone; <span class=\"ltx_text ltx_font_italic\">watson</span> may also benefit from attentional saliency, being a word that the subject consistently paid attention to.</p>\n\n",
                "matched_terms": [
                    "across",
                    "three",
                    "from",
                    "keyword"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To understand how keyword detection performance scales with available training data, we systematically varied the fraction of the 52-hour corpus used for training while keeping the validation and test sets fixed. For these scaling runs we used 0&#8201;s pre-onset and +0.25&#8201;s post-onset windows (per-instance window length of 1.05&#8201;s). Because training uses many overlapping windows around labelled events, the total windowed duration processed exceeds the 52&#8201;h of unique recordings: at 10% this corresponds to <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>14&#8201;h of windowed data (<math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>5.2&#8201;h unique), and at 100% to <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>143&#8201;h of windowed data.</p>\n\n",
                "matched_terms": [
                    "fixed",
                    "fraction",
                    "scaling",
                    "detection",
                    "runs",
                    "training",
                    "keyword",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.F5\" title=\"Figure 5 &#8227; 4.3 Data Scaling &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, AUPRC improves approximately log-linearly as we increase the training fraction from 10% to 100%, consistent with established within-subject scaling laws in neural decoding <cite class=\"ltx_cite ltx_citemacro_citep\">(d&#8217;Ascoli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib11\" title=\"\">2024</a>; Sato et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib49\" title=\"\">2024</a>)</cite>. Notably, even with just 10% of the training data (<math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>14&#8201;h windowed; <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m2\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>5.2&#8201;h unique), the model achieves meaningful performance above chance, suggesting that keyword detection remains feasible even in scenarios with limited recording time. Permutation tests confirm that AUPRC is not above chance at 5% (<math alttext=\"p=0.108\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.108</mn></mrow><annotation encoding=\"application/x-tex\">p=0.108</annotation></semantics></math>), but is already significant at 10% (<math alttext=\"p=0.0156\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.0156</mn></mrow><annotation encoding=\"application/x-tex\">p=0.0156</annotation></semantics></math>; one-sided, 10,000 draws), and remains strongly significant thereafter (20% <math alttext=\"p=6.0\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mrow><mn>6.0</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">p=6.0\\times 10^{-4}</annotation></semantics></math>; 40&#8211;100% <math alttext=\"p\\leq 2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m6\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&#8804;</mo><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">p\\leq 2\\times 10^{-4}</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "auprc",
                    "scaling",
                    "tests",
                    "detection",
                    "permutation",
                    "from",
                    "training",
                    "keyword",
                    "fraction",
                    "onesided"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Adding pre- and post-onset offsets modestly improves detection. Averaged over all non-zero offsets, AUPRC increases by <math alttext=\"\\sim\\!25\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mrow><mn>25</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\sim\\!25\\%</annotation></semantics></math> relative to the 0/0 baseline (absolute +0.0097). This improvement is statistically significant (paired per-seed mean +0.0099 AUPRC, SE 0.0028; 95% CI [0.0045, 0.0154]; one-sided <math alttext=\"p&lt;10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">p&lt;10^{-3}</annotation></semantics></math>). We observe the best AUPRC near (neg=0.1s, pos=0.3s), but no clear monotonic trend across offsets; performance is relatively flat in a small neighbourhood around this setting and declines for very short or overly long windows, suggesting a bias&#8211;variance trade-off between providing sufficient context and diluting signal with unrelated activity.</p>\n\n",
                "matched_terms": [
                    "over",
                    "across",
                    "auprc",
                    "detection",
                    "onesided"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study reports results for a single participant on a single corpus. Generalisability across participants remains an open question for Neural Keyword Spotting, though generalization is becoming less of a problem in decoding than it was <cite class=\"ltx_cite ltx_citemacro_citep\">(Csaky et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib10\" title=\"\">2023</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib16\" title=\"\">2023</a>; d&#8217;Ascoli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib11\" title=\"\">2024</a>; Jayalath et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib26\" title=\"\">2025b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib25\" title=\"\">a</a>)</cite>. Validation and test sessions were selected to maximise positives, stabilising metrics at the expense of a mild base-rate bias. Test sets contain few positives, so thresholded metrics carry substantial uncertainty. We evaluate a compact model only, without exploring richer encoders, self-supervised pretraining, streaming inference, or multi-keyword training. Finally, we use event-referenced windows; continuous-stream detection with latency and explicit false-alarm accounting remains open.</p>\n\n",
                "matched_terms": [
                    "across",
                    "detection",
                    "training",
                    "results",
                    "test",
                    "keyword"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the success of competitions like the 2025 LibriBrain Competition <cite class=\"ltx_cite ltx_citemacro_citep\">(Landau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib30\" title=\"\">2025</a>)</cite> or Brain-to-text &#8217;25 <cite class=\"ltx_cite ltx_citemacro_citep\">(Card et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib8\" title=\"\">2025</a>)</cite>, we will release a leaderboard system for the keyword detection task as part of a larger-scale competition later this year. We also plan to extend this work to additional datasets, including inner speech and multi-subject recordings. Further analysis, such as phoneme-informed keyword selection and a deeper examination of temporal offsets, should clarify where gains are available building on the exploratory results presented here.</p>\n\n",
                "matched_terms": [
                    "results",
                    "keyword",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the single- or multi-keyword task, sample length is inferred from the longest keyword duration and can be extended with the <span class=\"ltx_text ltx_font_typewriter\">positive_buffer</span> and <span class=\"ltx_text ltx_font_typewriter\">negative_buffer</span> arguments. Overwrites using <span class=\"ltx_text ltx_font_typewriter\">tmin</span> and <span class=\"ltx_text ltx_font_typewriter\">tmax</span> are of course possible. For full signal-to-word, that is rarely the intended behaviour, so these options are disabled and a reasonable default is used instead.</p>\n\n",
                "matched_terms": [
                    "from",
                    "keyword"
                ]
            }
        ]
    },
    "A3.T3": {
        "caption": "Table 3: Seed-averaged per-keyword metrics (absolute units): base rate (positive prevalence), AUPRC, AUROC, Accuracy, and Best F1 (per-seed best across thresholds). Means and ±\\pm SEM are computed across seeds. Bold marks the best improvement vs base rate for AUPRC, and the highest mean for other columns.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Keyword</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Base rate</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">AUPRC</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">AUROC</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Acc</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Best F1</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">and</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.039</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.218 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.014</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.825</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.002</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.756 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.010</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.292</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.008</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">the</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.077</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.213 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.005</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.728 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.006</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.673 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.004</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.278 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.006</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">i</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.039</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.191 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.005</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.784 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m10\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.004</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.708 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m11\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.015</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.279 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m12\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.006</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">watson</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.005</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.065</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m13\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.017</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.759 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m14\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.006</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.952 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m15\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.034</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.149 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m16\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.036</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">holmes</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.008</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.028 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m17\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.001</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.791 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m18\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.013</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.820 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m19\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.066</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.072 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m20\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.005</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">himself</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.003</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.013 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m21\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.001</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.758 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m22\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.021</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.993 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m23\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.001</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.062 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m24\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.008</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">considerable</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.001</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.012 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m25\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.007</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.684 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m26\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.066</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.997 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m27\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.002</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.041 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m28\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.025</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">inspector</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.004</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.008 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m29\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.001</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.593 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m30\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.020</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.994 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m31\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.001</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.040 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m32\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.012</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">mister</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.002</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.007 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m33\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.004</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.505 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m34\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.051</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.998 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m35\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.000</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.054 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m36\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.022</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">remarkable</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.001</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.003 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m37\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.001</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.658 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m38\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.070</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.991 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m39\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.008</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.010 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m40\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.004</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">understand</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.001</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.002 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m41\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.001</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.523 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m42\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.136</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.999</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m43\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.000</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.006 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m44\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.003</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">investigation</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.001</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.002 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m45\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.001</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.665 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m46\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.047</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.998 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m47\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.001</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.011 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T3.m48\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.007</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "perkeyword",
            "himself",
            "positive",
            "perseed",
            "absolute",
            "columns",
            "seeds",
            "thresholds",
            "marks",
            "mister",
            "understand",
            "investigation",
            "base",
            "rate",
            "remarkable",
            "considerable",
            "seedaveraged",
            "computed",
            "inspector",
            "accuracy",
            "sem",
            "across",
            "prevalence",
            "improvement",
            "metrics",
            "bold",
            "mean",
            "units",
            "auroc",
            "watson",
            "auprc",
            "means",
            "best",
            "±pm",
            "highest",
            "other",
            "acc",
            "keyword",
            "holmes"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Non-invasive brain-computer interfaces (BCIs) are beginning to benefit from large, public benchmarks. However, current benchmarks target relatively simple, foundational tasks like Speech Detection and Phoneme Classification, while application-ready results on tasks like Brain-to-Text remain elusive. We propose Keyword Spotting (KWS) as a practically applicable, privacy-aware intermediate task. Using the deep 52-hour, within-subject LibriBrain corpus, we provide standardized train/validation/test splits for reproducible benchmarking, and adopt an evaluation protocol tailored to extreme class imbalance. Concretely, we use area under the precision-recall curve (AUPRC) as a robust evaluation metric, complemented by false alarms per hour (FA/h) at fixed recall to capture user-facing trade-offs. To simplify deployment and further experimentation within the research community, we are releasing an updated version of the <span class=\"ltx_text ltx_font_typewriter\">pnpl</span> library with word-level dataloaders and Colab-ready tutorials. As an initial reference model, we present a compact 1-D Conv/ResNet baseline with focal loss and top-<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> pooling that is trainable on a single consumer-class GPU. The reference model achieves <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math><math alttext=\"13\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"m3\" intent=\":literal\"><semantics><mrow><mn>13</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">13\\times</annotation></semantics></math> the permutation-baseline AUPRC on held-out sessions, demonstrating the viability of the task. Exploratory analyses reveal: (i) predictable within-subject scaling&#8212;performance improves log-linearly with more training hours&#8212;and (ii) the existence of word-level factors (frequency and duration) that systematically modulate detectability.</p>\n\n",
                "matched_terms": [
                    "auprc",
                    "keyword"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The arrival of large and readily available datasets has begun to supply non-invasive brain-computer-interface (BCI) research with the kind of &#8220;common yard-stick&#8221; that ImageNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Russakovsky et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib45\" title=\"\">2015</a>)</cite> provided for computer vision. Among current non-invasive datasets for decoding speech, LibriBrain <cite class=\"ltx_cite ltx_citemacro_citep\">(&#214;zdogan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib42\" title=\"\">2025</a>)</cite> is the <span class=\"ltx_text ltx_font_italic\">deepest</span> (i.e., <span class=\"ltx_text ltx_font_italic\">largest within-subject</span>) with 52 hours of magnetoencephalography (MEG) recorded from a single participant. This dataset forms the foundation for the 2025 PNPL Competition <cite class=\"ltx_cite ltx_citemacro_citep\">(Landau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib30\" title=\"\">2025</a>)</cite>, an open machine-learning competition that has catalyzed progress on two foundational decoding tasks: Speech Detection and Phoneme Classification. Progress on these tasks can be seen by looking at the online leaderboards (<a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://libribrain.com/\" title=\"\">https://libribrain.com/</a>). For example, in just two months F1 macro scores on the Speech Detection task advanced rapidly from about 68% to a new state-of-the-art on the (extended) public leaderboard of 96%. Our aim in this paper is to build on this success by introducing a new standardized task: Word Detection (a.k.a.&#160;Keyword Spotting). Substantively, we provide the same kinds of supporting infrastructure for this task (e.g., data loader, reference model, reproducible metrics, public leaderboard) which led directly to accelerated improvements in Speech Detection and Phoneme Classification.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "keyword"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Brain-to-Text (B2T)</span> takes variable-length neural sequences (e.g., EEG, MEG, fMRI) as input and outputs text transcripts, typically evaluated with word error rate (WER) or semantic similarity metrics such as the BERTScore <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib62\" title=\"\">2020</a>)</cite>. B2T is the analogue of ASR and represents a long-term goal, though its difficulty has motivated the development of what we call intermediate tasks, which lie between more foundational tasks like Speech Detection and full B2T. Non-invasive B2T has been explored with EEG and MEG <cite class=\"ltx_cite ltx_citemacro_citep\">(Duan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib15\" title=\"\">2023</a>; Jo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib27\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib59\" title=\"\">2024b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib60\" title=\"\">c</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib58\" title=\"\">a</a>)</cite>, with semantic metrics often reported in place of WER, although recent work shows that competitive WERs are beginning to be achievable non-invasively <cite class=\"ltx_cite ltx_citemacro_citep\">(Jayalath et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib25\" title=\"\">2025a</a>)</cite>. In fMRI, the coarse temporal resolution makes word-level alignment unlikely, though remarkable paraphrases have been produced which retain some semantic similarities to the ground truth speech <cite class=\"ltx_cite ltx_citemacro_citep\">(e.g., Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib52\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "remarkable",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Keyword spotting in the traditional audio domain (also referred to as wake-word detection) is a mature, highly-imbalanced detection problem optimizsed for very low false-alarm (FA) rates at fixed recall. Early small-footprint CNN and CRNN systems established the modern operating regime (e.g., 0.5 FA/h at acceptable FRR) under tight on-device constraints\n<cite class=\"ltx_cite ltx_citemacro_citep\">(e.g.,  Sainath and Parada, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib46\" title=\"\">2015</a>; Ar&#237;k et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib3\" title=\"\">2017</a>)</cite>. Large benchmarks like Speech Commands <cite class=\"ltx_cite ltx_citemacro_citep\">(Warden, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib55\" title=\"\">2018</a>)</cite>) and efficient architectures like MatchboxNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Maidina et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib36\" title=\"\">2020</a>)</cite> and Keyword Transformer <cite class=\"ltx_cite ltx_citemacro_citep\">(Berg et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib5\" title=\"\">2021</a>)</cite> further drove accuracy/latency trade-offs for embedded devices, while industrial deployments (e.g., Apple&#8217;s &#8220;Hey Siri&#8221;) codified evaluation practices around FA/h and user-centric thresholds <cite class=\"ltx_cite ltx_citemacro_citep\">(Apple Siri Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib2\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "keyword",
                    "thresholds"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Non-invasive technologies (EEG/MEG) have dramatic benefits over surgical implants in terms of safety and scalability. The application of keyword spotting is motivated by two converging strands. First, segment identification decoders trained to predict self-supervised speech representations from brain signals reliably retrieve the matching few-second stimulus among large candidate sets and generalise across participants - evidence that non-invasive signals carry phonetic/lexical detail at the granularity needed for lexical identification <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib16\" title=\"\">2023</a>; d&#8217;Ascoli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib11\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "keyword"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The release spans 93 sessions (3,139&#8201;min; 52.32&#8201;h) with 466,230 word tokens (16,892 unique) and 1,511,732 phoneme tokens. See Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#A2.F6\" title=\"Figure 6 &#8227; Appendix B Dataset Figures &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#A2.F7\" title=\"Figure 7 &#8227; Appendix B Dataset Figures &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> for an overview of the dataset. Word frequencies are Zipfian, providing keywords across a wide base-rate spectrum (short, frequent function words vs. longer, rarer content/proper names). These properties suit event-referenced keyword detection with extreme class imbalance.</p>\n\n",
                "matched_terms": [
                    "across",
                    "keyword"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to KWS, full-vocabulary Brain-to-Text aims to identify <math alttext=\"w\\in\\mathcal{W}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><mi>w</mi><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119986;</mi></mrow><annotation encoding=\"application/x-tex\">w\\in\\mathcal{W}</annotation></semantics></math> (hundreds of thousands of words), which introduces severe long-tail sparsity and requires calibrating thresholds across many classes. KWS is a practical, fixed-lexicon task: it asks only whether any member of a small, predefined set <math alttext=\"\\mathcal{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><annotation encoding=\"application/x-tex\">\\mathcal{V}</annotation></semantics></math> occurred. This offers a simple, reliable trigger with clean control over latency and false alarms&#8212;useful both on its own (assistive or hands-free commands) and as a stepping stone towards richer decoders.</p>\n\n",
                "matched_terms": [
                    "across",
                    "thresholds"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By default, we adopt LibriBrain&#8217;s session-level train/val/test split <cite class=\"ltx_cite ltx_citemacro_citep\">(&#214;zdogan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib42\" title=\"\">2025</a>)</cite>. For a chosen set of keywords <math alttext=\"\\mathcal{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><annotation encoding=\"application/x-tex\">\\mathcal{V}</annotation></semantics></math>, we verify that positives occur in both validation and test. If not, we replace them with the two sessions containing the most positives for <math alttext=\"\\mathcal{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><annotation encoding=\"application/x-tex\">\\mathcal{V}</annotation></semantics></math>.\nThis ensures sufficient positive examples for reliable metric computation, particularly important given the extreme class imbalance in keyword detection. For a session <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> containing word tokens with strings <math alttext=\"\\{s_{i}\\}_{i=1}^{n_{S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m4\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>n</mi><mi>S</mi></msub></msubsup><annotation encoding=\"application/x-tex\">\\{s_{i}\\}_{i=1}^{n_{S}}</annotation></semantics></math>, let</p>\n\n",
                "matched_terms": [
                    "positive",
                    "keyword"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use area under the precision-recall curve (AUPRC) as the primary metric. This is well suited to keyword spotting because its base rate equals the empirical prevalence of positives, so gains are easy to interpret. It also summarises the trade-off we actually care about under heavy imbalance&#8212;how many of the system&#8217;s alarms are correct (precision) as we demand more coverage (recall). Finally, unlike alternative metrics (e.g., AUROC), AUPRC is not overly optimistic when precision is too low to be usable. We supplement AUPRC with additional metrics like AUROC to provide a comprehensive view.\nFor scenario-grounded reporting, we translate any validation-selected threshold (precision <math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math>, recall <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math>) into hourly rates under an assumed event frequency <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> (keywords/hour):</p>\n\n",
                "matched_terms": [
                    "auprc",
                    "prevalence",
                    "base",
                    "rate",
                    "metrics",
                    "auroc",
                    "keyword"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We consider two illustrative use cases: assistive access (<math alttext=\"\\lambda\\!\\approx\\!2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo lspace=\"0.108em\" rspace=\"0.108em\">&#8776;</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda\\!\\approx\\!2</annotation></semantics></math>/h) and hands-free control (<math alttext=\"\\lambda\\!\\approx\\!10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo lspace=\"0.108em\" rspace=\"0.108em\">&#8776;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda\\!\\approx\\!10</annotation></semantics></math>/h). Thresholds <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m6\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> are chosen on validation either (a) by maximising recall under a false-alarm budget, or (b) by minimising FA/h subject to a target recall; selected <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m7\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> are then frozen for testing. For clarity, FA/h can also be computed directly from test-set coverage (false positives per hour of labelled windows); unless otherwise noted, we report the scenario-translated FA/h using <math alttext=\"(P,R,\\lambda)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m8\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>P</mi><mo>,</mo><mi>R</mi><mo>,</mo><mi>&#955;</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(P,R,\\lambda)</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "computed",
                    "thresholds"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Where possible, all experiments use the standard train/validation/test splits provided by the <span class=\"ltx_text ltx_font_typewriter\">pnpl</span> dataset (using the logic described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S3.SS4\" title=\"3.4 Reference Model &#8227; 3 Methods &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a>) and are fully reproducible (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#A1\" title=\"Appendix A Open Resources: Code, Tutorial, and Leaderboard &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>). Unless noted, values are seed-averages over three runs. Error bars are standard errors across seeds. For Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Model Performance &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> we report standard errors approximated from 95% bootstrap CIs (4,000 resamples).</p>\n\n",
                "matched_terms": [
                    "across",
                    "seeds"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first establish that the dataset carries usable signal for keyword detection by evaluating a single model on the held-out test set (<math alttext=\"n=4660\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>4660</mn></mrow><annotation encoding=\"application/x-tex\">n=4660</annotation></semantics></math>, positives <math alttext=\"=24\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>=</mo><mn>24</mn></mrow><annotation encoding=\"application/x-tex\">=24</annotation></semantics></math>; base rate <math alttext=\"=0.00515\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>=</mo><mn>0.00515</mn></mrow><annotation encoding=\"application/x-tex\">=0.00515</annotation></semantics></math>). Given the absence of prior publicly reproducible MEG keyword spotting benchmarks, we evaluate against permutation-derived random baselines to demonstrate the presence of meaningful signal rather than competitive performance. While overall performance is modest, threshold-free metrics indicate clear signal (AUPRC <math alttext=\"\\approx 13.4\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mo>&#8776;</mo><mn>13.4</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">\\approx 13.4\\times</annotation></semantics></math> the permutation baseline; AUROC <math alttext=\"\\approx 0.80\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.80</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.80</annotation></semantics></math>). Full results are provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Model Performance &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nBeyond threshold-free metrics, we include an operational snapshot. At a target recall of <math alttext=\"\\sim\\!0.10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>0.10</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!0.10</annotation></semantics></math>, the scenario-translated FA/h for the assistive case (<math alttext=\"\\lambda=2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=2</annotation></semantics></math>/h) is <math alttext=\"\\sim\\!2.19\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>2.19</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!2.19</annotation></semantics></math> (SE <math alttext=\"\\approx\\!1.63\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m9\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8776;</mo><mn>1.63</mn></mrow><annotation encoding=\"application/x-tex\">\\approx\\!1.63</annotation></semantics></math>), corresponding to <math alttext=\"\\sim\\!13\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m10\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>13</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!13</annotation></semantics></math> alerts per correct detection. For reference, directly counting false positives per hour under the labelled test coverage yields <math alttext=\"\\sim\\!16.3\\,\\mathrm{FA/h}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m11\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mrow><mrow><mn>16.3</mn><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><mi>FA</mi></mrow><mo>/</mo><mi mathvariant=\"normal\">h</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\sim\\!16.3\\,\\mathrm{FA/h}</annotation></semantics></math> (seed-avg; SE <math alttext=\"\\approx\\!12.1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m12\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8776;</mo><mn>12.1</mn></mrow><annotation encoding=\"application/x-tex\">\\approx\\!12.1</annotation></semantics></math>). Under FA/h budgets (scenario scale, <math alttext=\"\\lambda=2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m13\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=2</annotation></semantics></math>/h), the model achieves recall <math alttext=\"\\sim\\!0.14\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m14\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>0.14</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!0.14</annotation></semantics></math> at 2.0 FA/h and <math alttext=\"\\sim\\!0.08\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m15\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>0.08</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!0.08</annotation></semantics></math> at 0.5 FA/h (seed-averaged). These numbers contextualise the ranking metrics and set a baseline for future improvements. The right panel of Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.F4\" title=\"Figure 4 &#8227; 4.2 Keyword Choice &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the mean recall&#8211;FA/h operating curve with per-seed traces. For this snapshot, operating points are chosen on the test PR curves for presentation; in deployment we would select thresholds on validation and freeze them before testing.</p>\n\n",
                "matched_terms": [
                    "auprc",
                    "perseed",
                    "base",
                    "rate",
                    "metrics",
                    "seedaveraged",
                    "mean",
                    "auroc",
                    "keyword",
                    "thresholds"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An important consideration in KWS is the choise of keyword(s). In LibriBrain, as in many real-world corpora, longer words are rarer: word length in phonemes is negatively correlated with token frequency (Spearman <math alttext=\"r=-0.28\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mrow><mo>&#8722;</mo><mn>0.28</mn></mrow></mrow><annotation encoding=\"application/x-tex\">r=-0.28</annotation></semantics></math> with <math alttext=\"\\log\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\"><semantics><mi>log</mi><annotation encoding=\"application/x-tex\">\\log</annotation></semantics></math> frequency; <math alttext=\"p=2.7\\times 10^{-185}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mrow><mn>2.7</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>185</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">p=2.7\\times 10^{-185}</annotation></semantics></math>; Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.F3\" title=\"Figure 3 &#8227; 4.2 Keyword Choice &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> left). This matters because length can reduce false alarms while frequency controls how many positives we can realistically train on. To navigate this trade-off, we selected the most frequent word at each phoneme length and measured %<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math>AUPRC over the empirical base rate. The length-%<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m5\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math>AUPRC relation is non-monotonic (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.F3\" title=\"Figure 3 &#8227; 4.2 Keyword Choice &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> right): among a 12-item shortlist spanning 1-12 phonemes, the 5-phoneme <em class=\"ltx_emph ltx_font_italic\">watson</em> yields the largest %<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m6\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math>AUPRC, whereas several longer items (e.g., 9-12 phonemes) underperform despite greater duration.</p>\n\n",
                "matched_terms": [
                    "base",
                    "watson",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A controlled comparison across three similarly frequent keywords of different lengths (<em class=\"ltx_emph ltx_font_italic\">walk</em>, <em class=\"ltx_emph ltx_font_italic\">surely</em>, <em class=\"ltx_emph ltx_font_italic\">excellent</em>; 3/5/8 phonemes) shows no detectable difference in %<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math>AUPRC within our precision (overlapping SEMs; Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.F4\" title=\"Figure 4 &#8227; 4.2 Keyword Choice &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). This indicates that, once frequency is matched, mere length is not the primary driver of detectability in MEG KWS. The pattern is consistent with established constraints on neural speech processing and KWS: benefits accrue less from duration per se and more from properties that improve time-locking and reduce lexical competition&#8212;salient acoustic onsets and early stress (stronger M100/M200), an early uniqueness point (UP)&#8212;i.e., the keyword becomes lexically unique after only a few initial phonemes (a small UP index relative to its length)&#8212;a sparse phonological neighborhood, and moderate frequency with lower contextual predictability <cite class=\"ltx_cite ltx_citemacro_citep\">(Gage et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib17\" title=\"\">1998</a>; Leminen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib33\" title=\"\">2011</a>; Vitevitch and Luce, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib54\" title=\"\">1999</a>; Halgren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib20\" title=\"\">2002</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib9\" title=\"\">2014</a>)</cite>. Empirically, <em class=\"ltx_emph ltx_font_italic\">watson</em> may profit from prosodic prominence in narrative speech and an early uniqueness point, outweighing any gains attributable to length alone; <span class=\"ltx_text ltx_font_italic\">watson</span> may also benefit from attentional saliency, being a word that the subject consistently paid attention to.</p>\n\n",
                "matched_terms": [
                    "watson",
                    "across",
                    "keyword"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To understand how keyword detection performance scales with available training data, we systematically varied the fraction of the 52-hour corpus used for training while keeping the validation and test sets fixed. For these scaling runs we used 0&#8201;s pre-onset and +0.25&#8201;s post-onset windows (per-instance window length of 1.05&#8201;s). Because training uses many overlapping windows around labelled events, the total windowed duration processed exceeds the 52&#8201;h of unique recordings: at 10% this corresponds to <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>14&#8201;h of windowed data (<math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>5.2&#8201;h unique), and at 100% to <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>143&#8201;h of windowed data.</p>\n\n",
                "matched_terms": [
                    "understand",
                    "keyword"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.F5\" title=\"Figure 5 &#8227; 4.3 Data Scaling &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, AUPRC improves approximately log-linearly as we increase the training fraction from 10% to 100%, consistent with established within-subject scaling laws in neural decoding <cite class=\"ltx_cite ltx_citemacro_citep\">(d&#8217;Ascoli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib11\" title=\"\">2024</a>; Sato et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib49\" title=\"\">2024</a>)</cite>. Notably, even with just 10% of the training data (<math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>14&#8201;h windowed; <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m2\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>5.2&#8201;h unique), the model achieves meaningful performance above chance, suggesting that keyword detection remains feasible even in scenarios with limited recording time. Permutation tests confirm that AUPRC is not above chance at 5% (<math alttext=\"p=0.108\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.108</mn></mrow><annotation encoding=\"application/x-tex\">p=0.108</annotation></semantics></math>), but is already significant at 10% (<math alttext=\"p=0.0156\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.0156</mn></mrow><annotation encoding=\"application/x-tex\">p=0.0156</annotation></semantics></math>; one-sided, 10,000 draws), and remains strongly significant thereafter (20% <math alttext=\"p=6.0\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mrow><mn>6.0</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">p=6.0\\times 10^{-4}</annotation></semantics></math>; 40&#8211;100% <math alttext=\"p\\leq 2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m6\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&#8804;</mo><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">p\\leq 2\\times 10^{-4}</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "auprc",
                    "keyword"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Adding pre- and post-onset offsets modestly improves detection. Averaged over all non-zero offsets, AUPRC increases by <math alttext=\"\\sim\\!25\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mrow><mn>25</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\sim\\!25\\%</annotation></semantics></math> relative to the 0/0 baseline (absolute +0.0097). This improvement is statistically significant (paired per-seed mean +0.0099 AUPRC, SE 0.0028; 95% CI [0.0045, 0.0154]; one-sided <math alttext=\"p&lt;10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">p&lt;10^{-3}</annotation></semantics></math>). We observe the best AUPRC near (neg=0.1s, pos=0.3s), but no clear monotonic trend across offsets; performance is relatively flat in a small neighbourhood around this setting and declines for very short or overly long windows, suggesting a bias&#8211;variance trade-off between providing sufficient context and diluting signal with unrelated activity.</p>\n\n",
                "matched_terms": [
                    "across",
                    "auprc",
                    "perseed",
                    "improvement",
                    "absolute",
                    "best",
                    "mean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the achieved improvement in metrics, performance is not yet sufficient for reliable hands-free use. In an assistive scenario (<math alttext=\"\\lambda{=}2\\,\\mathrm{h}^{-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mrow><mn>2</mn><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><msup><mi mathvariant=\"normal\">h</mi><mrow><mo>&#8722;</mo><mn>1</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\lambda{=}2\\,\\mathrm{h}^{-1}</annotation></semantics></math>), at recall <math alttext=\"\\approx\\!0.10\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8776;</mo><mn>0.10</mn></mrow><annotation encoding=\"application/x-tex\">\\approx\\!0.10</annotation></semantics></math> the system yields <math alttext=\"\\approx\\!2.2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8776;</mo><mn>2.2</mn></mrow><annotation encoding=\"application/x-tex\">\\approx\\!2.2</annotation></semantics></math> false alarms per hour (about 13 alerts per correct detection). Priorities for future work thus include: (i) stronger ranking, (ii) calibration and principled threshold selection, and (iii) deployment strategies that suppress false alarms (multi-confirmation, small ensembles, cascaded detectors with context-aware priors).</p>\n\n",
                "matched_terms": [
                    "improvement",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study reports results for a single participant on a single corpus. Generalisability across participants remains an open question for Neural Keyword Spotting, though generalization is becoming less of a problem in decoding than it was <cite class=\"ltx_cite ltx_citemacro_citep\">(Csaky et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib10\" title=\"\">2023</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib16\" title=\"\">2023</a>; d&#8217;Ascoli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib11\" title=\"\">2024</a>; Jayalath et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib26\" title=\"\">2025b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib25\" title=\"\">a</a>)</cite>. Validation and test sessions were selected to maximise positives, stabilising metrics at the expense of a mild base-rate bias. Test sets contain few positives, so thresholded metrics carry substantial uncertainty. We evaluate a compact model only, without exploring richer encoders, self-supervised pretraining, streaming inference, or multi-keyword training. Finally, we use event-referenced windows; continuous-stream detection with latency and explicit false-alarm accounting remains open.</p>\n\n",
                "matched_terms": [
                    "across",
                    "metrics",
                    "keyword"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Similarly, the <span class=\"ltx_text ltx_font_typewriter\">keyword_detection</span> variant will verify that the keyword(s) are present in the dataset and, if not, default to highest prevalence sessions as validation and test sets, while the signal-to-word variant will use the default validation and test sets.</p>\n\n",
                "matched_terms": [
                    "highest",
                    "prevalence"
                ]
            }
        ]
    },
    "A3.T4": {
        "caption": "Table 4: Operating-point snapshot (best-AUPRC buffer: neg=0.1s, pos=0.3s). Values are seed-averages ±\\pm SE (n=3n=3). Scenario-scale metrics use the assistive case (λ=2\\lambda=2/h).",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Scenario</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Metric</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Value</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SE</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Assistive (</span><math alttext=\"\\lambda=2\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T4.m1\" intent=\":literal\"><semantics><mrow><mi mathsize=\"0.900em\">&#955;</mi><mo mathsize=\"0.900em\">=</mo><mn mathsize=\"0.900em\">2</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=2</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">/h), target recall </span><math alttext=\"\\approx 0.10\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T4.m2\" intent=\":literal\"><semantics><mrow><mi/><mo mathsize=\"0.900em\">&#8776;</mo><mn mathsize=\"0.900em\">0.10</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.10</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">FA/h</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.194</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.629</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Assistive (</span><math alttext=\"\\lambda=2\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T4.m3\" intent=\":literal\"><semantics><mrow><mi mathsize=\"0.900em\">&#955;</mi><mo mathsize=\"0.900em\">=</mo><mn mathsize=\"0.900em\">2</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=2</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">/h), FA/h budget 2.0</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">Recall</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.139</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.050</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Assistive (</span><math alttext=\"\\lambda=2\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T4.m4\" intent=\":literal\"><semantics><mrow><mi mathsize=\"0.900em\">&#955;</mi><mo mathsize=\"0.900em\">=</mo><mn mathsize=\"0.900em\">2</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=2</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">/h), FA/h budget 0.5</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">Recall</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.083</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.024</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">Labelled test coverage</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">FP/h</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">16.3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.1</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "snapshot",
            "labelled",
            "assistive",
            "≈010approx",
            "operatingpoint",
            "pos03s",
            "fah",
            "target",
            "λ2lambda2h",
            "test",
            "case",
            "value",
            "seedaverages",
            "scenario",
            "neg01s",
            "budget",
            "metrics",
            "recall",
            "metric",
            "values",
            "scenarioscale",
            "fph",
            "n3n3",
            "use",
            "±pm",
            "bestauprc",
            "coverage",
            "buffer"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Non-invasive brain-computer interfaces (BCIs) are beginning to benefit from large, public benchmarks. However, current benchmarks target relatively simple, foundational tasks like Speech Detection and Phoneme Classification, while application-ready results on tasks like Brain-to-Text remain elusive. We propose Keyword Spotting (KWS) as a practically applicable, privacy-aware intermediate task. Using the deep 52-hour, within-subject LibriBrain corpus, we provide standardized train/validation/test splits for reproducible benchmarking, and adopt an evaluation protocol tailored to extreme class imbalance. Concretely, we use area under the precision-recall curve (AUPRC) as a robust evaluation metric, complemented by false alarms per hour (FA/h) at fixed recall to capture user-facing trade-offs. To simplify deployment and further experimentation within the research community, we are releasing an updated version of the <span class=\"ltx_text ltx_font_typewriter\">pnpl</span> library with word-level dataloaders and Colab-ready tutorials. As an initial reference model, we present a compact 1-D Conv/ResNet baseline with focal loss and top-<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> pooling that is trainable on a single consumer-class GPU. The reference model achieves <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math><math alttext=\"13\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"m3\" intent=\":literal\"><semantics><mrow><mn>13</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">13\\times</annotation></semantics></math> the permutation-baseline AUPRC on held-out sessions, demonstrating the viability of the task. Exploratory analyses reveal: (i) predictable within-subject scaling&#8212;performance improves log-linearly with more training hours&#8212;and (ii) the existence of word-level factors (frequency and duration) that systematically modulate detectability.</p>\n\n",
                "matched_terms": [
                    "use",
                    "fah",
                    "recall",
                    "target",
                    "metric"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Keyword spotting in the traditional audio domain (also referred to as wake-word detection) is a mature, highly-imbalanced detection problem optimizsed for very low false-alarm (FA) rates at fixed recall. Early small-footprint CNN and CRNN systems established the modern operating regime (e.g., 0.5 FA/h at acceptable FRR) under tight on-device constraints\n<cite class=\"ltx_cite ltx_citemacro_citep\">(e.g.,  Sainath and Parada, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib46\" title=\"\">2015</a>; Ar&#237;k et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib3\" title=\"\">2017</a>)</cite>. Large benchmarks like Speech Commands <cite class=\"ltx_cite ltx_citemacro_citep\">(Warden, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib55\" title=\"\">2018</a>)</cite>) and efficient architectures like MatchboxNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Maidina et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib36\" title=\"\">2020</a>)</cite> and Keyword Transformer <cite class=\"ltx_cite ltx_citemacro_citep\">(Berg et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib5\" title=\"\">2021</a>)</cite> further drove accuracy/latency trade-offs for embedded devices, while industrial deployments (e.g., Apple&#8217;s &#8220;Hey Siri&#8221;) codified evaluation practices around FA/h and user-centric thresholds <cite class=\"ltx_cite ltx_citemacro_citep\">(Apple Siri Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib2\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "recall",
                    "fah"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By default, we adopt LibriBrain&#8217;s session-level train/val/test split <cite class=\"ltx_cite ltx_citemacro_citep\">(&#214;zdogan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib42\" title=\"\">2025</a>)</cite>. For a chosen set of keywords <math alttext=\"\\mathcal{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><annotation encoding=\"application/x-tex\">\\mathcal{V}</annotation></semantics></math>, we verify that positives occur in both validation and test. If not, we replace them with the two sessions containing the most positives for <math alttext=\"\\mathcal{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><annotation encoding=\"application/x-tex\">\\mathcal{V}</annotation></semantics></math>.\nThis ensures sufficient positive examples for reliable metric computation, particularly important given the extreme class imbalance in keyword detection. For a session <math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m3\" intent=\":literal\"><semantics><mi>S</mi><annotation encoding=\"application/x-tex\">S</annotation></semantics></math> containing word tokens with strings <math alttext=\"\\{s_{i}\\}_{i=1}^{n_{S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p4.m4\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msub><mi>s</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>n</mi><mi>S</mi></msub></msubsup><annotation encoding=\"application/x-tex\">\\{s_{i}\\}_{i=1}^{n_{S}}</annotation></semantics></math>, let</p>\n\n",
                "matched_terms": [
                    "metric",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use area under the precision-recall curve (AUPRC) as the primary metric. This is well suited to keyword spotting because its base rate equals the empirical prevalence of positives, so gains are easy to interpret. It also summarises the trade-off we actually care about under heavy imbalance&#8212;how many of the system&#8217;s alarms are correct (precision) as we demand more coverage (recall). Finally, unlike alternative metrics (e.g., AUROC), AUPRC is not overly optimistic when precision is too low to be usable. We supplement AUPRC with additional metrics like AUROC to provide a comprehensive view.\nFor scenario-grounded reporting, we translate any validation-selected threshold (precision <math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math>, recall <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math>) into hourly rates under an assumed event frequency <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> (keywords/hour):</p>\n\n",
                "matched_terms": [
                    "use",
                    "metrics",
                    "recall",
                    "metric",
                    "coverage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We consider two illustrative use cases: assistive access (<math alttext=\"\\lambda\\!\\approx\\!2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo lspace=\"0.108em\" rspace=\"0.108em\">&#8776;</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda\\!\\approx\\!2</annotation></semantics></math>/h) and hands-free control (<math alttext=\"\\lambda\\!\\approx\\!10\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo lspace=\"0.108em\" rspace=\"0.108em\">&#8776;</mo><mn>10</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda\\!\\approx\\!10</annotation></semantics></math>/h). Thresholds <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m6\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> are chosen on validation either (a) by maximising recall under a false-alarm budget, or (b) by minimising FA/h subject to a target recall; selected <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m7\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> are then frozen for testing. For clarity, FA/h can also be computed directly from test-set coverage (false positives per hour of labelled windows); unless otherwise noted, we report the scenario-translated FA/h using <math alttext=\"(P,R,\\lambda)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m8\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>P</mi><mo>,</mo><mi>R</mi><mo>,</mo><mi>&#955;</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(P,R,\\lambda)</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "budget",
                    "labelled",
                    "use",
                    "fah",
                    "assistive",
                    "target",
                    "recall",
                    "coverage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Where possible, all experiments use the standard train/validation/test splits provided by the <span class=\"ltx_text ltx_font_typewriter\">pnpl</span> dataset (using the logic described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S3.SS4\" title=\"3.4 Reference Model &#8227; 3 Methods &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a>) and are fully reproducible (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#A1\" title=\"Appendix A Open Resources: Code, Tutorial, and Leaderboard &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>). Unless noted, values are seed-averages over three runs. Error bars are standard errors across seeds. For Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Model Performance &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> we report standard errors approximated from 95% bootstrap CIs (4,000 resamples).</p>\n\n",
                "matched_terms": [
                    "use",
                    "seedaverages",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first establish that the dataset carries usable signal for keyword detection by evaluating a single model on the held-out test set (<math alttext=\"n=4660\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>4660</mn></mrow><annotation encoding=\"application/x-tex\">n=4660</annotation></semantics></math>, positives <math alttext=\"=24\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>=</mo><mn>24</mn></mrow><annotation encoding=\"application/x-tex\">=24</annotation></semantics></math>; base rate <math alttext=\"=0.00515\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>=</mo><mn>0.00515</mn></mrow><annotation encoding=\"application/x-tex\">=0.00515</annotation></semantics></math>). Given the absence of prior publicly reproducible MEG keyword spotting benchmarks, we evaluate against permutation-derived random baselines to demonstrate the presence of meaningful signal rather than competitive performance. While overall performance is modest, threshold-free metrics indicate clear signal (AUPRC <math alttext=\"\\approx 13.4\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mo>&#8776;</mo><mn>13.4</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">\\approx 13.4\\times</annotation></semantics></math> the permutation baseline; AUROC <math alttext=\"\\approx 0.80\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.80</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.80</annotation></semantics></math>). Full results are provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Model Performance &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nBeyond threshold-free metrics, we include an operational snapshot. At a target recall of <math alttext=\"\\sim\\!0.10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>0.10</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!0.10</annotation></semantics></math>, the scenario-translated FA/h for the assistive case (<math alttext=\"\\lambda=2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=2</annotation></semantics></math>/h) is <math alttext=\"\\sim\\!2.19\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>2.19</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!2.19</annotation></semantics></math> (SE <math alttext=\"\\approx\\!1.63\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m9\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8776;</mo><mn>1.63</mn></mrow><annotation encoding=\"application/x-tex\">\\approx\\!1.63</annotation></semantics></math>), corresponding to <math alttext=\"\\sim\\!13\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m10\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>13</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!13</annotation></semantics></math> alerts per correct detection. For reference, directly counting false positives per hour under the labelled test coverage yields <math alttext=\"\\sim\\!16.3\\,\\mathrm{FA/h}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m11\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mrow><mrow><mn>16.3</mn><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><mi>FA</mi></mrow><mo>/</mo><mi mathvariant=\"normal\">h</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\sim\\!16.3\\,\\mathrm{FA/h}</annotation></semantics></math> (seed-avg; SE <math alttext=\"\\approx\\!12.1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m12\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8776;</mo><mn>12.1</mn></mrow><annotation encoding=\"application/x-tex\">\\approx\\!12.1</annotation></semantics></math>). Under FA/h budgets (scenario scale, <math alttext=\"\\lambda=2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m13\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=2</annotation></semantics></math>/h), the model achieves recall <math alttext=\"\\sim\\!0.14\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m14\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>0.14</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!0.14</annotation></semantics></math> at 2.0 FA/h and <math alttext=\"\\sim\\!0.08\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m15\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>0.08</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!0.08</annotation></semantics></math> at 0.5 FA/h (seed-averaged). These numbers contextualise the ranking metrics and set a baseline for future improvements. The right panel of Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.F4\" title=\"Figure 4 &#8227; 4.2 Keyword Choice &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the mean recall&#8211;FA/h operating curve with per-seed traces. For this snapshot, operating points are chosen on the test PR curves for presentation; in deployment we would select thresholds on validation and freeze them before testing.</p>\n\n",
                "matched_terms": [
                    "scenario",
                    "snapshot",
                    "labelled",
                    "metrics",
                    "fah",
                    "assistive",
                    "target",
                    "λ2lambda2h",
                    "recall",
                    "test",
                    "case",
                    "coverage"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To understand how keyword detection performance scales with available training data, we systematically varied the fraction of the 52-hour corpus used for training while keeping the validation and test sets fixed. For these scaling runs we used 0&#8201;s pre-onset and +0.25&#8201;s post-onset windows (per-instance window length of 1.05&#8201;s). Because training uses many overlapping windows around labelled events, the total windowed duration processed exceeds the 52&#8201;h of unique recordings: at 10% this corresponds to <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>14&#8201;h of windowed data (<math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>5.2&#8201;h unique), and at 100% to <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>143&#8201;h of windowed data.</p>\n\n",
                "matched_terms": [
                    "labelled",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Adding pre- and post-onset offsets modestly improves detection. Averaged over all non-zero offsets, AUPRC increases by <math alttext=\"\\sim\\!25\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mrow><mn>25</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\sim\\!25\\%</annotation></semantics></math> relative to the 0/0 baseline (absolute +0.0097). This improvement is statistically significant (paired per-seed mean +0.0099 AUPRC, SE 0.0028; 95% CI [0.0045, 0.0154]; one-sided <math alttext=\"p&lt;10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">p&lt;10^{-3}</annotation></semantics></math>). We observe the best AUPRC near (neg=0.1s, pos=0.3s), but no clear monotonic trend across offsets; performance is relatively flat in a small neighbourhood around this setting and declines for very short or overly long windows, suggesting a bias&#8211;variance trade-off between providing sufficient context and diluting signal with unrelated activity.</p>\n\n",
                "matched_terms": [
                    "pos03s",
                    "neg01s"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the achieved improvement in metrics, performance is not yet sufficient for reliable hands-free use. In an assistive scenario (<math alttext=\"\\lambda{=}2\\,\\mathrm{h}^{-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mrow><mn>2</mn><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><msup><mi mathvariant=\"normal\">h</mi><mrow><mo>&#8722;</mo><mn>1</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">\\lambda{=}2\\,\\mathrm{h}^{-1}</annotation></semantics></math>), at recall <math alttext=\"\\approx\\!0.10\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8776;</mo><mn>0.10</mn></mrow><annotation encoding=\"application/x-tex\">\\approx\\!0.10</annotation></semantics></math> the system yields <math alttext=\"\\approx\\!2.2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8776;</mo><mn>2.2</mn></mrow><annotation encoding=\"application/x-tex\">\\approx\\!2.2</annotation></semantics></math> false alarms per hour (about 13 alerts per correct detection). Priorities for future work thus include: (i) stronger ranking, (ii) calibration and principled threshold selection, and (iii) deployment strategies that suppress false alarms (multi-confirmation, small ensembles, cascaded detectors with context-aware priors).</p>\n\n",
                "matched_terms": [
                    "scenario",
                    "use",
                    "metrics",
                    "assistive",
                    "recall"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study reports results for a single participant on a single corpus. Generalisability across participants remains an open question for Neural Keyword Spotting, though generalization is becoming less of a problem in decoding than it was <cite class=\"ltx_cite ltx_citemacro_citep\">(Csaky et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib10\" title=\"\">2023</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib16\" title=\"\">2023</a>; d&#8217;Ascoli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib11\" title=\"\">2024</a>; Jayalath et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib26\" title=\"\">2025b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib25\" title=\"\">a</a>)</cite>. Validation and test sessions were selected to maximise positives, stabilising metrics at the expense of a mild base-rate bias. Test sets contain few positives, so thresholded metrics carry substantial uncertainty. We evaluate a compact model only, without exploring richer encoders, self-supervised pretraining, streaming inference, or multi-keyword training. Finally, we use event-referenced windows; continuous-stream detection with latency and explicit false-alarm accounting remains open.</p>\n\n",
                "matched_terms": [
                    "use",
                    "metrics",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Similarly, the <span class=\"ltx_text ltx_font_typewriter\">keyword_detection</span> variant will verify that the keyword(s) are present in the dataset and, if not, default to highest prevalence sessions as validation and test sets, while the signal-to-word variant will use the default validation and test sets.</p>\n\n",
                "matched_terms": [
                    "use",
                    "test"
                ]
            }
        ]
    },
    "A3.T5": {
        "caption": "Table 5: Matched-frequency keyword comparison (seed-averaged over three runs). Results show no significant differences (overlapping SEMs).",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Keyword</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Chars</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Base rate</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">AUPRC</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">%<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T5.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math>AUPRC over base</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">walk</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">4</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.00056</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.00136 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T5.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.00010</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">144.9 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T5.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 18.0</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">surely</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">6</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.00056</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.00141 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T5.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.00031</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">153.7 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T5.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 59.3</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">excellent</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">9</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.00056</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">0.00133 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T5.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 0.00033</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">138.9 </span><math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T5.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> 57.6</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "sems",
            "chars",
            "overlapping",
            "significant",
            "δdeltaauprc",
            "comparison",
            "base",
            "rate",
            "differences",
            "seedaveraged",
            "show",
            "runs",
            "surely",
            "matchedfrequency",
            "excellent",
            "over",
            "results",
            "auprc",
            "three",
            "±pm",
            "keyword",
            "walk"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Non-invasive brain-computer interfaces (BCIs) are beginning to benefit from large, public benchmarks. However, current benchmarks target relatively simple, foundational tasks like Speech Detection and Phoneme Classification, while application-ready results on tasks like Brain-to-Text remain elusive. We propose Keyword Spotting (KWS) as a practically applicable, privacy-aware intermediate task. Using the deep 52-hour, within-subject LibriBrain corpus, we provide standardized train/validation/test splits for reproducible benchmarking, and adopt an evaluation protocol tailored to extreme class imbalance. Concretely, we use area under the precision-recall curve (AUPRC) as a robust evaluation metric, complemented by false alarms per hour (FA/h) at fixed recall to capture user-facing trade-offs. To simplify deployment and further experimentation within the research community, we are releasing an updated version of the <span class=\"ltx_text ltx_font_typewriter\">pnpl</span> library with word-level dataloaders and Colab-ready tutorials. As an initial reference model, we present a compact 1-D Conv/ResNet baseline with focal loss and top-<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> pooling that is trainable on a single consumer-class GPU. The reference model achieves <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math><math alttext=\"13\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"m3\" intent=\":literal\"><semantics><mrow><mn>13</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">13\\times</annotation></semantics></math> the permutation-baseline AUPRC on held-out sessions, demonstrating the viability of the task. Exploratory analyses reveal: (i) predictable within-subject scaling&#8212;performance improves log-linearly with more training hours&#8212;and (ii) the existence of word-level factors (frequency and duration) that systematically modulate detectability.</p>\n\n",
                "matched_terms": [
                    "keyword",
                    "auprc",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As <cite class=\"ltx_cite ltx_citemacro_citet\">Landau et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib30\" title=\"\">2025</a>)</cite> explain, the two tasks in the 2025 PNPL Competition were selected for their simplicity. Over time, the idea was to increase the complexity, and utility, of benchmark decoding tasks. Keyword Spotting (KWS) is an exciting landmark as it represents the first decoding task on this curriculum with practical utility for BCIs. In the established domain of voice computing, Keyword Spotting is commonly used to detect &#8220;wake words&#8221; (e.g., &#8220;Hey Siri&#8221;, &#8220;Alexa&#8221;, &#8220;OK Google&#8221;). Wake words like these can be used to indicate that subsequent speech should be interpreted as a command, or they can be used themselves as commands. In the emerging domain of <span class=\"ltx_text ltx_font_italic\">brain</span> computing, even a single wake word (e.g., &#8220;help&#8221;) could be profoundly meaningful to someone with severe paralysis. Only slightly further along the curriculum, a small set of working keywords (e.g., &#8220;hungry&#8221;, &#8220;tired&#8221;, &#8220;thirsty&#8221;, &#8220;toilet&#8221;, &#8220;pain&#8221;) would transform their quality of life. The benchmark task established in this paper is intended, ultimately, to lead to such an outcome. For clarity, and to contrast the use of acoustic inputs, we use the term <span class=\"ltx_text ltx_font_italic\">Neural Keyword Spotting</span> to denote Keyword Spotting from continuous brain data, a task represented schematically in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S0.F1\" title=\"Figure 1 &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "over",
                    "keyword"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Non-invasive technologies (EEG/MEG) have dramatic benefits over surgical implants in terms of safety and scalability. The application of keyword spotting is motivated by two converging strands. First, segment identification decoders trained to predict self-supervised speech representations from brain signals reliably retrieve the matching few-second stimulus among large candidate sets and generalise across participants - evidence that non-invasive signals carry phonetic/lexical detail at the granularity needed for lexical identification <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib16\" title=\"\">2023</a>; d&#8217;Ascoli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib11\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "over",
                    "keyword"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, converging MEG/EEG results show sensitivity to phoneme sequence structure and higher-level linguistic content, and recent deep models capture meaningful portions of the speech-to-language transform in these signals <cite class=\"ltx_cite ltx_citemacro_citep\">(Gwilliams et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib18\" title=\"\">2022</a>; Tezcan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib53\" title=\"\">2023</a>; Desai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib14\" title=\"\">2021</a>)</cite>.\nAgainst this recent progress, LibriBrain allows testing whether the same brain-speech representations that enable segment retrieval also support lexical selectivity for pre-specified words in its long-form, naturalistic stories. Due to its larger scale, it also allows building on prior EEG-based KWS pilots, which have largely remained at small-lexicon trialwise classification/onset detection <cite class=\"ltx_cite ltx_citemacro_citep\">(Sakthi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib48\" title=\"\">2021</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "show",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use area under the precision-recall curve (AUPRC) as the primary metric. This is well suited to keyword spotting because its base rate equals the empirical prevalence of positives, so gains are easy to interpret. It also summarises the trade-off we actually care about under heavy imbalance&#8212;how many of the system&#8217;s alarms are correct (precision) as we demand more coverage (recall). Finally, unlike alternative metrics (e.g., AUROC), AUPRC is not overly optimistic when precision is too low to be usable. We supplement AUPRC with additional metrics like AUROC to provide a comprehensive view.\nFor scenario-grounded reporting, we translate any validation-selected threshold (precision <math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math>, recall <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math>) into hourly rates under an assumed event frequency <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> (keywords/hour):</p>\n\n",
                "matched_terms": [
                    "base",
                    "keyword",
                    "auprc",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Where possible, all experiments use the standard train/validation/test splits provided by the <span class=\"ltx_text ltx_font_typewriter\">pnpl</span> dataset (using the logic described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S3.SS4\" title=\"3.4 Reference Model &#8227; 3 Methods &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a>) and are fully reproducible (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#A1\" title=\"Appendix A Open Resources: Code, Tutorial, and Leaderboard &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>). Unless noted, values are seed-averages over three runs. Error bars are standard errors across seeds. For Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Model Performance &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> we report standard errors approximated from 95% bootstrap CIs (4,000 resamples).</p>\n\n",
                "matched_terms": [
                    "over",
                    "runs",
                    "three"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first establish that the dataset carries usable signal for keyword detection by evaluating a single model on the held-out test set (<math alttext=\"n=4660\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>4660</mn></mrow><annotation encoding=\"application/x-tex\">n=4660</annotation></semantics></math>, positives <math alttext=\"=24\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>=</mo><mn>24</mn></mrow><annotation encoding=\"application/x-tex\">=24</annotation></semantics></math>; base rate <math alttext=\"=0.00515\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>=</mo><mn>0.00515</mn></mrow><annotation encoding=\"application/x-tex\">=0.00515</annotation></semantics></math>). Given the absence of prior publicly reproducible MEG keyword spotting benchmarks, we evaluate against permutation-derived random baselines to demonstrate the presence of meaningful signal rather than competitive performance. While overall performance is modest, threshold-free metrics indicate clear signal (AUPRC <math alttext=\"\\approx 13.4\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mo>&#8776;</mo><mn>13.4</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">\\approx 13.4\\times</annotation></semantics></math> the permutation baseline; AUROC <math alttext=\"\\approx 0.80\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.80</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.80</annotation></semantics></math>). Full results are provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Model Performance &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nBeyond threshold-free metrics, we include an operational snapshot. At a target recall of <math alttext=\"\\sim\\!0.10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>0.10</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!0.10</annotation></semantics></math>, the scenario-translated FA/h for the assistive case (<math alttext=\"\\lambda=2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=2</annotation></semantics></math>/h) is <math alttext=\"\\sim\\!2.19\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>2.19</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!2.19</annotation></semantics></math> (SE <math alttext=\"\\approx\\!1.63\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m9\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8776;</mo><mn>1.63</mn></mrow><annotation encoding=\"application/x-tex\">\\approx\\!1.63</annotation></semantics></math>), corresponding to <math alttext=\"\\sim\\!13\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m10\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>13</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!13</annotation></semantics></math> alerts per correct detection. For reference, directly counting false positives per hour under the labelled test coverage yields <math alttext=\"\\sim\\!16.3\\,\\mathrm{FA/h}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m11\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mrow><mrow><mn>16.3</mn><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><mi>FA</mi></mrow><mo>/</mo><mi mathvariant=\"normal\">h</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\sim\\!16.3\\,\\mathrm{FA/h}</annotation></semantics></math> (seed-avg; SE <math alttext=\"\\approx\\!12.1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m12\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8776;</mo><mn>12.1</mn></mrow><annotation encoding=\"application/x-tex\">\\approx\\!12.1</annotation></semantics></math>). Under FA/h budgets (scenario scale, <math alttext=\"\\lambda=2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m13\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=2</annotation></semantics></math>/h), the model achieves recall <math alttext=\"\\sim\\!0.14\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m14\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>0.14</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!0.14</annotation></semantics></math> at 2.0 FA/h and <math alttext=\"\\sim\\!0.08\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m15\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>0.08</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!0.08</annotation></semantics></math> at 0.5 FA/h (seed-averaged). These numbers contextualise the ranking metrics and set a baseline for future improvements. The right panel of Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.F4\" title=\"Figure 4 &#8227; 4.2 Keyword Choice &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the mean recall&#8211;FA/h operating curve with per-seed traces. For this snapshot, operating points are chosen on the test PR curves for presentation; in deployment we would select thresholds on validation and freeze them before testing.</p>\n\n",
                "matched_terms": [
                    "auprc",
                    "base",
                    "rate",
                    "seedaveraged",
                    "keyword",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An important consideration in KWS is the choise of keyword(s). In LibriBrain, as in many real-world corpora, longer words are rarer: word length in phonemes is negatively correlated with token frequency (Spearman <math alttext=\"r=-0.28\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mrow><mo>&#8722;</mo><mn>0.28</mn></mrow></mrow><annotation encoding=\"application/x-tex\">r=-0.28</annotation></semantics></math> with <math alttext=\"\\log\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\"><semantics><mi>log</mi><annotation encoding=\"application/x-tex\">\\log</annotation></semantics></math> frequency; <math alttext=\"p=2.7\\times 10^{-185}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mrow><mn>2.7</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>185</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">p=2.7\\times 10^{-185}</annotation></semantics></math>; Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.F3\" title=\"Figure 3 &#8227; 4.2 Keyword Choice &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> left). This matters because length can reduce false alarms while frequency controls how many positives we can realistically train on. To navigate this trade-off, we selected the most frequent word at each phoneme length and measured %<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math>AUPRC over the empirical base rate. The length-%<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m5\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math>AUPRC relation is non-monotonic (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.F3\" title=\"Figure 3 &#8227; 4.2 Keyword Choice &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> right): among a 12-item shortlist spanning 1-12 phonemes, the 5-phoneme <em class=\"ltx_emph ltx_font_italic\">watson</em> yields the largest %<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m6\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math>AUPRC, whereas several longer items (e.g., 9-12 phonemes) underperform despite greater duration.</p>\n\n",
                "matched_terms": [
                    "base",
                    "over",
                    "δdeltaauprc",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A controlled comparison across three similarly frequent keywords of different lengths (<em class=\"ltx_emph ltx_font_italic\">walk</em>, <em class=\"ltx_emph ltx_font_italic\">surely</em>, <em class=\"ltx_emph ltx_font_italic\">excellent</em>; 3/5/8 phonemes) shows no detectable difference in %<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math>AUPRC within our precision (overlapping SEMs; Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.F4\" title=\"Figure 4 &#8227; 4.2 Keyword Choice &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). This indicates that, once frequency is matched, mere length is not the primary driver of detectability in MEG KWS. The pattern is consistent with established constraints on neural speech processing and KWS: benefits accrue less from duration per se and more from properties that improve time-locking and reduce lexical competition&#8212;salient acoustic onsets and early stress (stronger M100/M200), an early uniqueness point (UP)&#8212;i.e., the keyword becomes lexically unique after only a few initial phonemes (a small UP index relative to its length)&#8212;a sparse phonological neighborhood, and moderate frequency with lower contextual predictability <cite class=\"ltx_cite ltx_citemacro_citep\">(Gage et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib17\" title=\"\">1998</a>; Leminen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib33\" title=\"\">2011</a>; Vitevitch and Luce, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib54\" title=\"\">1999</a>; Halgren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib20\" title=\"\">2002</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib9\" title=\"\">2014</a>)</cite>. Empirically, <em class=\"ltx_emph ltx_font_italic\">watson</em> may profit from prosodic prominence in narrative speech and an early uniqueness point, outweighing any gains attributable to length alone; <span class=\"ltx_text ltx_font_italic\">watson</span> may also benefit from attentional saliency, being a word that the subject consistently paid attention to.</p>\n\n",
                "matched_terms": [
                    "sems",
                    "overlapping",
                    "three",
                    "δdeltaauprc",
                    "keyword",
                    "surely",
                    "walk",
                    "comparison",
                    "excellent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To understand how keyword detection performance scales with available training data, we systematically varied the fraction of the 52-hour corpus used for training while keeping the validation and test sets fixed. For these scaling runs we used 0&#8201;s pre-onset and +0.25&#8201;s post-onset windows (per-instance window length of 1.05&#8201;s). Because training uses many overlapping windows around labelled events, the total windowed duration processed exceeds the 52&#8201;h of unique recordings: at 10% this corresponds to <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>14&#8201;h of windowed data (<math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>5.2&#8201;h unique), and at 100% to <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>143&#8201;h of windowed data.</p>\n\n",
                "matched_terms": [
                    "runs",
                    "keyword",
                    "overlapping"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.F5\" title=\"Figure 5 &#8227; 4.3 Data Scaling &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, AUPRC improves approximately log-linearly as we increase the training fraction from 10% to 100%, consistent with established within-subject scaling laws in neural decoding <cite class=\"ltx_cite ltx_citemacro_citep\">(d&#8217;Ascoli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib11\" title=\"\">2024</a>; Sato et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib49\" title=\"\">2024</a>)</cite>. Notably, even with just 10% of the training data (<math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>14&#8201;h windowed; <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m2\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>5.2&#8201;h unique), the model achieves meaningful performance above chance, suggesting that keyword detection remains feasible even in scenarios with limited recording time. Permutation tests confirm that AUPRC is not above chance at 5% (<math alttext=\"p=0.108\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.108</mn></mrow><annotation encoding=\"application/x-tex\">p=0.108</annotation></semantics></math>), but is already significant at 10% (<math alttext=\"p=0.0156\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.0156</mn></mrow><annotation encoding=\"application/x-tex\">p=0.0156</annotation></semantics></math>; one-sided, 10,000 draws), and remains strongly significant thereafter (20% <math alttext=\"p=6.0\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mrow><mn>6.0</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">p=6.0\\times 10^{-4}</annotation></semantics></math>; 40&#8211;100% <math alttext=\"p\\leq 2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m6\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&#8804;</mo><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">p\\leq 2\\times 10^{-4}</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "auprc",
                    "keyword",
                    "significant"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Adding pre- and post-onset offsets modestly improves detection. Averaged over all non-zero offsets, AUPRC increases by <math alttext=\"\\sim\\!25\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mrow><mn>25</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\sim\\!25\\%</annotation></semantics></math> relative to the 0/0 baseline (absolute +0.0097). This improvement is statistically significant (paired per-seed mean +0.0099 AUPRC, SE 0.0028; 95% CI [0.0045, 0.0154]; one-sided <math alttext=\"p&lt;10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">p&lt;10^{-3}</annotation></semantics></math>). We observe the best AUPRC near (neg=0.1s, pos=0.3s), but no clear monotonic trend across offsets; performance is relatively flat in a small neighbourhood around this setting and declines for very short or overly long windows, suggesting a bias&#8211;variance trade-off between providing sufficient context and diluting signal with unrelated activity.</p>\n\n",
                "matched_terms": [
                    "over",
                    "auprc",
                    "significant"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study reports results for a single participant on a single corpus. Generalisability across participants remains an open question for Neural Keyword Spotting, though generalization is becoming less of a problem in decoding than it was <cite class=\"ltx_cite ltx_citemacro_citep\">(Csaky et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib10\" title=\"\">2023</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib16\" title=\"\">2023</a>; d&#8217;Ascoli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib11\" title=\"\">2024</a>; Jayalath et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib26\" title=\"\">2025b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib25\" title=\"\">a</a>)</cite>. Validation and test sessions were selected to maximise positives, stabilising metrics at the expense of a mild base-rate bias. Test sets contain few positives, so thresholded metrics carry substantial uncertainty. We evaluate a compact model only, without exploring richer encoders, self-supervised pretraining, streaming inference, or multi-keyword training. Finally, we use event-referenced windows; continuous-stream detection with latency and explicit false-alarm accounting remains open.</p>\n\n",
                "matched_terms": [
                    "keyword",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the success of competitions like the 2025 LibriBrain Competition <cite class=\"ltx_cite ltx_citemacro_citep\">(Landau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib30\" title=\"\">2025</a>)</cite> or Brain-to-text &#8217;25 <cite class=\"ltx_cite ltx_citemacro_citep\">(Card et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib8\" title=\"\">2025</a>)</cite>, we will release a leaderboard system for the keyword detection task as part of a larger-scale competition later this year. We also plan to extend this work to additional datasets, including inner speech and multi-subject recordings. Further analysis, such as phoneme-informed keyword selection and a deeper examination of temporal offsets, should clarify where gains are available building on the exploratory results presented here.</p>\n\n",
                "matched_terms": [
                    "keyword",
                    "results"
                ]
            }
        ]
    },
    "A3.T6": {
        "caption": "Table 6: Seed-averaged performance across temporal offsets around the keyword onset. Values are mean ±\\pm standard error across seeds. The row with the highest %Δ\\DeltaAUPRC over the 0/0 baseline is typeset in bold.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Neg [s]</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Pos [s]</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">AUPRC (mean <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> SE)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">AUROC (mean <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m2\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> SE)</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Seeds</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"0.039\\pm 0.009\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m3\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.039</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.009</mn></mrow><annotation encoding=\"application/x-tex\">0.039\\pm 0.009</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"0.811\\pm 0.011\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m4\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.811</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.011</mn></mrow><annotation encoding=\"application/x-tex\">0.811\\pm 0.011</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.05</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.043\\pm 0.003\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m5\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.043</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.003</mn></mrow><annotation encoding=\"application/x-tex\">0.043\\pm 0.003</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.813\\pm 0.008\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m6\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.813</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.008</mn></mrow><annotation encoding=\"application/x-tex\">0.813\\pm 0.008</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.10</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.030\\pm 0.007\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m7\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.030</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.007</mn></mrow><annotation encoding=\"application/x-tex\">0.030\\pm 0.007</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.779\\pm 0.015\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m8\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.779</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.015</mn></mrow><annotation encoding=\"application/x-tex\">0.779\\pm 0.015</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.15</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.039\\pm 0.009\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m9\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.039</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.009</mn></mrow><annotation encoding=\"application/x-tex\">0.039\\pm 0.009</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.799\\pm 0.021\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m10\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.799</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.021</mn></mrow><annotation encoding=\"application/x-tex\">0.799\\pm 0.021</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.20</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.037\\pm 0.003\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m11\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.037</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.003</mn></mrow><annotation encoding=\"application/x-tex\">0.037\\pm 0.003</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.781\\pm 0.011\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m12\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.781</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.011</mn></mrow><annotation encoding=\"application/x-tex\">0.781\\pm 0.011</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.25</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.064\\pm 0.007\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m13\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.064</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.007</mn></mrow><annotation encoding=\"application/x-tex\">0.064\\pm 0.007</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.820\\pm 0.002\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m14\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.820</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.002</mn></mrow><annotation encoding=\"application/x-tex\">0.820\\pm 0.002</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.30</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.052\\pm 0.006\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m15\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.052</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.006</mn></mrow><annotation encoding=\"application/x-tex\">0.052\\pm 0.006</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.821\\pm 0.007\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m16\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.821</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.007</mn></mrow><annotation encoding=\"application/x-tex\">0.821\\pm 0.007</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.05</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.083\\pm 0.029\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m17\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.083</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.029</mn></mrow><annotation encoding=\"application/x-tex\">0.083\\pm 0.029</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.799\\pm 0.025\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m18\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.799</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.025</mn></mrow><annotation encoding=\"application/x-tex\">0.799\\pm 0.025</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.05</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.05</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.049\\pm 0.010\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m19\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.049</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.010</mn></mrow><annotation encoding=\"application/x-tex\">0.049\\pm 0.010</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.781\\pm 0.010\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m20\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.781</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.010</mn></mrow><annotation encoding=\"application/x-tex\">0.781\\pm 0.010</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.05</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.10</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.039\\pm 0.006\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m21\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.039</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.006</mn></mrow><annotation encoding=\"application/x-tex\">0.039\\pm 0.006</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.780\\pm 0.013\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m22\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.780</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.013</mn></mrow><annotation encoding=\"application/x-tex\">0.780\\pm 0.013</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.05</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.15</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.040\\pm 0.013\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m23\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.040</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.013</mn></mrow><annotation encoding=\"application/x-tex\">0.040\\pm 0.013</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.758\\pm 0.002\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m24\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.758</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.002</mn></mrow><annotation encoding=\"application/x-tex\">0.758\\pm 0.002</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.05</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.20</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.053\\pm 0.007\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m25\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.053</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.007</mn></mrow><annotation encoding=\"application/x-tex\">0.053\\pm 0.007</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.812\\pm 0.009\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m26\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.812</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.009</mn></mrow><annotation encoding=\"application/x-tex\">0.812\\pm 0.009</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.05</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.25</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.069\\pm 0.021\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m27\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.069</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.021</mn></mrow><annotation encoding=\"application/x-tex\">0.069\\pm 0.021</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.800\\pm 0.010\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m28\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.800</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.010</mn></mrow><annotation encoding=\"application/x-tex\">0.800\\pm 0.010</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.05</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.30</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.045\\pm 0.014\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m29\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.045</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.014</mn></mrow><annotation encoding=\"application/x-tex\">0.045\\pm 0.014</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.796\\pm 0.003\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m30\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.796</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.003</mn></mrow><annotation encoding=\"application/x-tex\">0.796\\pm 0.003</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.10</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.025\\pm 0.005\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m31\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.025</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.005</mn></mrow><annotation encoding=\"application/x-tex\">0.025\\pm 0.005</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.730\\pm 0.011\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m32\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.730</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.011</mn></mrow><annotation encoding=\"application/x-tex\">0.730\\pm 0.011</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.10</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.05</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.069\\pm 0.031\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m33\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.069</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.031</mn></mrow><annotation encoding=\"application/x-tex\">0.069\\pm 0.031</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.826\\pm 0.005\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m34\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.826</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.005</mn></mrow><annotation encoding=\"application/x-tex\">0.826\\pm 0.005</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.10</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.10</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.047\\pm 0.011\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m35\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.047</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.011</mn></mrow><annotation encoding=\"application/x-tex\">0.047\\pm 0.011</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.789\\pm 0.005\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m36\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.789</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.005</mn></mrow><annotation encoding=\"application/x-tex\">0.789\\pm 0.005</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.10</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.15</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.070\\pm 0.003\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m37\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.070</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.003</mn></mrow><annotation encoding=\"application/x-tex\">0.070\\pm 0.003</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.787\\pm 0.020\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m38\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.787</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.020</mn></mrow><annotation encoding=\"application/x-tex\">0.787\\pm 0.020</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.10</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.20</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.080\\pm 0.029\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m39\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.080</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.029</mn></mrow><annotation encoding=\"application/x-tex\">0.080\\pm 0.029</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.836\\pm 0.006\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m40\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.836</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.006</mn></mrow><annotation encoding=\"application/x-tex\">0.836\\pm 0.006</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.10</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.25</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.071\\pm 0.007\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m41\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.071</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.007</mn></mrow><annotation encoding=\"application/x-tex\">0.071\\pm 0.007</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.787\\pm 0.014\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m42\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.787</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.014</mn></mrow><annotation encoding=\"application/x-tex\">0.787\\pm 0.014</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.10</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.30</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\mathbf{0.094}\\pm\\mathbf{0.032}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m43\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathsize=\"0.900em\" mathvariant=\"bold\">0.094</mn><mo mathsize=\"0.900em\">&#177;</mo><mn class=\"ltx_mathvariant_bold\" mathsize=\"0.900em\" mathvariant=\"bold\">0.032</mn></mrow><annotation encoding=\"application/x-tex\">\\mathbf{0.094}\\pm\\mathbf{0.032}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\mathbf{0.804}\\pm\\mathbf{0.017}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m44\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathsize=\"0.900em\" mathvariant=\"bold\">0.804</mn><mo mathsize=\"0.900em\">&#177;</mo><mn class=\"ltx_mathvariant_bold\" mathsize=\"0.900em\" mathvariant=\"bold\">0.017</mn></mrow><annotation encoding=\"application/x-tex\">\\mathbf{0.804}\\pm\\mathbf{0.017}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.15</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.038\\pm 0.012\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m45\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.038</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.012</mn></mrow><annotation encoding=\"application/x-tex\">0.038\\pm 0.012</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.764\\pm 0.027\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m46\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.764</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.027</mn></mrow><annotation encoding=\"application/x-tex\">0.764\\pm 0.027</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.15</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.05</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.027\\pm 0.005\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m47\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.027</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.005</mn></mrow><annotation encoding=\"application/x-tex\">0.027\\pm 0.005</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.781\\pm 0.028\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m48\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.781</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.028</mn></mrow><annotation encoding=\"application/x-tex\">0.781\\pm 0.028</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.15</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.10</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.053\\pm 0.014\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m49\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.053</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.014</mn></mrow><annotation encoding=\"application/x-tex\">0.053\\pm 0.014</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.760\\pm 0.010\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m50\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.760</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.010</mn></mrow><annotation encoding=\"application/x-tex\">0.760\\pm 0.010</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.15</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.15</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.036\\pm 0.003\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m51\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.036</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.003</mn></mrow><annotation encoding=\"application/x-tex\">0.036\\pm 0.003</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.779\\pm 0.013\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m52\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.779</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.013</mn></mrow><annotation encoding=\"application/x-tex\">0.779\\pm 0.013</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.15</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.20</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.030\\pm 0.003\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m53\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.030</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.003</mn></mrow><annotation encoding=\"application/x-tex\">0.030\\pm 0.003</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.795\\pm 0.014\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m54\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.795</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.014</mn></mrow><annotation encoding=\"application/x-tex\">0.795\\pm 0.014</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.15</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.25</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.027\\pm 0.006\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m55\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.027</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.006</mn></mrow><annotation encoding=\"application/x-tex\">0.027\\pm 0.006</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.789\\pm 0.017\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m56\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.789</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.017</mn></mrow><annotation encoding=\"application/x-tex\">0.789\\pm 0.017</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.15</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.30</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.034\\pm 0.007\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m57\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.034</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.007</mn></mrow><annotation encoding=\"application/x-tex\">0.034\\pm 0.007</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.797\\pm 0.017\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m58\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.797</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.017</mn></mrow><annotation encoding=\"application/x-tex\">0.797\\pm 0.017</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.20</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.00</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.029\\pm 0.000\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m59\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.029</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.000</mn></mrow><annotation encoding=\"application/x-tex\">0.029\\pm 0.000</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.764\\pm 0.000\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m60\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.764</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.000</mn></mrow><annotation encoding=\"application/x-tex\">0.764\\pm 0.000</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.20</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.05</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.045\\pm 0.010\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m61\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.045</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.010</mn></mrow><annotation encoding=\"application/x-tex\">0.045\\pm 0.010</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.819\\pm 0.009\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m62\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.819</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.009</mn></mrow><annotation encoding=\"application/x-tex\">0.819\\pm 0.009</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.20</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.10</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.046\\pm 0.015\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m63\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.046</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.015</mn></mrow><annotation encoding=\"application/x-tex\">0.046\\pm 0.015</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.810\\pm 0.004\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m64\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.810</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.004</mn></mrow><annotation encoding=\"application/x-tex\">0.810\\pm 0.004</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.20</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.15</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.036\\pm 0.003\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m65\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.036</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.003</mn></mrow><annotation encoding=\"application/x-tex\">0.036\\pm 0.003</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.801\\pm 0.020\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m66\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.801</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.020</mn></mrow><annotation encoding=\"application/x-tex\">0.801\\pm 0.020</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.20</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.20</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.042\\pm 0.003\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m67\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.042</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.003</mn></mrow><annotation encoding=\"application/x-tex\">0.042\\pm 0.003</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.792\\pm 0.010\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m68\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.792</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.010</mn></mrow><annotation encoding=\"application/x-tex\">0.792\\pm 0.010</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.20</span></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.25</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.037\\pm 0.009\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m69\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.037</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.009</mn></mrow><annotation encoding=\"application/x-tex\">0.037\\pm 0.009</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"0.822\\pm 0.011\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m70\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.822</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.011</mn></mrow><annotation encoding=\"application/x-tex\">0.822\\pm 0.011</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.20</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.30</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><math alttext=\"0.050\\pm 0.012\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m71\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.050</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.012</mn></mrow><annotation encoding=\"application/x-tex\">0.050\\pm 0.012</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><math alttext=\"0.836\\pm 0.008\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T6.m72\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">0.836</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.008</mn></mrow><annotation encoding=\"application/x-tex\">0.836\\pm 0.008</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_right ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "0730±00110730pm",
            "0758±00020758pm",
            "0789±00050789pm",
            "seeds",
            "0780±00130780pm",
            "onset",
            "error",
            "0801±00200801pm",
            "0070±00030070pm",
            "0039±00090039pm",
            "seedaveraged",
            "offsets",
            "0810±00040810pm",
            "over",
            "bold",
            "0045±00140045pm",
            "0053±00070053pm",
            "0781±00100781pm",
            "0039±00060039pm",
            "0764±00000764pm",
            "0030±00070030pm",
            "0795±00140795pm",
            "0836±00060836pm",
            "keyword",
            "0836±00080836pm",
            "0069±00310069pm",
            "0789±00170789pm",
            "0040±00130040pm",
            "0083±00290083pm",
            "0820±00020820pm",
            "0030±00030030pm",
            "0037±00030037pm",
            "0036±00030036pm",
            "0799±00250799pm",
            "0800±00100800pm",
            "across",
            "0027±00060027pm",
            "0792±00100792pm",
            "mean",
            "auroc",
            "0027±00050027pm",
            "0029±00000029pm",
            "0826±00050826pm",
            "neg",
            "0797±00170797pm",
            "0781±00110781pm",
            "standard",
            "around",
            "0064±00070064pm",
            "±pm",
            "0787±00140787pm",
            "0047±00110047pm",
            "typeset",
            "0038±00120038pm",
            "0094±0032mathbf0094pmmathbf0032",
            "pos",
            "0760±00100760pm",
            "0050±00120050pm",
            "0822±00110822pm",
            "0812±00090812pm",
            "0045±00100045pm",
            "0811±00110811pm",
            "performance",
            "0025±00050025pm",
            "0779±00130779pm",
            "0796±00030796pm",
            "0037±00090037pm",
            "values",
            "0049±00100049pm",
            "0034±00070034pm",
            "highest",
            "0069±00210069pm",
            "baseline",
            "temporal",
            "0052±00060052pm",
            "0804±0017mathbf0804pmmathbf0017",
            "δdeltaauprc",
            "0821±00070821pm",
            "0781±00280781pm",
            "row",
            "0053±00140053pm",
            "0819±00090819pm",
            "0080±00290080pm",
            "0046±00150046pm",
            "0787±00200787pm",
            "0764±00270764pm",
            "0813±00080813pm",
            "0043±00030043pm",
            "0071±00070071pm",
            "0779±00150779pm",
            "0042±00030042pm",
            "auprc",
            "0799±00210799pm"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Non-invasive brain-computer interfaces (BCIs) are beginning to benefit from large, public benchmarks. However, current benchmarks target relatively simple, foundational tasks like Speech Detection and Phoneme Classification, while application-ready results on tasks like Brain-to-Text remain elusive. We propose Keyword Spotting (KWS) as a practically applicable, privacy-aware intermediate task. Using the deep 52-hour, within-subject LibriBrain corpus, we provide standardized train/validation/test splits for reproducible benchmarking, and adopt an evaluation protocol tailored to extreme class imbalance. Concretely, we use area under the precision-recall curve (AUPRC) as a robust evaluation metric, complemented by false alarms per hour (FA/h) at fixed recall to capture user-facing trade-offs. To simplify deployment and further experimentation within the research community, we are releasing an updated version of the <span class=\"ltx_text ltx_font_typewriter\">pnpl</span> library with word-level dataloaders and Colab-ready tutorials. As an initial reference model, we present a compact 1-D Conv/ResNet baseline with focal loss and top-<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> pooling that is trainable on a single consumer-class GPU. The reference model achieves <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math><math alttext=\"13\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"m3\" intent=\":literal\"><semantics><mrow><mn>13</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">13\\times</annotation></semantics></math> the permutation-baseline AUPRC on held-out sessions, demonstrating the viability of the task. Exploratory analyses reveal: (i) predictable within-subject scaling&#8212;performance improves log-linearly with more training hours&#8212;and (ii) the existence of word-level factors (frequency and duration) that systematically modulate detectability.</p>\n\n",
                "matched_terms": [
                    "auprc",
                    "baseline",
                    "keyword"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As <cite class=\"ltx_cite ltx_citemacro_citet\">Landau et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib30\" title=\"\">2025</a>)</cite> explain, the two tasks in the 2025 PNPL Competition were selected for their simplicity. Over time, the idea was to increase the complexity, and utility, of benchmark decoding tasks. Keyword Spotting (KWS) is an exciting landmark as it represents the first decoding task on this curriculum with practical utility for BCIs. In the established domain of voice computing, Keyword Spotting is commonly used to detect &#8220;wake words&#8221; (e.g., &#8220;Hey Siri&#8221;, &#8220;Alexa&#8221;, &#8220;OK Google&#8221;). Wake words like these can be used to indicate that subsequent speech should be interpreted as a command, or they can be used themselves as commands. In the emerging domain of <span class=\"ltx_text ltx_font_italic\">brain</span> computing, even a single wake word (e.g., &#8220;help&#8221;) could be profoundly meaningful to someone with severe paralysis. Only slightly further along the curriculum, a small set of working keywords (e.g., &#8220;hungry&#8221;, &#8220;tired&#8221;, &#8220;thirsty&#8221;, &#8220;toilet&#8221;, &#8220;pain&#8221;) would transform their quality of life. The benchmark task established in this paper is intended, ultimately, to lead to such an outcome. For clarity, and to contrast the use of acoustic inputs, we use the term <span class=\"ltx_text ltx_font_italic\">Neural Keyword Spotting</span> to denote Keyword Spotting from continuous brain data, a task represented schematically in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S0.F1\" title=\"Figure 1 &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "over",
                    "keyword"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prior to the release of LibriBrain <cite class=\"ltx_cite ltx_citemacro_citep\">(&#214;zdogan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib42\" title=\"\">2025</a>)</cite>, together with a standard Python library (<span class=\"ltx_text ltx_font_typewriter\">pnpl</span>) for loading predefined data splits <cite class=\"ltx_cite ltx_citemacro_citep\">(Landau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib30\" title=\"\">2025</a>)</cite>, a number of open datasets <cite class=\"ltx_cite ltx_citemacro_citep\">(e.g., Schoffelen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib51\" title=\"\">2019</a>; Nastase et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib40\" title=\"\">2022</a>; Gwilliams et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib19\" title=\"\">2023</a>)</cite> were starting to reappear across large-scale studies <cite class=\"ltx_cite ltx_citemacro_citep\">(e.g., D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib16\" title=\"\">2023</a>; d&#8217;Ascoli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib11\" title=\"\">2024</a>; Ridge and Parker&#160;Jones, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib43\" title=\"\">2024</a>; Jayalath et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib26\" title=\"\">2025b</a>)</cite>.\nHowever, data splits were not generally replicated making it difficult to compare methods. The same model architectures and weights were neither generally shared nor used as baselines and there were no public leaderboards.</p>\n\n",
                "matched_terms": [
                    "across",
                    "standard"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Brain-to-Text (B2T)</span> takes variable-length neural sequences (e.g., EEG, MEG, fMRI) as input and outputs text transcripts, typically evaluated with word error rate (WER) or semantic similarity metrics such as the BERTScore <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib62\" title=\"\">2020</a>)</cite>. B2T is the analogue of ASR and represents a long-term goal, though its difficulty has motivated the development of what we call intermediate tasks, which lie between more foundational tasks like Speech Detection and full B2T. Non-invasive B2T has been explored with EEG and MEG <cite class=\"ltx_cite ltx_citemacro_citep\">(Duan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib15\" title=\"\">2023</a>; Jo et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib27\" title=\"\">2024</a>; Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib59\" title=\"\">2024b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib60\" title=\"\">c</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib58\" title=\"\">a</a>)</cite>, with semantic metrics often reported in place of WER, although recent work shows that competitive WERs are beginning to be achievable non-invasively <cite class=\"ltx_cite ltx_citemacro_citep\">(Jayalath et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib25\" title=\"\">2025a</a>)</cite>. In fMRI, the coarse temporal resolution makes word-level alignment unlikely, though remarkable paraphrases have been produced which retain some semantic similarities to the ground truth speech <cite class=\"ltx_cite ltx_citemacro_citep\">(e.g., Tang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib52\" title=\"\">2023</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "temporal",
                    "error"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Keyword spotting in the traditional audio domain (also referred to as wake-word detection) is a mature, highly-imbalanced detection problem optimizsed for very low false-alarm (FA) rates at fixed recall. Early small-footprint CNN and CRNN systems established the modern operating regime (e.g., 0.5 FA/h at acceptable FRR) under tight on-device constraints\n<cite class=\"ltx_cite ltx_citemacro_citep\">(e.g.,  Sainath and Parada, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib46\" title=\"\">2015</a>; Ar&#237;k et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib3\" title=\"\">2017</a>)</cite>. Large benchmarks like Speech Commands <cite class=\"ltx_cite ltx_citemacro_citep\">(Warden, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib55\" title=\"\">2018</a>)</cite>) and efficient architectures like MatchboxNet <cite class=\"ltx_cite ltx_citemacro_citep\">(Maidina et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib36\" title=\"\">2020</a>)</cite> and Keyword Transformer <cite class=\"ltx_cite ltx_citemacro_citep\">(Berg et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib5\" title=\"\">2021</a>)</cite> further drove accuracy/latency trade-offs for embedded devices, while industrial deployments (e.g., Apple&#8217;s &#8220;Hey Siri&#8221;) codified evaluation practices around FA/h and user-centric thresholds <cite class=\"ltx_cite ltx_citemacro_citep\">(Apple Siri Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib2\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "keyword",
                    "around"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Non-invasive technologies (EEG/MEG) have dramatic benefits over surgical implants in terms of safety and scalability. The application of keyword spotting is motivated by two converging strands. First, segment identification decoders trained to predict self-supervised speech representations from brain signals reliably retrieve the matching few-second stimulus among large candidate sets and generalise across participants - evidence that non-invasive signals carry phonetic/lexical detail at the granularity needed for lexical identification <cite class=\"ltx_cite ltx_citemacro_citep\">(D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib16\" title=\"\">2023</a>; d&#8217;Ascoli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib11\" title=\"\">2024</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "over",
                    "across",
                    "keyword"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The release spans 93 sessions (3,139&#8201;min; 52.32&#8201;h) with 466,230 word tokens (16,892 unique) and 1,511,732 phoneme tokens. See Figures <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#A2.F6\" title=\"Figure 6 &#8227; Appendix B Dataset Figures &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#A2.F7\" title=\"Figure 7 &#8227; Appendix B Dataset Figures &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> for an overview of the dataset. Word frequencies are Zipfian, providing keywords across a wide base-rate spectrum (short, frequent function words vs. longer, rarer content/proper names). These properties suit event-referenced keyword detection with extreme class imbalance.</p>\n\n",
                "matched_terms": [
                    "across",
                    "keyword"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We cast neural keyword spotting (KWS) from MEG as an event-referenced detection task using LibriBrain word onsets <cite class=\"ltx_cite ltx_citemacro_citep\">(&#214;zdogan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib42\" title=\"\">2025</a>)</cite>. This can be formalized as follows: First, we fix a small keyword set <math alttext=\"\\mathcal{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><annotation encoding=\"application/x-tex\">\\mathcal{V}</annotation></semantics></math> (minimally <math alttext=\"|\\mathcal{V}|=1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo stretchy=\"false\">|</mo></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">|\\mathcal{V}|=1</annotation></semantics></math>). For each keyword <math alttext=\"k\\in\\mathcal{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi></mrow><annotation encoding=\"application/x-tex\">k\\in\\mathcal{V}</annotation></semantics></math>, let <math alttext=\"d_{\\max}(k)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><msub><mi>d</mi><mi>max</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">d_{\\max}(k)</annotation></semantics></math> be the maximum duration of any instance of <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> in the corpus. Given that we may want to extract brain recordings that start and end before and after audio event boundaries (e.g., because neural processing continues after the presentation of a stimulus), we can select fixed pre/post buffers <math alttext=\"\\beta^{-}\\!\\geq 0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m6\" intent=\":literal\"><semantics><mrow><msup><mi>&#946;</mi><mo>&#8722;</mo></msup><mo lspace=\"0.108em\">&#8805;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\beta^{-}\\!\\geq 0</annotation></semantics></math> and <math alttext=\"\\beta^{+}\\!\\geq 0\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m7\" intent=\":literal\"><semantics><mrow><msup><mi>&#946;</mi><mo>+</mo></msup><mo lspace=\"0.108em\">&#8805;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\beta^{+}\\!\\geq 0</annotation></semantics></math>.\nThese offsets can then be used to define <math alttext=\"D(\\mathcal{V})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m8\" intent=\":literal\"><semantics><mrow><mi>D</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">D(\\mathcal{V})</annotation></semantics></math>, which is the total window duration (in seconds) of neural data to extract around any keyword in <math alttext=\"\\mathcal{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m9\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><annotation encoding=\"application/x-tex\">\\mathcal{V}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "keyword",
                    "around",
                    "offsets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast to KWS, full-vocabulary Brain-to-Text aims to identify <math alttext=\"w\\in\\mathcal{W}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><mi>w</mi><mo>&#8712;</mo><mi class=\"ltx_font_mathcaligraphic\">&#119986;</mi></mrow><annotation encoding=\"application/x-tex\">w\\in\\mathcal{W}</annotation></semantics></math> (hundreds of thousands of words), which introduces severe long-tail sparsity and requires calibrating thresholds across many classes. KWS is a practical, fixed-lexicon task: it asks only whether any member of a small, predefined set <math alttext=\"\\mathcal{V}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119985;</mi><annotation encoding=\"application/x-tex\">\\mathcal{V}</annotation></semantics></math> occurred. This offers a simple, reliable trigger with clean control over latency and false alarms&#8212;useful both on its own (assistive or hands-free commands) and as a stepping stone towards richer decoders.</p>\n\n",
                "matched_terms": [
                    "over",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use area under the precision-recall curve (AUPRC) as the primary metric. This is well suited to keyword spotting because its base rate equals the empirical prevalence of positives, so gains are easy to interpret. It also summarises the trade-off we actually care about under heavy imbalance&#8212;how many of the system&#8217;s alarms are correct (precision) as we demand more coverage (recall). Finally, unlike alternative metrics (e.g., AUROC), AUPRC is not overly optimistic when precision is too low to be usable. We supplement AUPRC with additional metrics like AUROC to provide a comprehensive view.\nFor scenario-grounded reporting, we translate any validation-selected threshold (precision <math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mi>P</mi><annotation encoding=\"application/x-tex\">P</annotation></semantics></math>, recall <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math>) into hourly rates under an assumed event frequency <math alttext=\"\\lambda\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m3\" intent=\":literal\"><semantics><mi>&#955;</mi><annotation encoding=\"application/x-tex\">\\lambda</annotation></semantics></math> (keywords/hour):</p>\n\n",
                "matched_terms": [
                    "auroc",
                    "auprc",
                    "keyword"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our reference system ingests 306-channel MEG windows of length <span class=\"ltx_text ltx_font_italic\">T</span> and processes them with a compact temporal convolutional trunk that includes a residual block and a time-downsampling layer, yielding a\n<math alttext=\"128\\times\\textit{T'}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>128</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mtext class=\"ltx_mathvariant_italic\">T&#8217;</mtext></mrow><annotation encoding=\"application/x-tex\">128\\times\\textit{T'}</annotation></semantics></math> representation (temporal CNNs are strong sequence/biosignal decoders; residual connections stabilize deeper stacks and enlarge receptive fields efficiently <cite class=\"ltx_cite ltx_citemacro_citep\">(Bai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib4\" title=\"\">2018</a>; Schirrmeister et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib50\" title=\"\">2017</a>; Lawhern et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib32\" title=\"\">2018</a>; He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib23\" title=\"\">2016</a>)</cite>). A projection stage produces a 512-channel sequence, from which two <math alttext=\"1\\times 1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1\\times 1</annotation></semantics></math> temporal heads compute (i) per-time logits and (ii) attention scores normalised along time. The final output is a scalar logit obtained by attention-weighted summation of the per-time logits (a learned MIL-style pooling well-suited to brief events within longer windows <cite class=\"ltx_cite ltx_citemacro_citep\">(Ilse et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib24\" title=\"\">2018</a>; Kong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib28\" title=\"\">2020</a>; McFee et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib37\" title=\"\">2018</a>)</cite>; in MEG, this lets the model emphasise time-locked acoustic/lexical responses such as M100/N400 components <cite class=\"ltx_cite ltx_citemacro_citep\">(Gage et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib17\" title=\"\">1998</a>; Halgren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib20\" title=\"\">2002</a>; Hari and Salmelin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib21\" title=\"\">2012</a>)</cite>). Training uses focal loss with a small pairwise ranking term: focal down-weights abundant easy negatives and focuses gradient on rare, hard positives under extreme imbalance <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib34\" title=\"\">2017</a>)</cite>, while the pairwise (logistic) ranking aux loss encourages correct ordering of positives above negatives, supporting PR/Average-Precision-aligned selection <cite class=\"ltx_cite ltx_citemacro_citep\">(Burges, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib7\" title=\"\">2010</a>; Yue et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib61\" title=\"\">2007</a>; Davis and Goadrich, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib13\" title=\"\">2006</a>; Saito and Rehmsmeier, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib47\" title=\"\">2015</a>)</cite>. Batches are class-balanced by oversampling positives, and we apply light temporal jitter and additive noise (both standard, effective regularizers for EEG/MEG time-series <cite class=\"ltx_cite ltx_citemacro_citep\">(Buda et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib6\" title=\"\">2018</a>; Lashgari et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib31\" title=\"\">2020</a>; He et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib22\" title=\"\">2021</a>; Rommel et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib44\" title=\"\">2022</a>)</cite>). We optimise with AdamW <cite class=\"ltx_cite ltx_citemacro_citep\">(Loshchilov and Hutter, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib35\" title=\"\">2019</a>)</cite> and select checkpoints by validation AUPRC (preferred under heavy class imbalance <cite class=\"ltx_cite ltx_citemacro_citep\">(Saito and Rehmsmeier, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib47\" title=\"\">2015</a>; Davis and Goadrich, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib13\" title=\"\">2006</a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "standard",
                    "auprc",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Where possible, all experiments use the standard train/validation/test splits provided by the <span class=\"ltx_text ltx_font_typewriter\">pnpl</span> dataset (using the logic described in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S3.SS4\" title=\"3.4 Reference Model &#8227; 3 Methods &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">3.4</span></a>) and are fully reproducible (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#A1\" title=\"Appendix A Open Resources: Code, Tutorial, and Leaderboard &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">A</span></a>). Unless noted, values are seed-averages over three runs. Error bars are standard errors across seeds. For Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Model Performance &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> we report standard errors approximated from 95% bootstrap CIs (4,000 resamples).</p>\n\n",
                "matched_terms": [
                    "over",
                    "across",
                    "standard",
                    "seeds",
                    "error",
                    "values"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first establish that the dataset carries usable signal for keyword detection by evaluating a single model on the held-out test set (<math alttext=\"n=4660\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>n</mi><mo>=</mo><mn>4660</mn></mrow><annotation encoding=\"application/x-tex\">n=4660</annotation></semantics></math>, positives <math alttext=\"=24\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>=</mo><mn>24</mn></mrow><annotation encoding=\"application/x-tex\">=24</annotation></semantics></math>; base rate <math alttext=\"=0.00515\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>=</mo><mn>0.00515</mn></mrow><annotation encoding=\"application/x-tex\">=0.00515</annotation></semantics></math>). Given the absence of prior publicly reproducible MEG keyword spotting benchmarks, we evaluate against permutation-derived random baselines to demonstrate the presence of meaningful signal rather than competitive performance. While overall performance is modest, threshold-free metrics indicate clear signal (AUPRC <math alttext=\"\\approx 13.4\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mo>&#8776;</mo><mn>13.4</mn><mo lspace=\"0.222em\">&#215;</mo></mrow><annotation encoding=\"application/x-tex\">\\approx 13.4\\times</annotation></semantics></math> the permutation baseline; AUROC <math alttext=\"\\approx 0.80\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.80</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.80</annotation></semantics></math>). Full results are provided in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.T1\" title=\"Table 1 &#8227; 4.1 Model Performance &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nBeyond threshold-free metrics, we include an operational snapshot. At a target recall of <math alttext=\"\\sim\\!0.10\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>0.10</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!0.10</annotation></semantics></math>, the scenario-translated FA/h for the assistive case (<math alttext=\"\\lambda=2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=2</annotation></semantics></math>/h) is <math alttext=\"\\sim\\!2.19\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m8\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>2.19</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!2.19</annotation></semantics></math> (SE <math alttext=\"\\approx\\!1.63\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m9\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8776;</mo><mn>1.63</mn></mrow><annotation encoding=\"application/x-tex\">\\approx\\!1.63</annotation></semantics></math>), corresponding to <math alttext=\"\\sim\\!13\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m10\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>13</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!13</annotation></semantics></math> alerts per correct detection. For reference, directly counting false positives per hour under the labelled test coverage yields <math alttext=\"\\sim\\!16.3\\,\\mathrm{FA/h}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m11\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mrow><mrow><mn>16.3</mn><mo lspace=\"0.170em\" rspace=\"0em\">&#8203;</mo><mi>FA</mi></mrow><mo>/</mo><mi mathvariant=\"normal\">h</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\sim\\!16.3\\,\\mathrm{FA/h}</annotation></semantics></math> (seed-avg; SE <math alttext=\"\\approx\\!12.1\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m12\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8776;</mo><mn>12.1</mn></mrow><annotation encoding=\"application/x-tex\">\\approx\\!12.1</annotation></semantics></math>). Under FA/h budgets (scenario scale, <math alttext=\"\\lambda=2\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m13\" intent=\":literal\"><semantics><mrow><mi>&#955;</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">\\lambda=2</annotation></semantics></math>/h), the model achieves recall <math alttext=\"\\sim\\!0.14\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m14\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>0.14</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!0.14</annotation></semantics></math> at 2.0 FA/h and <math alttext=\"\\sim\\!0.08\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m15\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>0.08</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!0.08</annotation></semantics></math> at 0.5 FA/h (seed-averaged). These numbers contextualise the ranking metrics and set a baseline for future improvements. The right panel of Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.F4\" title=\"Figure 4 &#8227; 4.2 Keyword Choice &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the mean recall&#8211;FA/h operating curve with per-seed traces. For this snapshot, operating points are chosen on the test PR curves for presentation; in deployment we would select thresholds on validation and freeze them before testing.</p>\n\n",
                "matched_terms": [
                    "auprc",
                    "seedaveraged",
                    "mean",
                    "auroc",
                    "keyword",
                    "baseline",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An important consideration in KWS is the choise of keyword(s). In LibriBrain, as in many real-world corpora, longer words are rarer: word length in phonemes is negatively correlated with token frequency (Spearman <math alttext=\"r=-0.28\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>r</mi><mo>=</mo><mrow><mo>&#8722;</mo><mn>0.28</mn></mrow></mrow><annotation encoding=\"application/x-tex\">r=-0.28</annotation></semantics></math> with <math alttext=\"\\log\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\"><semantics><mi>log</mi><annotation encoding=\"application/x-tex\">\\log</annotation></semantics></math> frequency; <math alttext=\"p=2.7\\times 10^{-185}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mrow><mn>2.7</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>185</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">p=2.7\\times 10^{-185}</annotation></semantics></math>; Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.F3\" title=\"Figure 3 &#8227; 4.2 Keyword Choice &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> left). This matters because length can reduce false alarms while frequency controls how many positives we can realistically train on. To navigate this trade-off, we selected the most frequent word at each phoneme length and measured %<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m4\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math>AUPRC over the empirical base rate. The length-%<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m5\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math>AUPRC relation is non-monotonic (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.F3\" title=\"Figure 3 &#8227; 4.2 Keyword Choice &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> right): among a 12-item shortlist spanning 1-12 phonemes, the 5-phoneme <em class=\"ltx_emph ltx_font_italic\">watson</em> yields the largest %<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m6\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math>AUPRC, whereas several longer items (e.g., 9-12 phonemes) underperform despite greater duration.</p>\n\n",
                "matched_terms": [
                    "over",
                    "δdeltaauprc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A controlled comparison across three similarly frequent keywords of different lengths (<em class=\"ltx_emph ltx_font_italic\">walk</em>, <em class=\"ltx_emph ltx_font_italic\">surely</em>, <em class=\"ltx_emph ltx_font_italic\">excellent</em>; 3/5/8 phonemes) shows no detectable difference in %<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math>AUPRC within our precision (overlapping SEMs; Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.F4\" title=\"Figure 4 &#8227; 4.2 Keyword Choice &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). This indicates that, once frequency is matched, mere length is not the primary driver of detectability in MEG KWS. The pattern is consistent with established constraints on neural speech processing and KWS: benefits accrue less from duration per se and more from properties that improve time-locking and reduce lexical competition&#8212;salient acoustic onsets and early stress (stronger M100/M200), an early uniqueness point (UP)&#8212;i.e., the keyword becomes lexically unique after only a few initial phonemes (a small UP index relative to its length)&#8212;a sparse phonological neighborhood, and moderate frequency with lower contextual predictability <cite class=\"ltx_cite ltx_citemacro_citep\">(Gage et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib17\" title=\"\">1998</a>; Leminen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib33\" title=\"\">2011</a>; Vitevitch and Luce, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib54\" title=\"\">1999</a>; Halgren et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib20\" title=\"\">2002</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib9\" title=\"\">2014</a>)</cite>. Empirically, <em class=\"ltx_emph ltx_font_italic\">watson</em> may profit from prosodic prominence in narrative speech and an early uniqueness point, outweighing any gains attributable to length alone; <span class=\"ltx_text ltx_font_italic\">watson</span> may also benefit from attentional saliency, being a word that the subject consistently paid attention to.</p>\n\n",
                "matched_terms": [
                    "across",
                    "δdeltaauprc",
                    "keyword"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To understand how keyword detection performance scales with available training data, we systematically varied the fraction of the 52-hour corpus used for training while keeping the validation and test sets fixed. For these scaling runs we used 0&#8201;s pre-onset and +0.25&#8201;s post-onset windows (per-instance window length of 1.05&#8201;s). Because training uses many overlapping windows around labelled events, the total windowed duration processed exceeds the 52&#8201;h of unique recordings: at 10% this corresponds to <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>14&#8201;h of windowed data (<math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m2\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>5.2&#8201;h unique), and at 100% to <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p1.m3\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>143&#8201;h of windowed data.</p>\n\n",
                "matched_terms": [
                    "keyword",
                    "around",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#S4.F5\" title=\"Figure 5 &#8227; 4.3 Data Scaling &#8227; 4 Results &#8227; Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, AUPRC improves approximately log-linearly as we increase the training fraction from 10% to 100%, consistent with established within-subject scaling laws in neural decoding <cite class=\"ltx_cite ltx_citemacro_citep\">(d&#8217;Ascoli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib11\" title=\"\">2024</a>; Sato et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib49\" title=\"\">2024</a>)</cite>. Notably, even with just 10% of the training data (<math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>14&#8201;h windowed; <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m2\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>5.2&#8201;h unique), the model achieves meaningful performance above chance, suggesting that keyword detection remains feasible even in scenarios with limited recording time. Permutation tests confirm that AUPRC is not above chance at 5% (<math alttext=\"p=0.108\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m3\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.108</mn></mrow><annotation encoding=\"application/x-tex\">p=0.108</annotation></semantics></math>), but is already significant at 10% (<math alttext=\"p=0.0156\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m4\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.0156</mn></mrow><annotation encoding=\"application/x-tex\">p=0.0156</annotation></semantics></math>; one-sided, 10,000 draws), and remains strongly significant thereafter (20% <math alttext=\"p=6.0\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m5\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>=</mo><mrow><mn>6.0</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">p=6.0\\times 10^{-4}</annotation></semantics></math>; 40&#8211;100% <math alttext=\"p\\leq 2\\times 10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS3.p2.m6\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&#8804;</mo><mrow><mn>2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow></mrow><annotation encoding=\"application/x-tex\">p\\leq 2\\times 10^{-4}</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "auprc",
                    "keyword",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Adding pre- and post-onset offsets modestly improves detection. Averaged over all non-zero offsets, AUPRC increases by <math alttext=\"\\sim\\!25\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mrow><mn>25</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\sim\\!25\\%</annotation></semantics></math> relative to the 0/0 baseline (absolute +0.0097). This improvement is statistically significant (paired per-seed mean +0.0099 AUPRC, SE 0.0028; 95% CI [0.0045, 0.0154]; one-sided <math alttext=\"p&lt;10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.p1.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">p&lt;10^{-3}</annotation></semantics></math>). We observe the best AUPRC near (neg=0.1s, pos=0.3s), but no clear monotonic trend across offsets; performance is relatively flat in a small neighbourhood around this setting and declines for very short or overly long windows, suggesting a bias&#8211;variance trade-off between providing sufficient context and diluting signal with unrelated activity.</p>\n\n",
                "matched_terms": [
                    "over",
                    "across",
                    "auprc",
                    "around",
                    "mean",
                    "offsets",
                    "baseline",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This study reports results for a single participant on a single corpus. Generalisability across participants remains an open question for Neural Keyword Spotting, though generalization is becoming less of a problem in decoding than it was <cite class=\"ltx_cite ltx_citemacro_citep\">(Csaky et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib10\" title=\"\">2023</a>; D&#233;fossez et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib16\" title=\"\">2023</a>; d&#8217;Ascoli et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib11\" title=\"\">2024</a>; Jayalath et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib26\" title=\"\">2025b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib25\" title=\"\">a</a>)</cite>. Validation and test sessions were selected to maximise positives, stabilising metrics at the expense of a mild base-rate bias. Test sets contain few positives, so thresholded metrics carry substantial uncertainty. We evaluate a compact model only, without exploring richer encoders, self-supervised pretraining, streaming inference, or multi-keyword training. Finally, we use event-referenced windows; continuous-stream detection with latency and explicit false-alarm accounting remains open.</p>\n\n",
                "matched_terms": [
                    "across",
                    "keyword"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the success of competitions like the 2025 LibriBrain Competition <cite class=\"ltx_cite ltx_citemacro_citep\">(Landau et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib30\" title=\"\">2025</a>)</cite> or Brain-to-text &#8217;25 <cite class=\"ltx_cite ltx_citemacro_citep\">(Card et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.21038v2#bib.bib8\" title=\"\">2025</a>)</cite>, we will release a leaderboard system for the keyword detection task as part of a larger-scale competition later this year. We also plan to extend this work to additional datasets, including inner speech and multi-subject recordings. Further analysis, such as phoneme-informed keyword selection and a deeper examination of temporal offsets, should clarify where gains are available building on the exploratory results presented here.</p>\n\n",
                "matched_terms": [
                    "keyword",
                    "offsets",
                    "temporal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To encourage further exploration within the community, we also release a tutorial in the format of a Jupyter Notebook. Within the compute limits of the Colab Free Tier (<span class=\"ltx_text ltx_font_typewriter\">T4</span> GPU), the notebook allows for training a model around 10% of the LibriBrain dataset, reaching significantly above chance performance in under 30 minutes.\nThe notebook is available in the <span class=\"ltx_text ltx_font_typewriter\">tutorial</span> folder of the <span class=\"ltx_text ltx_font_typewriter\">keyword-experiments</span> repository<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>http://github.com/neural-processing-lab/libribrain-keyword-experiments</span></span></span>.</p>\n\n",
                "matched_terms": [
                    "around",
                    "performance"
                ]
            }
        ]
    }
}