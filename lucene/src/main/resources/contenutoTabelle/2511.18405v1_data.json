{
    "S2.T1": {
        "source_file": "A Multimodal Conversational Agent for Tabular Data Analysis",
        "caption": "TABLE I: Comparison of representative voice/data analytics systems.",
        "body": "System\nInput Modality\nExecution\nVisualization\nMultimodal\nMemory\nAdaptive\nLicense / Deploy\n\n\n\n\nTableau Ask Data\nText\n— (no code)\nYes\nNo\nYes\nNo\nProp.; On​-prem\n\n\nPower BI Q&A\nText\n— (no code)\nYes\nNo\nPartial\nNo\nProprietary; SaaS\n\n\nChatCSV\nText (CSV)\n— (auto charts)\nYes\nNo\nNo\nNo\nProprietary; SaaS\n\n\nPowerdrill AI\nText\nAuto analytics\nYes\nNo\nYes\nNo\nProprietary; SaaS\n\n\nLangChain Demos\nVoice + Text\nPrototype code gen\nBasic\nPartial\nYes\nNo\nOpen-source; On​-prem\n\n\nAmazon QuickSight Q\nText\nAuto analytics\nYes\nNo\nYes\nNo\nProprietary; SaaS\n\n\nQlik Sense\nVoice + Text\nAuto suggestions\nYes\nNo\nYes\nNo\nProprietary; SaaS + On​-prem\n\n\nTalk2Data (Ours)\nVoice + Text\nSecure code execution\nYes\nVoice & Visual\nYes\nYes\nOpen-source; On​-prem",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">System</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Input Modality</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Execution</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Visualization</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Multimodal</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Memory</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Adaptive</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">License / Deploy</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Tableau Ask Data</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Text</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#8212; (no code)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Yes</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">No</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Yes</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">No</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Prop.; On&#8203;-prem</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Power BI Q&amp;A</th>\n<td class=\"ltx_td ltx_align_center\">Text</td>\n<td class=\"ltx_td ltx_align_center\">&#8212; (no code)</td>\n<td class=\"ltx_td ltx_align_center\">Yes</td>\n<td class=\"ltx_td ltx_align_center\">No</td>\n<td class=\"ltx_td ltx_align_center\">Partial</td>\n<td class=\"ltx_td ltx_align_center\">No</td>\n<td class=\"ltx_td ltx_align_center\">Proprietary; SaaS</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ChatCSV</th>\n<td class=\"ltx_td ltx_align_center\">Text (CSV)</td>\n<td class=\"ltx_td ltx_align_center\">&#8212; (auto charts)</td>\n<td class=\"ltx_td ltx_align_center\">Yes</td>\n<td class=\"ltx_td ltx_align_center\">No</td>\n<td class=\"ltx_td ltx_align_center\">No</td>\n<td class=\"ltx_td ltx_align_center\">No</td>\n<td class=\"ltx_td ltx_align_center\">Proprietary; SaaS</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Powerdrill AI</th>\n<td class=\"ltx_td ltx_align_center\">Text</td>\n<td class=\"ltx_td ltx_align_center\">Auto analytics</td>\n<td class=\"ltx_td ltx_align_center\">Yes</td>\n<td class=\"ltx_td ltx_align_center\">No</td>\n<td class=\"ltx_td ltx_align_center\">Yes</td>\n<td class=\"ltx_td ltx_align_center\">No</td>\n<td class=\"ltx_td ltx_align_center\">Proprietary; SaaS</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">LangChain Demos</th>\n<td class=\"ltx_td ltx_align_center\">Voice + Text</td>\n<td class=\"ltx_td ltx_align_center\">Prototype code gen</td>\n<td class=\"ltx_td ltx_align_center\">Basic</td>\n<td class=\"ltx_td ltx_align_center\">Partial</td>\n<td class=\"ltx_td ltx_align_center\">Yes</td>\n<td class=\"ltx_td ltx_align_center\">No</td>\n<td class=\"ltx_td ltx_align_center\">Open-source; On&#8203;-prem</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Amazon QuickSight Q</th>\n<td class=\"ltx_td ltx_align_center\">Text</td>\n<td class=\"ltx_td ltx_align_center\">Auto analytics</td>\n<td class=\"ltx_td ltx_align_center\">Yes</td>\n<td class=\"ltx_td ltx_align_center\">No</td>\n<td class=\"ltx_td ltx_align_center\">Yes</td>\n<td class=\"ltx_td ltx_align_center\">No</td>\n<td class=\"ltx_td ltx_align_center\">Proprietary; SaaS</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Qlik Sense</th>\n<td class=\"ltx_td ltx_align_center\">Voice + Text</td>\n<td class=\"ltx_td ltx_align_center\">Auto suggestions</td>\n<td class=\"ltx_td ltx_align_center\">Yes</td>\n<td class=\"ltx_td ltx_align_center\">No</td>\n<td class=\"ltx_td ltx_align_center\">Yes</td>\n<td class=\"ltx_td ltx_align_center\">No</td>\n<td class=\"ltx_td ltx_align_center\">Proprietary; SaaS + On&#8203;-prem</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">Talk2Data (Ours)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Voice + Text</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">Secure code execution</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">Yes</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">Voice &amp; Visual</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">Yes</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">Yes</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Open-source; On&#8203;-prem</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "tableau",
            "representative",
            "voicedata",
            "code",
            "powerdrill",
            "langchain",
            "basic",
            "power",
            "data",
            "charts",
            "visual",
            "ask",
            "amazon",
            "quicksight",
            "deploy",
            "text",
            "qlik",
            "license",
            "proprietary",
            "visualization",
            "csv",
            "multimodal",
            "prototype",
            "modality",
            "system",
            "prop",
            "systems",
            "voice",
            "yes",
            "analytics",
            "comparison",
            "chatcsv",
            "sense",
            "memory",
            "opensource",
            "saas",
            "input",
            "gen",
            "auto",
            "ours",
            "secure",
            "adaptive",
            "partial",
            "on​prem",
            "execution",
            "talk2data",
            "suggestions",
            "demos"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We contrast representative systems along input/output modality, execution model, memory, and adaptivity.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18405v1#S2.T1\" title=\"TABLE I &#8227; II-A System Comparison &#8227; II Related Work &#8227; A Multimodal Conversational Agent for Tabular Data Analysis\"><span class=\"ltx_text ltx_ref_tag\">I</span></a> summarizes key properties of existing systems. Unlike prior work, our design uniquely integrates (1) dual input and output modalities (voice and text), (2) LLM-powered code generation with safe and transparent sandboxed execution, (3) adaptive modality switching, i.e., flexibly choosing between charts, tables, or brief spoken/text responses within one dialogue, and (4) conversational memory for multi-turn analysis.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large language models (LLMs) can reshape information processing by handling data analysis, visualization, and interpretation in an interactive, context-aware dialogue with users, including voice interaction, while maintaining high performance. In this article, we present Talk2Data, a multimodal LLM-driven conversational agent for intuitive data exploration. The system lets users query datasets with voice or text instructions and receive answers as plots, tables, statistics, or spoken explanations. Built on LLMs, the suggested design combines OpenAI Whisper automatic speech recognition (ASR) system, Qwen-coder code generation LLM/model, custom sandboxed execution tools, and Coqui library for text-to-speech (TTS) within an agentic orchestration loop. Unlike text-only analysis tools, it adapts responses across modalities and supports multi-turn dialogues grounded in dataset context. In an evaluation of 48 tasks on three datasets, our prototype achieved 95.8% accuracy with model-only generation time under 1.7 seconds (excluding ASR and execution time). A comparison across five LLM sizes (1.5B&#8211;32B) revealed accuracy&#8211;latency&#8211;cost trade-offs, with a 7B model providing the best balance for interactive use. By routing between conversation with user and code execution, constrained to a transparent sandbox, with simultaneously grounding prompts in schema-level context, the Talk2Data agent reliably retrieves actionable insights from tables while making computations verifiable. In the article, except for the Talk2Data agent itself, we discuss implications for human&#8211;data interaction, trust in LLM-driven analytics, and future extensions toward large-scale multimodal assistants.</p>\n\n",
                "matched_terms": [
                    "text",
                    "system",
                    "voice",
                    "code",
                    "visualization",
                    "analytics",
                    "data",
                    "comparison",
                    "execution",
                    "talk2data",
                    "multimodal",
                    "prototype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Interacting with data often requires programming skills or statistical expertise, creating barriers for managers, analysts, and other non-technical users&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Zheng2024MultimodalTableUnderstanding</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Sundar2023cTBLS</span>]</cite>. Natural language interfaces (NLIs) aim to improve this information seeking process by allowing users to query data conversationally&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Li2024NLITabular</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">CIS2022</span>]</cite>. At the same time, voice interfaces are becoming increasingly common in daily life, yet existing voice assistants remain limited: they can answer factual questions or control devices, but they lack the analytical capabilities needed for meaningful data exploration.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LLMs now provide a powerful foundation for code generation and complex reasoning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Chen2021Codex</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Roziere2023CodeLlama</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Hui2024Qwen25</span>]</cite>. Systems such as OpenAI&#8217;s Code Interpreter&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">OpenAI2023CodeInterpreter</span>]</cite> demonstrate this potential, but they typically support only text-based input/output. Another problem is that features like multimodal responses and transparent execution of generated code (capabilities that are increasingly important for reliable human&#8211;data interaction in the information seeking process) are less present in current voice assistant designs.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "systems",
                    "code",
                    "execution",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present Talk2Data, a multimodal conversational agent that enables users to effectively seek, retrieve, and analyze data stored in tabular datasets through either voice or text instructions, and to receive answers and insights as plots, tables, or spoken explanations. For example, users can ask &#8220;What&#8217;s the average delay for United flights?&#8221; or &#8220;Plot a histogram of age,&#8221; and the agent dynamically determines whether to generate Python code or provide a direct natural language response. Code is executed in a secure sandbox, results are narrated through text-to-speech (TTS), and multi-turn dialogue is supported via conversational memory. This design allows to adapt its responses to user intent, offering both analytical depth and accessibility.</p>\n\n",
                "matched_terms": [
                    "text",
                    "voice",
                    "code",
                    "data",
                    "talk2data",
                    "ask",
                    "multimodal",
                    "memory",
                    "secure"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At the core of this agent is a orchestration module that reasons about each query&#8217;s intent and selects the appropriate response path. Dataset metadata and conversation history are injected into structured prompts, enabling grounded, context-aware behavior. By integrating these components, this design demonstrates how LLMs can support natural, multimodal workflows for data analysis.</p>\n\n",
                "matched_terms": [
                    "multimodal",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multimodal information-seeking agent.</span> An end-to-end system that unifies voice/text input with visual, tabular, and spoken outputs for exploratory analysis, supporting multi-turn, back-and-forth dialogue for information seeking over tabular corpora.</p>\n\n",
                "matched_terms": [
                    "system",
                    "visual",
                    "multimodal",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Orchestration with transparent code execution.</span> A router that adaptively selects narration vs. code generation within one dialog using grounded prompts, paired with a secure sandbox that exposes code for provenance.</p>\n\n",
                "matched_terms": [
                    "code",
                    "execution",
                    "secure"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We review prior research in three areas most relevant to our work: (i) NLIs for data analytics and information retrieval, (ii) LLM-based code generation for data science, and (iii) voice-enabled data assistants.</p>\n\n",
                "matched_terms": [
                    "code",
                    "analytics",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-Based NLIs.</span> Commercial tools such as Tableau Ask Data&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Tableau2025AskData</span>]</cite>, Power BI Q&amp;A&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Microsoft2025PowerBIQA</span>]</cite>, AWS QuickSight Q&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">AWS2025QuickSightQ</span>]</cite>, and Qlik Sense&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Qlik2025InsightAdvisor</span>]</cite> enable querying structured data in natural language&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Zheng2024MultimodalTableUnderstanding</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Sundar2023cTBLS</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Li2024NLITabular</span>]</cite>. These interfaces lower the barrier to entry for exploration, but remain limited to text-based input/output. Recent surveys document the resurgence of neural methods for conversational information retrieval, highlighting the growing importance of multi-turn dialogue in facilitating interaction&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Gao2022CIRSurvey</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Hambarde2023IRSurvey</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">IRBook2024</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Zhang2024AIRetrieval</span>]</cite>. More recent LLM-powered utilities such as Powerdrill AI and ChatCSV&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Powerdrill2025CSVAssistant</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ChatCSV2025</span>]</cite> automate chart generation, but still do not support spoken interaction or adaptive response modalities.</p>\n\n",
                "matched_terms": [
                    "tableau",
                    "qlik",
                    "adaptive",
                    "powerdrill",
                    "power",
                    "data",
                    "chatcsv",
                    "sense",
                    "ask",
                    "quicksight"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM-Based Code Generation.</span> Advances in program synthesis with LLMs&#8212;including OpenAI Codex&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Chen2021Codex</span>]</cite>, Code Llama&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Roziere2023CodeLlama</span>]</cite>, and Qwen-2.5-Coder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Hui2024Qwen25</span>]</cite>&#8212;have demonstrated strong performance in generating analysis pipelines and data visualizations from text instructions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Nascimento2024LLM4DS</span>]</cite> which largely aids the process of retrieving insights using the generated code. However, these models are typically deployed in static, text-only settings. They lack conversational memory, execution tool, transparency of execution, and the ability to adapt outputs to user context or modality preferences.</p>\n\n",
                "matched_terms": [
                    "text",
                    "code",
                    "data",
                    "execution",
                    "memory",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Voice-Enabled Data Assistants.</span> Early prototypes built with LangChain, Streamlit, and gTTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Ngonidzashe2023LangChainDemo</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Kumar2023VoiceViz</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">GomezVazquez2024AutomaticGeneration</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Zhao2023QTSumm</span>]</cite> have demonstrated proof-of-concept voice interaction with tabular data. While promising, these systems remain limited: they provide only one-way voice interfaces, offer minimal multimodal feedback, and rely on heuristics rather than agentic decision-making about when to generate code versus when to respond conversationally.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "systems",
                    "code",
                    "langchain",
                    "data",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We position our work within prior art by noting that earlier systems typically emphasize one or two axes of the problem: (i) text-only NLIs that translate simple questions into charts or aggregations&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Tableau2025AskData</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Microsoft2025PowerBIQA</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">AWS2025QuickSightQ</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Qlik2025InsightAdvisor</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Powerdrill2025CSVAssistant</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ChatCSV2025</span>]</cite>, (ii) LLM-based code synthesis for analysis and visualization&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Chen2021Codex</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Roziere2023CodeLlama</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Hui2024Qwen25</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Nascimento2024LLM4DS</span>]</cite>, or (iii) voice front-ends that surface data with limited analytics depth&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Ngonidzashe2023LangChainDemo</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Kumar2023VoiceViz</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">GomezVazquez2024AutomaticGeneration</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Zhao2023QTSumm</span>]</cite>. Several recent demos combine two of these (e.g., voice <em class=\"ltx_emph ltx_font_italic\">plus</em> auto-charts, or text <em class=\"ltx_emph ltx_font_italic\">plus</em> code execution), but typically stop short of a unified conversation loop with memory, guarded execution, and modality-aware responses.</p>\n\n",
                "matched_terms": [
                    "text",
                    "systems",
                    "voice",
                    "code",
                    "visualization",
                    "charts",
                    "data",
                    "analytics",
                    "execution",
                    "demos",
                    "memory"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another dimension of differentiation is deployment: most commercial NLIs such as Tableau Ask Data, Power BI Q&amp;A, or Amazon QuickSight Q are proprietary SaaS features with limited or no on-prem support. By contrast, our Talk2Data prototype follows an open-source, self-hostable trajectory, emphasizing transparency and feasibility for on-prem deployments.</p>\n\n",
                "matched_terms": [
                    "tableau",
                    "saas",
                    "proprietary",
                    "power",
                    "data",
                    "talk2data",
                    "ask",
                    "amazon",
                    "prototype",
                    "opensource",
                    "quicksight"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The suggested design stays within this lineage and assembles known components&#8212;ASR, an LLM, and TTS&#8212;into a single workflow. The incremental aspects we explore are: (1) an explicit routing step (via a lightweight LangGraph node) that decides between code generation and direct narration using dataset-aware prompts; and (2) a restricted execution path that treats LLM code as a first-class artifact but runs it with guardrails (timeouts, whitelisting) and returns structured errors. Neither routing nor sandboxing is novel in isolation; our contribution is an empirical look at how these choices affect <em class=\"ltx_emph ltx_font_italic\">end-to-end</em> usability (accuracy, latency, and dialogue continuity) for tabular analysis, including when speech is the entry point.</p>\n\n",
                "matched_terms": [
                    "code",
                    "execution"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared with text-only NLIs in commercial business intelligence (BI) tools&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Tableau2025AskData</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Microsoft2025PowerBIQA</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">AWS2025QuickSightQ</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Qlik2025InsightAdvisor</span>]</cite>, the suggested design emphasizes transparency and adaptability over enterprise-specific heuristics. Whereas BI platforms often rely on curated semantic layers, governed vocabularies, and policy enforcement, our system favors flexibility by exposing generated code and supporting free-form queries across arbitrary tabular datasets. This design trades some enterprise features (e.g., lineage tracking, synonym governance) for openness and interpretability&#8212;qualities important for exploratory analysis and educational use.</p>\n\n",
                "matched_terms": [
                    "system",
                    "code"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Relative to LLM &#8220;code interpreter&#8221; environments&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">OpenAI2023CodeInterpreter</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Nascimento2024LLM4DS</span>]</cite>, our scope extends by adding dual input/output modalities (voice and text, visuals and speech) and an explicit routing layer that determines when to generate code and when to respond conversationally. These choices enable multimodal, multi-turn interaction rather than single-shot code execution.</p>\n\n",
                "matched_terms": [
                    "text",
                    "voice",
                    "code",
                    "execution",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Voice-centric prototypes&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Ngonidzashe2023LangChainDemo</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Kumar2023VoiceViz</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">GomezVazquez2024AutomaticGeneration</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Zhao2023QTSumm</span>]</cite> demonstrate the feasibility of ASR&#8211;LLM&#8211;TTS pipelines but treat prompts as one-shot. By contrast, this design integrates a guarded execution environment with explicit error surfacing and conversational memory, supporting iterative refinement of queries while maintaining predictability and safety.</p>\n\n",
                "matched_terms": [
                    "memory",
                    "execution"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In that sense, this work demonstrates how combining speech recognition, LLM-based reasoning, guarded code execution, and text-to-speech within a single orchestration loop can yield a dependable multimodal assistant for tabular data. The suggested design offers an open-ended yet safe exploration of datasets through natural conversation, which forms a path toward more accessible and trustworthy multimodal data assistants.</p>\n\n",
                "matched_terms": [
                    "code",
                    "data",
                    "sense",
                    "execution",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18405v1#S3.F1\" title=\"Figure 1 &#8227; III-A Input and Context Assembly &#8227; III System Architecture &#8227; A Multimodal Conversational Agent for Tabular Data Analysis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the architecture and turn-level interaction loop. A user query enters a conversational workflow that initializes the agent state, selects an action, and returns a structured response. The pipeline is organized into four conceptual stages: preparing inputs and dataset context, deciding how to answer (code vs. narration), executing with safeguards, and delivering multimodal outputs while updating conversation state.</p>\n\n",
                "matched_terms": [
                    "code",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This stage aligns two inputs&#8212;the dataset and the user&#8217;s utterance&#8212;into a shared, compact context that the rest of the pipeline can reliably consume (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18405v1#S3.F1\" title=\"Figure 1 &#8227; III-A Input and Context Assembly &#8227; III System Architecture &#8227; A Multimodal Conversational Agent for Tabular Data Analysis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). On the data side, we parse a lightweight schema and exemplars: column names and inferred types, a few sample rows, numeric ranges for quantitative fields, and representative values for categoricals. On the query side, both voice and text are normalized into a single text form (speech <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> ASR <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> text) using OpenAI&#8217;s Whisper&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Radford2022Whisper</span>]</cite> for English speech; the ASR component is drop-in replaceable and can be extended to other languages by switching Whisper checkpoints or equivalent multilingual models.</p>\n\n",
                "matched_terms": [
                    "text",
                    "representative",
                    "voice",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Two invariants make this stage effective: (i) <em class=\"ltx_emph ltx_font_italic\">faithfulness</em>&#8212;only facts derived from the uploaded data (schema, ranges, exemplars) are injected; (ii) <em class=\"ltx_emph ltx_font_italic\">compactness</em>&#8212;the context is trimmed to remain stable across turns (e.g., small samples and capped categorical exemplars) so later stages see a consistent, prompt-friendly view. When the dataset cannot be parsed or violates basic expectations, the pipeline fails fast with a user-facing message rather than proceeding with an ungrounded context. This keeps subsequent decisions predictable and makes downstream successes attributable to a single, explicit source of grounding.</p>\n\n",
                "matched_terms": [
                    "basic",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">At the core of the loop is a single decision point (Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18405v1#S3.F1\" title=\"Figure 1 &#8227; III-A Input and Context Assembly &#8227; III System Architecture &#8227; A Multimodal Conversational Agent for Tabular Data Analysis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, <em class=\"ltx_emph ltx_font_italic\">decide_action</em>) that classifies each turn as either <em class=\"ltx_emph ltx_font_italic\">code generation</em> or <em class=\"ltx_emph ltx_font_italic\">chat response</em>. Concentrating modality choice in one place keeps the rest of the pipeline simple: only one branch executes per turn, and every downstream component can assume a clear contract about what arrives next (either narration to render or code to check and run).</p>\n\n",
                "matched_terms": [
                    "code",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">By separating <em class=\"ltx_emph ltx_font_italic\">decision</em> (what to do) from <em class=\"ltx_emph ltx_font_italic\">realization</em> (how it is done), the architecture admits evolution without churn: improving examples or style in the decision layer changes behavior immediately, while the execution and rendering layers remain stable. This division of concerns is what allows the whole design to be both adaptive in interaction and steady in operation.</p>\n\n",
                "matched_terms": [
                    "execution",
                    "adaptive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prompting is the policy layer of our architecture: it encodes how the agent interprets intent, how it speaks, and how it produces computable artifacts&#8212;without hard-coding logic. The prompts operate at the three decision points highlighted in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18405v1#S3.F1\" title=\"Figure 1 &#8227; III-A Input and Context Assembly &#8227; III System Architecture &#8227; A Multimodal Conversational Agent for Tabular Data Analysis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>: (i) selecting the response mode, (ii) producing narration suited for TTS, and (iii) emitting code that downstream components can execute and capture reliably. Each prompt consumes the same structured context (dataset metadata and dialogue history) so that routing, language, and code are grounded in the same view of the data.</p>\n\n",
                "matched_terms": [
                    "code",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This prompt helps the LLM to classify the user&#8217;s request into <span class=\"ltx_text ltx_font_typewriter\">code_generation</span> or <span class=\"ltx_text ltx_font_typewriter\">chat_response</span>, returning the decision as minimal JSON. It is seeded with short, contrastive few-shot examples (&#8220;distribution&#8221; <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> code; &#8220;what columns&#8221; <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> chat) to bias toward the most useful modality while still allowing clarification. Architecturally, this concentrates adaptivity into a single, auditable step: the rest of the pipeline remains simple because the mode is fixed before execution. When the request is ambiguous, the prompt favors a low-risk chat response and invites follow-up.</p>\n\n",
                "matched_terms": [
                    "code",
                    "execution",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here the policy is stylistic and accessibility-driven: produce brief, speakable text with no formatting, suitable for TTS, and grounded in the provided metadata/history. The prompt explicitly allows the assistant to ask clarifying questions and to propose switching to computation when the user&#8217;s intent is analytical. This preserves continuity (the conversation advances even when the dataset or request is unclear) and keeps narration aligned with what the code path would compute later.</p>\n\n",
                "matched_terms": [
                    "text",
                    "code",
                    "ask"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This prompt constrains outputs to executable, comment-free Python that assumes a preloaded <span class=\"ltx_text ltx_font_typewriter\">df</span>. Two invariants matter architecturally: (1) <em class=\"ltx_emph ltx_font_italic\">grounding</em>&#8212;the code should reference only columns/types present in metadata; (2) <em class=\"ltx_emph ltx_font_italic\">capturability</em>&#8212;outputs end with an expression (e.g., a figure or dataframe variable) rather than prints. These constraints make execution predictable and simplify capture/rendering, while the few-shot patterns (simple histogram; head of a table) keep generations within the capabilities of the guarded runtime.</p>\n\n",
                "matched_terms": [
                    "code",
                    "execution"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When the decision module selects the computational path, the agents constructs the code generation prompt. This prompt instructs the LLM to produce concise, expression-oriented Python code. The generated snippets typically leverage standard data analysis libraries such as <span class=\"ltx_text ltx_font_typewriter\">pandas</span>, <span class=\"ltx_text ltx_font_typewriter\">matplotlib</span>, <span class=\"ltx_text ltx_font_typewriter\">plotly</span>, or <span class=\"ltx_text ltx_font_typewriter\">seaborn</span>. By encouraging expression-based outputs rather than long scripts, the agent ensures that results are captured cleanly (e.g., figures, summary tables) and remain interpretable for users.</p>\n\n",
                "matched_terms": [
                    "code",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generated code is passed to a sandboxed runtime that enforces strict constraints. Only whitelisted libraries are exposed, with file system, operating system, and network access disabled. Execution is bounded by timeouts and resource quotas to avoid denial-of-service or runaway computations. The pipeline captures outputs in structured form: numerical results are returned as JSON, and figures are converted to Base64 images that can be rendered in the user interface. This design also facilitates error handling: exceptions are intercepted, logged, and summarized into user-friendly explanations.</p>\n\n",
                "matched_terms": [
                    "system",
                    "code",
                    "execution"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Allowing an LLM to generate and execute code introduces security and reliability risks. Without safeguards, generated code could accidentally overwrite files, exhaust memory, or leak private data. This sandbox design prioritizes safety and transparency by restricting system calls, limiting resource usage, and surfacing errors explicitly. This is a trade-off: some advanced operations (e.g., dynamic imports, external APIs) are not supported, which occasionally leads to task failures (see Section&#160;<span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:sec:experiments_benchmarks</span>). However, we argue that constrained but safe execution is preferable to unrestricted but opaque pipelines, particularly for non-technical users who may not detect silent errors.</p>\n\n",
                "matched_terms": [
                    "system",
                    "code",
                    "data",
                    "execution",
                    "memory"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This secure execution pipeline provides several advantages to users. First, it builds trust: users can see and verify the code that produced their results, rather than relying on a &#8220;black box.&#8221; Second, it supports transparency: errors and limitations are explained instead of hidden, encouraging realistic expectations. Third, it maintains reproducibility: since outputs are generated from explicit code, they can be re-run or adapted manually by analysts.</p>\n\n",
                "matched_terms": [
                    "code",
                    "execution",
                    "secure"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Providing natural explanations addresses a common limitation of analytic systems: non-technical users may not understand why a result looks the way it does. By embedding interpretive narratives into the workflow, this design increases trust, usability, and inclusivity. This functionality also enables accessibility scenarios (e.g., blind users receiving spoken feedback), underscoring the value of a multimodal conversational design.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "multimodal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To support multi-turn conversations, the agent maintains a lightweight dialogue memory that includes active dataset metadata and a running history of prior queries and outputs (code or explanations). This context is injected into each new LLM prompt, allowing the agent to generate coherent, contextually aware responses. This design allows users to interact iteratively without restating full queries. For example:</p>\n\n",
                "matched_terms": [
                    "memory",
                    "code"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Memory transforms the design from a query engine into a conversational partner. Users can build on prior steps naturally, explore alternative perspectives, and carry forward insights across turns. This supports more exploratory and human-like interaction with data.</p>\n\n",
                "matched_terms": [
                    "memory",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, these capabilities&#8212;natural explanations, conversational memory, and multimodal output&#8212;differentiate this design from text-only NLIs and prototype voice demos, supporting the idea of a general-purpose conversational agent for data analysis.</p>\n\n",
                "matched_terms": [
                    "voice",
                    "data",
                    "demos",
                    "multimodal",
                    "memory",
                    "prototype"
                ]
            }
        ]
    },
    "S3.SS6.tab1": {
        "source_file": "A Multimodal Conversational Agent for Tabular Data Analysis",
        "caption": "TABLE II: Stepwise refinement scenario on an insurance dataset: user command, generated code, and resulting plot.",
        "body": "Step\n\n\n\n\nUser Command\n\n\n\n\nGenerated Code (snippet)\n\n\n\n\nResulting Plot\n\n\n\n\n\n\n\n\n1\n\n\n\n\nPlot charges vs BMI\n\n\n\n\n⬇\n\n\nplt.scatter(df[’bmi’], df[’charges’])\n\nplt.xlabel(’BMI’)\n\nplt.ylabel(’Charges’)\n\nplt.title(’Charges vs BMI’)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2\n\n\n\n\nnow color by smoker status\n\n\n\n\n⬇\n\n\nplt.scatter(df[’bmi’], df[’charges’], c=df[’smoker’].map({’yes’: ’red’, ’no’: ’blue’}))\n\nplt.xlabel(’BMI’)\n\nplt.ylabel(’Charges’)\n\nplt.title(’Charges vs BMI by Smoker Status’)\n\nplt.legend([’Smoker’, ’Non-Smoker’])\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3\n\n\n\n\nnow add a regression line",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt\" style=\"width:13.8pt;padding-top:0.4pt;padding-bottom:0.4pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Step</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt\" style=\"width:34.5pt;padding-top:0.4pt;padding-bottom:0.4pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">User Command</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt\" style=\"width:172.5pt;padding-top:0.4pt;padding-bottom:0.4pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Generated Code (snippet)</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_tt\" style=\"width:103.5pt;padding-top:0.4pt;padding-bottom:0.4pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Resulting Plot</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:13.8pt;padding-top:0.4pt;padding-bottom:0.4pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p ltx_minipage ltx_align_center ltx_align_middle\" style=\"width:433.6pt;\"><span class=\"ltx_text ltx_font_bold\">1</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:34.5pt;padding-top:0.4pt;padding-bottom:0.4pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p ltx_align_left ltx_minipage ltx_align_middle\" style=\"width:433.6pt;\">Plot charges vs BMI</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:172.5pt;padding-top:0.4pt;padding-bottom:0.4pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_listing ltx_align_left ltx_minipage ltx_align_middle ltx_framed ltx_framed_rectangle ltx_listing\" style=\"width:433.6pt;\"><span class=\"ltx_listing_data\"><a download=\"\" href=\"data:text/plain;base64,CQkJCQpwbHQuc2NhdHRlcihkZlsnYm1pJ10sIGRmWydjaGFyZ2VzJ10pCnBsdC54bGFiZWwoJ0JNSScpCnBsdC55bGFiZWwoJ0NoYXJnZXMnKQpwbHQudGl0bGUoJ0NoYXJnZXMgdnMgQk1JJykKcGx0LnNob3coKQ==\">&#11015;</a></span>\n<span class=\"ltx_listingline\" id=\"lstnumberx1\">\n</span>\n<span class=\"ltx_listingline\" id=\"lstnumberx2\"><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\" style=\"font-size:70%;\">plt</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">.</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\" style=\"font-size:70%;\">scatter</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\" style=\"font-size:70%;\">df</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">[</span><span class=\"ltx_text ltx_lst_string ltx_font_typewriter\" style=\"font-size:70%;\">&#8217;bmi&#8217;</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">],</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\" style=\"font-size:70%;\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\" style=\"font-size:70%;\">df</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">[</span><span class=\"ltx_text ltx_lst_string ltx_font_typewriter\" style=\"font-size:70%;\">&#8217;charges&#8217;</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">])</span>\n</span>\n<span class=\"ltx_listingline\" id=\"lstnumberx3\"><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\" style=\"font-size:70%;\">plt</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">.</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\" style=\"font-size:70%;\">xlabel</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">(</span><span class=\"ltx_text ltx_lst_string ltx_font_typewriter\" style=\"font-size:70%;\">&#8217;BMI&#8217;</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">)</span>\n</span>\n<span class=\"ltx_listingline\" id=\"lstnumberx4\"><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\" style=\"font-size:70%;\">plt</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">.</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\" style=\"font-size:70%;\">ylabel</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">(</span><span class=\"ltx_text ltx_lst_string ltx_font_typewriter\" style=\"font-size:70%;\">&#8217;Charges&#8217;</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">)</span>\n</span>\n<span class=\"ltx_listingline\" id=\"lstnumberx5\"><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\" style=\"font-size:70%;\">plt</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">.</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\" style=\"font-size:70%;\">title</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">(</span><span class=\"ltx_text ltx_lst_string ltx_font_typewriter\" style=\"font-size:70%;\">&#8217;Charges<span class=\"ltx_text ltx_lst_space\"> </span>vs<span class=\"ltx_text ltx_lst_space\"> </span>BMI&#8217;</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">)</span>\n</span>\n<span class=\"ltx_listingline\" id=\"lstnumberx6\"><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\" style=\"font-size:70%;\">plt</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">.</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\" style=\"font-size:70%;\">show</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">()</span>\n</span>\n</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_t\" style=\"width:103.5pt;padding-top:0.4pt;padding-bottom:0.4pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_block ltx_minipage ltx_align_center ltx_align_middle\" style=\"width:433.6pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"130\" id=\"S3.SS6.g1\" src=\"1.png\" width=\"172\"/>\n</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:13.8pt;padding-top:0.4pt;padding-bottom:0.4pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p ltx_minipage ltx_align_center ltx_align_middle\" style=\"width:433.6pt;\"><span class=\"ltx_text ltx_font_bold\">2</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:34.5pt;padding-top:0.4pt;padding-bottom:0.4pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p ltx_align_left ltx_minipage ltx_align_middle\" style=\"width:433.6pt;\">now color by smoker status</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:172.5pt;padding-top:0.4pt;padding-bottom:0.4pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_listing ltx_align_left ltx_minipage ltx_align_middle ltx_framed ltx_framed_rectangle ltx_listing\" style=\"width:433.6pt;\"><span class=\"ltx_listing_data\"><a download=\"\" href=\"data:text/plain;base64,CQkJCQpwbHQuc2NhdHRlcihkZlsnYm1pJ10sIGRmWydjaGFyZ2VzJ10sIGM9ZGZbJ3Ntb2tlciddLm1hcCh7J3llcyc6ICdyZWQnLCAnbm8nOiAnYmx1ZSd9KSkKcGx0LnhsYWJlbCgnQk1JJykKcGx0LnlsYWJlbCgnQ2hhcmdlcycpCnBsdC50aXRsZSgnQ2hhcmdlcyB2cyBCTUkgYnkgU21va2VyIFN0YXR1cycpCnBsdC5sZWdlbmQoWydTbW9rZXInLCAnTm9uLVNtb2tlciddKQpwbHQuc2hvdygp\">&#11015;</a></span>\n<span class=\"ltx_listingline\" id=\"lstnumberx7\">\n</span>\n<span class=\"ltx_listingline\" id=\"lstnumberx8\"><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\" style=\"font-size:70%;\">plt</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">.</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\" style=\"font-size:70%;\">scatter</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">(</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\" style=\"font-size:70%;\">df</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">[</span><span class=\"ltx_text ltx_lst_string ltx_font_typewriter\" style=\"font-size:70%;\">&#8217;bmi&#8217;</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">],</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\" style=\"font-size:70%;\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\" style=\"font-size:70%;\">df</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">[</span><span class=\"ltx_text ltx_lst_string ltx_font_typewriter\" style=\"font-size:70%;\">&#8217;charges&#8217;</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">],</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\" style=\"font-size:70%;\"> </span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\" style=\"font-size:70%;\">c</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">=</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\" style=\"font-size:70%;\">df</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">[</span><span class=\"ltx_text ltx_lst_string ltx_font_typewriter\" style=\"font-size:70%;\">&#8217;smoker&#8217;</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">].</span><span class=\"ltx_text ltx_lst_keyword ltx_lst_keywords2 ltx_font_typewriter ltx_font_bold\" style=\"font-size:70%;\">map</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">({</span><span class=\"ltx_text ltx_lst_string ltx_font_typewriter\" style=\"font-size:70%;\">&#8217;yes&#8217;</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">:</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\" style=\"font-size:70%;\"> </span><span class=\"ltx_text ltx_lst_string ltx_font_typewriter\" style=\"font-size:70%;\">&#8217;red&#8217;</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">,</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\" style=\"font-size:70%;\"> </span><span class=\"ltx_text ltx_lst_string ltx_font_typewriter\" style=\"font-size:70%;\">&#8217;no&#8217;</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">:</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\" style=\"font-size:70%;\"> </span><span class=\"ltx_text ltx_lst_string ltx_font_typewriter\" style=\"font-size:70%;\">&#8217;blue&#8217;</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">}))</span>\n</span>\n<span class=\"ltx_listingline\" id=\"lstnumberx9\"><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\" style=\"font-size:70%;\">plt</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">.</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\" style=\"font-size:70%;\">xlabel</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">(</span><span class=\"ltx_text ltx_lst_string ltx_font_typewriter\" style=\"font-size:70%;\">&#8217;BMI&#8217;</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">)</span>\n</span>\n<span class=\"ltx_listingline\" id=\"lstnumberx10\"><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\" style=\"font-size:70%;\">plt</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">.</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\" style=\"font-size:70%;\">ylabel</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">(</span><span class=\"ltx_text ltx_lst_string ltx_font_typewriter\" style=\"font-size:70%;\">&#8217;Charges&#8217;</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">)</span>\n</span>\n<span class=\"ltx_listingline\" id=\"lstnumberx11\"><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\" style=\"font-size:70%;\">plt</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">.</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\" style=\"font-size:70%;\">title</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">(</span><span class=\"ltx_text ltx_lst_string ltx_font_typewriter\" style=\"font-size:70%;\">&#8217;Charges<span class=\"ltx_text ltx_lst_space\"> </span>vs<span class=\"ltx_text ltx_lst_space\"> </span>BMI<span class=\"ltx_text ltx_lst_space\"> </span>by<span class=\"ltx_text ltx_lst_space\"> </span>Smoker<span class=\"ltx_text ltx_lst_space\"> </span>Status&#8217;</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">)</span>\n</span>\n<span class=\"ltx_listingline\" id=\"lstnumberx12\"><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\" style=\"font-size:70%;\">plt</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">.</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\" style=\"font-size:70%;\">legend</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">([</span><span class=\"ltx_text ltx_lst_string ltx_font_typewriter\" style=\"font-size:70%;\">&#8217;Smoker&#8217;</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">,</span><span class=\"ltx_text ltx_lst_space ltx_font_typewriter\" style=\"font-size:70%;\"> </span><span class=\"ltx_text ltx_lst_string ltx_font_typewriter\" style=\"font-size:70%;\">&#8217;Non-Smoker&#8217;</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">])</span>\n</span>\n<span class=\"ltx_listingline\" id=\"lstnumberx13\"><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\" style=\"font-size:70%;\">plt</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">.</span><span class=\"ltx_text ltx_lst_identifier ltx_font_typewriter\" style=\"font-size:70%;\">show</span><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:70%;\">()</span>\n</span>\n</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:103.5pt;padding-top:0.4pt;padding-bottom:0.4pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_block ltx_minipage ltx_align_center ltx_align_middle\" style=\"width:433.6pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"130\" id=\"S3.SS6.g2\" src=\"2.png\" width=\"172\"/>\n</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:13.8pt;padding-top:0.4pt;padding-bottom:0.4pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p ltx_minipage ltx_align_center ltx_align_middle\" style=\"width:433.6pt;\"><span class=\"ltx_text ltx_font_bold\">3</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:34.5pt;padding-top:0.4pt;padding-bottom:0.4pt;\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p ltx_align_left ltx_minipage ltx_align_middle\" style=\"width:433.6pt;\">now add a regression line</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:172.5pt;padding-top:0.4pt;padding-bottom:0.4pt;\"/>\n<td class=\"ltx_td ltx_align_justify ltx_align_middle\" style=\"width:103.5pt;padding-top:0.4pt;padding-bottom:0.4pt;\"/>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "df’charges’",
            "code",
            "status",
            "status’",
            "pltxlabel’bmi’",
            "scenario",
            "refinement",
            "pltscatterdf’bmi’",
            "pltlegend’smoker’",
            "charges",
            "bmi’",
            "bmi",
            "pltylabel’charges’",
            "snippet",
            "step",
            "’no’",
            "command",
            "’blue’",
            "add",
            "pltshow",
            "line",
            "color",
            "now",
            "dataset",
            "’red’",
            "plttitle’charges",
            "generated",
            "resulting",
            "plot",
            "smoker",
            "cdf’smoker’map’yes’",
            "user",
            "’nonsmoker’",
            "stepwise",
            "regression",
            "insurance"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Large language models (LLMs) can reshape information processing by handling data analysis, visualization, and interpretation in an interactive, context-aware dialogue with users, including voice interaction, while maintaining high performance. In this article, we present Talk2Data, a multimodal LLM-driven conversational agent for intuitive data exploration. The system lets users query datasets with voice or text instructions and receive answers as plots, tables, statistics, or spoken explanations. Built on LLMs, the suggested design combines OpenAI Whisper automatic speech recognition (ASR) system, Qwen-coder code generation LLM/model, custom sandboxed execution tools, and Coqui library for text-to-speech (TTS) within an agentic orchestration loop. Unlike text-only analysis tools, it adapts responses across modalities and supports multi-turn dialogues grounded in dataset context. In an evaluation of 48 tasks on three datasets, our prototype achieved 95.8% accuracy with model-only generation time under 1.7 seconds (excluding ASR and execution time). A comparison across five LLM sizes (1.5B&#8211;32B) revealed accuracy&#8211;latency&#8211;cost trade-offs, with a 7B model providing the best balance for interactive use. By routing between conversation with user and code execution, constrained to a transparent sandbox, with simultaneously grounding prompts in schema-level context, the Talk2Data agent reliably retrieves actionable insights from tables while making computations verifiable. In the article, except for the Talk2Data agent itself, we discuss implications for human&#8211;data interaction, trust in LLM-driven analytics, and future extensions toward large-scale multimodal assistants.</p>\n\n",
                "matched_terms": [
                    "code",
                    "user",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">LLMs now provide a powerful foundation for code generation and complex reasoning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Chen2021Codex</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Roziere2023CodeLlama</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Hui2024Qwen25</span>]</cite>. Systems such as OpenAI&#8217;s Code Interpreter&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">OpenAI2023CodeInterpreter</span>]</cite> demonstrate this potential, but they typically support only text-based input/output. Another problem is that features like multimodal responses and transparent execution of generated code (capabilities that are increasingly important for reliable human&#8211;data interaction in the information seeking process) are less present in current voice assistant designs.</p>\n\n",
                "matched_terms": [
                    "code",
                    "generated",
                    "now"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present Talk2Data, a multimodal conversational agent that enables users to effectively seek, retrieve, and analyze data stored in tabular datasets through either voice or text instructions, and to receive answers and insights as plots, tables, or spoken explanations. For example, users can ask &#8220;What&#8217;s the average delay for United flights?&#8221; or &#8220;Plot a histogram of age,&#8221; and the agent dynamically determines whether to generate Python code or provide a direct natural language response. Code is executed in a secure sandbox, results are narrated through text-to-speech (TTS), and multi-turn dialogue is supported via conversational memory. This design allows to adapt its responses to user intent, offering both analytical depth and accessibility.</p>\n\n",
                "matched_terms": [
                    "code",
                    "user"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM-Based Code Generation.</span> Advances in program synthesis with LLMs&#8212;including OpenAI Codex&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Chen2021Codex</span>]</cite>, Code Llama&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Roziere2023CodeLlama</span>]</cite>, and Qwen-2.5-Coder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Hui2024Qwen25</span>]</cite>&#8212;have demonstrated strong performance in generating analysis pipelines and data visualizations from text instructions&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Nascimento2024LLM4DS</span>]</cite> which largely aids the process of retrieving insights using the generated code. However, these models are typically deployed in static, text-only settings. They lack conversational memory, execution tool, transparency of execution, and the ability to adapt outputs to user context or modality preferences.</p>\n\n",
                "matched_terms": [
                    "code",
                    "generated",
                    "user"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The suggested design stays within this lineage and assembles known components&#8212;ASR, an LLM, and TTS&#8212;into a single workflow. The incremental aspects we explore are: (1) an explicit routing step (via a lightweight LangGraph node) that decides between code generation and direct narration using dataset-aware prompts; and (2) a restricted execution path that treats LLM code as a first-class artifact but runs it with guardrails (timeouts, whitelisting) and returns structured errors. Neither routing nor sandboxing is novel in isolation; our contribution is an empirical look at how these choices affect <em class=\"ltx_emph ltx_font_italic\">end-to-end</em> usability (accuracy, latency, and dialogue continuity) for tabular analysis, including when speech is the entry point.</p>\n\n",
                "matched_terms": [
                    "code",
                    "step"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Compared with text-only NLIs in commercial business intelligence (BI) tools&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Tableau2025AskData</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Microsoft2025PowerBIQA</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">AWS2025QuickSightQ</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">Qlik2025InsightAdvisor</span>]</cite>, the suggested design emphasizes transparency and adaptability over enterprise-specific heuristics. Whereas BI platforms often rely on curated semantic layers, governed vocabularies, and policy enforcement, our system favors flexibility by exposing generated code and supporting free-form queries across arbitrary tabular datasets. This design trades some enterprise features (e.g., lineage tracking, synonym governance) for openness and interpretability&#8212;qualities important for exploratory analysis and educational use.</p>\n\n",
                "matched_terms": [
                    "code",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18405v1#S3.F1\" title=\"Figure 1 &#8227; III-A Input and Context Assembly &#8227; III System Architecture &#8227; A Multimodal Conversational Agent for Tabular Data Analysis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents the architecture and turn-level interaction loop. A user query enters a conversational workflow that initializes the agent state, selects an action, and returns a structured response. The pipeline is organized into four conceptual stages: preparing inputs and dataset context, deciding how to answer (code vs. narration), executing with safeguards, and delivering multimodal outputs while updating conversation state.</p>\n\n",
                "matched_terms": [
                    "code",
                    "user",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The result is a <em class=\"ltx_emph ltx_font_italic\">context pack</em> that travels with the turn: it grounds routing (the decision prompt), constrains narration (the chat prompt), and anchors code synthesis (the code prompt) to what actually exists in the table. Ambiguities are reduced by aligning user words to schema tokens (e.g., resolving &#8220;GPA four&#8221; to <span class=\"ltx_text ltx_font_typewriter\">GPA4</span> when present) and by exposing recent conversation state so follow-ups (&#8220;now color by gender&#8221;) inherit prior choices without restating them.</p>\n\n",
                "matched_terms": [
                    "code",
                    "color",
                    "user"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Orchestration is lightweight; it passes the same grounded context forward, selects the branch, and records the choice in the conversation state. This achieves three properties we rely on throughout the paper:\n(i) <em class=\"ltx_emph ltx_font_italic\">Adaptivity</em> &#8212; the agent aligns response mode with user intent and dataset characteristics;\n(ii) <em class=\"ltx_emph ltx_font_italic\">Predictability</em> &#8212; heavy operations (policy checks, execution) only occur when the decision warrants them;\n(iii) <em class=\"ltx_emph ltx_font_italic\">Auditability</em> &#8212; each turn carries an explicit decision and rationale, so failures can be traced to a visible fork rather than hidden heuristics.</p>\n\n",
                "matched_terms": [
                    "user",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Prompting is the policy layer of our architecture: it encodes how the agent interprets intent, how it speaks, and how it produces computable artifacts&#8212;without hard-coding logic. The prompts operate at the three decision points highlighted in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18405v1#S3.F1\" title=\"Figure 1 &#8227; III-A Input and Context Assembly &#8227; III System Architecture &#8227; A Multimodal Conversational Agent for Tabular Data Analysis\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>: (i) selecting the response mode, (ii) producing narration suited for TTS, and (iii) emitting code that downstream components can execute and capture reliably. Each prompt consumes the same structured context (dataset metadata and dialogue history) so that routing, language, and code are grounded in the same view of the data.</p>\n\n",
                "matched_terms": [
                    "code",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This prompt helps the LLM to classify the user&#8217;s request into <span class=\"ltx_text ltx_font_typewriter\">code_generation</span> or <span class=\"ltx_text ltx_font_typewriter\">chat_response</span>, returning the decision as minimal JSON. It is seeded with short, contrastive few-shot examples (&#8220;distribution&#8221; <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> code; &#8220;what columns&#8221; <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> chat) to bias toward the most useful modality while still allowing clarification. Architecturally, this concentrates adaptivity into a single, auditable step: the rest of the pipeline remains simple because the mode is fixed before execution. When the request is ambiguous, the prompt favors a low-risk chat response and invites follow-up.</p>\n\n",
                "matched_terms": [
                    "code",
                    "step"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here the policy is stylistic and accessibility-driven: produce brief, speakable text with no formatting, suitable for TTS, and grounded in the provided metadata/history. The prompt explicitly allows the assistant to ask clarifying questions and to propose switching to computation when the user&#8217;s intent is analytical. This preserves continuity (the conversation advances even when the dataset or request is unclear) and keeps narration aligned with what the code path would compute later.</p>\n\n",
                "matched_terms": [
                    "code",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When the decision module selects the computational path, the agents constructs the code generation prompt. This prompt instructs the LLM to produce concise, expression-oriented Python code. The generated snippets typically leverage standard data analysis libraries such as <span class=\"ltx_text ltx_font_typewriter\">pandas</span>, <span class=\"ltx_text ltx_font_typewriter\">matplotlib</span>, <span class=\"ltx_text ltx_font_typewriter\">plotly</span>, or <span class=\"ltx_text ltx_font_typewriter\">seaborn</span>. By encouraging expression-based outputs rather than long scripts, the agent ensures that results are captured cleanly (e.g., figures, summary tables) and remain interpretable for users.</p>\n\n",
                "matched_terms": [
                    "code",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Generated code is passed to a sandboxed runtime that enforces strict constraints. Only whitelisted libraries are exposed, with file system, operating system, and network access disabled. Execution is bounded by timeouts and resource quotas to avoid denial-of-service or runaway computations. The pipeline captures outputs in structured form: numerical results are returned as JSON, and figures are converted to Base64 images that can be rendered in the user interface. This design also facilitates error handling: exceptions are intercepted, logged, and summarized into user-friendly explanations.</p>\n\n",
                "matched_terms": [
                    "code",
                    "generated",
                    "user"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Allowing an LLM to generate and execute code introduces security and reliability risks. Without safeguards, generated code could accidentally overwrite files, exhaust memory, or leak private data. This sandbox design prioritizes safety and transparency by restricting system calls, limiting resource usage, and surfacing errors explicitly. This is a trade-off: some advanced operations (e.g., dynamic imports, external APIs) are not supported, which occasionally leads to task failures (see Section&#160;<span class=\"ltx_ref ltx_missing_label ltx_ref_self\">LABEL:sec:experiments_benchmarks</span>). However, we argue that constrained but safe execution is preferable to unrestricted but opaque pipelines, particularly for non-technical users who may not detect silent errors.</p>\n\n",
                "matched_terms": [
                    "code",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This secure execution pipeline provides several advantages to users. First, it builds trust: users can see and verify the code that produced their results, rather than relying on a &#8220;black box.&#8221; Second, it supports transparency: errors and limitations are explained instead of hidden, encouraging realistic expectations. Third, it maintains reproducibility: since outputs are generated from explicit code, they can be re-run or adapted manually by analysts.</p>\n\n",
                "matched_terms": [
                    "code",
                    "generated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">When the agent chooses the <span class=\"ltx_text ltx_font_typewriter\">narrative response</span> path, it advances the conversation without running code: it interprets the user&#8217;s request against the current dataset context and recent turns, then produces a brief, speakable explanation. The aim is to keep momentum&#8212;clarify what a field means, summarize what a prior chart already showed, or resolve ambiguity with the smallest possible clarification&#8212;while remaining grounded in the same context that drives the analytic path.</p>\n\n",
                "matched_terms": [
                    "code",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To support multi-turn conversations, the agent maintains a lightweight dialogue memory that includes active dataset metadata and a running history of prior queries and outputs (code or explanations). This context is injected into each new LLM prompt, allowing the agent to generate coherent, contextually aware responses. This design allows users to interact iteratively without restating full queries. For example:</p>\n\n",
                "matched_terms": [
                    "code",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">&#8220;Plot math vs. reading score.&#8221;<span class=\"ltx_text ltx_font_upright\"> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> [scatter plot] \n<br class=\"ltx_break\"/></span>&#8220;Now color by gender.&#8221;<span class=\"ltx_text ltx_font_upright\"> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS6.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> [updated plot with hue]</span></span>\n</p>\n\n",
                "matched_terms": [
                    "color",
                    "plot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.18405v1#S3.SS6\" title=\"III-F Multi-turn Interactions &#8227; III System Architecture &#8227; A Multimodal Conversational Agent for Tabular Data Analysis\"><span class=\"ltx_text ltx_ref_tag\">III-F</span></a> illustrates a three-turn refinement on the insurance dataset: the agent plots charges vs. BMI, colors by smoker status, then adds a regression line.</p>\n\n",
                "matched_terms": [
                    "status",
                    "charges",
                    "smoker",
                    "line",
                    "refinement",
                    "bmi",
                    "regression",
                    "insurance",
                    "dataset"
                ]
            }
        ]
    }
}