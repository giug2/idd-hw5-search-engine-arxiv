{
    "S3.T1": {
        "source_file": "ProLAP: Probabilistic Language-Audio Pre-Training",
        "caption": "Table 1: Results on audio-text retrieval tasks using AudioCaps (AC) and ClothoV2 (CL).",
        "body": "AudioCaps Dataset\n\n\n\n\nMethod\nTraining\nText-to-Audio Retrieval\nAudio-to-Text Retrieval\n\n\nDataset\nR@1\nR@5\nR@10\nmAP@10\nR@1\nR@5\nR@10\nmAP@10\n\n\nCLAP (InfoNCE)\nAC\n41.90\n77.24\n89.24\n57.30\n39.75\n78.32\n89.35\n55.84\n\n\nCLAP (SigLIP)\nAC\n41.45\n76.22\n88.67\n56.22\n40.88\n76.90\n88.34\n56.22\n\n\nProLAP (Ours)\nAC\n43.15\n77.01\n89.01\n57.65\n41.79\n77.12\n88.45\n56.52\n\n\n\nProLAP w/ ℒinch,ℒMR\\mathcal{L}^{h}_{\\mathrm{inc}},\\mathcal{L}_{\\mathrm{MR}} (Ours)\n\nAC\n42.36\n76.56\n88.22\n57.23\n41.79\n78.48\n88.67\n56.72\n\n\nCLAP (InfoNCE)\nCL\n27.63\n62.85\n76.78\n42.45\n26.73\n57.19\n71.57\n39.43\n\n\nCLAP (SigLIP)\nCL\n27.97\n62.40\n77.01\n42.49\n24.46\n59.23\n76.10\n39.20\n\n\nProLAP (Ours)\nCL\n28.65\n63.65\n77.46\n43.54\n25.71\n60.93\n76.44\n40.56\n\n\n\nProLAP w/ ℒinch,ℒMR\\mathcal{L}^{h}_{\\mathrm{inc}},\\mathcal{L}_{\\mathrm{MR}}(Ours)\n\nCL\n28.77\n63.53\n78.26\n43.72\n26.05\n61.27\n77.01\n40.99\n\n\nClothoV2 Dataset\n\n\nMethod\nTraining\nText-to-Audio Retrieval\nAudio-to-Text Retrieval\n\n\nDataset\nR@1\nR@5\nR@10\nmAP@10\nR@1\nR@5\nR@10\nmAP@10\n\n\nCLAP (InfoNCE)\nAC\n20.00\n44.98\n59.23\n31.03\n17.22\n44.69\n57.42\n28.63\n\n\nCLAP (SigLIP)\nAC\n19.14\n45.07\n58.66\n30.13\n17.99\n44.31\n59.52\n29.19\n\n\nProLAP (Ours)\nAC\n20.48\n45.65\n59.81\n31.46\n18.95\n44.40\n58.66\n29.77\n\n\n\nProLAP w/ ℒinch,ℒMR\\mathcal{L}^{h}_{\\mathrm{inc}},\\mathcal{L}_{\\mathrm{MR}} (Ours)\n\nAC\n20.57\n45.74\n59.33\n31.53\n18.18\n45.17\n58.28\n29.30\n\n\nCLAP (InfoNCE)\nCL\n20.67\n46.12\n59.43\n31.72\n19.43\n45.65\n59.71\n30.62\n\n\nCLAP (SigLIP)\nCL\n18.85\n45.07\n57.80\n29.90\n17.89\n45.26\n59.71\n29.85\n\n\nProLAP (Ours)\nCL\n20.38\n46.99\n60.29\n31.92\n19.14\n48.23\n62.20\n31.53\n\n\n\nProLAP w/ ℒinch,ℒMR\\mathcal{L}^{h}_{\\mathrm{inc}},\\mathcal{L}_{\\mathrm{MR}} (Ours)\n\nCL\n20.86\n46.41\n60.00\n32.17\n19.90\n47.75\n62.68\n31.99",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" colspan=\"10\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">AudioCaps Dataset</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Method</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Text-to-Audio Retrieval</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Audio-to-Text Retrieval</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">R@1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">R@5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">R@10</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">mAP@10</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">R@1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">R@5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">R@10</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">mAP@10</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">CLAP (InfoNCE)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">AC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">41.90</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">77.24</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">89.24</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">57.30</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">39.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">78.32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">89.35</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">55.84</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">CLAP (SigLIP)</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">AC</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">41.45</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">76.22</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">88.67</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">56.22</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">40.88</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">76.90</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">88.34</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">56.22</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">ProLAP (Ours)</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">AC</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">43.15</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">77.01</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">89.01</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">57.65</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">41.79</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">77.12</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">88.45</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">56.52</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">ProLAP w/ </span><math alttext=\"\\mathcal{L}^{h}_{\\mathrm{inc}},\\mathcal{L}_{\\mathrm{MR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi><mi mathsize=\"0.900em\">inc</mi><mi mathsize=\"0.900em\">h</mi></msubsup><mo mathsize=\"0.900em\">,</mo><msub><mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi><mi mathsize=\"0.900em\">MR</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}^{h}_{\\mathrm{inc}},\\mathcal{L}_{\\mathrm{MR}}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> (Ours)</span>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">AC</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">42.36</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">76.56</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">88.22</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">57.23</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">41.79</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">78.48</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">88.67</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">56.72</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">CLAP (InfoNCE)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">CL</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.63</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">62.85</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">76.78</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">42.45</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">26.73</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">57.19</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">71.57</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">39.43</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">CLAP (SigLIP)</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">CL</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">27.97</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">62.40</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">77.01</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">42.49</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">24.46</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">59.23</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">76.10</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">39.20</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">ProLAP (Ours)</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">CL</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">28.65</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">63.65</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">77.46</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">43.54</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">25.71</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">60.93</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">76.44</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">40.56</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">ProLAP w/ </span><math alttext=\"\\mathcal{L}^{h}_{\\mathrm{inc}},\\mathcal{L}_{\\mathrm{MR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m2\" intent=\":literal\"><semantics><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi><mi mathsize=\"0.900em\">inc</mi><mi mathsize=\"0.900em\">h</mi></msubsup><mo mathsize=\"0.900em\">,</mo><msub><mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi><mi mathsize=\"0.900em\">MR</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}^{h}_{\\mathrm{inc}},\\mathcal{L}_{\\mathrm{MR}}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">(Ours)</span>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">CL</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">28.77</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">63.53</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">78.26</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">43.72</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">26.05</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">61.27</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">77.01</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">40.99</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" colspan=\"10\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">ClothoV2 Dataset</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Method</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Training</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Text-to-Audio Retrieval</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Audio-to-Text Retrieval</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">R@1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">R@5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">R@10</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">mAP@10</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">R@1</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">R@5</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">R@10</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">mAP@10</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">CLAP (InfoNCE)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">AC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">44.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">59.23</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.03</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">17.22</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">44.69</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">57.42</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">28.63</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">CLAP (SigLIP)</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">AC</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">19.14</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">45.07</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">58.66</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">30.13</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">17.99</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">44.31</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">59.52</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">29.19</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">ProLAP (Ours)</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">AC</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.48</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">45.65</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">59.81</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.46</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">18.95</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">44.40</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">58.66</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">29.77</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">ProLAP w/ </span><math alttext=\"\\mathcal{L}^{h}_{\\mathrm{inc}},\\mathcal{L}_{\\mathrm{MR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m3\" intent=\":literal\"><semantics><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi><mi mathsize=\"0.900em\">inc</mi><mi mathsize=\"0.900em\">h</mi></msubsup><mo mathsize=\"0.900em\">,</mo><msub><mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi><mi mathsize=\"0.900em\">MR</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}^{h}_{\\mathrm{inc}},\\mathcal{L}_{\\mathrm{MR}}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> (Ours)</span>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">AC</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">20.57</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">45.74</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">59.33</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">31.53</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">18.18</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">45.17</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">58.28</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">29.30</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">CLAP (InfoNCE)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">CL</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.67</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">46.12</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">59.43</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.72</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">19.43</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">45.65</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">59.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">30.62</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">CLAP (SigLIP)</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">CL</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">18.85</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">45.07</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">57.80</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">29.90</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">17.89</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">45.26</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">59.71</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">29.85</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">ProLAP (Ours)</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">CL</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.38</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">46.99</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">60.29</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.92</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">19.14</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">48.23</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">62.20</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">31.53</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">ProLAP w/ </span><math alttext=\"\\mathcal{L}^{h}_{\\mathrm{inc}},\\mathcal{L}_{\\mathrm{MR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m4\" intent=\":literal\"><semantics><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi><mi mathsize=\"0.900em\">inc</mi><mi mathsize=\"0.900em\">h</mi></msubsup><mo mathsize=\"0.900em\">,</mo><msub><mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi><mi mathsize=\"0.900em\">MR</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}^{h}_{\\mathrm{inc}},\\mathcal{L}_{\\mathrm{MR}}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> (Ours)</span>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">CL</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">20.86</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">46.41</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">60.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">32.17</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">19.90</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">47.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">62.68</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">31.99</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "siglip",
            "training",
            "tasks",
            "ours",
            "retrieval",
            "audiototext",
            "audiocaps",
            "ℒinchℒmrmathcallhmathrmincmathcallmathrmmr",
            "audiotext",
            "ℒinchℒmrmathcallhmathrmincmathcallmathrmmrours",
            "clap",
            "texttoaudio",
            "results",
            "map10",
            "infonce",
            "dataset",
            "prolap",
            "clothov2",
            "method",
            "r10"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#S3.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 3 Experiments &#8227; ProLAP: Probabilistic Language-Audio Pre-Training\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> compares the performance of models\ntrained either on AudioCaps or on ClothoV2.\nOverall, ProLAP outperforms existing methods on both datasets.\nNotably, the improvements are more pronounced on datasets not used for training,\nsuggesting that ProLAP is more robust to out-of-domain data than the baselines.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Language-audio joint representation learning frameworks typically depend on deterministic embeddings,\nassuming a one-to-one correspondence between audio and text.\nIn real-world settings, however, the language-audio relationship is inherently many-to-many:\none audio segment can be described by multiple captions and vice versa.\nTo address this, we propose Probabilistic Language-Audio Pre-training (ProLAP),\nwhich models multiplicity as the spread of probability distributions\nin a joint language-audio embedding space.\nTo train the intra-modal hierarchical relationship effectively,\nwe also introduce two objectives:\n(i) hierarchical inclusion loss to promote semantic hierarchical understanding of inputs\nand (ii) mask repulsive loss to improve the efficiency of learning when optimizing the hierarchical inclusion loss.\nWith this training strategy, our model can learn the hierarchical structure inherent in the data even from small datasets, in contrast to prior probabilistic approaches that rely on large-scale datasets.\nIn our experiments, ProLAP outperforms existing deterministic approaches on audio-text retrieval tasks.\nMoreover, through experiments on the audio traversal task introduced in this paper,\nwe demonstrate that ProLAP captures the plausible semantic hierarchy.\n</span>\n</p>\n\n",
                "matched_terms": [
                    "training",
                    "tasks",
                    "audiotext",
                    "prolap",
                    "retrieval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;<span class=\"ltx_text ltx_font_medium\">\ncontrastive learning, general-purpose audio representation, audio-text retrieval</span></span></span>\n</p>\n\n",
                "matched_terms": [
                    "retrieval",
                    "audiotext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Learning joint representations of language and audio\nhas emerged as a powerful paradigm for open-vocabulary audio understanding.\nContrastive Language-Audio Pre-training (CLAP)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\naligns audio clips and textual descriptions in a shared embedding space,\nenabling zero-shot retrieval and classification in diverse acoustic domains.</span>\n</p>\n\n",
                "matched_terms": [
                    "retrieval",
                    "clap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We propose Probabilistic Language-Audio Pre-training (ProLAP), a probabilistic extension of CLAP&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nProLAP represents each input (audio or text) as a probability distribution over\nthe joint embedding space rather than as a deterministic vector.\nHowever, we find empirically that a straightforward probabilistic extension\nis insufficient to capture the semantic hierarchy in audio and text.\nTherefore, we introduce hierarchical inclusion loss and mask repulsive loss to promote hierarchical learning.\nWith this training strategy, ProLAP explicitly captures\n(i) multimodal semantics arising from many-to-many audio&#8211;text correspondences\nand (ii) hierarchical relations between general and specific concepts.</span>\n</p>\n\n",
                "matched_terms": [
                    "training",
                    "prolap",
                    "clap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In the experiments, ProLAP outperforms standard CLAP and CLAP with sigmoid loss&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\non audio-text retrieval tasks.\nAdditionally, to evaluate semantic hierarchical understanding, we introduce audio traversal task, analogous to the image traversal task&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe audio traversal task traces a straight line\nin embedding space from an audio embedding to the abstract anchor,\nretrieving the nearest text embedding at sampled points.\nThe results indicate that ProLAP better captures the semantic hierarchy than baselines.</span>\n</p>\n\n",
                "matched_terms": [
                    "tasks",
                    "audiotext",
                    "prolap",
                    "retrieval",
                    "clap",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we propose Probabilistic Language-Audio Pre-training (ProLAP),\nwhich learns the joint language-audio representations\nwith probabilistic modeling of inputs.\nBuilding on ProLIP&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">,\nwe follow its probabilistic formulation,\nand our baseline ProLAP is a straightforward application of the ProLIP for audio-text data.\nWe also propose two loss functions to promote hierarchical learning:\nhierarchical inclusion loss and mask repulsive loss.\nIn the following sections, we first provide an overview of ProLAP,\nand then we explain the two loss objectives used in ProLIP to learn the cross-modal probabilistic representations.\nFinally, we introduce two additional loss functions to promote semantic hierarchy understanding.\n</span>\n</p>\n\n",
                "matched_terms": [
                    "prolap",
                    "audiotext"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To demonstrate the effectiveness of ProLAP,\nwe evaluate it on text-to-audio and audio-to-text retrieval tasks.\nIn our experiments, we first fine-tune the models\ninitialized with pre-trained CLAP&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> weights,\nand then evaluate the models on the retrieval tasks.\nAs baselines, we also train CLAP with the InfoNCE and\nCLAP with the sigmoid loss proposed in SigLIP&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nWe use the similarity </span>\n  <math alttext=\"s(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mo lspace=\"0em\" mathsize=\"0.900em\" rspace=\"0em\">&#8901;</mo>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">s(\\cdot)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> defined in Eq.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#S2.E1\" style=\"font-size:90%;\" title=\"In 2.2.1 Probabilistic Pairwise Contrastive Loss &#8227; 2.2 ProLAP Baseline Objective &#8227; 2 Proposed Method &#8227; ProLAP: Probabilistic Language-Audio Pre-Training\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nwhen training and evaluating ProLAP.\nFollowing prior works&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib12\" title=\"\">12</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">,\nwe report Recall@K (R@K) for </span>\n  <math alttext=\"K\\in\\{1,5,10\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">K</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">5</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">K\\in\\{1,5,10\\}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nand mean average precision among the top 10 results (mAP@10).</span>\n</p>\n\n",
                "matched_terms": [
                    "siglip",
                    "map10",
                    "training",
                    "tasks",
                    "infonce",
                    "prolap",
                    "retrieval",
                    "audiototext",
                    "clap",
                    "texttoaudio",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Datasets.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe use three benchmarks for audio-text retrieval:\nAudioCaps (AC)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">]\nand ClothoV2 (CL)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nAudioCaps contains 51,308 audio clips, each paired with a single caption.\nClothoV2 comprises 5,930 audio clips and has five captions per audio clip.</span>\n</p>\n\n",
                "matched_terms": [
                    "clothov2",
                    "retrieval",
                    "audiotext",
                    "audiocaps"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Input length vs. uncertainty. </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe first examine the relationship between input text length\nand predicted uncertainty on the AudioCaps dataset.\nIntuitively, longer captions tend to be more specific (i.e., convey lower uncertainty).\nAs shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#S3.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 3.3 Understanding Learned Uncertainty &#8227; 3 Experiments &#8227; ProLAP: Probabilistic Language-Audio Pre-Training\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">,\nthe baseline predicts nearly constant uncertainty across text lengths,\nindicating that it fails to capture text uncertainty.\nOne plausible factor is data characteristics: compared with AudioCaps,\nthe DataComp&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> corpus used by ProLIP\ndiffers markedly in both size (1.28B vs. 46K examples)\nand caption length (1&#8211;75 vs. 2&#8211;30 tokens).\nIn contrast, when we apply our proposed loss,\nwe observe a downward trend in uncertainty as text length increases.\nThese results suggest that the proposed method enables more data-efficient\nuncertainty estimation, even with limited training data.</span>\n</p>\n\n",
                "matched_terms": [
                    "training",
                    "dataset",
                    "method",
                    "audiocaps",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To assess the hierarchy learned by ProLAP, we introduce audio traversals.\nAnalogous to image traversals&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">,\nan audio traversal traces a straight line\nin the embedding space from an audio embedding\nto the most abstract anchor, </span>\n  <code class=\"ltx_verbatim ltx_font_typewriter\" style=\"font-size:90%;\">[ROOT]</code>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">,\nand retrieves the nearest text embedding at each of a sequence of points along this path.\nAudio traversals also have practical value:\nthey can flag audio&#8211;text examples that show mismatches in abstraction level,\nthereby supporting dataset curation or other downstream applications.</span>\n</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "prolap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For deterministic models, </span>\n  <code class=\"ltx_verbatim ltx_font_typewriter\" style=\"font-size:90%;\">[ROOT]</code>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the empty caption (</span>\n  <code class=\"ltx_verbatim ltx_font_typewriter\" style=\"font-size:90%;\">\"\"</code>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nFor probabilistic models, following ProLIP&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">,\n</span>\n  <code class=\"ltx_verbatim ltx_font_typewriter\" style=\"font-size:90%;\">[ROOT]</code>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is defined as the average of the empty-caption embedding\nand the most inclusive caption embedding for the query.\nFor evaluation, we use 50 equally spaced points\nalong the line segment between [ROOT] and the query embedding.\nTable&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#S3.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 3.4 Audio Traversal &#8227; 3 Experiments &#8227; ProLAP: Probabilistic Language-Audio Pre-Training\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows traversal performance on HierarAudioCaps.\nThe results imply that ProLAP has a better understanding of the semantic hierarchy,\nand that our proposed losses contribute to improving the hierarchical representations.</span>\n</p>\n\n",
                "matched_terms": [
                    "results",
                    "prolap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate the effectiveness of the hierarchical inclusion loss\nand the mask-repulsive loss for learning hierarchical structure,\nwe conduct an ablation study.\nTable&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.5 Ablation Studies &#8227; 3 Experiments &#8227; ProLAP: Probabilistic Language-Audio Pre-Training\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> reports retrieval performance on AudioCaps and audio traversal performance on HierarAudioCaps,\nThe results indicate that the hierarchical inclusion loss\ncontributes substantially to learning the hierarchy.\nBy contrast, the mask repulsive loss alone slightly\ndegrades performance; however, when combined with the hierarchical inclusion loss,\nit has a complementary effect that further promotes hierarchical learning.\n\n</span>\n</p>\n\n",
                "matched_terms": [
                    "retrieval",
                    "results",
                    "audiocaps"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper,\nwe propose Probabilistic Language&#8211;Audio Pre-training (ProLAP),\nwhich integrates probabilistic representations into CLAP\nto assess their effectiveness for language&#8211;audio learning.\nTo encourage hierarchical understanding,\nwe further introduce hierarchical inclusion loss and mask repulsive loss.\nExperiments show that ProLAP outperforms CLAP on audio&#8211;text retrieval benchmarks\nand likewise surpasses it on the audio traversal analysis,\nthus validating the effectiveness of our loss design in capturing semantic hierarchy.</span>\n</p>\n\n",
                "matched_terms": [
                    "retrieval",
                    "prolap",
                    "clap"
                ]
            }
        ]
    },
    "S3.T2": {
        "source_file": "ProLAP: Probabilistic Language-Audio Pre-Training",
        "caption": "Table 2: \nAudio traversal.\nR@1Lv.1{}^{\\text{Lv.1}} indicates R@1 of the most abstract caption in HierarAudioCaps.",
        "body": "Method\nPrec.\nR@1\n\nR@1Lv.1{}^{\\text{Lv.1}}\n\n\n\n\n\nCLAP (InfoNCE)\n12.77\n13.46\n8.48\n\n\nCLAP (SigLIP)\n10.60\n10.66\n8.48\n\n\nProLAP (Ours)\n19.98\n13.74\n9.39\n\n\n\nProLAP w/ ℒinch,ℒMR\\mathcal{L}^{h}_{\\mathrm{inc}},\\mathcal{L}_{\\mathrm{MR}} (Ours)\n\n27.33\n15.16\n11.76",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Method</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Prec.</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">R@1</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">R@1</span><math alttext=\"{}^{\\text{Lv.1}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m3\" intent=\":literal\"><semantics><msup><mi/><mtext mathsize=\"0.900em\">Lv.1</mtext></msup><annotation encoding=\"application/x-tex\">{}^{\\text{Lv.1}}</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">CLAP (InfoNCE)</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.77</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.46</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.48</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">CLAP (SigLIP)</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.60</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.66</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.48</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">ProLAP (Ours)</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">19.98</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.74</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.39</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">ProLAP w/ </span><math alttext=\"\\mathcal{L}^{h}_{\\mathrm{inc}},\\mathcal{L}_{\\mathrm{MR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m4\" intent=\":literal\"><semantics><mrow><msubsup><mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi><mi mathsize=\"0.900em\">inc</mi><mi mathsize=\"0.900em\">h</mi></msubsup><mo mathsize=\"0.900em\">,</mo><msub><mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi><mi mathsize=\"0.900em\">MR</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\mathcal{L}^{h}_{\\mathrm{inc}},\\mathcal{L}_{\\mathrm{MR}}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> (Ours)</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">27.33</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">15.16</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">11.76</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "siglip",
            "audio",
            "prec",
            "ℒinchℒmrmathcallhmathrmincmathcallmathrmmr",
            "caption",
            "abstract",
            "most",
            "infonce",
            "r1lv1textlv1",
            "hieraraudiocaps",
            "ours",
            "prolap",
            "method",
            "clap",
            "indicates",
            "traversal"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For deterministic models, </span>\n  <code class=\"ltx_verbatim ltx_font_typewriter\" style=\"font-size:90%;\">[ROOT]</code>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the empty caption (</span>\n  <code class=\"ltx_verbatim ltx_font_typewriter\" style=\"font-size:90%;\">\"\"</code>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nFor probabilistic models, following ProLIP&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">,\n</span>\n  <code class=\"ltx_verbatim ltx_font_typewriter\" style=\"font-size:90%;\">[ROOT]</code>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is defined as the average of the empty-caption embedding\nand the most inclusive caption embedding for the query.\nFor evaluation, we use 50 equally spaced points\nalong the line segment between [ROOT] and the query embedding.\nTable&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#S3.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 3.4 Audio Traversal &#8227; 3 Experiments &#8227; ProLAP: Probabilistic Language-Audio Pre-Training\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows traversal performance on HierarAudioCaps.\nThe results imply that ProLAP has a better understanding of the semantic hierarchy,\nand that our proposed losses contribute to improving the hierarchical representations.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Language-audio joint representation learning frameworks typically depend on deterministic embeddings,\nassuming a one-to-one correspondence between audio and text.\nIn real-world settings, however, the language-audio relationship is inherently many-to-many:\none audio segment can be described by multiple captions and vice versa.\nTo address this, we propose Probabilistic Language-Audio Pre-training (ProLAP),\nwhich models multiplicity as the spread of probability distributions\nin a joint language-audio embedding space.\nTo train the intra-modal hierarchical relationship effectively,\nwe also introduce two objectives:\n(i) hierarchical inclusion loss to promote semantic hierarchical understanding of inputs\nand (ii) mask repulsive loss to improve the efficiency of learning when optimizing the hierarchical inclusion loss.\nWith this training strategy, our model can learn the hierarchical structure inherent in the data even from small datasets, in contrast to prior probabilistic approaches that rely on large-scale datasets.\nIn our experiments, ProLAP outperforms existing deterministic approaches on audio-text retrieval tasks.\nMoreover, through experiments on the audio traversal task introduced in this paper,\nwe demonstrate that ProLAP captures the plausible semantic hierarchy.\n</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "traversal",
                    "prolap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Learning joint representations of language and audio\nhas emerged as a powerful paradigm for open-vocabulary audio understanding.\nContrastive Language-Audio Pre-training (CLAP)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\naligns audio clips and textual descriptions in a shared embedding space,\nenabling zero-shot retrieval and classification in diverse acoustic domains.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "clap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Most language&#8211;audio models&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib3\" title=\"\">3</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> implicitly\nassume a one-to-one correspondence between audio and text\nand map each input to a single deterministic point in the embedding space.\nIn real-world settings, however, the language-audio relationship is inherently many-to-many and involves uncertainty:\na single audio segment can be described by multiple valid captions\nat different levels of specificity\n(e.g., string instrument </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> guitar </span>\n  <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo>\n      <annotation encoding=\"application/x-tex\">\\to</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> acoustic guitar),\nand multiple paraphrases at the same level (e.g. &#8220;Men talking followed by an engine starting&#8221; and &#8220;An engine starts after men talk&#8221;).</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "most"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We propose Probabilistic Language-Audio Pre-training (ProLAP), a probabilistic extension of CLAP&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nProLAP represents each input (audio or text) as a probability distribution over\nthe joint embedding space rather than as a deterministic vector.\nHowever, we find empirically that a straightforward probabilistic extension\nis insufficient to capture the semantic hierarchy in audio and text.\nTherefore, we introduce hierarchical inclusion loss and mask repulsive loss to promote hierarchical learning.\nWith this training strategy, ProLAP explicitly captures\n(i) multimodal semantics arising from many-to-many audio&#8211;text correspondences\nand (ii) hierarchical relations between general and specific concepts.</span>\n</p>\n\n",
                "matched_terms": [
                    "clap",
                    "audio",
                    "prolap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In the experiments, ProLAP outperforms standard CLAP and CLAP with sigmoid loss&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\non audio-text retrieval tasks.\nAdditionally, to evaluate semantic hierarchical understanding, we introduce audio traversal task, analogous to the image traversal task&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe audio traversal task traces a straight line\nin embedding space from an audio embedding to the abstract anchor,\nretrieving the nearest text embedding at sampled points.\nThe results indicate that ProLAP better captures the semantic hierarchy than baselines.</span>\n</p>\n\n",
                "matched_terms": [
                    "abstract",
                    "prolap",
                    "clap",
                    "audio",
                    "traversal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; ProLAP: Probabilistic Language-Audio Pre-Training\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, ProLAP consists of audio and text encoders,\nand aims to learn joint language-audio representations.\nWe model each input as a Gaussian random variable with diagonal covariance, parameterized by a mean vector </span>\n  <math alttext=\"\\mu\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">&#956;</mi>\n      <annotation encoding=\"application/x-tex\">\\mu</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and a variance vector </span>\n  <math alttext=\"\\sigma^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">&#963;</mi>\n        <mn mathsize=\"0.900em\">2</mn>\n      </msup>\n      <annotation encoding=\"application/x-tex\">\\sigma^{2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "prolap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">c</mi>\n      <annotation encoding=\"application/x-tex\">c</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a positive scalar constant.\nWe designed ProLAP to learn two types of inclusion relationships: cross-modal and intra-modal (as shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; ProLAP: Probabilistic Language-Audio Pre-Training\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nFor the cross-modal case, we assume that text captions is more uncertain\nthan its audio segment.\nWe encode this relationship with the inclusion loss </span>\n  <math alttext=\"\\mathcal{L}_{\\mathrm{inc}}(Z_{a}\\subset Z_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p3.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n          <mi mathsize=\"0.900em\">inc</mi>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mrow>\n            <msub>\n              <mi mathsize=\"0.900em\">Z</mi>\n              <mi mathsize=\"0.900em\">a</mi>\n            </msub>\n            <mo mathsize=\"0.900em\">&#8834;</mo>\n            <msub>\n              <mi mathsize=\"0.900em\">Z</mi>\n              <mi mathsize=\"0.900em\">t</mi>\n            </msub>\n          </mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\mathrm{inc}}(Z_{a}\\subset Z_{t})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFor the intra-modal case, we assume that a masked input is\nconsidered more uncertain than its raw counterpart.\nWe encode this relationship with the inclusion loss </span>\n  <math alttext=\"\\mathcal{L}_{\\mathrm{inc}}(Z_{p}\\subset Z_{p^{M}})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p3.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n          <mi mathsize=\"0.900em\">inc</mi>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mrow>\n            <msub>\n              <mi mathsize=\"0.900em\">Z</mi>\n              <mi mathsize=\"0.900em\">p</mi>\n            </msub>\n            <mo mathsize=\"0.900em\">&#8834;</mo>\n            <msub>\n              <mi mathsize=\"0.900em\">Z</mi>\n              <msup>\n                <mi mathsize=\"0.900em\">p</mi>\n                <mi mathsize=\"0.900em\">M</mi>\n              </msup>\n            </msub>\n          </mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\mathrm{inc}}(Z_{p}\\subset Z_{p^{M}})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p3.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">p</mi>\n      <annotation encoding=\"application/x-tex\">p</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> indexes the modality (audio </span>\n  <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p3.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">a</mi>\n      <annotation encoding=\"application/x-tex\">a</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> or text </span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p3.m6\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) and </span>\n  <math alttext=\"p^{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p3.m7\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mi mathsize=\"0.900em\">M</mi>\n      </msup>\n      <annotation encoding=\"application/x-tex\">p^{M}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the masked input.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "prolap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Empirically, we find the ProLAP baseline\nis insufficient for learning the semantic hierarchy in audio and text\n(see details in Sec.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#S3.SS3\" style=\"font-size:90%;\" title=\"3.3 Understanding Learned Uncertainty &#8227; 3 Experiments &#8227; ProLAP: Probabilistic Language-Audio Pre-Training\">\n    <span class=\"ltx_text ltx_ref_tag\">3.3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nTo address this, we introduce two loss functions to promote semantic hierarchical understanding.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "prolap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To demonstrate the effectiveness of ProLAP,\nwe evaluate it on text-to-audio and audio-to-text retrieval tasks.\nIn our experiments, we first fine-tune the models\ninitialized with pre-trained CLAP&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> weights,\nand then evaluate the models on the retrieval tasks.\nAs baselines, we also train CLAP with the InfoNCE and\nCLAP with the sigmoid loss proposed in SigLIP&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nWe use the similarity </span>\n  <math alttext=\"s(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mo lspace=\"0em\" mathsize=\"0.900em\" rspace=\"0em\">&#8901;</mo>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">s(\\cdot)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> defined in Eq.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#S2.E1\" style=\"font-size:90%;\" title=\"In 2.2.1 Probabilistic Pairwise Contrastive Loss &#8227; 2.2 ProLAP Baseline Objective &#8227; 2 Proposed Method &#8227; ProLAP: Probabilistic Language-Audio Pre-Training\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nwhen training and evaluating ProLAP.\nFollowing prior works&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib12\" title=\"\">12</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">,\nwe report Recall@K (R@K) for </span>\n  <math alttext=\"K\\in\\{1,5,10\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">K</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">5</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">K\\in\\{1,5,10\\}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nand mean average precision among the top 10 results (mAP@10).</span>\n</p>\n\n",
                "matched_terms": [
                    "siglip",
                    "clap",
                    "infonce",
                    "prolap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Datasets.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe use three benchmarks for audio-text retrieval:\nAudioCaps (AC)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">]\nand ClothoV2 (CL)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nAudioCaps contains 51,308 audio clips, each paired with a single caption.\nClothoV2 comprises 5,930 audio clips and has five captions per audio clip.</span>\n</p>\n\n",
                "matched_terms": [
                    "caption",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">ProLAP is trained to estimate predictive uncertainty for the text and audio modalities.\nTo assess the quality of these estimates, we conduct two experiments.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "prolap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Input length vs. uncertainty. </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe first examine the relationship between input text length\nand predicted uncertainty on the AudioCaps dataset.\nIntuitively, longer captions tend to be more specific (i.e., convey lower uncertainty).\nAs shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#S3.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 3.3 Understanding Learned Uncertainty &#8227; 3 Experiments &#8227; ProLAP: Probabilistic Language-Audio Pre-Training\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">,\nthe baseline predicts nearly constant uncertainty across text lengths,\nindicating that it fails to capture text uncertainty.\nOne plausible factor is data characteristics: compared with AudioCaps,\nthe DataComp&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> corpus used by ProLIP\ndiffers markedly in both size (1.28B vs. 46K examples)\nand caption length (1&#8211;75 vs. 2&#8211;30 tokens).\nIn contrast, when we apply our proposed loss,\nwe observe a downward trend in uncertainty as text length increases.\nThese results suggest that the proposed method enables more data-efficient\nuncertainty estimation, even with limited training data.</span>\n</p>\n\n",
                "matched_terms": [
                    "method",
                    "caption"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Qualitative Evaluation. </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nFig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#S3.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 3.3 Understanding Learned Uncertainty &#8227; 3 Experiments &#8227; ProLAP: Probabilistic Language-Audio Pre-Training\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows a visualization\nof probabilistic audio embeddings of ProLAP.\nOur baseline (ProLAP without the proposed losses) fails to distinguish inputs with different masks.\nIn contrast, ProLAP with the proposed loss functions produces distinguishable embeddings\nfor these inputs.\nMoreover, it yields more appropriate inclusion relationships among the resulting distributions.</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "prolap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To assess the hierarchy learned by ProLAP, we introduce audio traversals.\nAnalogous to image traversals&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">,\nan audio traversal traces a straight line\nin the embedding space from an audio embedding\nto the most abstract anchor, </span>\n  <code class=\"ltx_verbatim ltx_font_typewriter\" style=\"font-size:90%;\">[ROOT]</code>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">,\nand retrieves the nearest text embedding at each of a sequence of points along this path.\nAudio traversals also have practical value:\nthey can flag audio&#8211;text examples that show mismatches in abstraction level,\nthereby supporting dataset curation or other downstream applications.</span>\n</p>\n\n",
                "matched_terms": [
                    "abstract",
                    "most",
                    "prolap",
                    "audio",
                    "traversal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate audio traversal quantitatively, we introduce HierarAudioCaps,\nwhich includes four levels of abstract captions\nfor each AudioCaps clip.\nLevel 4 denotes the original caption, while Level 1 denotes the most abstract caption.\nAbstract captions are generated by gpt-oss&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "caption",
                    "abstract",
                    "most",
                    "hieraraudiocaps",
                    "audio",
                    "traversal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate the effectiveness of the hierarchical inclusion loss\nand the mask-repulsive loss for learning hierarchical structure,\nwe conduct an ablation study.\nTable&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.5 Ablation Studies &#8227; 3 Experiments &#8227; ProLAP: Probabilistic Language-Audio Pre-Training\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> reports retrieval performance on AudioCaps and audio traversal performance on HierarAudioCaps,\nThe results indicate that the hierarchical inclusion loss\ncontributes substantially to learning the hierarchy.\nBy contrast, the mask repulsive loss alone slightly\ndegrades performance; however, when combined with the hierarchical inclusion loss,\nit has a complementary effect that further promotes hierarchical learning.\n\n</span>\n</p>\n\n",
                "matched_terms": [
                    "hieraraudiocaps",
                    "audio",
                    "traversal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper,\nwe propose Probabilistic Language&#8211;Audio Pre-training (ProLAP),\nwhich integrates probabilistic representations into CLAP\nto assess their effectiveness for language&#8211;audio learning.\nTo encourage hierarchical understanding,\nwe further introduce hierarchical inclusion loss and mask repulsive loss.\nExperiments show that ProLAP outperforms CLAP on audio&#8211;text retrieval benchmarks\nand likewise surpasses it on the audio traversal analysis,\nthus validating the effectiveness of our loss design in capturing semantic hierarchy.</span>\n</p>\n\n",
                "matched_terms": [
                    "clap",
                    "audio",
                    "traversal",
                    "prolap"
                ]
            }
        ]
    },
    "S3.T3": {
        "source_file": "ProLAP: Probabilistic Language-Audio Pre-Training",
        "caption": "Table 3: \nAblation study.\nInclusion test indicates the proportion of cases in which Level 1 caption includes Level 4 counterpart.",
        "body": "ℒinch\\mathcal{L}^{h}_{\\mathrm{inc}}\nℒMR\\mathcal{L}_{\\mathrm{MR}}\nRetrieval\nTraversal\nInclusion\n\n\nmAP@10\nPrec.\nR@1\ntest (%)\n\n\n\n\n\n\n57.65\n19.98\n13.74\n63.19\n\n\n✓\n\n57.27\n23.28\n15.21\n82.79\n\n\n\n✓\n57.08\n16.16\n13.63\n75.42\n\n\n✓\n✓\n57.23\n27.33\n15.16\n89.47",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\"><math alttext=\"\\mathcal{L}^{h}_{\\mathrm{inc}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m1\" intent=\":literal\"><semantics><msubsup><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>inc</mi><mi>h</mi></msubsup><annotation encoding=\"application/x-tex\">\\mathcal{L}^{h}_{\\mathrm{inc}}</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\"><math alttext=\"\\mathcal{L}_{\\mathrm{MR}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m2\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mi>MR</mi></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\mathrm{MR}}</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Retrieval</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">Traversal</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Inclusion</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">mAP@10</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">Prec.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">R@1</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">test (%)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">57.65</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">19.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">63.19</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">57.27</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">23.28</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">15.21</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">82.79</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">57.08</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">16.16</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">13.63</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">75.42</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#10003;</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">57.23</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">27.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">15.16</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">89.47</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "cases",
            "prec",
            "map10",
            "caption",
            "which",
            "test",
            "study",
            "ablation",
            "inclusion",
            "traversal",
            "ℒinchmathcallhmathrminc",
            "retrieval",
            "ℒmrmathcallmathrmmr",
            "includes",
            "counterpart",
            "indicates",
            "proportion",
            "level"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate the effectiveness of the hierarchical inclusion loss\nand the mask-repulsive loss for learning hierarchical structure,\nwe conduct an ablation study.\nTable&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.5 Ablation Studies &#8227; 3 Experiments &#8227; ProLAP: Probabilistic Language-Audio Pre-Training\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> reports retrieval performance on AudioCaps and audio traversal performance on HierarAudioCaps,\nThe results indicate that the hierarchical inclusion loss\ncontributes substantially to learning the hierarchy.\nBy contrast, the mask repulsive loss alone slightly\ndegrades performance; however, when combined with the hierarchical inclusion loss,\nit has a complementary effect that further promotes hierarchical learning.\n\n</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Language-audio joint representation learning frameworks typically depend on deterministic embeddings,\nassuming a one-to-one correspondence between audio and text.\nIn real-world settings, however, the language-audio relationship is inherently many-to-many:\none audio segment can be described by multiple captions and vice versa.\nTo address this, we propose Probabilistic Language-Audio Pre-training (ProLAP),\nwhich models multiplicity as the spread of probability distributions\nin a joint language-audio embedding space.\nTo train the intra-modal hierarchical relationship effectively,\nwe also introduce two objectives:\n(i) hierarchical inclusion loss to promote semantic hierarchical understanding of inputs\nand (ii) mask repulsive loss to improve the efficiency of learning when optimizing the hierarchical inclusion loss.\nWith this training strategy, our model can learn the hierarchical structure inherent in the data even from small datasets, in contrast to prior probabilistic approaches that rely on large-scale datasets.\nIn our experiments, ProLAP outperforms existing deterministic approaches on audio-text retrieval tasks.\nMoreover, through experiments on the audio traversal task introduced in this paper,\nwe demonstrate that ProLAP captures the plausible semantic hierarchy.\n</span>\n</p>\n\n",
                "matched_terms": [
                    "retrieval",
                    "inclusion",
                    "which",
                    "traversal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In the experiments, ProLAP outperforms standard CLAP and CLAP with sigmoid loss&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\non audio-text retrieval tasks.\nAdditionally, to evaluate semantic hierarchical understanding, we introduce audio traversal task, analogous to the image traversal task&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nThe audio traversal task traces a straight line\nin embedding space from an audio embedding to the abstract anchor,\nretrieving the nearest text embedding at sampled points.\nThe results indicate that ProLAP better captures the semantic hierarchy than baselines.</span>\n</p>\n\n",
                "matched_terms": [
                    "retrieval",
                    "traversal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we propose Probabilistic Language-Audio Pre-training (ProLAP),\nwhich learns the joint language-audio representations\nwith probabilistic modeling of inputs.\nBuilding on ProLIP&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">,\nwe follow its probabilistic formulation,\nand our baseline ProLAP is a straightforward application of the ProLIP for audio-text data.\nWe also propose two loss functions to promote hierarchical learning:\nhierarchical inclusion loss and mask repulsive loss.\nIn the following sections, we first provide an overview of ProLAP,\nand then we explain the two loss objectives used in ProLIP to learn the cross-modal probabilistic representations.\nFinally, we introduce two additional loss functions to promote semantic hierarchy understanding.\n</span>\n</p>\n\n",
                "matched_terms": [
                    "inclusion",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To model the semantic hierarchy, we use the inclusion loss following ProLIP&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFirst, we define the following inclusion score (test statistic) to quantify the asymmetric inclusion </span>\n  <math alttext=\"Z_{1}\\subset Z_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">Z</mi>\n          <mn mathsize=\"0.900em\">1</mn>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8834;</mo>\n        <msub>\n          <mi mathsize=\"0.900em\">Z</mi>\n          <mn mathsize=\"0.900em\">2</mn>\n        </msub>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">Z_{1}\\subset Z_{2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">:</span>\n</p>\n\n",
                "matched_terms": [
                    "inclusion",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"p_{1}(x)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">p</mi>\n          <mn mathsize=\"0.900em\">1</mn>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mi mathsize=\"0.900em\">x</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">p_{1}(x)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"p_{2}(x)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi mathsize=\"0.900em\">p</mi>\n          <mn mathsize=\"0.900em\">2</mn>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mi mathsize=\"0.900em\">x</mi>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">p_{2}(x)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are the probability density functions (pdf) corresponding to </span>\n  <math alttext=\"Z_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p2.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">Z</mi>\n        <mn mathsize=\"0.900em\">1</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">Z_{1}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and </span>\n  <math alttext=\"Z_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p2.m4\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">Z</mi>\n        <mn mathsize=\"0.900em\">2</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">Z_{2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, respectively.\nWe then map this score to a probability via a logistic link\nand use its negative log-likelihood as the inclusion loss,\nwhich encourages inclusion of </span>\n  <math alttext=\"Z_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p2.m5\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">Z</mi>\n        <mn mathsize=\"0.900em\">1</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">Z_{1}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> in </span>\n  <math alttext=\"Z_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p2.m6\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">Z</mi>\n        <mn mathsize=\"0.900em\">2</mn>\n      </msub>\n      <annotation encoding=\"application/x-tex\">Z_{2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">:</span>\n</p>\n\n",
                "matched_terms": [
                    "inclusion",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">where </span>\n  <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p3.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">c</mi>\n      <annotation encoding=\"application/x-tex\">c</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is a positive scalar constant.\nWe designed ProLAP to learn two types of inclusion relationships: cross-modal and intra-modal (as shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; ProLAP: Probabilistic Language-Audio Pre-Training\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nFor the cross-modal case, we assume that text captions is more uncertain\nthan its audio segment.\nWe encode this relationship with the inclusion loss </span>\n  <math alttext=\"\\mathcal{L}_{\\mathrm{inc}}(Z_{a}\\subset Z_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p3.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n          <mi mathsize=\"0.900em\">inc</mi>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mrow>\n            <msub>\n              <mi mathsize=\"0.900em\">Z</mi>\n              <mi mathsize=\"0.900em\">a</mi>\n            </msub>\n            <mo mathsize=\"0.900em\">&#8834;</mo>\n            <msub>\n              <mi mathsize=\"0.900em\">Z</mi>\n              <mi mathsize=\"0.900em\">t</mi>\n            </msub>\n          </mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\mathrm{inc}}(Z_{a}\\subset Z_{t})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nFor the intra-modal case, we assume that a masked input is\nconsidered more uncertain than its raw counterpart.\nWe encode this relationship with the inclusion loss </span>\n  <math alttext=\"\\mathcal{L}_{\\mathrm{inc}}(Z_{p}\\subset Z_{p^{M}})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p3.m3\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mi class=\"ltx_font_mathcaligraphic\" mathsize=\"0.900em\">&#8466;</mi>\n          <mi mathsize=\"0.900em\">inc</mi>\n        </msub>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mrow>\n            <msub>\n              <mi mathsize=\"0.900em\">Z</mi>\n              <mi mathsize=\"0.900em\">p</mi>\n            </msub>\n            <mo mathsize=\"0.900em\">&#8834;</mo>\n            <msub>\n              <mi mathsize=\"0.900em\">Z</mi>\n              <msup>\n                <mi mathsize=\"0.900em\">p</mi>\n                <mi mathsize=\"0.900em\">M</mi>\n              </msup>\n            </msub>\n          </mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\mathrm{inc}}(Z_{p}\\subset Z_{p^{M}})</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, where </span>\n  <math alttext=\"p\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p3.m4\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">p</mi>\n      <annotation encoding=\"application/x-tex\">p</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> indexes the modality (audio </span>\n  <math alttext=\"a\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p3.m5\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">a</mi>\n      <annotation encoding=\"application/x-tex\">a</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> or text </span>\n  <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p3.m6\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">t</mi>\n      <annotation encoding=\"application/x-tex\">t</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) and </span>\n  <math alttext=\"p^{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.SSS2.p3.m7\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">p</mi>\n        <mi mathsize=\"0.900em\">M</mi>\n      </msup>\n      <annotation encoding=\"application/x-tex\">p^{M}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> denotes the masked input.</span>\n</p>\n\n",
                "matched_terms": [
                    "inclusion",
                    "counterpart"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Moreover, our ProLAP baseline produces degenerate representations for masked inputs\n(See Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#S3.F3.sf1\" style=\"font-size:90%;\" title=\"In Figure 3 &#8227; 3.3 Understanding Learned Uncertainty &#8227; 3 Experiments &#8227; ProLAP: Probabilistic Language-Audio Pre-Training\">\n    <span class=\"ltx_text ltx_ref_tag\">3(a)</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nIf the model cannot adequately distinguish the representations of masked inputs,\nit fails to faithfully encode the unmasked content, thereby undermining\nhierarchical learning with the mask-based inclusion loss described above.\nTherefore, we introduce mask repulsive loss,\nwhich pushes representations for masked inputs away from each other.\nMask repulsive loss is defined as a negative-only PPCL:</span>\n</p>\n\n",
                "matched_terms": [
                    "inclusion",
                    "which"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To demonstrate the effectiveness of ProLAP,\nwe evaluate it on text-to-audio and audio-to-text retrieval tasks.\nIn our experiments, we first fine-tune the models\ninitialized with pre-trained CLAP&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> weights,\nand then evaluate the models on the retrieval tasks.\nAs baselines, we also train CLAP with the InfoNCE and\nCLAP with the sigmoid loss proposed in SigLIP&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nWe use the similarity </span>\n  <math alttext=\"s(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">s</mi>\n        <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n          <mo lspace=\"0em\" mathsize=\"0.900em\" rspace=\"0em\">&#8901;</mo>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">s(\\cdot)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> defined in Eq.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#S2.E1\" style=\"font-size:90%;\" title=\"In 2.2.1 Probabilistic Pairwise Contrastive Loss &#8227; 2.2 ProLAP Baseline Objective &#8227; 2 Proposed Method &#8227; ProLAP: Probabilistic Language-Audio Pre-Training\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nwhen training and evaluating ProLAP.\nFollowing prior works&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib12\" title=\"\">12</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">,\nwe report Recall@K (R@K) for </span>\n  <math alttext=\"K\\in\\{1,5,10\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mi mathsize=\"0.900em\">K</mi>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">5</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">10</mn>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">K\\in\\{1,5,10\\}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nand mean average precision among the top 10 results (mAP@10).</span>\n</p>\n\n",
                "matched_terms": [
                    "retrieval",
                    "map10"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Datasets.</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nWe use three benchmarks for audio-text retrieval:\nAudioCaps (AC)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">]\nand ClothoV2 (CL)&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.\nAudioCaps contains 51,308 audio clips, each paired with a single caption.\nClothoV2 comprises 5,930 audio clips and has five captions per audio clip.</span>\n</p>\n\n",
                "matched_terms": [
                    "retrieval",
                    "caption"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To assess the hierarchy learned by ProLAP, we introduce audio traversals.\nAnalogous to image traversals&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">,\nan audio traversal traces a straight line\nin the embedding space from an audio embedding\nto the most abstract anchor, </span>\n  <code class=\"ltx_verbatim ltx_font_typewriter\" style=\"font-size:90%;\">[ROOT]</code>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">,\nand retrieves the nearest text embedding at each of a sequence of points along this path.\nAudio traversals also have practical value:\nthey can flag audio&#8211;text examples that show mismatches in abstraction level,\nthereby supporting dataset curation or other downstream applications.</span>\n</p>\n\n",
                "matched_terms": [
                    "traversal",
                    "level"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To evaluate audio traversal quantitatively, we introduce HierarAudioCaps,\nwhich includes four levels of abstract captions\nfor each AudioCaps clip.\nLevel 4 denotes the original caption, while Level 1 denotes the most abstract caption.\nAbstract captions are generated by gpt-oss&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "caption",
                    "which",
                    "includes",
                    "traversal",
                    "level"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For deterministic models, </span>\n  <code class=\"ltx_verbatim ltx_font_typewriter\" style=\"font-size:90%;\">[ROOT]</code>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is the empty caption (</span>\n  <code class=\"ltx_verbatim ltx_font_typewriter\" style=\"font-size:90%;\">\"\"</code>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).\nFor probabilistic models, following ProLIP&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">,\n</span>\n  <code class=\"ltx_verbatim ltx_font_typewriter\" style=\"font-size:90%;\">[ROOT]</code>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is defined as the average of the empty-caption embedding\nand the most inclusive caption embedding for the query.\nFor evaluation, we use 50 equally spaced points\nalong the line segment between [ROOT] and the query embedding.\nTable&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.18423v1#S3.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 3.4 Audio Traversal &#8227; 3 Experiments &#8227; ProLAP: Probabilistic Language-Audio Pre-Training\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows traversal performance on HierarAudioCaps.\nThe results imply that ProLAP has a better understanding of the semantic hierarchy,\nand that our proposed losses contribute to improving the hierarchical representations.</span>\n</p>\n\n",
                "matched_terms": [
                    "caption",
                    "traversal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper,\nwe propose Probabilistic Language&#8211;Audio Pre-training (ProLAP),\nwhich integrates probabilistic representations into CLAP\nto assess their effectiveness for language&#8211;audio learning.\nTo encourage hierarchical understanding,\nwe further introduce hierarchical inclusion loss and mask repulsive loss.\nExperiments show that ProLAP outperforms CLAP on audio&#8211;text retrieval benchmarks\nand likewise surpasses it on the audio traversal analysis,\nthus validating the effectiveness of our loss design in capturing semantic hierarchy.</span>\n</p>\n\n",
                "matched_terms": [
                    "retrieval",
                    "inclusion",
                    "which",
                    "traversal"
                ]
            }
        ]
    }
}