{
    "S3.T1": {
        "caption": "Table 1: Text evaluation dimensions of the SCs.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#ECECEC;\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#ECECEC;\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text ltx_align_left ltx_font_bold\">Eval. Dimension</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#ECECEC;\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Description &amp; Scale Anchors (1&#8211;5)</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\">Semantic Coherence</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Coherence of continuation with the given prompt:</span>\n<br class=\"ltx_break\"/>1 = Off-topic or incoherent; Additionally reads as an audiobook narration.\n<br class=\"ltx_break\"/>5 = Highly coherent and consistent with prompt context. \n<br class=\"ltx_break\"/></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\">Sentence \n<br class=\"ltx_break ltx_align_left\"/>Polarity</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Sentiment consistency between continuation and prompt:</span>\n<br class=\"ltx_break\"/>1 = Strongly mismatched polarity (e.g., cheerful tone in a tragic context or vice-versa). \n<br class=\"ltx_break\"/>5 = Polarity is consistent with and reinforces the prompt&#8217;s sentiment.\n\n<br class=\"ltx_break\"/></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\">Agency &amp; \n<br class=\"ltx_break ltx_align_left\"/>Competence&#160;<cite class=\"ltx_cite ltx_align_left ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib27\" title=\"\">27</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Portrayal of speaker as agentic and competent:</span>\n<br class=\"ltx_break\"/>1 = Low agency (passive, helpless, lacking initiative);\n<br class=\"ltx_break\"/>5 = High agency (assertive, accomplished, decision-making). \n<br class=\"ltx_break\"/></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\">Emotionalisation \n<br class=\"ltx_break ltx_align_left\"/>(Affect Framing)&#160;<cite class=\"ltx_cite ltx_align_left ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib28\" title=\"\">28</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Treatment of emotions in the continuation:</span>\n<br class=\"ltx_break\"/>1 = Emotion framed as weakness or irrationality, gendered fragility;\n<br class=\"ltx_break\"/>5 = Emotions handled neutrally or validated without gendered framing. \n<br class=\"ltx_break\"/></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\">Appearance \n<br class=\"ltx_break ltx_align_left\"/>(Objectification) \n<br class=\"ltx_break ltx_align_left\"/>&#160;<cite class=\"ltx_cite ltx_align_left ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:80%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib27\" title=\"\">27</a><span class=\"ltx_text\" style=\"font-size:80%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Undue focus on looks, body, or sexualisation:</span>\n<br class=\"ltx_break\"/>1 = Strong appearance or objectifying focus;\n<br class=\"ltx_break\"/>5 = No undue emphasis on appearance, focus on actions/agency.</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "irrationality",
            "appearance",
            "high",
            "evaluation",
            "reads",
            "highly",
            "low",
            "gendered",
            "competent",
            "neutrally",
            "eval",
            "audiobook",
            "mismatched",
            "handled",
            "objectifying",
            "treatment",
            "additionally",
            "given",
            "undue",
            "1–5",
            "framing",
            "consistency",
            "semantic",
            "agentic",
            "context",
            "speaker",
            "cheerful",
            "sentence",
            "text",
            "emotionalisation",
            "coherent",
            "incoherent",
            "without",
            "reinforces",
            "decisionmaking",
            "emphasis",
            "viceversa",
            "emotion",
            "fragility",
            "coherence",
            "polarity",
            "validated",
            "description",
            "consistent",
            "assertive",
            "offtopic",
            "emotions",
            "strong",
            "sexualisation",
            "portrayal",
            "weakness",
            "narration",
            "scs",
            "between",
            "scale",
            "tragic",
            "framed",
            "actionsagency",
            "accomplished",
            "agency",
            "anchors",
            "prompt",
            "affect",
            "prompt’s",
            "strongly",
            "lacking",
            "passive",
            "looks",
            "competence",
            "sentiment",
            "objectification",
            "tone",
            "helpless",
            "initiative",
            "continuation",
            "dimensions",
            "dimension",
            "focus",
            "body"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Textual Evaluation:</span> We evaluate the textual content of the SCs for logical coherence/sentence polarity preservation and for gender bias; see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#S3.T1\" title=\"Table 1 &#8227; 3.2 Procedure &#8227; 3 Experiments\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. To obtain the textual content of the SCs, we use the <span class=\"ltx_text ltx_font_typewriter\">azure-cognitiveservices-speech</span> SDK and perform automatic speech recognition of the SC. Then, we use the <span class=\"ltx_text ltx_font_typewriter\">gemini-2.5-flash-lite-preview-06-17</span> API as an LLM judge and rate the textual content on a scale of 1&#8211;5 on five dimensions, without exposing any knowledge of the input gender from the speech prompt to the API. Previous research has shown LLM judges as being capable of matching crowdsourced human performance on open-ended text evaluation tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib22\" title=\"\">22</a>]</cite>. Our rubrics assess continuation coherence and sentiment preservation, while the other metrics draw on prior bias research to capture gender bias.\n<span class=\"ltx_text ltx_font_bold\">Semantic Coherence and Sentence Polarity:</span> These measure the degree to which the continuation follows logically from the prompt and preserves its intended emotional stance.\n<span class=\"ltx_text ltx_font_bold\">Social Bias Dimensions:</span> We adapt constructs from social psychology (e.g., agency/communality, ambivalent sexism, stereotype content model) and prior works in bias with text (e.g., gendered language, appearance focus) to the setting of first-person SCs, ensuring that they capture empirically documented harms relevant to gender bias&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib26\" title=\"\">26</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib27\" title=\"\">27</a>]</cite>.</p>\n\n",
            "<p class=\"ltx_p\">Since the absence of speaker similarity does not necessarily imply that voice conditioning has no effect, we proceeded to evaluate bias in the lexical content of the continuations in all 4 models.\nAmong the five metrics presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#S3.T1\" title=\"Table 1 &#8227; 3.2 Procedure &#8227; 3 Experiments\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, we did not observe a significant effect of VQ and gender on <span class=\"ltx_text ltx_font_italic\">Emotionalisation</span> and <span class=\"ltx_text ltx_font_italic\">Appearance</span>, and the only effect of model demonstrated a small improvement of <span class=\"ltx_text ltx_font_italic\">Emotionalisation</span> by SpeechGPT compared to both variants of SpiritLM. Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#S4.F1\" title=\"Figure 1 &#8227; 4.2 Evaluation of Bias &#8227; 4 Results and Discussion\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reports mean scores and standard deviations for <span class=\"ltx_text ltx_font_italic\">Semantic Coherence</span>, <span class=\"ltx_text ltx_font_italic\">Sentence Polarity</span> and <span class=\"ltx_text ltx_font_italic\">Agency &amp; Competence</span>. Statistical tests reveal an interaction between model and gender for all metrics, but no impact of VQ. Therefore, all further comparisons are made while considering all VQ conditions together (last row of each subfigure).</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Speech Continuation (SC) is the task of generating a coherent extension of a spoken prompt while preserving both semantic context and speaker identity. Because SC is constrained to a single audio stream, it offers a more direct setting for probing biases in speech foundation models than dialogue does. In this work we present the first systematic evaluation of bias in SC, investigating how gender and phonation type (breathy, creaky, end-creak) affect continuation behaviour. We evaluate three recent models: SpiritLM (base and expressive), VAE-GSLM, and SpeechGPT across speaker similarity, voice quality preservation, and text-based bias metrics. Results show that while both speaker similarity and coherence remain a challenge, textual evaluations reveal significant model and gender interactions: once coherence is sufficiently high (for VAE-GSLM), gender effects emerge on text-metrics such as agency and sentence polarity.\nIn addition, continuations revert toward modal phonation more strongly for female prompts than for male ones, revealing a systematic voice-quality bias.\nThese findings highlight SC as a controlled probe of socially relevant representational biases in speech foundation models, and suggest that it will become an increasingly informative diagnostic as continuation quality improves.\n</p>\n\n",
                "matched_terms": [
                    "coherent",
                    "coherence",
                    "polarity",
                    "agency",
                    "prompt",
                    "semantic",
                    "context",
                    "high",
                    "speaker",
                    "evaluation",
                    "continuation",
                    "affect",
                    "sentence",
                    "strongly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in large language model (LLM)-based speech generation have introduced the Speech Continuation (SC) task as a new model capability.\nIn this task, the system is provided with a short audio prompt of a speaker and is required to generate a continuation that preserves speaker identity, prosody, and linguistic content <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib1\" title=\"\">1</a>]</cite>.\nThe SC task has been adopted as a benchmark in recent models such as AudioLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib2\" title=\"\">2</a>]</cite>, SpeechGPT-Gen <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib3\" title=\"\">3</a>]</cite>, SpiritLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib4\" title=\"\">4</a>]</cite> and VAE-GSLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib5\" title=\"\">5</a>]</cite>, where it is used to evaluate zero-shot voice preservation and prosodic consistency.\nWhile the evaluation of SC models has largely focused on performance metrics such as speaker similarity, much less is known about the social and representational biases that speech foundation models may exhibit through this task.</p>\n\n",
                "matched_terms": [
                    "consistency",
                    "prompt",
                    "continuation",
                    "speaker",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Bias evaluation in speech generation has only recently developed as a research area<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib8\" title=\"\">8</a>]</cite>, following earlier work on bias in speech recognition <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib10\" title=\"\">10</a>]</cite>.\nFor instance, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib6\" title=\"\">6</a>]</cite> introduce a toolkit for assessing semantic gender bias in SpeechLLMs across spoken QA and multiple-choice continuation.\nIn Conversational AI, speech foundation model bias evaluations are complicated by the inherently interactive nature of conversations. Hence, observed bias may be difficult to disentangle from the joint influence of the interlocutor&#8217;s voice and role-based framing effects\n(prompts) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib11\" title=\"\">11</a>]</cite>. Arguing for a more direct way to probe bias in speech models, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib8\" title=\"\">8</a>]</cite> examine &#8220;default speaker assignment&#8221; in a text-to-audio model: that is, how the model selects a voice when none is specified. They show that for certain prompts &#8211; such as stereotyped professions or gender-associated words &#8211; the model exhibits systematic gendered tendencies in its voice assignments.\nThe SC task provides a similar monologic setting that has the potential to provide a much cleaner probe of representational bias,\nrevealing how a model&#8217;s linguistic and acoustic predictions vary as a function of the speaker it is asked to imitate.\nIn other words, while bias in dialogue speech foundation models shows how someone would have been responded to, the question remains: <em class=\"ltx_emph ltx_font_italic\">&#8220;By whom?&#8221;</em>. SC bias shows what someone with this voice identity would have said according to the model.\nNotably, beyond serving as a benchmark for demonstrating raw model capability, the SC task has so far lacked a clear practical use case. We highlight probing for voice-based bias as a compelling and socially relevant application.</p>\n\n",
                "matched_terms": [
                    "gendered",
                    "framing",
                    "semantic",
                    "continuation",
                    "speaker",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To the best of our knowledge, this is the first paper to (1) evaluate bias in voice-conditioned speech continuation models, and (2) systematically vary voice quality (e.g., breathy voice, creaky phonation) &#8211; a socially salient but previously overlooked dimension of variation &#8211; in the evaluation of large speech generation models in general.</p>\n\n",
                "matched_terms": [
                    "continuation",
                    "evaluation",
                    "dimension"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We develop a methodology to probe paralinguistic gender bias via the SC task, where a spoken prompt is extended by the model. We next describe the test data, evaluation dimensions, and experimental protocol, guided by the hypotheses that (i) gender and (ii) voice quality can systematically shape continuation outputs once sufficient coherence is achieved.</p>\n\n",
                "matched_terms": [
                    "coherence",
                    "prompt",
                    "continuation",
                    "dimensions",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">1. <span class=\"ltx_text ltx_font_bold\">Spoken StereoSet<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text ltx_font_medium\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib12\" title=\"\">12</a><span class=\"ltx_text ltx_font_medium\">]</span></cite></span>: We select 149 (<math alttext=\"&gt;3\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">&gt;3</annotation></semantics></math> s) generations from the gender subset of the Spoken StereoSet (Mean duration: <math alttext=\"5.25\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\"><semantics><mn>5.25</mn><annotation encoding=\"application/x-tex\">5.25</annotation></semantics></math> s), a previously validated multiple choice benchmark derived from the original StereoSet dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib13\" title=\"\">13</a>]</cite> and designed to evaluate social biases in SpeechLLMs. Each instance consists of a spoken context synthesised via TTS, and three candidate textual continuations: one stereotypical, one anti-stereotypical, and one irrelevant. Speaker attributes such as gender are expressed through the speech signal but not the text. To repurpose the benchmark for generative SC, we use the original speech prompt as a continuation prompt. We call the prompts from this set SS<sub class=\"ltx_sub\">set</sub>.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "context",
                    "validated",
                    "continuation",
                    "speaker",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">2. <span class=\"ltx_text ltx_font_bold\">Neutral open-ended prompts</span>: To supplement this, we also constructed an evaluation set of 150 neutral, open-ended sentence starters. These prompts were first generated with OpenAI GPT-5 and subsequently validated and filtered by a human annotator. Then, we used the same Azure TTS voices from SS<sub class=\"ltx_sub\">set</sub> to synthesise spoken versions (Mean duration: <math alttext=\"4.31\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m1\" intent=\":literal\"><semantics><mn>4.31</mn><annotation encoding=\"application/x-tex\">4.31</annotation></semantics></math> s). The prompts cover 15 pragmatic categories (e.g., expressing opinion, posing possibilities) and are underspecified, to permit diverse continuations.\nWe call the prompts from this set NOP<sub class=\"ltx_sub\">set</sub>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "sentence",
                    "validated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to gender, we investigated whether <em class=\"ltx_emph ltx_font_italic\">voice quality</em> (VQ) modulations can influence bias patterns in speech continuation. For this, we used <span class=\"ltx_text ltx_font_bold\">VoiceQualityVC</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib14\" title=\"\">14</a>]</cite>, a recently introduced voice conversion system designed to systematically manipulate phonation types such as breathy and creaky voice, while preserving speaker identity. According to the literature, breathy and creaky phonation serve pragmatic <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib16\" title=\"\">16</a>]</cite> and paralinguistic <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib17\" title=\"\">17</a>]</cite> functions and influence social perception. Creaky voice, especially in female speakers, has been linked to lower competence, education, trustworthiness, and employability <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib18\" title=\"\">18</a>]</cite>, though phrase-final creak appears less marked <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib19\" title=\"\">19</a>]</cite>. In contrast, breathy voice is associated with increased attractiveness and likeability <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib20\" title=\"\">20</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "continuation",
                    "speaker",
                    "competence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We rendered each prompt in four VQ conditions, as described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#S3.SS2\" title=\"3.2 Procedure &#8227; 3 Experiments\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, yielding a total of 4,784 distinct speech inputs for evaluation.\nThis allows us to go beyond gender categories and probe intersectional biases that may more closely reflect human perceptual tendencies. The questions we are asking concerning voice quality are twofold: first, whether voice quality affects the semantic bias in the continuation, and second, whether there are gender effects in VQ preservation.</p>\n\n",
                "matched_terms": [
                    "continuation",
                    "evaluation",
                    "prompt",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaker Similarity:</span> We assess speaker preservation by computing cosine similarity between ECAPA-TDNN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib21\" title=\"\">21</a>]</cite> embeddings of the continuation and reference prompt.</p>\n\n",
                "matched_terms": [
                    "continuation",
                    "speaker",
                    "prompt",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Voice Quality Preservation:</span>\nOne indirect way to measure whether models extract VQ information from the input prompts, is if they demonstrate an\nability to maintain phonation characteristics from the prompt throughout the continuation. To measure and compare VQ between prompts and continuations, we extracted two glottal source parameters, representative of the opening (H1&#8211;H2) and closing (H1&#8211;A3) of the vocal folds, which aids in distinguishing breathy and creaky voice for male speakers <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib15\" title=\"\">15</a>]</cite>.\nWe also included CPPS as a noise parameter to distinguish breathy female voices and capture voice quality in male speakers.</p>\n\n",
                "matched_terms": [
                    "continuation",
                    "prompt",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated three models with public checkpoints that support voice-conditioned SC: SpiritLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib4\" title=\"\">4</a>]</cite> (in two variants), VAE-GSLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib5\" title=\"\">5</a>]</cite>, and SpeechGPT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib3\" title=\"\">3</a>]</cite>.\n<span class=\"ltx_text ltx_font_bold\">SpiritLM</span> Base is a LLaMA-2&#8211;based model trained on interleaved text and speech tokens; while the expressive (Expr.) variant further conditions on pitch and style tokens to reproduce prosodic cues.\n<span class=\"ltx_text ltx_font_bold\">VAE-GSLM</span> combines discrete semantic tokens with a VAE over continuous speech features, enabling more fine-grained voice preservation.\n<span class=\"ltx_text ltx_font_bold\">SpeechGPT</span> is an 8B-parameter model with a semantic LM and a flow-matching decoder, designed for TTS and dialogue but also supports SC.</p>\n\n",
                "matched_terms": [
                    "text",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We design four experimental conditions:\n(1) Baseline Condition: Unmodified speech prompts from SS<sub class=\"ltx_sub\">set</sub> and NOP<sub class=\"ltx_sub\">set</sub>; (2) Breathy voice condition;\n(3) Creaky voice condition;\n(4) End creak condition.\nThe parallel versions, in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#S2.SS2\" title=\"2.2 Voice Quality Manipulations &#8227; 2 Method\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, were created using VoiceQualityVC with the original speaker of the prompt as the target speaker and the following parameters as conditioning for breathy voice: high H1&#8211;H2 and high H1&#8211;A3 (both <math alttext=\"+3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo>+</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">+3</annotation></semantics></math> st.d. from the mean) and low creak (<math alttext=\"-2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">-2</annotation></semantics></math> st.d.) and low CPPS (<math alttext=\"-1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">-1</annotation></semantics></math> st.d.), and the following for creaky voice: high creak (<math alttext=\"+2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mo>+</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">+2</annotation></semantics></math> st.d.), low CPPS (<math alttext=\"-1.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1.5</mn></mrow><annotation encoding=\"application/x-tex\">-1.5</annotation></semantics></math> st.d.) and low H1&#8211;H2 and H1&#8211;A3 (<math alttext=\"-2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m6\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">-2</annotation></semantics></math> st.d.). For end creak, the conditioning starts from the midway point of the audio, increasing linearly to: extremely high creak (<math alttext=\"+7\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m7\" intent=\":literal\"><semantics><mrow><mo>+</mo><mn>7</mn></mrow><annotation encoding=\"application/x-tex\">+7</annotation></semantics></math> st.d.), and low H1&#8211;H2, H1&#8211;A3, and CPPS (<math alttext=\"-2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m8\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">-2</annotation></semantics></math> st.d.).\nEach model was prompted with 3&#8211;5 s reference audio files from SS<sub class=\"ltx_sub\">set</sub> and NOP<sub class=\"ltx_sub\">set</sub>, and tasked with generating a 5&#8211;8 s continuation that was semantically coherent and preserved the input speaker&#8217;s voice.\nThe impact of voice quality, gender, and model on each metric was investigated using beta regression. Interactions were removed stepwise if ANOVA comparisons showed no significance.</p>\n\n",
                "matched_terms": [
                    "coherent",
                    "low",
                    "prompt",
                    "high",
                    "speaker",
                    "continuation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaker similarity:</span> By contrast with results reported in original papers&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib4\" title=\"\">4</a>]</cite>, SpeechGPT and SpiritLM Base both generated speech with a single speaker identity which was independent from the input prompt, female for SpeechGPT, and male for SpiritLM Base.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#S4.T2\" title=\"Table 2 &#8227; 4.1 Evaluation of Continuation &#8227; 4 Results and Discussion\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reports speaker similarity scores of the SpiritLM Expr. and VAE-GSLM models.\nModel differences to speaker-gender are significant: VAE-GSLM yields higher speaker similarity than SpiritLM Expr., while SpiritLM Expr. itself shows gender-specific variation. Qualitative observations suggest SpiritLM Expr. systematically generates a female-presenting voice that adapts to the prompt (e.g., lowering pitch for male inputs). VAE-GSLM is the only model to fully reproduce distinct speaker identities.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "prompt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Voice Quality Similarity:</span>\nxTo examine how well voice quality was maintained and study voice-quality related bias in the continuations, the H1&#8211;H2, H1&#8211;A3, and CPPS of the prompts and continuations were compared using a Linear Mixed-Effects model with type III ANOVA. Post-hoc pairwise comparisons were performed on the estimated marginal means, with Tukey adjustment for multiple comparisons. In the prompts, female voices showed slightly lower H1&#8211;H2 and H1&#8211;A3 than males (<math alttext=\"p&lt;0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.05</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.05</annotation></semantics></math>), consistent with somewhat creakier phonation. In the continuations, this pattern inverted: female outputs were systematically breathier and less creaky than male ones, particularly after breathy and creaky prompts (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>). End-creak prompts behaved as an intermediate case.\nFor CPPS, baseline female prompts were slightly lower than male ones, indicating noisier or creakier voice. In the continuations, the effect reversed: female outputs had higher CPPS (i.e., more modal phonation) than males across modal and breathy prompts (<math alttext=\"p&lt;0.0001\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m3\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.0001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.0001</annotation></semantics></math>). For creaky and end-creak prompts, the pattern depended on model: VAE-GSLM produced higher CPPS for males, whereas SpiritLM Expr. produced higher CPPS for females, strongly reducing creak in female voices (<math alttext=\"p&lt;0.0001\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m4\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.0001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.0001</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "consistent",
                    "strongly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effect of Model:</span>\nAmong the <math alttext=\"36\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mn>36</mn><annotation encoding=\"application/x-tex\">36</annotation></semantics></math> pairs of model comparisons made for the two genders and three metrics, <math alttext=\"31\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><mn>31</mn><annotation encoding=\"application/x-tex\">31</annotation></semantics></math> were significant, demonstrating a clear distinction in generation quality among the models.\nSpiritLM Base, SpiritLM Expr., and VAE-GSLM consistently produce text with reasonable <span class=\"ltx_text ltx_font_italic\">Semantic Coherence</span>, with mean scores generally above <math alttext=\"2.4\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><mn>2.4</mn><annotation encoding=\"application/x-tex\">2.4</annotation></semantics></math>. SpeechGPT&#8217;s outputs are of markedly lower quality, with its highest <span class=\"ltx_text ltx_font_italic\">Semantic Coherence</span> score being just <math alttext=\"1.45\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mn>1.45</mn><annotation encoding=\"application/x-tex\">1.45</annotation></semantics></math>. Its mean score for a breathy female voice prompt was particularly low at <math alttext=\"1.37\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mn>1.37</mn><annotation encoding=\"application/x-tex\">1.37</annotation></semantics></math>.\nA similar trend is observed for the two other metrics. SpiritLM Base provides the highest scores on all metrics, with the exception of VAE-GSLM on males outperforming other models on <span class=\"ltx_text ltx_font_italic\">Sentence Polarity</span>.\nInterestingly, while VAE-GSLM performs best on audio features (speaker and VQ similarity), SpiritLM is more consistent in textual coherence.</p>\n\n",
                "matched_terms": [
                    "coherence",
                    "polarity",
                    "low",
                    "prompt",
                    "semantic",
                    "speaker",
                    "consistent",
                    "sentence",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effect of VQ:</span>\nFig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#S4.F1\" title=\"Figure 1 &#8227; 4.2 Evaluation of Bias &#8227; 4 Results and Discussion\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reveals variations on <span class=\"ltx_text ltx_font_italic\">Sentence Polarity</span>: between Creaky and other VQ for Female with SpiritLM Expr., and between Modal and other VQ for Male with VAE-GSLM; on <span class=\"ltx_text ltx_font_italic\">Agency &amp; Competence</span>: between Modal and other VQ for Female with SpiritLM Base, and between Breathy and other VQ for VAE-GSLM.\nThese appear as isolated outliers in our statistical model, which shows no VQ effect. Yet the low VQ similarity across models suggests this reflects model limitations rather than absence of bias. VQ may emerge as a bias source once models capture it more effectively.</p>\n\n",
                "matched_terms": [
                    "polarity",
                    "agency",
                    "low",
                    "competence",
                    "between",
                    "sentence"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluations reveal that current SC models vary widely in continuation quality and robustness. Once semantic coherence is high enough (for VAE-GSLM), significant gender differences begin to appear, specifically in the <span class=\"ltx_text ltx_font_italic\">Sentence Polarity</span> and <span class=\"ltx_text ltx_font_italic\">Agency &amp; Competence</span> metrics. We find that models disproportionately suppress non-modal phonation in female voices, reflecting documented societal bias regarding how women are expected to sound. This highlights voice-quality bias as a key issue to monitor and mitigate in speech foundation models. Although current systems struggle with preserving speaker identity, rapid progress in large speech models makes it important to treat bias in continuation as a central, not peripheral, evaluation dimension.</p>\n\n",
                "matched_terms": [
                    "coherence",
                    "polarity",
                    "agency",
                    "semantic",
                    "high",
                    "speaker",
                    "evaluation",
                    "competence",
                    "continuation",
                    "dimension",
                    "sentence"
                ]
            }
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Average speaker similarity (ECAPA-TDNN cosine) per model by VQ modification and gender.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_italic\">VAE-GSLM</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_italic\">SpiritLM Expr.</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\"><span class=\"ltx_text ltx_font_bold\">VQ Mod.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Male</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Female</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Male</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\">Female</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Unmod.</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.50 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.57 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.08 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.06</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">0.12 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.09</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Breathy</th>\n<td class=\"ltx_td ltx_align_center\">0.42 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.26</td>\n<td class=\"ltx_td ltx_align_center\">0.49 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m6\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.25</td>\n<td class=\"ltx_td ltx_align_center\">0.08 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m7\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.06</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">0.21 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m8\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.09</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Creaky</th>\n<td class=\"ltx_td ltx_align_center\">0.46 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m9\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.23</td>\n<td class=\"ltx_td ltx_align_center\">0.44 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m10\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.25</td>\n<td class=\"ltx_td ltx_align_center\">0.10 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m11\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.06</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">0.28 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m12\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.08</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">EndCr.</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.51 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m13\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.51 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m14\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.09 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m15\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.06</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\">0.24 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m16\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.08</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "modification",
            "similarity",
            "breathy",
            "ecapatdnn",
            "average",
            "expr",
            "speaker",
            "gender",
            "male",
            "mod",
            "endcr",
            "spiritlm",
            "cosine",
            "creaky",
            "model",
            "vaegslm",
            "±pm",
            "female",
            "unmod"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaker similarity:</span> By contrast with results reported in original papers&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib4\" title=\"\">4</a>]</cite>, SpeechGPT and SpiritLM Base both generated speech with a single speaker identity which was independent from the input prompt, female for SpeechGPT, and male for SpiritLM Base.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#S4.T2\" title=\"Table 2 &#8227; 4.1 Evaluation of Continuation &#8227; 4 Results and Discussion\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reports speaker similarity scores of the SpiritLM Expr. and VAE-GSLM models.\nModel differences to speaker-gender are significant: VAE-GSLM yields higher speaker similarity than SpiritLM Expr., while SpiritLM Expr. itself shows gender-specific variation. Qualitative observations suggest SpiritLM Expr. systematically generates a female-presenting voice that adapts to the prompt (e.g., lowering pitch for male inputs). VAE-GSLM is the only model to fully reproduce distinct speaker identities.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Speech Continuation (SC) is the task of generating a coherent extension of a spoken prompt while preserving both semantic context and speaker identity. Because SC is constrained to a single audio stream, it offers a more direct setting for probing biases in speech foundation models than dialogue does. In this work we present the first systematic evaluation of bias in SC, investigating how gender and phonation type (breathy, creaky, end-creak) affect continuation behaviour. We evaluate three recent models: SpiritLM (base and expressive), VAE-GSLM, and SpeechGPT across speaker similarity, voice quality preservation, and text-based bias metrics. Results show that while both speaker similarity and coherence remain a challenge, textual evaluations reveal significant model and gender interactions: once coherence is sufficiently high (for VAE-GSLM), gender effects emerge on text-metrics such as agency and sentence polarity.\nIn addition, continuations revert toward modal phonation more strongly for female prompts than for male ones, revealing a systematic voice-quality bias.\nThese findings highlight SC as a controlled probe of socially relevant representational biases in speech foundation models, and suggest that it will become an increasingly informative diagnostic as continuation quality improves.\n</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "creaky",
                    "breathy",
                    "model",
                    "vaegslm",
                    "speaker",
                    "gender",
                    "female",
                    "spiritlm",
                    "male"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent advances in large language model (LLM)-based speech generation have introduced the Speech Continuation (SC) task as a new model capability.\nIn this task, the system is provided with a short audio prompt of a speaker and is required to generate a continuation that preserves speaker identity, prosody, and linguistic content <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib1\" title=\"\">1</a>]</cite>.\nThe SC task has been adopted as a benchmark in recent models such as AudioLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib2\" title=\"\">2</a>]</cite>, SpeechGPT-Gen <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib3\" title=\"\">3</a>]</cite>, SpiritLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib4\" title=\"\">4</a>]</cite> and VAE-GSLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib5\" title=\"\">5</a>]</cite>, where it is used to evaluate zero-shot voice preservation and prosodic consistency.\nWhile the evaluation of SC models has largely focused on performance metrics such as speaker similarity, much less is known about the social and representational biases that speech foundation models may exhibit through this task.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "model",
                    "vaegslm",
                    "speaker",
                    "spiritlm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Bias evaluation in speech generation has only recently developed as a research area<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib8\" title=\"\">8</a>]</cite>, following earlier work on bias in speech recognition <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib10\" title=\"\">10</a>]</cite>.\nFor instance, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib6\" title=\"\">6</a>]</cite> introduce a toolkit for assessing semantic gender bias in SpeechLLMs across spoken QA and multiple-choice continuation.\nIn Conversational AI, speech foundation model bias evaluations are complicated by the inherently interactive nature of conversations. Hence, observed bias may be difficult to disentangle from the joint influence of the interlocutor&#8217;s voice and role-based framing effects\n(prompts) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib11\" title=\"\">11</a>]</cite>. Arguing for a more direct way to probe bias in speech models, <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib8\" title=\"\">8</a>]</cite> examine &#8220;default speaker assignment&#8221; in a text-to-audio model: that is, how the model selects a voice when none is specified. They show that for certain prompts &#8211; such as stereotyped professions or gender-associated words &#8211; the model exhibits systematic gendered tendencies in its voice assignments.\nThe SC task provides a similar monologic setting that has the potential to provide a much cleaner probe of representational bias,\nrevealing how a model&#8217;s linguistic and acoustic predictions vary as a function of the speaker it is asked to imitate.\nIn other words, while bias in dialogue speech foundation models shows how someone would have been responded to, the question remains: <em class=\"ltx_emph ltx_font_italic\">&#8220;By whom?&#8221;</em>. SC bias shows what someone with this voice identity would have said according to the model.\nNotably, beyond serving as a benchmark for demonstrating raw model capability, the SC task has so far lacked a clear practical use case. We highlight probing for voice-based bias as a compelling and socially relevant application.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "gender",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To the best of our knowledge, this is the first paper to (1) evaluate bias in voice-conditioned speech continuation models, and (2) systematically vary voice quality (e.g., breathy voice, creaky phonation) &#8211; a socially salient but previously overlooked dimension of variation &#8211; in the evaluation of large speech generation models in general.</p>\n\n",
                "matched_terms": [
                    "breathy",
                    "creaky"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We develop a methodology to probe paralinguistic gender bias via the SC task, where a spoken prompt is extended by the model. We next describe the test data, evaluation dimensions, and experimental protocol, guided by the hypotheses that (i) gender and (ii) voice quality can systematically shape continuation outputs once sufficient coherence is achieved.</p>\n\n",
                "matched_terms": [
                    "gender",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">1. <span class=\"ltx_text ltx_font_bold\">Spoken StereoSet<cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text ltx_font_medium\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib12\" title=\"\">12</a><span class=\"ltx_text ltx_font_medium\">]</span></cite></span>: We select 149 (<math alttext=\"&gt;3\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&gt;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">&gt;3</annotation></semantics></math> s) generations from the gender subset of the Spoken StereoSet (Mean duration: <math alttext=\"5.25\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\"><semantics><mn>5.25</mn><annotation encoding=\"application/x-tex\">5.25</annotation></semantics></math> s), a previously validated multiple choice benchmark derived from the original StereoSet dataset <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib13\" title=\"\">13</a>]</cite> and designed to evaluate social biases in SpeechLLMs. Each instance consists of a spoken context synthesised via TTS, and three candidate textual continuations: one stereotypical, one anti-stereotypical, and one irrelevant. Speaker attributes such as gender are expressed through the speech signal but not the text. To repurpose the benchmark for generative SC, we use the original speech prompt as a continuation prompt. We call the prompts from this set SS<sub class=\"ltx_sub\">set</sub>.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "gender"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to gender, we investigated whether <em class=\"ltx_emph ltx_font_italic\">voice quality</em> (VQ) modulations can influence bias patterns in speech continuation. For this, we used <span class=\"ltx_text ltx_font_bold\">VoiceQualityVC</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib14\" title=\"\">14</a>]</cite>, a recently introduced voice conversion system designed to systematically manipulate phonation types such as breathy and creaky voice, while preserving speaker identity. According to the literature, breathy and creaky phonation serve pragmatic <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib16\" title=\"\">16</a>]</cite> and paralinguistic <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib17\" title=\"\">17</a>]</cite> functions and influence social perception. Creaky voice, especially in female speakers, has been linked to lower competence, education, trustworthiness, and employability <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib18\" title=\"\">18</a>]</cite>, though phrase-final creak appears less marked <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib19\" title=\"\">19</a>]</cite>. In contrast, breathy voice is associated with increased attractiveness and likeability <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib20\" title=\"\">20</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "creaky",
                    "breathy",
                    "speaker",
                    "gender",
                    "female"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speaker Similarity:</span> We assess speaker preservation by computing cosine similarity between ECAPA-TDNN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib21\" title=\"\">21</a>]</cite> embeddings of the continuation and reference prompt.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "similarity",
                    "ecapatdnn",
                    "cosine"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Voice Quality Preservation:</span>\nOne indirect way to measure whether models extract VQ information from the input prompts, is if they demonstrate an\nability to maintain phonation characteristics from the prompt throughout the continuation. To measure and compare VQ between prompts and continuations, we extracted two glottal source parameters, representative of the opening (H1&#8211;H2) and closing (H1&#8211;A3) of the vocal folds, which aids in distinguishing breathy and creaky voice for male speakers <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib15\" title=\"\">15</a>]</cite>.\nWe also included CPPS as a noise parameter to distinguish breathy female voices and capture voice quality in male speakers.</p>\n\n",
                "matched_terms": [
                    "breathy",
                    "female",
                    "male",
                    "creaky"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Textual Evaluation:</span> We evaluate the textual content of the SCs for logical coherence/sentence polarity preservation and for gender bias; see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#S3.T1\" title=\"Table 1 &#8227; 3.2 Procedure &#8227; 3 Experiments\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. To obtain the textual content of the SCs, we use the <span class=\"ltx_text ltx_font_typewriter\">azure-cognitiveservices-speech</span> SDK and perform automatic speech recognition of the SC. Then, we use the <span class=\"ltx_text ltx_font_typewriter\">gemini-2.5-flash-lite-preview-06-17</span> API as an LLM judge and rate the textual content on a scale of 1&#8211;5 on five dimensions, without exposing any knowledge of the input gender from the speech prompt to the API. Previous research has shown LLM judges as being capable of matching crowdsourced human performance on open-ended text evaluation tasks <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib22\" title=\"\">22</a>]</cite>. Our rubrics assess continuation coherence and sentiment preservation, while the other metrics draw on prior bias research to capture gender bias.\n<span class=\"ltx_text ltx_font_bold\">Semantic Coherence and Sentence Polarity:</span> These measure the degree to which the continuation follows logically from the prompt and preserves its intended emotional stance.\n<span class=\"ltx_text ltx_font_bold\">Social Bias Dimensions:</span> We adapt constructs from social psychology (e.g., agency/communality, ambivalent sexism, stereotype content model) and prior works in bias with text (e.g., gendered language, appearance focus) to the setting of first-person SCs, ensuring that they capture empirically documented harms relevant to gender bias&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib26\" title=\"\">26</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib27\" title=\"\">27</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "gender",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated three models with public checkpoints that support voice-conditioned SC: SpiritLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib4\" title=\"\">4</a>]</cite> (in two variants), VAE-GSLM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib5\" title=\"\">5</a>]</cite>, and SpeechGPT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#biba.bib3\" title=\"\">3</a>]</cite>.\n<span class=\"ltx_text ltx_font_bold\">SpiritLM</span> Base is a LLaMA-2&#8211;based model trained on interleaved text and speech tokens; while the expressive (Expr.) variant further conditions on pitch and style tokens to reproduce prosodic cues.\n<span class=\"ltx_text ltx_font_bold\">VAE-GSLM</span> combines discrete semantic tokens with a VAE over continuous speech features, enabling more fine-grained voice preservation.\n<span class=\"ltx_text ltx_font_bold\">SpeechGPT</span> is an 8B-parameter model with a semantic LM and a flow-matching decoder, designed for TTS and dialogue but also supports SC.</p>\n\n",
                "matched_terms": [
                    "expr",
                    "spiritlm",
                    "model",
                    "vaegslm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We design four experimental conditions:\n(1) Baseline Condition: Unmodified speech prompts from SS<sub class=\"ltx_sub\">set</sub> and NOP<sub class=\"ltx_sub\">set</sub>; (2) Breathy voice condition;\n(3) Creaky voice condition;\n(4) End creak condition.\nThe parallel versions, in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#S2.SS2\" title=\"2.2 Voice Quality Manipulations &#8227; 2 Method\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>, were created using VoiceQualityVC with the original speaker of the prompt as the target speaker and the following parameters as conditioning for breathy voice: high H1&#8211;H2 and high H1&#8211;A3 (both <math alttext=\"+3\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo>+</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">+3</annotation></semantics></math> st.d. from the mean) and low creak (<math alttext=\"-2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">-2</annotation></semantics></math> st.d.) and low CPPS (<math alttext=\"-1\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">-1</annotation></semantics></math> st.d.), and the following for creaky voice: high creak (<math alttext=\"+2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mo>+</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">+2</annotation></semantics></math> st.d.), low CPPS (<math alttext=\"-1.5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>1.5</mn></mrow><annotation encoding=\"application/x-tex\">-1.5</annotation></semantics></math> st.d.) and low H1&#8211;H2 and H1&#8211;A3 (<math alttext=\"-2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m6\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">-2</annotation></semantics></math> st.d.). For end creak, the conditioning starts from the midway point of the audio, increasing linearly to: extremely high creak (<math alttext=\"+7\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m7\" intent=\":literal\"><semantics><mrow><mo>+</mo><mn>7</mn></mrow><annotation encoding=\"application/x-tex\">+7</annotation></semantics></math> st.d.), and low H1&#8211;H2, H1&#8211;A3, and CPPS (<math alttext=\"-2\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m8\" intent=\":literal\"><semantics><mrow><mo>&#8722;</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">-2</annotation></semantics></math> st.d.).\nEach model was prompted with 3&#8211;5 s reference audio files from SS<sub class=\"ltx_sub\">set</sub> and NOP<sub class=\"ltx_sub\">set</sub>, and tasked with generating a 5&#8211;8 s continuation that was semantically coherent and preserved the input speaker&#8217;s voice.\nThe impact of voice quality, gender, and model on each metric was investigated using beta regression. Interactions were removed stepwise if ANOVA comparisons showed no significance.</p>\n\n",
                "matched_terms": [
                    "creaky",
                    "breathy",
                    "model",
                    "speaker",
                    "gender"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Continuation:</span> The first criterion is the ability of models to perform continuation, i.e., producing a speech signal as output. We obtained success scores of <math alttext=\"100\\text{\\,}\\mathrm{\\char 37\\relax}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>100</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\" mathvariant=\"normal\">%</mi></mrow><annotation encoding=\"application/x-tex\">100\\text{\\,}\\mathrm{\\char 37\\relax}</annotation></semantics></math> for SpeechGPT, <math alttext=\"100\\text{\\,}\\mathrm{\\char 37\\relax}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mn>100</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\" mathvariant=\"normal\">%</mi></mrow><annotation encoding=\"application/x-tex\">100\\text{\\,}\\mathrm{\\char 37\\relax}</annotation></semantics></math> for SpiritLM Base and Expr., and <math alttext=\"53\\text{\\,}\\mathrm{\\char 37\\relax}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><mrow><mn>53</mn><mtext>&#160;</mtext><mi class=\"ltx_unit\" mathvariant=\"normal\">%</mi></mrow><annotation encoding=\"application/x-tex\">53\\text{\\,}\\mathrm{\\char 37\\relax}</annotation></semantics></math> for VAE-GSLM. As a result, further evaluations are performed on utterances where all models were successful, i.e., <math alttext=\"635\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><mn>635</mn><annotation encoding=\"application/x-tex\">635</annotation></semantics></math> prompts including <math alttext=\"390\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mn>390</mn><annotation encoding=\"application/x-tex\">390</annotation></semantics></math> from SS<sub class=\"ltx_sub\">set</sub> and <math alttext=\"245\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mn>245</mn><annotation encoding=\"application/x-tex\">245</annotation></semantics></math> from NOP<sub class=\"ltx_sub\">set</sub>.</p>\n\n",
                "matched_terms": [
                    "expr",
                    "spiritlm",
                    "vaegslm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Voice Quality Similarity:</span>\nxTo examine how well voice quality was maintained and study voice-quality related bias in the continuations, the H1&#8211;H2, H1&#8211;A3, and CPPS of the prompts and continuations were compared using a Linear Mixed-Effects model with type III ANOVA. Post-hoc pairwise comparisons were performed on the estimated marginal means, with Tukey adjustment for multiple comparisons. In the prompts, female voices showed slightly lower H1&#8211;H2 and H1&#8211;A3 than males (<math alttext=\"p&lt;0.05\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.05</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.05</annotation></semantics></math>), consistent with somewhat creakier phonation. In the continuations, this pattern inverted: female outputs were systematically breathier and less creaky than male ones, particularly after breathy and creaky prompts (<math alttext=\"p&lt;0.001\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.001</annotation></semantics></math>). End-creak prompts behaved as an intermediate case.\nFor CPPS, baseline female prompts were slightly lower than male ones, indicating noisier or creakier voice. In the continuations, the effect reversed: female outputs had higher CPPS (i.e., more modal phonation) than males across modal and breathy prompts (<math alttext=\"p&lt;0.0001\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m3\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.0001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.0001</annotation></semantics></math>). For creaky and end-creak prompts, the pattern depended on model: VAE-GSLM produced higher CPPS for males, whereas SpiritLM Expr. produced higher CPPS for females, strongly reducing creak in female voices (<math alttext=\"p&lt;0.0001\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m4\" intent=\":literal\"><semantics><mrow><mi>p</mi><mo>&lt;</mo><mn>0.0001</mn></mrow><annotation encoding=\"application/x-tex\">p&lt;0.0001</annotation></semantics></math>).</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "creaky",
                    "breathy",
                    "model",
                    "vaegslm",
                    "expr",
                    "female",
                    "spiritlm",
                    "male"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, continuations consistently reverted toward modal phonation, reducing both creakiness and breathiness. This &#8220;regularisation&#8221; was stronger for female prompts, effectively reversing the natural gender difference observed in the inputs. This reflects a voice-quality bias: SC models disproportionately suppress non-modal phonation in female voices.</p>\n\n",
                "matched_terms": [
                    "female",
                    "gender"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Since the absence of speaker similarity does not necessarily imply that voice conditioning has no effect, we proceeded to evaluate bias in the lexical content of the continuations in all 4 models.\nAmong the five metrics presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#S3.T1\" title=\"Table 1 &#8227; 3.2 Procedure &#8227; 3 Experiments\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, we did not observe a significant effect of VQ and gender on <span class=\"ltx_text ltx_font_italic\">Emotionalisation</span> and <span class=\"ltx_text ltx_font_italic\">Appearance</span>, and the only effect of model demonstrated a small improvement of <span class=\"ltx_text ltx_font_italic\">Emotionalisation</span> by SpeechGPT compared to both variants of SpiritLM. Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#S4.F1\" title=\"Figure 1 &#8227; 4.2 Evaluation of Bias &#8227; 4 Results and Discussion\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reports mean scores and standard deviations for <span class=\"ltx_text ltx_font_italic\">Semantic Coherence</span>, <span class=\"ltx_text ltx_font_italic\">Sentence Polarity</span> and <span class=\"ltx_text ltx_font_italic\">Agency &amp; Competence</span>. Statistical tests reveal an interaction between model and gender for all metrics, but no impact of VQ. Therefore, all further comparisons are made while considering all VQ conditions together (last row of each subfigure).</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "model",
                    "speaker",
                    "gender",
                    "spiritlm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effect of Model:</span>\nAmong the <math alttext=\"36\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m1\" intent=\":literal\"><semantics><mn>36</mn><annotation encoding=\"application/x-tex\">36</annotation></semantics></math> pairs of model comparisons made for the two genders and three metrics, <math alttext=\"31\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m2\" intent=\":literal\"><semantics><mn>31</mn><annotation encoding=\"application/x-tex\">31</annotation></semantics></math> were significant, demonstrating a clear distinction in generation quality among the models.\nSpiritLM Base, SpiritLM Expr., and VAE-GSLM consistently produce text with reasonable <span class=\"ltx_text ltx_font_italic\">Semantic Coherence</span>, with mean scores generally above <math alttext=\"2.4\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m3\" intent=\":literal\"><semantics><mn>2.4</mn><annotation encoding=\"application/x-tex\">2.4</annotation></semantics></math>. SpeechGPT&#8217;s outputs are of markedly lower quality, with its highest <span class=\"ltx_text ltx_font_italic\">Semantic Coherence</span> score being just <math alttext=\"1.45\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m4\" intent=\":literal\"><semantics><mn>1.45</mn><annotation encoding=\"application/x-tex\">1.45</annotation></semantics></math>. Its mean score for a breathy female voice prompt was particularly low at <math alttext=\"1.37\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p2.m5\" intent=\":literal\"><semantics><mn>1.37</mn><annotation encoding=\"application/x-tex\">1.37</annotation></semantics></math>.\nA similar trend is observed for the two other metrics. SpiritLM Base provides the highest scores on all metrics, with the exception of VAE-GSLM on males outperforming other models on <span class=\"ltx_text ltx_font_italic\">Sentence Polarity</span>.\nInterestingly, while VAE-GSLM performs best on audio features (speaker and VQ similarity), SpiritLM is more consistent in textual coherence.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "breathy",
                    "model",
                    "vaegslm",
                    "expr",
                    "speaker",
                    "female",
                    "spiritlm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effect of VQ:</span>\nFig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#S4.F1\" title=\"Figure 1 &#8227; 4.2 Evaluation of Bias &#8227; 4 Results and Discussion\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> reveals variations on <span class=\"ltx_text ltx_font_italic\">Sentence Polarity</span>: between Creaky and other VQ for Female with SpiritLM Expr., and between Modal and other VQ for Male with VAE-GSLM; on <span class=\"ltx_text ltx_font_italic\">Agency &amp; Competence</span>: between Modal and other VQ for Female with SpiritLM Base, and between Breathy and other VQ for VAE-GSLM.\nThese appear as isolated outliers in our statistical model, which shows no VQ effect. Yet the low VQ similarity across models suggests this reflects model limitations rather than absence of bias. VQ may emerge as a bias source once models capture it more effectively.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "creaky",
                    "breathy",
                    "model",
                    "vaegslm",
                    "expr",
                    "female",
                    "spiritlm",
                    "male"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effect of Gender:</span>\nWe observe systematic gender effects across all three metrics with VAE-GSLM only, as displayed by the black rectangles on Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.22061v1#S4.F1\" title=\"Figure 1 &#8227; 4.2 Evaluation of Bias &#8227; 4 Results and Discussion\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nThis supports our hypothesis that SC models can exhibit voice-driven gender bias. Notably, such effects appear only in the model capable of reproducing speaker voices with reasonable similarity.</p>\n\n",
                "matched_terms": [
                    "similarity",
                    "model",
                    "vaegslm",
                    "speaker",
                    "gender"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our evaluations reveal that current SC models vary widely in continuation quality and robustness. Once semantic coherence is high enough (for VAE-GSLM), significant gender differences begin to appear, specifically in the <span class=\"ltx_text ltx_font_italic\">Sentence Polarity</span> and <span class=\"ltx_text ltx_font_italic\">Agency &amp; Competence</span> metrics. We find that models disproportionately suppress non-modal phonation in female voices, reflecting documented societal bias regarding how women are expected to sound. This highlights voice-quality bias as a key issue to monitor and mitigate in speech foundation models. Although current systems struggle with preserving speaker identity, rapid progress in large speech models makes it important to treat bias in continuation as a central, not peripheral, evaluation dimension.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "gender",
                    "vaegslm",
                    "female"
                ]
            }
        ]
    }
}