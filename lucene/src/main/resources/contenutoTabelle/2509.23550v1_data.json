{
    "S3.T1": {
        "source_file": "Automatic Speech Recognition for Greek Medical Dictation",
        "caption": "Table 1: Trainable Parameters Comparison of Whisper Models Using LoRA",
        "body": "Model\nParams (B)\nTrainable Params (M)\n\n\n\n\nWhisper Small\n0.249\n\n∼\\sim7.1 (2.8%)\n\n\nWhisper Medium\n0.783\n\n∼\\sim18.9 (2.4%)\n\n\nWhisper Large-v2\n1.574\n\n∼\\sim31.5 (2%)",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Params (B)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Trainable Params (M)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Whisper Small</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.249</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>7.1 (2.8%)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Whisper Medium</th>\n<td class=\"ltx_td ltx_align_center\">0.783</td>\n<td class=\"ltx_td ltx_align_center\">\n<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>18.9 (2.4%)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\">Whisper Large-v2</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">1.574</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">\n<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m3\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>31.5 (2%)</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "largev2",
            "whisper",
            "model",
            "medium",
            "∼sim315",
            "params",
            "models",
            "small",
            "lora",
            "parameters",
            "∼sim189",
            "∼sim71",
            "trainable",
            "comparison"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">We develop a system that combines state-of-the-art automatic speech recognition techniques with domain adapted language model for Greek medical dictation. Our ASR component uses the pre-trained Whisper model <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib13\" title=\"\">2023</a>)</cite>, adapted to Greek. Additionaly, a Greek version of GPT-2 <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib14\" title=\"\">2019</a>)</cite> language model, fine-tuned with Greek medical text, is used to evaluate and select the best transcription hypothesis from the multiple candidates generated by the ASR. This approach integrates both acoustic and linguistic information to improve transcription accuracy and and better handle specialized medical terminology.</p>\n\n",
                "matched_terms": [
                    "model",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fine-tuning pretrained models to specific domains is essential for improving performance without requiring massive datasets or training from scratch. Transfer learning allows models trained on general speech or text corpora to adapt to specialized tasks, such as medical dictation, by learning domain-specific terminology and phrasing. Parameter-efficient methods, such as Low-Rank Adaptation (LoRA)<cite class=\"ltx_cite ltx_citemacro_cite\">Hu et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib8\" title=\"\">2021</a>)</cite>, enable large models to be adapted with minimal computational overhead. Instead of updating all parameters, LoRA introduces small learnable matrices that approximate the necessary weight updates while keeping the original model frozen. In this work, we use LoRA to fine-tune each model, allowing the system to handle specialized terminology and domain-specific phrasing, ultimately improving transcription accuracy and reliability.</p>\n\n",
                "matched_terms": [
                    "model",
                    "models",
                    "small",
                    "parameters",
                    "lora"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents the design and implementation of the Automatic Speech Recognition (ASR) system for Greek medical dictation. The core of this work involves adapting OpenAI&#8217;s Whisper model to the specific features of the Greek language through a controlled fine-tuning process. A key aspect of our methodology is the comparative analysis of three different sizes of the Whisper model, small, medium, and large-v2. To further enhance transcription quality, we integrated a re-ranking mechanism based on a fine-tuned GPT-2 model, which was used to select the most contextually appropriate transcription among Whisper&#8217;s alternatives.</p>\n\n",
                "matched_terms": [
                    "largev2",
                    "model",
                    "medium",
                    "small",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The foundation of our system is the Whisper model <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib13\" title=\"\">2023</a>)</cite>, a state-of-the-art ASR model using an encoder-decoder architecture. Whisper has been trained on a large variety of labeled speech data, enabling robust performance across multiple languages and noisy environments. The model converts raw audio into log-Mel spectrogram features, which are processed by convolutional layers and transformer encoder blocks, and decoded into text using cross-attention and a transformer decoder <cite class=\"ltx_cite ltx_citemacro_cite\">Vaswani et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib18\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We examine three pre-trained versions of Whisper: small, medium, and large-v2. The primary motive for training three distinct models was to analyze the trade-off between performance and computational cost. While larger models like large-v2 are expected to have higher accuracy due to their increased parameters and greater representational capacity, they are also more computationally expensive and harder to deploy in resource-constrained environments. By fine-tuning and evaluating all three models we can determine the optimal one that meets our desired accuracy benchmarks while being practical and efficient for real-world medical dictation.</p>\n\n",
                "matched_terms": [
                    "largev2",
                    "medium",
                    "models",
                    "small",
                    "parameters",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To adapt Whisper model to the Greek language we applied a fine-tuning methodology to each of the three models. A diverse set of different Greek speech datasets was aggregate to create a robust corpus of data, containing speech data with different domains, with varying acoustic environments, and from multiple speakers. The need for such diversity is motivated by the challenges posed by Greek dialectal variation <cite class=\"ltx_cite ltx_citemacro_cite\">Vakirtzian et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib17\" title=\"\">2024</a>)</cite>. To make fine-tuning more efficient, Low-Rank Adaptation (LoRA) <cite class=\"ltx_cite ltx_citemacro_cite\">Hu et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib8\" title=\"\">2021</a>)</cite> was applied to reduce the number of trainable parameters. In this way, only about <math alttext=\"2\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">2\\%</annotation></semantics></math> of the full model&#8217;s trainable parameters are trainable, allowing efficient fine-tuning under limited resources.</p>\n\n",
                "matched_terms": [
                    "whisper",
                    "model",
                    "models",
                    "parameters",
                    "trainable",
                    "lora"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To improve transcription quality, the Greek GPT-2 model <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib14\" title=\"\">2019</a>)</cite> was used to re-rank N-best hypotheses generated by Whisper. GPT-2 uses a unidirectional transformer decoder with masked multi-head self-attention to model dependencies from left to right <cite class=\"ltx_cite ltx_citemacro_cite\">Vaswani et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib18\" title=\"\">2017</a>)</cite>. Our variant, fine-tuned for Greek text, consists of 12 decoder layers, each with 12 attention heads and a hidden size of 768. This model was further fine-tuned on a domain-specific medical text corpus using LoRA <cite class=\"ltx_cite ltx_citemacro_cite\">Hu et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib8\" title=\"\">2021</a>)</cite>. Attention and projection layers were adapted with low-rank matrices, updating only 1.29% of the total parameters. The fine-tuned model was evaluated using perplexity, showing improved confidence in next-token predictions and better alignment with medical terminology.</p>\n\n",
                "matched_terms": [
                    "lora",
                    "model",
                    "whisper",
                    "parameters"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our final pipeline integrates both Whisper and Greek GPT-2 (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#S3.F1\" title=\"Figure 1 &#8227; 3 System Design &#8227; Automatic Speech Recognition for Greek Medical Dictation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Whisper generates a set of N candidate hypotheses, representing possible transcriptions. It converts raw audio waveforms into text using its pre-trained encoder-decoder architecture and produces multiple transcription variants to capture potential ambiguities in the audio. The language model evaluates the N candidate hypotheses. Each candidate is scored based on grammatical correctness, contextual relevance, and semantic coherence. Through this re-ranking process, the most accurate candidate is selected, ensuring that the final transcription aligns well with Greek language conventions. A critical design choice is selecting the optimal value of N, balancing computational load and the diversity of transcription options, enabling GPT-2 to effectively re-rank and select the best candidate. The optimal value of N was determined through empirical testing, ensuring robust performance for Greek audio inputs.</p>\n\n",
                "matched_terms": [
                    "model",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Whisper, we fine-tuned the model on a composite dataset of Greek speech audio paired with transcriptions. This dataset combined three publicly available sources to ensure a variety of speakers, accents, and acoustic conditions. The Mosel dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Gaido et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib7\" title=\"\">2024</a>)</cite> contains European Parliament recordings in Greek. Some recordings, however, had missing timestamps or misaligned segments. To address this, we curated a filtered dataset of well-aligned audio-transcription pairs for model training <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/datasets/Vardis/Greek_Mosel\" title=\"\">https://huggingface.co/datasets/Vardis/Greek_Mosel</a></span></span></span>. Mozilla Common Voice 11.0 <cite class=\"ltx_cite ltx_citemacro_cite\">Ardila et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib3\" title=\"\">2020</a>)</cite> contributed crowd-sourced recordings from volunteers across different accents and environments. Google FLEURS <cite class=\"ltx_cite ltx_citemacro_cite\">Conneau et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib6\" title=\"\">2022</a>)</cite> provided read-aloud sentences from multiple domains, complementing the dataset. The final dataset contains approximately 49 hours of speech, covering diverse domains, speaker variability, and acoustic conditions, enabling robust model fine-tuning.</p>\n\n",
                "matched_terms": [
                    "model",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Greek GPT-2 model was trained on a custom medical text dataset containing 20,430 samples from multiple sources <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/datasets/Vardis/Greek_Medical_Text\" title=\"\">https://huggingface.co/datasets/Vardis/Greek_Medical_Text</a></span></span></span>. Medical e-books provided detailed clinical terminology covering diagnostics, procedures, and patient care <cite class=\"ltx_cite ltx_citemacro_cite\">Iatrakis (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib9\" title=\"\">2015</a>); Sfikakis et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib15\" title=\"\">2015</a>); Tsipouras et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib16\" title=\"\">2015</a>)</cite>. The QTLP Greek CC Corpus for the medical domain <cite class=\"ltx_cite ltx_citemacro_cite\">ath (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib1\" title=\"\">2015</a>)</cite> added web documents automatically classified as medical, including reference materials, news, discussions, and other genres. Dialogues from medical podcasts, collected from <span class=\"ltx_text ltx_font_typewriter\">istorima.org</span>, introduced conversational medical language, enriching the dataset with contextual and informal expressions. This corpus allows GPT-2 to rank candidate sentences produced by Whisper based on perplexity, improving transcription selection and alignment with domain-specific terminology.</p>\n\n",
                "matched_terms": [
                    "model",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated Whisper and GPT-2, focusing on the final Whisper-GPT-2 pipeline. Each model was evaluated on its respective test dataset to assess its effectiveness in handling Greek medical speech tasks. The Whisper model was fine-tuned in three configurations (Small, Medium, Large-v2) on a composite dataset comprising Greek speech data <cite class=\"ltx_cite ltx_citemacro_cite\">Gaido et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib7\" title=\"\">2024</a>); Ardila et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib3\" title=\"\">2020</a>); Conneau et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib6\" title=\"\">2022</a>)</cite>. The results are summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#S5.T3\" title=\"Table 3 &#8227; 5 Evaluation and Results &#8227; Automatic Speech Recognition for Greek Medical Dictation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, reporting the Word Error Rate (WER), normalized Word Error Rate (nWER), Character Error Rate (CER), and BLEU <cite class=\"ltx_cite ltx_citemacro_cite\">Papineni et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib12\" title=\"\">2002</a>)</cite>. Fine-tuning improved performance across all model sizes. Whisper Small reduced WER from 43.62% to 30.31%, Whisper Medium from 34.71% to 19.45%, and Whisper Large-v2 from 26.41% to 14.90%. Similar gains were observed in CER, confirming the effectiveness of fine-tuning, while nWER, which disregards differences in case, punctuation, and spacing, followed the same trends, further demonstrating Whisper&#8217;s improved adaptation to Greek.</p>\n\n",
                "matched_terms": [
                    "largev2",
                    "model",
                    "medium",
                    "small",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Whisper-GPT-2 pipeline was evaluated on the test dataset using WER, nWER, CER, and BLEU metrics, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#S5.T3\" title=\"Table 3 &#8227; 5 Evaluation and Results &#8227; Automatic Speech Recognition for Greek Medical Dictation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The pipeline incorporates a re-ranking step in which the Whisper model first generates <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p3.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> candidate transcriptions for each audio segment. Subsequently, the fine-tuned GPT-2 model evaluates these candidates and selects the most probable sentence. Experiments with <math alttext=\"N=3,5,8\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p3.m2\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mrow><mn>3</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>8</mn></mrow></mrow><annotation encoding=\"application/x-tex\">N=3,5,8</annotation></semantics></math> showed that <math alttext=\"N=5\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p3.m3\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">N=5</annotation></semantics></math> provides the best balance between transcription quality and computational efficiency. Increasing <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p3.m4\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> gave only marginal gains but significantly increased computation time, while smaller <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p3.m5\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> values limited GPT-2&#8217;s re-ranking capabilities.</p>\n\n",
                "matched_terms": [
                    "model",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Re-ranking consistently improves performance across all tested Whisper model sizes. The WER reduction is approximately 9.66% for the Whisper Small, 6.27% for the Whisper Medium, and 1.41% for the Whisper Large-v2. CER and BLEU scores show corresponding gains, highlighting that re-ranking enhances both word-level accuracy and overall sentence quality. Compared to the original models, the full pipeline achieves WER reductions of 37.23% for Small, 47.45% for Medium, and 44.38% for Large-v2.</p>\n\n",
                "matched_terms": [
                    "largev2",
                    "model",
                    "medium",
                    "models",
                    "small",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Choosing the optimal model depends on deployment constraints. While Whisper Large-v2 achieves the lowest WER, it requires significantly more computational resources, making it less practical for routine deployment. Whisper Medium offers strong performance with lower WER and CER while being faster, making it more practical for real-world applications.</p>\n\n",
                "matched_terms": [
                    "largev2",
                    "model",
                    "whisper",
                    "medium"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings highlight the effectiveness of combining a strong pre-trained ASR model with a domain-adapted language model. The re-ranking stage is very important especially for medical dictation, where even small improvements in transcription accuracy can be vital for understanding specialized terminology and avoiding critical misinterpretations. Overall, this demonstrates that the Whisper-GPT-2 pipeline is an effective approach for improving transcription accuracy and producing higher quality outputs in Greek medical dictation.</p>\n\n",
                "matched_terms": [
                    "small",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper presented an Automatic Speech Recognition (ASR) pipeline tailored to Greek medical dictation, integrating fine-tuned Whisper models with a domain-adapted Greek GPT-2 language model for re-ranking. Through fine-tuning, we achieved substantial reductions in Word Error Rate (WER) and Character Error Rate (CER), while the re-ranking step provided consistent gains across all model sizes. A key contribution of this work is the curation of a high-quality Greek speech-to-text dataset, which addresses issues in existing resources and enables reproducibility.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "whisper"
                ]
            }
        ]
    },
    "S5.T2": {
        "source_file": "Automatic Speech Recognition for Greek Medical Dictation",
        "caption": "Table 2: Perplexity of Greek GPT-2 (Pre-trained vs Fine-tuned) on Medical and Speech Datasets",
        "body": "Dataset\nPre-trained\nFine-tuned\n\nΔ\\Delta (%)\n\n\n\n\n\nMedical Texts\n45.73\n35.36\n22.7\n\n\nSpeech Transcriptions\n103.21\n67.67\n34.4\n\n\nCombined (All Data)\n53.15\n39.86\n25.0",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Pre-trained</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Fine-tuned</span></th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_t\">\n<math alttext=\"\\Delta\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#916;</mi><annotation encoding=\"application/x-tex\">\\Delta</annotation></semantics></math><span class=\"ltx_text ltx_font_bold\"> (%)</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t\">Medical Texts</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">45.73</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">35.36</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\">22.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row\">Speech Transcriptions</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\">103.21</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\">67.67</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">34.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_b\">Combined (All Data)</th>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_b\">53.15</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_b\">39.86</td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_b\">25.0</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "medical",
            "speech",
            "texts",
            "combined",
            "finetuned",
            "all",
            "dataset",
            "gpt2",
            "transcriptions",
            "datasets",
            "pretrained",
            "data",
            "greek",
            "δdelta",
            "perplexity"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The Greek GPT-2 model was fine-tuned on a domain-specific corpus designed to adapt the model to the medical context. The training set combined medical texts and transcribed speech data. This hybrid approach allowed the model to learn not only specialized vocabulary but also the stylistic and syntactic patterns typical of oral communication, which are highly relevant for correcting ASR outputs. The evaluation was conducted using perplexity on both medical text , speech transcription, and their combination. Across all three evaluation settings, the fine-tuned model consistently outperformed the original pre-trained Greek GPT-2. More specifically, perplexity was substantially reduced on both medical texts and speech data, confirming that the model successfully learned domain-specific terminology as well as the idiomatic patterns of spoken language. The combined results further highlight the overall effectiveness of the fine-tuning strategy (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#S5.T2\" title=\"Table 2 &#8227; 5 Evaluation and Results &#8227; Automatic Speech Recognition for Greek Medical Dictation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>).</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Medical dictation systems are essential tools in modern healthcare, enabling accurate and efficient conversion of speech into written medical documentation. The main objective of this paper is to create a domain-specific system for Greek medical speech transcriptions. The ultimate goal is to assist healthcare professionals by reducing the overload of manual documentation and improving workflow efficiency. Towards this goal, we develop a system that combines automatic speech recognition techniques with text correction model, allowing better handling of domain-specific terminology and linguistic variations in Greek. Our approach leverages both acoustic and textual modeling to create more realistic and reliable transcriptions. We focused on adapting existing language and speech technologies to the Greek medical context, addressing challenges such as complex medical terminology and linguistic inconsistencies. Through domain-specific fine-tuning, our system achieves more accurate and coherent transcriptions, contributing to the development of practical language technologies for the Greek healthcare sector.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "greek",
                    "transcriptions",
                    "medical"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">Automatic Speech Recognition for Greek Medical Dictation</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "greek",
                    "medical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Medical dictation plays a pivotal role in modern healthcare workflows <cite class=\"ltx_cite ltx_citemacro_cite\">Al Hadidi et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib2\" title=\"\">2017</a>)</cite>. Precise and timely documentation of medical information enhances diagnostic accuracy, ensures continuity of care, and supports legal protection. Traditional documentation is time-consuming for healthcare professionals, requiring them to spend more time in each case. Dictation systems provide a fast and natural alternative to manual data entry, reducing documentation workload and improving healthcare professionals efficiency.</p>\n\n",
                "matched_terms": [
                    "data",
                    "medical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Greek medical dictation is under-resourced compared to English systems. Existing speech recognition systems perform poorly in Greek medical domain due to complex domain-specific terminology, the linguistic characteristics of the Greek language and possibly due to variations in individual speech patterns.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "greek",
                    "medical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We develop a system that combines state-of-the-art automatic speech recognition techniques with domain adapted language model for Greek medical dictation. Our ASR component uses the pre-trained Whisper model <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib13\" title=\"\">2023</a>)</cite>, adapted to Greek. Additionaly, a Greek version of GPT-2 <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib14\" title=\"\">2019</a>)</cite> language model, fine-tuned with Greek medical text, is used to evaluate and select the best transcription hypothesis from the multiple candidates generated by the ASR. This approach integrates both acoustic and linguistic information to improve transcription accuracy and and better handle specialized medical terminology.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "finetuned",
                    "gpt2",
                    "pretrained",
                    "greek",
                    "medical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Modern speech recognition systems tend to combine acoustic, pronunciation, and language models into a single unified network that can be trained end-to-end. This approach simplifies the pipeline, reduces errors, and improves accuracy. Recent advances in self-supervised learning allow models to be pre-trained on large amounts of unlabeled speech and then fine-tuned on smaller, specialized datasets <cite class=\"ltx_cite ltx_citemacro_cite\">Baevski et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib4\" title=\"\">2020</a>)</cite>, which is especially important for low-resource languages such as Greek.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "finetuned",
                    "datasets",
                    "pretrained",
                    "greek"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Medical speech introduces unique challenges, including specialized terminology, abbreviations, complex sentence structures, and context-specific phrases. Clinical environments often involve background noise, multiple speakers, and time constrained communication. General-purpose ASR systems frequently fail in this setting, and the problem is compounded in low-resource languages. Accurate transcription is critical for proper clinical documentation, workflow efficiency, and reducing the risk of medical errors <cite class=\"ltx_cite ltx_citemacro_cite\">Ng et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib11\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "medical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Language Models play a crucial role in the post-processing of texts generated by automatic speech recognition systems. After the initial conversion from speech to text, the resulting text often contains errors, omissions or inconsistencies. These models help with the correction of those errors, resulting in a more coherent and well-structured transcription. Transformer-based models, like GPT-2 <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib14\" title=\"\">2019</a>)</cite>, have the capability to understand the context of each sentence and provide improvements that make the text more natural and understandable. Domain-specific LMs can be trained using task-relevant text to expand available resources, which has been shown to reduce perplexity and improve transcription quality <cite class=\"ltx_cite ltx_citemacro_cite\">Jha (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib10\" title=\"\">2021</a>)</cite>. By training these models on large volumes of text, they can identify incorrect sentences or unusual expressions, and improve the flow and clarity. Using them for post-processing is especially important in this task, where accurate transcription and clear understanding of texts are critical.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "texts",
                    "perplexity",
                    "gpt2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fine-tuning pretrained models to specific domains is essential for improving performance without requiring massive datasets or training from scratch. Transfer learning allows models trained on general speech or text corpora to adapt to specialized tasks, such as medical dictation, by learning domain-specific terminology and phrasing. Parameter-efficient methods, such as Low-Rank Adaptation (LoRA)<cite class=\"ltx_cite ltx_citemacro_cite\">Hu et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib8\" title=\"\">2021</a>)</cite>, enable large models to be adapted with minimal computational overhead. Instead of updating all parameters, LoRA introduces small learnable matrices that approximate the necessary weight updates while keeping the original model frozen. In this work, we use LoRA to fine-tune each model, allowing the system to handle specialized terminology and domain-specific phrasing, ultimately improving transcription accuracy and reliability.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "all",
                    "datasets",
                    "pretrained",
                    "medical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents the design and implementation of the Automatic Speech Recognition (ASR) system for Greek medical dictation. The core of this work involves adapting OpenAI&#8217;s Whisper model to the specific features of the Greek language through a controlled fine-tuning process. A key aspect of our methodology is the comparative analysis of three different sizes of the Whisper model, small, medium, and large-v2. To further enhance transcription quality, we integrated a re-ranking mechanism based on a fine-tuned GPT-2 model, which was used to select the most contextually appropriate transcription among Whisper&#8217;s alternatives.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "finetuned",
                    "gpt2",
                    "greek",
                    "medical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The foundation of our system is the Whisper model <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib13\" title=\"\">2023</a>)</cite>, a state-of-the-art ASR model using an encoder-decoder architecture. Whisper has been trained on a large variety of labeled speech data, enabling robust performance across multiple languages and noisy environments. The model converts raw audio into log-Mel spectrogram features, which are processed by convolutional layers and transformer encoder blocks, and decoded into text using cross-attention and a transformer decoder <cite class=\"ltx_cite ltx_citemacro_cite\">Vaswani et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib18\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We examine three pre-trained versions of Whisper: small, medium, and large-v2. The primary motive for training three distinct models was to analyze the trade-off between performance and computational cost. While larger models like large-v2 are expected to have higher accuracy due to their increased parameters and greater representational capacity, they are also more computationally expensive and harder to deploy in resource-constrained environments. By fine-tuning and evaluating all three models we can determine the optimal one that meets our desired accuracy benchmarks while being practical and efficient for real-world medical dictation.</p>\n\n",
                "matched_terms": [
                    "pretrained",
                    "all",
                    "medical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To adapt Whisper model to the Greek language we applied a fine-tuning methodology to each of the three models. A diverse set of different Greek speech datasets was aggregate to create a robust corpus of data, containing speech data with different domains, with varying acoustic environments, and from multiple speakers. The need for such diversity is motivated by the challenges posed by Greek dialectal variation <cite class=\"ltx_cite ltx_citemacro_cite\">Vakirtzian et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib17\" title=\"\">2024</a>)</cite>. To make fine-tuning more efficient, Low-Rank Adaptation (LoRA) <cite class=\"ltx_cite ltx_citemacro_cite\">Hu et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib8\" title=\"\">2021</a>)</cite> was applied to reduce the number of trainable parameters. In this way, only about <math alttext=\"2\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">2\\%</annotation></semantics></math> of the full model&#8217;s trainable parameters are trainable, allowing efficient fine-tuning under limited resources.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data",
                    "greek",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To improve transcription quality, the Greek GPT-2 model <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib14\" title=\"\">2019</a>)</cite> was used to re-rank N-best hypotheses generated by Whisper. GPT-2 uses a unidirectional transformer decoder with masked multi-head self-attention to model dependencies from left to right <cite class=\"ltx_cite ltx_citemacro_cite\">Vaswani et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib18\" title=\"\">2017</a>)</cite>. Our variant, fine-tuned for Greek text, consists of 12 decoder layers, each with 12 attention heads and a hidden size of 768. This model was further fine-tuned on a domain-specific medical text corpus using LoRA <cite class=\"ltx_cite ltx_citemacro_cite\">Hu et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib8\" title=\"\">2021</a>)</cite>. Attention and projection layers were adapted with low-rank matrices, updating only 1.29% of the total parameters. The fine-tuned model was evaluated using perplexity, showing improved confidence in next-token predictions and better alignment with medical terminology.</p>\n\n",
                "matched_terms": [
                    "medical",
                    "finetuned",
                    "gpt2",
                    "greek",
                    "perplexity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our final pipeline integrates both Whisper and Greek GPT-2 (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#S3.F1\" title=\"Figure 1 &#8227; 3 System Design &#8227; Automatic Speech Recognition for Greek Medical Dictation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Whisper generates a set of N candidate hypotheses, representing possible transcriptions. It converts raw audio waveforms into text using its pre-trained encoder-decoder architecture and produces multiple transcription variants to capture potential ambiguities in the audio. The language model evaluates the N candidate hypotheses. Each candidate is scored based on grammatical correctness, contextual relevance, and semantic coherence. Through this re-ranking process, the most accurate candidate is selected, ensuring that the final transcription aligns well with Greek language conventions. A critical design choice is selecting the optimal value of N, balancing computational load and the diversity of transcription options, enabling GPT-2 to effectively re-rank and select the best candidate. The optimal value of N was determined through empirical testing, ensuring robust performance for Greek audio inputs.</p>\n\n",
                "matched_terms": [
                    "pretrained",
                    "greek",
                    "transcriptions",
                    "gpt2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work uses several Greek audio datasets used to fine-tune Whisper and a medical text corpus for adapting the Greek GPT-2. Each dataset was assembled with the needs of its respective task in mind, aiming to support effective training, reliable evaluation, and accurate representation of Greek linguistic features.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "gpt2",
                    "datasets",
                    "greek",
                    "medical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Whisper, we fine-tuned the model on a composite dataset of Greek speech audio paired with transcriptions. This dataset combined three publicly available sources to ensure a variety of speakers, accents, and acoustic conditions. The Mosel dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Gaido et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib7\" title=\"\">2024</a>)</cite> contains European Parliament recordings in Greek. Some recordings, however, had missing timestamps or misaligned segments. To address this, we curated a filtered dataset of well-aligned audio-transcription pairs for model training <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/datasets/Vardis/Greek_Mosel\" title=\"\">https://huggingface.co/datasets/Vardis/Greek_Mosel</a></span></span></span>. Mozilla Common Voice 11.0 <cite class=\"ltx_cite ltx_citemacro_cite\">Ardila et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib3\" title=\"\">2020</a>)</cite> contributed crowd-sourced recordings from volunteers across different accents and environments. Google FLEURS <cite class=\"ltx_cite ltx_citemacro_cite\">Conneau et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib6\" title=\"\">2022</a>)</cite> provided read-aloud sentences from multiple domains, complementing the dataset. The final dataset contains approximately 49 hours of speech, covering diverse domains, speaker variability, and acoustic conditions, enabling robust model fine-tuning.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "combined",
                    "finetuned",
                    "dataset",
                    "transcriptions",
                    "greek"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Greek GPT-2 model was trained on a custom medical text dataset containing 20,430 samples from multiple sources <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/datasets/Vardis/Greek_Medical_Text\" title=\"\">https://huggingface.co/datasets/Vardis/Greek_Medical_Text</a></span></span></span>. Medical e-books provided detailed clinical terminology covering diagnostics, procedures, and patient care <cite class=\"ltx_cite ltx_citemacro_cite\">Iatrakis (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib9\" title=\"\">2015</a>); Sfikakis et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib15\" title=\"\">2015</a>); Tsipouras et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib16\" title=\"\">2015</a>)</cite>. The QTLP Greek CC Corpus for the medical domain <cite class=\"ltx_cite ltx_citemacro_cite\">ath (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib1\" title=\"\">2015</a>)</cite> added web documents automatically classified as medical, including reference materials, news, discussions, and other genres. Dialogues from medical podcasts, collected from <span class=\"ltx_text ltx_font_typewriter\">istorima.org</span>, introduced conversational medical language, enriching the dataset with contextual and informal expressions. This corpus allows GPT-2 to rank candidate sentences produced by Whisper based on perplexity, improving transcription selection and alignment with domain-specific terminology.</p>\n\n",
                "matched_terms": [
                    "medical",
                    "dataset",
                    "gpt2",
                    "greek",
                    "perplexity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluated Whisper and GPT-2, focusing on the final Whisper-GPT-2 pipeline. Each model was evaluated on its respective test dataset to assess its effectiveness in handling Greek medical speech tasks. The Whisper model was fine-tuned in three configurations (Small, Medium, Large-v2) on a composite dataset comprising Greek speech data <cite class=\"ltx_cite ltx_citemacro_cite\">Gaido et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib7\" title=\"\">2024</a>); Ardila et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib3\" title=\"\">2020</a>); Conneau et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib6\" title=\"\">2022</a>)</cite>. The results are summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#S5.T3\" title=\"Table 3 &#8227; 5 Evaluation and Results &#8227; Automatic Speech Recognition for Greek Medical Dictation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, reporting the Word Error Rate (WER), normalized Word Error Rate (nWER), Character Error Rate (CER), and BLEU <cite class=\"ltx_cite ltx_citemacro_cite\">Papineni et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib12\" title=\"\">2002</a>)</cite>. Fine-tuning improved performance across all model sizes. Whisper Small reduced WER from 43.62% to 30.31%, Whisper Medium from 34.71% to 19.45%, and Whisper Large-v2 from 26.41% to 14.90%. Similar gains were observed in CER, confirming the effectiveness of fine-tuning, while nWER, which disregards differences in case, punctuation, and spacing, followed the same trends, further demonstrating Whisper&#8217;s improved adaptation to Greek.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "finetuned",
                    "all",
                    "dataset",
                    "gpt2",
                    "data",
                    "greek",
                    "medical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Whisper-GPT-2 pipeline was evaluated on the test dataset using WER, nWER, CER, and BLEU metrics, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#S5.T3\" title=\"Table 3 &#8227; 5 Evaluation and Results &#8227; Automatic Speech Recognition for Greek Medical Dictation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The pipeline incorporates a re-ranking step in which the Whisper model first generates <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p3.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> candidate transcriptions for each audio segment. Subsequently, the fine-tuned GPT-2 model evaluates these candidates and selects the most probable sentence. Experiments with <math alttext=\"N=3,5,8\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p3.m2\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mrow><mn>3</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>8</mn></mrow></mrow><annotation encoding=\"application/x-tex\">N=3,5,8</annotation></semantics></math> showed that <math alttext=\"N=5\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p3.m3\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">N=5</annotation></semantics></math> provides the best balance between transcription quality and computational efficiency. Increasing <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p3.m4\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> gave only marginal gains but significantly increased computation time, while smaller <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p3.m5\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> values limited GPT-2&#8217;s re-ranking capabilities.</p>\n\n",
                "matched_terms": [
                    "transcriptions",
                    "finetuned",
                    "dataset",
                    "gpt2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings highlight the effectiveness of combining a strong pre-trained ASR model with a domain-adapted language model. The re-ranking stage is very important especially for medical dictation, where even small improvements in transcription accuracy can be vital for understanding specialized terminology and avoiding critical misinterpretations. Overall, this demonstrates that the Whisper-GPT-2 pipeline is an effective approach for improving transcription accuracy and producing higher quality outputs in Greek medical dictation.</p>\n\n",
                "matched_terms": [
                    "pretrained",
                    "greek",
                    "medical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper presented an Automatic Speech Recognition (ASR) pipeline tailored to Greek medical dictation, integrating fine-tuned Whisper models with a domain-adapted Greek GPT-2 language model for re-ranking. Through fine-tuning, we achieved substantial reductions in Word Error Rate (WER) and Character Error Rate (CER), while the re-ranking step provided consistent gains across all model sizes. A key contribution of this work is the curation of a high-quality Greek speech-to-text dataset, which addresses issues in existing resources and enables reproducibility.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "finetuned",
                    "all",
                    "dataset",
                    "gpt2",
                    "greek",
                    "medical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond quantitative improvements, the proposed pipeline demonstrates the feasibility of combining state-of-the-art ASR with domain-specific language modeling to support the challenging task of Greek medical transcription. By improving accuracy in handling specialized terminology, homophones, and spoken-language variability, this work moves toward reducing the documentation burden for healthcare professionals.</p>\n\n",
                "matched_terms": [
                    "greek",
                    "medical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For future work, we aim to incorporate authentic Greek medical speech data, such as doctor&#8211;patient interactions and clinical dictations, and explore real-time deployment on GPU-backed systems. These steps will further enhance robustness and bring the system closer to practical adoption in clinical settings.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data",
                    "greek",
                    "medical"
                ]
            }
        ]
    },
    "S5.T3": {
        "source_file": "Automatic Speech Recognition for Greek Medical Dictation",
        "caption": "Table 3: Progression of Whisper models (Original, Fine-tuned, and Fine-tuned + GPT-2 reranking) for Greek ASR.",
        "body": "Model / Pipeline\nWER (%)\nnWER (%)\nCER (%)\nBLEU (%)\n\n\n\n\nOriginal Whisper Small\n43.62\n36.69\n21.61\n72.61\n\n\nFine-tuned Whisper Small\n30.31\n26.54\n13.28\n82.35\n\n\nFine-tuned Whisper Small + GPT-2\n27.38\n23.57\n11.80\n84.17\n\n\nOriginal Whisper Medium\n34.71\n27.21\n19.30\n79.14\n\n\nFine-tuned Whisper Medium\n19.45\n16.17\n8.96\n88.93\n\n\nFine-tuned Whisper Medium + GPT-2\n18.23\n14.86\n8.35\n89.60\n\n\nOriginal Whisper Large-v2\n26.41\n18.86\n14.55\n82.49\n\n\nFine-tuned Whisper Large-v2\n14.90\n12.06\n8.45\n92.03\n\n\nFine-tuned Whisper Large-v2 + GPT-2\n14.69\n11.98\n8.66\n92.06",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Model / Pipeline</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">WER (%)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">nWER (%)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">CER (%)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">BLEU (%)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Original Whisper Small</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">43.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">36.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">21.61</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">72.61</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-tuned Whisper Small</td>\n<td class=\"ltx_td ltx_align_center\">30.31</td>\n<td class=\"ltx_td ltx_align_center\">26.54</td>\n<td class=\"ltx_td ltx_align_center\">13.28</td>\n<td class=\"ltx_td ltx_align_center\">82.35</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-tuned Whisper Small + GPT-2</td>\n<td class=\"ltx_td ltx_align_center\">27.38</td>\n<td class=\"ltx_td ltx_align_center\">23.57</td>\n<td class=\"ltx_td ltx_align_center\">11.80</td>\n<td class=\"ltx_td ltx_align_center\">84.17</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Original Whisper Medium</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">34.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">27.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">19.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">79.14</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-tuned Whisper Medium</td>\n<td class=\"ltx_td ltx_align_center\">19.45</td>\n<td class=\"ltx_td ltx_align_center\">16.17</td>\n<td class=\"ltx_td ltx_align_center\">8.96</td>\n<td class=\"ltx_td ltx_align_center\">88.93</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-tuned Whisper Medium + GPT-2</td>\n<td class=\"ltx_td ltx_align_center\">18.23</td>\n<td class=\"ltx_td ltx_align_center\">14.86</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">8.35</span></td>\n<td class=\"ltx_td ltx_align_center\">89.60</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Original Whisper Large-v2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">26.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">18.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">14.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">82.49</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fine-tuned Whisper Large-v2</td>\n<td class=\"ltx_td ltx_align_center\">14.90</td>\n<td class=\"ltx_td ltx_align_center\">12.06</td>\n<td class=\"ltx_td ltx_align_center\">8.45</td>\n<td class=\"ltx_td ltx_align_center\">92.03</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\">Fine-tuned Whisper Large-v2 + GPT-2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\">14.69</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\">11.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">8.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\">92.06</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "largev2",
            "model",
            "finetuned",
            "bleu",
            "medium",
            "models",
            "greek",
            "gpt2",
            "pipeline",
            "reranking",
            "small",
            "nwer",
            "wer",
            "cer",
            "original",
            "progression",
            "whisper",
            "asr"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We evaluated Whisper and GPT-2, focusing on the final Whisper-GPT-2 pipeline. Each model was evaluated on its respective test dataset to assess its effectiveness in handling Greek medical speech tasks. The Whisper model was fine-tuned in three configurations (Small, Medium, Large-v2) on a composite dataset comprising Greek speech data <cite class=\"ltx_cite ltx_citemacro_cite\">Gaido et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib7\" title=\"\">2024</a>); Ardila et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib3\" title=\"\">2020</a>); Conneau et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib6\" title=\"\">2022</a>)</cite>. The results are summarized in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#S5.T3\" title=\"Table 3 &#8227; 5 Evaluation and Results &#8227; Automatic Speech Recognition for Greek Medical Dictation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, reporting the Word Error Rate (WER), normalized Word Error Rate (nWER), Character Error Rate (CER), and BLEU <cite class=\"ltx_cite ltx_citemacro_cite\">Papineni et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib12\" title=\"\">2002</a>)</cite>. Fine-tuning improved performance across all model sizes. Whisper Small reduced WER from 43.62% to 30.31%, Whisper Medium from 34.71% to 19.45%, and Whisper Large-v2 from 26.41% to 14.90%. Similar gains were observed in CER, confirming the effectiveness of fine-tuning, while nWER, which disregards differences in case, punctuation, and spacing, followed the same trends, further demonstrating Whisper&#8217;s improved adaptation to Greek.</p>\n\n",
            "<p class=\"ltx_p\">The Whisper-GPT-2 pipeline was evaluated on the test dataset using WER, nWER, CER, and BLEU metrics, as shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#S5.T3\" title=\"Table 3 &#8227; 5 Evaluation and Results &#8227; Automatic Speech Recognition for Greek Medical Dictation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. The pipeline incorporates a re-ranking step in which the Whisper model first generates <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p3.m1\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> candidate transcriptions for each audio segment. Subsequently, the fine-tuned GPT-2 model evaluates these candidates and selects the most probable sentence. Experiments with <math alttext=\"N=3,5,8\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p3.m2\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mrow><mn>3</mn><mo>,</mo><mn>5</mn><mo>,</mo><mn>8</mn></mrow></mrow><annotation encoding=\"application/x-tex\">N=3,5,8</annotation></semantics></math> showed that <math alttext=\"N=5\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p3.m3\" intent=\":literal\"><semantics><mrow><mi>N</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">N=5</annotation></semantics></math> provides the best balance between transcription quality and computational efficiency. Increasing <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p3.m4\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> gave only marginal gains but significantly increased computation time, while smaller <math alttext=\"N\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p3.m5\" intent=\":literal\"><semantics><mi>N</mi><annotation encoding=\"application/x-tex\">N</annotation></semantics></math> values limited GPT-2&#8217;s re-ranking capabilities.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Medical dictation systems are essential tools in modern healthcare, enabling accurate and efficient conversion of speech into written medical documentation. The main objective of this paper is to create a domain-specific system for Greek medical speech transcriptions. The ultimate goal is to assist healthcare professionals by reducing the overload of manual documentation and improving workflow efficiency. Towards this goal, we develop a system that combines automatic speech recognition techniques with text correction model, allowing better handling of domain-specific terminology and linguistic variations in Greek. Our approach leverages both acoustic and textual modeling to create more realistic and reliable transcriptions. We focused on adapting existing language and speech technologies to the Greek medical context, addressing challenges such as complex medical terminology and linguistic inconsistencies. Through domain-specific fine-tuning, our system achieves more accurate and coherent transcriptions, contributing to the development of practical language technologies for the Greek healthcare sector.</p>\n\n",
                "matched_terms": [
                    "greek",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We develop a system that combines state-of-the-art automatic speech recognition techniques with domain adapted language model for Greek medical dictation. Our ASR component uses the pre-trained Whisper model <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib13\" title=\"\">2023</a>)</cite>, adapted to Greek. Additionaly, a Greek version of GPT-2 <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib14\" title=\"\">2019</a>)</cite> language model, fine-tuned with Greek medical text, is used to evaluate and select the best transcription hypothesis from the multiple candidates generated by the ASR. This approach integrates both acoustic and linguistic information to improve transcription accuracy and and better handle specialized medical terminology.</p>\n\n",
                "matched_terms": [
                    "model",
                    "finetuned",
                    "gpt2",
                    "greek",
                    "whisper",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early ASR systems were based on statistical models such as Hidden Markov Models (HMMs) with Gaussian Mixture Models (GMMs) for acoustic modeling <cite class=\"ltx_cite ltx_citemacro_cite\">Bahl et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib5\" title=\"\">1983</a>)</cite>. These approaches relied on strong independent assumptions and struggled with complex phonetic patterns. Neural networks, especially RNNs and LSTMs, improved performance by modeling long-term dependencies in speech. In addition, the introduction of the Transformer architecture changed the approach to sequence processing, further advanced ASR with self-attention, enabling efficient end-to-end architectures <cite class=\"ltx_cite ltx_citemacro_cite\">Zhang et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib19\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Modern speech recognition systems tend to combine acoustic, pronunciation, and language models into a single unified network that can be trained end-to-end. This approach simplifies the pipeline, reduces errors, and improves accuracy. Recent advances in self-supervised learning allow models to be pre-trained on large amounts of unlabeled speech and then fine-tuned on smaller, specialized datasets <cite class=\"ltx_cite ltx_citemacro_cite\">Baevski et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib4\" title=\"\">2020</a>)</cite>, which is especially important for low-resource languages such as Greek.</p>\n\n",
                "matched_terms": [
                    "models",
                    "greek",
                    "finetuned",
                    "pipeline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Language Models play a crucial role in the post-processing of texts generated by automatic speech recognition systems. After the initial conversion from speech to text, the resulting text often contains errors, omissions or inconsistencies. These models help with the correction of those errors, resulting in a more coherent and well-structured transcription. Transformer-based models, like GPT-2 <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib14\" title=\"\">2019</a>)</cite>, have the capability to understand the context of each sentence and provide improvements that make the text more natural and understandable. Domain-specific LMs can be trained using task-relevant text to expand available resources, which has been shown to reduce perplexity and improve transcription quality <cite class=\"ltx_cite ltx_citemacro_cite\">Jha (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib10\" title=\"\">2021</a>)</cite>. By training these models on large volumes of text, they can identify incorrect sentences or unusual expressions, and improve the flow and clarity. Using them for post-processing is especially important in this task, where accurate transcription and clear understanding of texts are critical.</p>\n\n",
                "matched_terms": [
                    "models",
                    "gpt2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fine-tuning pretrained models to specific domains is essential for improving performance without requiring massive datasets or training from scratch. Transfer learning allows models trained on general speech or text corpora to adapt to specialized tasks, such as medical dictation, by learning domain-specific terminology and phrasing. Parameter-efficient methods, such as Low-Rank Adaptation (LoRA)<cite class=\"ltx_cite ltx_citemacro_cite\">Hu et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib8\" title=\"\">2021</a>)</cite>, enable large models to be adapted with minimal computational overhead. Instead of updating all parameters, LoRA introduces small learnable matrices that approximate the necessary weight updates while keeping the original model frozen. In this work, we use LoRA to fine-tune each model, allowing the system to handle specialized terminology and domain-specific phrasing, ultimately improving transcription accuracy and reliability.</p>\n\n",
                "matched_terms": [
                    "models",
                    "original",
                    "model",
                    "small"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents the design and implementation of the Automatic Speech Recognition (ASR) system for Greek medical dictation. The core of this work involves adapting OpenAI&#8217;s Whisper model to the specific features of the Greek language through a controlled fine-tuning process. A key aspect of our methodology is the comparative analysis of three different sizes of the Whisper model, small, medium, and large-v2. To further enhance transcription quality, we integrated a re-ranking mechanism based on a fine-tuned GPT-2 model, which was used to select the most contextually appropriate transcription among Whisper&#8217;s alternatives.</p>\n\n",
                "matched_terms": [
                    "largev2",
                    "model",
                    "medium",
                    "finetuned",
                    "gpt2",
                    "reranking",
                    "small",
                    "greek",
                    "whisper",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The foundation of our system is the Whisper model <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib13\" title=\"\">2023</a>)</cite>, a state-of-the-art ASR model using an encoder-decoder architecture. Whisper has been trained on a large variety of labeled speech data, enabling robust performance across multiple languages and noisy environments. The model converts raw audio into log-Mel spectrogram features, which are processed by convolutional layers and transformer encoder blocks, and decoded into text using cross-attention and a transformer decoder <cite class=\"ltx_cite ltx_citemacro_cite\">Vaswani et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib18\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "whisper",
                    "model",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We examine three pre-trained versions of Whisper: small, medium, and large-v2. The primary motive for training three distinct models was to analyze the trade-off between performance and computational cost. While larger models like large-v2 are expected to have higher accuracy due to their increased parameters and greater representational capacity, they are also more computationally expensive and harder to deploy in resource-constrained environments. By fine-tuning and evaluating all three models we can determine the optimal one that meets our desired accuracy benchmarks while being practical and efficient for real-world medical dictation.</p>\n\n",
                "matched_terms": [
                    "largev2",
                    "medium",
                    "models",
                    "small",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To adapt Whisper model to the Greek language we applied a fine-tuning methodology to each of the three models. A diverse set of different Greek speech datasets was aggregate to create a robust corpus of data, containing speech data with different domains, with varying acoustic environments, and from multiple speakers. The need for such diversity is motivated by the challenges posed by Greek dialectal variation <cite class=\"ltx_cite ltx_citemacro_cite\">Vakirtzian et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib17\" title=\"\">2024</a>)</cite>. To make fine-tuning more efficient, Low-Rank Adaptation (LoRA) <cite class=\"ltx_cite ltx_citemacro_cite\">Hu et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib8\" title=\"\">2021</a>)</cite> was applied to reduce the number of trainable parameters. In this way, only about <math alttext=\"2\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p4.m1\" intent=\":literal\"><semantics><mrow><mn>2</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">2\\%</annotation></semantics></math> of the full model&#8217;s trainable parameters are trainable, allowing efficient fine-tuning under limited resources.</p>\n\n",
                "matched_terms": [
                    "models",
                    "greek",
                    "whisper",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To improve transcription quality, the Greek GPT-2 model <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib14\" title=\"\">2019</a>)</cite> was used to re-rank N-best hypotheses generated by Whisper. GPT-2 uses a unidirectional transformer decoder with masked multi-head self-attention to model dependencies from left to right <cite class=\"ltx_cite ltx_citemacro_cite\">Vaswani et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib18\" title=\"\">2017</a>)</cite>. Our variant, fine-tuned for Greek text, consists of 12 decoder layers, each with 12 attention heads and a hidden size of 768. This model was further fine-tuned on a domain-specific medical text corpus using LoRA <cite class=\"ltx_cite ltx_citemacro_cite\">Hu et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib8\" title=\"\">2021</a>)</cite>. Attention and projection layers were adapted with low-rank matrices, updating only 1.29% of the total parameters. The fine-tuned model was evaluated using perplexity, showing improved confidence in next-token predictions and better alignment with medical terminology.</p>\n\n",
                "matched_terms": [
                    "model",
                    "finetuned",
                    "gpt2",
                    "greek",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our final pipeline integrates both Whisper and Greek GPT-2 (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#S3.F1\" title=\"Figure 1 &#8227; 3 System Design &#8227; Automatic Speech Recognition for Greek Medical Dictation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Whisper generates a set of N candidate hypotheses, representing possible transcriptions. It converts raw audio waveforms into text using its pre-trained encoder-decoder architecture and produces multiple transcription variants to capture potential ambiguities in the audio. The language model evaluates the N candidate hypotheses. Each candidate is scored based on grammatical correctness, contextual relevance, and semantic coherence. Through this re-ranking process, the most accurate candidate is selected, ensuring that the final transcription aligns well with Greek language conventions. A critical design choice is selecting the optimal value of N, balancing computational load and the diversity of transcription options, enabling GPT-2 to effectively re-rank and select the best candidate. The optimal value of N was determined through empirical testing, ensuring robust performance for Greek audio inputs.</p>\n\n",
                "matched_terms": [
                    "model",
                    "pipeline",
                    "gpt2",
                    "reranking",
                    "greek",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work uses several Greek audio datasets used to fine-tune Whisper and a medical text corpus for adapting the Greek GPT-2. Each dataset was assembled with the needs of its respective task in mind, aiming to support effective training, reliable evaluation, and accurate representation of Greek linguistic features.</p>\n\n",
                "matched_terms": [
                    "greek",
                    "whisper",
                    "gpt2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For Whisper, we fine-tuned the model on a composite dataset of Greek speech audio paired with transcriptions. This dataset combined three publicly available sources to ensure a variety of speakers, accents, and acoustic conditions. The Mosel dataset <cite class=\"ltx_cite ltx_citemacro_cite\">Gaido et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib7\" title=\"\">2024</a>)</cite> contains European Parliament recordings in Greek. Some recordings, however, had missing timestamps or misaligned segments. To address this, we curated a filtered dataset of well-aligned audio-transcription pairs for model training <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/datasets/Vardis/Greek_Mosel\" title=\"\">https://huggingface.co/datasets/Vardis/Greek_Mosel</a></span></span></span>. Mozilla Common Voice 11.0 <cite class=\"ltx_cite ltx_citemacro_cite\">Ardila et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib3\" title=\"\">2020</a>)</cite> contributed crowd-sourced recordings from volunteers across different accents and environments. Google FLEURS <cite class=\"ltx_cite ltx_citemacro_cite\">Conneau et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib6\" title=\"\">2022</a>)</cite> provided read-aloud sentences from multiple domains, complementing the dataset. The final dataset contains approximately 49 hours of speech, covering diverse domains, speaker variability, and acoustic conditions, enabling robust model fine-tuning.</p>\n\n",
                "matched_terms": [
                    "greek",
                    "whisper",
                    "model",
                    "finetuned"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Greek GPT-2 model was trained on a custom medical text dataset containing 20,430 samples from multiple sources <span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/datasets/Vardis/Greek_Medical_Text\" title=\"\">https://huggingface.co/datasets/Vardis/Greek_Medical_Text</a></span></span></span>. Medical e-books provided detailed clinical terminology covering diagnostics, procedures, and patient care <cite class=\"ltx_cite ltx_citemacro_cite\">Iatrakis (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib9\" title=\"\">2015</a>); Sfikakis et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib15\" title=\"\">2015</a>); Tsipouras et al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib16\" title=\"\">2015</a>)</cite>. The QTLP Greek CC Corpus for the medical domain <cite class=\"ltx_cite ltx_citemacro_cite\">ath (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#bib.bib1\" title=\"\">2015</a>)</cite> added web documents automatically classified as medical, including reference materials, news, discussions, and other genres. Dialogues from medical podcasts, collected from <span class=\"ltx_text ltx_font_typewriter\">istorima.org</span>, introduced conversational medical language, enriching the dataset with contextual and informal expressions. This corpus allows GPT-2 to rank candidate sentences produced by Whisper based on perplexity, improving transcription selection and alignment with domain-specific terminology.</p>\n\n",
                "matched_terms": [
                    "greek",
                    "whisper",
                    "model",
                    "gpt2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Greek GPT-2 model was fine-tuned on a domain-specific corpus designed to adapt the model to the medical context. The training set combined medical texts and transcribed speech data. This hybrid approach allowed the model to learn not only specialized vocabulary but also the stylistic and syntactic patterns typical of oral communication, which are highly relevant for correcting ASR outputs. The evaluation was conducted using perplexity on both medical text , speech transcription, and their combination. Across all three evaluation settings, the fine-tuned model consistently outperformed the original pre-trained Greek GPT-2. More specifically, perplexity was substantially reduced on both medical texts and speech data, confirming that the model successfully learned domain-specific terminology as well as the idiomatic patterns of spoken language. The combined results further highlight the overall effectiveness of the fine-tuning strategy (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.23550v1#S5.T2\" title=\"Table 2 &#8227; 5 Evaluation and Results &#8227; Automatic Speech Recognition for Greek Medical Dictation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>).</p>\n\n",
                "matched_terms": [
                    "model",
                    "finetuned",
                    "greek",
                    "gpt2",
                    "original",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Re-ranking consistently improves performance across all tested Whisper model sizes. The WER reduction is approximately 9.66% for the Whisper Small, 6.27% for the Whisper Medium, and 1.41% for the Whisper Large-v2. CER and BLEU scores show corresponding gains, highlighting that re-ranking enhances both word-level accuracy and overall sentence quality. Compared to the original models, the full pipeline achieves WER reductions of 37.23% for Small, 47.45% for Medium, and 44.38% for Large-v2.</p>\n\n",
                "matched_terms": [
                    "largev2",
                    "model",
                    "medium",
                    "bleu",
                    "models",
                    "pipeline",
                    "reranking",
                    "small",
                    "wer",
                    "cer",
                    "original",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Choosing the optimal model depends on deployment constraints. While Whisper Large-v2 achieves the lowest WER, it requires significantly more computational resources, making it less practical for routine deployment. Whisper Medium offers strong performance with lower WER and CER while being faster, making it more practical for real-world applications.</p>\n\n",
                "matched_terms": [
                    "largev2",
                    "model",
                    "medium",
                    "wer",
                    "cer",
                    "whisper"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These findings highlight the effectiveness of combining a strong pre-trained ASR model with a domain-adapted language model. The re-ranking stage is very important especially for medical dictation, where even small improvements in transcription accuracy can be vital for understanding specialized terminology and avoiding critical misinterpretations. Overall, this demonstrates that the Whisper-GPT-2 pipeline is an effective approach for improving transcription accuracy and producing higher quality outputs in Greek medical dictation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "pipeline",
                    "reranking",
                    "small",
                    "greek",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper presented an Automatic Speech Recognition (ASR) pipeline tailored to Greek medical dictation, integrating fine-tuned Whisper models with a domain-adapted Greek GPT-2 language model for re-ranking. Through fine-tuning, we achieved substantial reductions in Word Error Rate (WER) and Character Error Rate (CER), while the re-ranking step provided consistent gains across all model sizes. A key contribution of this work is the curation of a high-quality Greek speech-to-text dataset, which addresses issues in existing resources and enables reproducibility.</p>\n\n",
                "matched_terms": [
                    "model",
                    "finetuned",
                    "models",
                    "gpt2",
                    "pipeline",
                    "reranking",
                    "wer",
                    "cer",
                    "greek",
                    "whisper",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Beyond quantitative improvements, the proposed pipeline demonstrates the feasibility of combining state-of-the-art ASR with domain-specific language modeling to support the challenging task of Greek medical transcription. By improving accuracy in handling specialized terminology, homophones, and spoken-language variability, this work moves toward reducing the documentation burden for healthcare professionals.</p>\n\n",
                "matched_terms": [
                    "asr",
                    "greek",
                    "pipeline"
                ]
            }
        ]
    }
}