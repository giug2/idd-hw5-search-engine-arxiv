{
    "S3.T1": {
        "source_file": "SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision",
        "caption": "Table 1: Characteristics of the datasets used in our experiments.",
        "body": "Dataset\nLanguage\nDomain\n#Videos\n#Sent.\nVocab.\nSplit (train/dev/test)\n\n\n\n\nPHOENIX-2014T\nDGS →\\rightarrow German\nWeather Forecast\n\n∼\\sim7k\n\n∼\\sim8k\n\n∼\\sim3k\n7,096 / 519 / 642\n\n\nCSL-Daily\nCSL →\\rightarrow Chinese\nDaily Communication\n\n∼\\sim20k\n\n∼\\sim25k\n\n∼\\sim5k\n18,401 / 1,078 / 1,057",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Dataset</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Language</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Domain</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">#Videos</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">#Sent.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Vocab.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Split (train/dev/test)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">PHOENIX-2014T</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">DGS <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> German</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Weather Forecast</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>7k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m3\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>8k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m4\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>3k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">7,096 / 519 / 642</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">CSL-Daily</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">CSL <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Chinese</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Daily Communication</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m6\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>20k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m7\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>25k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m8\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>5k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">18,401 / 1,078 / 1,057</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "∼sim7k",
            "phoenix2014t",
            "csl",
            "characteristics",
            "datasets",
            "experiments",
            "videos",
            "chinese",
            "split",
            "sent",
            "used",
            "∼sim5k",
            "domain",
            "∼sim25k",
            "vocab",
            "∼sim8k",
            "∼sim3k",
            "traindevtest",
            "weather",
            "dgs",
            "dataset",
            "communication",
            "csldaily",
            "language",
            "daily",
            "∼sim20k",
            "→rightarrow",
            "german",
            "forecast",
            "our"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Statistics of both datasets are summarizes in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S3.T1\" title=\"Table 1 &#8227; 3.7 Joint Training Objective &#8227; 3 Methodology &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Sign language translation (SLT) is typically trained with text in a single spoken language, which limits scalability and cross-language generalization. Earlier approaches have replaced gloss supervision with text-based sentence embeddings, but up to now, these remain tied to a specific language and modality. In contrast, here we employ language-agnostic, multimodal embeddings trained on text and speech from multiple languages to supervise SLT, enabling direct multilingual translation.\nTo address data scarcity, we propose a coupled augmentation method that combines multilingual target augmentations (i.e. translations into many languages) with video-level perturbations, improving model robustness. Experiments show consistent BLEURT gains over text-only sentence embedding supervision, with larger improvements in low-resource settings. Our results demonstrate that language-agnostic embedding supervision, combined with coupled augmentation, provides a scalable and semantically robust alternative to traditional SLT training.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We release the code, models, and features to facilitate further research. Github repository: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/DFKI-SignLanguage/sonar-slt.git\" title=\"\">https://github.com/DFKI-SignLanguage/sonar-slt.git</a>; Huggingface: <a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/mtmlt\" title=\"\">https://huggingface.co/mtmlt</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "our",
                    "experiments",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Sign languages (SLs) are inherently visual and culturally embedded. Each SL has evolved independently and is closely tied to the communities and spoken languages of its region. As a result, most sign language translation (SLT) datasets are built around a <em class=\"ltx_emph ltx_font_italic\">single</em> sign&#8211;spoken language pair (e.g., DGS<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>German), which makes it difficult to scale models across languages or to combine datasets. Training a system for a new target language typically requires a separate model and fresh parallel data collection.</p>\n\n",
                "matched_terms": [
                    "language",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent work by <cite class=\"ltx_cite ltx_citemacro_citet\">Hamidullah et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib11\" title=\"\">2024</a>)</cite> has reduced the reliance on glosses by supervising SLT with <em class=\"ltx_emph ltx_font_italic\">text-based sentence embeddings</em>. This yields better semantic alignment, but the embeddings remain modality-specific and typically require dataset-specific fine-tuning. Furthermore, compared to large pre-trained models that exploit vast text corpora, these text-only embeddings show limited cross-lingual transfer and reduced robustness. This raises the key question: <em class=\"ltx_emph ltx_font_italic\">Can language-agnostic, multimodal sentence embedding supervision replace text-only alignment in SLT?</em> We hypothesize that <span class=\"ltx_text ltx_font_bold\">language-agnostic, multimodal sentence embeddings</span> can reduce the residual dependence on text. Concretely, we build on <span class=\"ltx_text ltx_font_smallcaps\">SONAR</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Duquenne et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib4\" title=\"\">2023</a>)</cite>, a pretrained multilingual and multimodal embedding space that jointly represents text and speech. SONAR embeddings are claimed to be language-agnostic.\nOur approach aligns sign representations directly with language-agnostic semantic vectors, thereby decoupling supervision from any specific spoken language and removing the need for glosses. Our model integrates multiple modalities and supports direct supervision across all 200 languages covered by <span class=\"ltx_text ltx_font_smallcaps\">SONAR</span> (see Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). In contrast to prior systems that relied on additional stages or separate models for multi-target translation, our method enables <em class=\"ltx_emph ltx_font_italic\">direct</em> translation into multiple languages within a single model.</p>\n\n",
                "matched_terms": [
                    "our",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our <span class=\"ltx_text ltx_font_bold\">coupled multiple target language and video perturbation augmentation</span> strategy addresses these challenges by combining (i) <em class=\"ltx_emph ltx_font_italic\">target-language augmentation</em>, which pairs each sign sample with parallel sentences in multiple languages, and (ii) <em class=\"ltx_emph ltx_font_italic\">video augmentation</em>, which perturbs the visual stream through spatial, temporal, and photometric transformations. These augmentations are complementary: multiple target-language augmentation strengthens semantic supervision without requiring new sign recordings, while video augmentation improves the invariance of the sign encoder. Together, they yield a more robust SLT model and provide a scalable, semantically grounded alternative to traditional training, unifying supervision across languages and modalities while reducing dependence on language-, culture-, and region-specific annotations. In all, our contributions can be summarized as:</p>\n\n",
                "matched_terms": [
                    "our",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In parallel, gloss-free SLT has emerged, enabling training on weakly annotated datasets exceeding 1,000 hours for some sign languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Uthus et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib24\" title=\"\">2023</a>)</cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>A <em class=\"ltx_emph ltx_font_italic\">weakly annotated dataset</em> provides only coarse or noisy supervision. For instance, YouTube-ASL datasets are collected from online videos where annotations rely solely on automatically generated or the provided subtitles, without manual realignment,\nleading to potential inaccuracies and temporal misalignments.</span></span></span>\n<cite class=\"ltx_cite ltx_citemacro_citet\">Hamidullah et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib11\" title=\"\">2024</a>)</cite> aligns sign language videos with sentence-level text embeddings. This supervision avoids feeding long, fine-grained frame sequences to the decoder, thereby reducing redundancy in video features, lowering the need for aggressive masking, and encouraging learning at the sentence-semantic level. While intermediate supervision of visual blocks is common in multimodal models, compressing video into a sentence-level embedding before decoding improves semantic grounding and flexibility in target text generation. Nevertheless, current approaches <cite class=\"ltx_cite ltx_citemacro_citep\">(Hamidullah et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib11\" title=\"\">2024</a>; Gueuwou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib8\" title=\"\">2025b</a>)</cite> remain limited by their reliance on <em class=\"ltx_emph ltx_font_italic\">text-only</em> embedding spaces with restricted language coverage, constraining augmentation and cross-sign transfer.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "language",
                    "datasets",
                    "videos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Complementary approaches leverage large language models (LLMs). SignLLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib5\" title=\"\">2024</a>)</cite> discretizes videos into tokens and prompts a frozen LLM; Sign2GPT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib26\" title=\"\">2024</a>)</cite> feeds pseudo-glosses to XGLM, reporting <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>22 BLEU on PHOENIX-2014T and <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>15 BLEU on CSL-Daily. SpaMo&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hwang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib12\" title=\"\">2025</a>)</cite> employs a straightforward approach that extracts spatial and motion features from sign language videos and utilizes a low-rank adapter to fine-tune an LLM for sign language translation. <cite class=\"ltx_cite ltx_citemacro_citet\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib2\" title=\"\">2024</a>)</cite> introduced FLa-LLM, a two-stage, gloss-free framework that first pre-trains the visual encoder and then fine-tunes a pre-trained LLM for the downstream SLT task. These methods inherit LLM fluency but are largely monolingual and require substantial tokenization and training overhead. In contrast, our PEFT-based SONAR adapters maintain multilinguality without retraining a large decoder on discretized video tokens. More recent work has explored large-scale pre-training to improve sign language understanding, with Uni-Sign <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib13\" title=\"\">2025</a>)</cite> proposing a unified generative framework that treats downstream tasks as SLT and incorporates prior-guided fusion.</p>\n\n",
                "matched_terms": [
                    "csldaily",
                    "phoenix2014t",
                    "language",
                    "videos",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, large-scale multilingual datasets <cite class=\"ltx_cite ltx_citemacro_citep\">(Uthus et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib24\" title=\"\">2023</a>; Yazdani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib28\" title=\"\">2025b</a>)</cite> remain scarce and noisy. Crawled web data increases coverage but introduces label and alignment errors that current models struggle to absorb, leading many studies to focus on a single language or a small set of cleaner corpora. Additionally, performance often varies widely even within the same language due to differences in feature pipelines and recording conditions.</p>\n\n",
                "matched_terms": [
                    "language",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multilingual SLT models also remain in their early stages. MLSLT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib29\" title=\"\">2022</a>)</cite> covers ten European sign languages via a routing mechanism, while JWSign&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gueuwou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib9\" title=\"\">2023</a>)</cite> scales to 98 languages with language-ID tokens.\nMore recently, Sign2(LID+Text)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib22\" title=\"\">2025</a>)</cite> incorporated token-level language identification with a CTC loss, achieving competitive results. In addition, <cite class=\"ltx_cite ltx_citemacro_citet\">Yazdani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib27\" title=\"\">2025a</a>)</cite> explored continual learning for multilingual SLT.\nRecent work applies heavy pre-processing <cite class=\"ltx_cite ltx_citemacro_citep\">(Gueuwou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib8\" title=\"\">2025b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib7\" title=\"\">a</a>)</cite>, sometimes obscuring whether improvements arise from better SLT modeling or dataset-specific engineering. Both gloss-based and gloss-free methods perform best when signer distance, camera setup, and motion characteristics closely match training conditions.</p>\n\n",
                "matched_terms": [
                    "language",
                    "characteristics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_bold\">SONAR-SLT</span>, a modular SLT framework that decouples\n<em class=\"ltx_emph ltx_font_italic\">semantic understanding</em> from <em class=\"ltx_emph ltx_font_italic\">text generation</em>.\nAs illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the system first maps an input sign language video into a multilingual, multimodal semantic space, and then (optionally) decodes from this space into a chosen spoken language.\nThis design allows training on heterogeneous sign language datasets, supports multilingual supervision, and removes the need for gloss annotations. A detailed architecture is presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#A1.SS1\" title=\"A.1 Detailed Architecture &#8227; Appendix A Appendix &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a> and summarized in the next subsections.</p>\n\n",
                "matched_terms": [
                    "language",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we leverage the language-agnostic semantic space for dataset fusion.\nBecause supervision is defined independently of any specific spoken language, videos from different sign languages can be trained jointly with textual supervision in <em class=\"ltx_emph ltx_font_italic\">any</em> available language. For example, German sign language videos annotated in German can\nbe re-aligned with English, French, or Chinese translations via <math alttext=\"\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><annotation encoding=\"application/x-tex\">\\mathcal{E}</annotation></semantics></math>, allowing unified training across datasets such as PHOENIX-2014T and CSL-Daily.\nThis enables direct multi-target translation without glosses and facilitates fusion of heterogeneous sign-language corpora.</p>\n\n",
                "matched_terms": [
                    "csldaily",
                    "phoenix2014t",
                    "language",
                    "german",
                    "datasets",
                    "videos",
                    "chinese",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our approach on the following datasets:</p>\n\n",
                "matched_terms": [
                    "our",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PHOENIX-2014T</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Camgoz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib1\" title=\"\">2018</a>)</cite>: German Sign Language (DGS) weather forecast videos with parallel German text.</p>\n\n",
                "matched_terms": [
                    "phoenix2014t",
                    "language",
                    "german",
                    "forecast",
                    "weather",
                    "videos",
                    "dgs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CSL-Daily</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib32\" title=\"\">2021</a>)</cite>: A Chinese Sign Language (CSL) corpus tailored for sign-to-Chinese SLT, emphasizing interactions in daily communication contexts.</p>\n\n",
                "matched_terms": [
                    "csldaily",
                    "language",
                    "daily",
                    "csl",
                    "chinese",
                    "communication"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our method against several strong recent state-of-the-art systems within the gloss-free paradigm. CSGCR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib30\" title=\"\">2021</a>)</cite> improves SLT accuracy and fluency through three modules: word existence verification, conditional sentence generation, and cross-modal re-ranking for richer grammatical representations. GFSLT-VLP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib31\" title=\"\">2023</a>)</cite> leverages vision&#8211;language pretraining, while FLa-LLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib2\" title=\"\">2024</a>)</cite> adopts a two-stage gloss-free pipeline that first pre-trains the visual encoder and then fine-tunes a pre-trained LLM for SLT. Sign2GPT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib26\" title=\"\">2024</a>)</cite> maps visual inputs to pseudo-gloss sequences and decodes them with GPT-style language modeling, whereas SignLLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib5\" title=\"\">2024</a>)</cite> discretizes sign features into visual tokens to prompt a frozen LLM. SEM-SLT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hamidullah et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib11\" title=\"\">2024</a>)</cite> aligns sign language videos with sentence embeddings and serves as the foundation of our work. For multilingual settings, Sign2(LID+Text)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib22\" title=\"\">2025</a>)</cite> combines token-level sign language identification with a CTC objective to generate spoken text.</p>\n\n",
                "matched_terms": [
                    "our",
                    "language",
                    "videos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because our model operates on embedding vectors with small magnitudes, the MSE loss can rapidly fall to <math alttext=\"\\sim 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\sim 10^{-5}</annotation></semantics></math> even when cosine similarity remains suboptimal. Empirically, we observed that <span class=\"ltx_text ltx_font_bold\">cosine and MSE only begin to correlate at <math alttext=\"\\sim 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.SSS0.Px2.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\sim 10^{-6}</annotation></semantics></math></span>. Optimizing cosine alone often stalls, as MSE ceases to decrease, while optimizing MSE alone improves fidelity but does not guarantee angular alignment. To address this, we up-weight MSE to maintain shrinkage and retain a non-negligible cosine term to enforce directional consistency. We also experimented with InfoNCE, but under our effective batch size (with few hard negatives) it led to slower convergence and negligible improvements and we do not use it in our final experiments.</p>\n\n",
                "matched_terms": [
                    "our",
                    "experiments"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt the best-performing visual representation strategies reported in prior work, noting that optimal choices vary across datasets. To ensure comparability in our multi&#8211;sign language experiments, we restrict evaluation to datasets with similar video settings and select the strongest corresponding model. The SpaMo Visual Block performs best with global, high-quality cues e.g., high-resolution videos with a moderate signer&#8211;camera distance (CSL-Daily), or lower-resolution videos where the signer is close and centered (PHOENIX-2014T). Consequently, we conduct multilingual experiments on CSL-Daily and PHOENIX-2014T.</p>\n\n",
                "matched_terms": [
                    "csldaily",
                    "phoenix2014t",
                    "language",
                    "datasets",
                    "experiments",
                    "videos",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare our approach with other gloss-free methods on both PHOENIX-2014T and CSL-Daily datasets in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S4.T2\" title=\"Table 2 &#8227; &#8729; Sentence embedding pooling. &#8227; 4.4 Implementation Details &#8227; 4 Experiments &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nOur method shows a clear advantage on the semantics-oriented BLEURT metric. It reaches a <span class=\"ltx_text ltx_font_bold\">BLEURT of 0.545</span>, outperforming the sentence-based supervision model using text-only sentence embedding (SEM-SLT). BLEURT uses a BERT-based scorer and is designed to capture meaning and fluency, unlike BLEU and ROUGE, which primarily measure <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram overlap. Moreover, our model outperforms previous monolingual and multilingual systems on CSL-Daily in terms of BLEU and achieves comparable results on PHOENIX-2014T.</p>\n\n",
                "matched_terms": [
                    "csldaily",
                    "phoenix2014t",
                    "datasets",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe a decrease in BLEU compared to the SEM-SLT system, which is expected since our model is not fine-tuned on sign-language text. Our language-agnostic, sentence embedding-based supervision preserves semantics without requiring fine-tuning on specific dataset: it goes beyond surface <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram matching to produce translations that are contextually accurate, grammatically correct, and cross-lingually robust. Part of the remaining gap stems from dataset capture conditions. Our feature extractor <cite class=\"ltx_cite ltx_citemacro_citep\">(Hwang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib12\" title=\"\">2025</a>)</cite> is tuned for global cues and can be less accurate in cases where fine-grained articulations, such as facial expressions and finger movements, are critical. Recent top systems address this with keypoint-based representations and extensive preprocessing <cite class=\"ltx_cite ltx_citemacro_citep\">(Gueuwou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib8\" title=\"\">2025b</a>)</cite>, which help preserve these fine-grained details.</p>\n\n",
                "matched_terms": [
                    "our",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate target-side augmentation, where language translations are included in training. Results for both low- and high-resource languages on PHOENIX-2014T are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S5.T3\" title=\"Table 3 &#8227; &#8729; Observed gaps. &#8227; 5.1 Comparative Analysis &#8227; 5 Results and Analysis &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. In our experiments, we augmented the target set in training with three high-resource languages&#8212;French, Spanish, and English&#8212;while the model was evaluated on other unseen languages. Using this augmented target set yields a modest improvement over training with a single target language. However, we observe a gap in performance between high- and low-resource languages, which primarily stems from lower reference translation quality in the low-resource languages.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Reference translations were obtained using <span class=\"ltx_text ltx_font_typewriter\">facebook/nllb-200-distilled-600M</span> model.</span></span></span> The narrow domain of PHOENIX-2014T can also introduce dataset-specific idiosyncrasies, complicating fair comparisons.</p>\n\n",
                "matched_terms": [
                    "phoenix2014t",
                    "language",
                    "domain",
                    "experiments",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S5.T4\" title=\"Table 4 &#8227; &#8729; Observed gaps. &#8227; 5.1 Comparative Analysis &#8227; 5 Results and Analysis &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that pre-training on concatenated multi-sign corpora followed by monolingual fine-tuning proves most effective. In contrast, joint multi-sign fine-tuning risks resembling another full training run without yielding substantial gains. In our experiments, we first pre-train on the combined data and then fine-tune monolingually, consistent with&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hamidullah et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib11\" title=\"\">2024</a>)</cite>; post-fine-tuning performance remains largely unchanged (see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S5.T4\" title=\"Table 4 &#8227; &#8729; Observed gaps. &#8227; 5.1 Comparative Analysis &#8227; 5 Results and Analysis &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, mono vs. multilingual setup). Differences in dataset capture conditions still matter&#8212;for example, methods that rely solely on global visual features can underperform when fine-grained articulations, such as hand or facial details, are crucial. Pipelines that integrate keypoints with extensive preprocessing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gueuwou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib8\" title=\"\">2025b</a>)</cite> help mitigate such losses and achieve stronger results.</p>\n\n",
                "matched_terms": [
                    "our",
                    "experiments",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, introducing the auto-encoding loss provides a second cross-entropy signal,\nwhich exerts a stronger influence on the Visual Block.\nHere, intermediate supervision continues to be beneficial, and the auto-encoding objective itself\naccelerates convergence.\nWe consistently observed this effect in CSL-Daily and in the augmented translation setup on PHOENIX-2014T.</p>\n\n",
                "matched_terms": [
                    "csldaily",
                    "phoenix2014t"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Language-specific tendencies.</span> We deep into the analysis of two languages: German, a language trained with original data and French, a language trained via machine translation augmentation.</p>\n\n",
                "matched_terms": [
                    "language",
                    "german"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">French:</span> Our analysis shows more frequent noun substitutions, agreement mismatches, and text modality shifts (e.g., hedging with <span class=\"ltx_text ltx_font_italic\">&#8220;sont possibles&#8221;</span>).\nRegister differences from determiners or prepositions are also common. Incorrect date and numeric substitutions occur more frequently than in German,\nlikely due to segmentation differences in temporal expressions.</p>\n\n",
                "matched_terms": [
                    "our",
                    "german"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Semantically near correct and correct paraphrases (German).</span>\nAs illustrated by the green-highlighted examples in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S5.T5\" title=\"Table 5 &#8227; &#8729; Semantic analysis. &#8227; 5.3 Qualitative and Semantic Error Analysis &#8227; 5 Results and Analysis &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, incorrect lexical or numeric substitutions leave most of the remaining meaning intact\n(e.g., date shifts: <span class=\"ltx_text ltx_font_italic\">&#8220;Sonntag, den neunzehnten Dezember&#8221;</span> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;Sonntag, den siebzehnten August&#8221;</span>;\ntemperature adjustments: <span class=\"ltx_text ltx_font_italic\">&#8220;sechs Grad an den Alpen&#8221;</span> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;neun Grad am Alpenrand&#8221;</span>).\nWe also observe benign stylistic reformulations (<span class=\"ltx_text ltx_font_italic\">&#8220;es gelten entsprechende Warnungen&#8221;</span> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;es bestehen Unwetterwarnungen&#8221;</span>)\nand word-order changes without semantic effect (<span class=\"ltx_text ltx_font_italic\">&#8220;aus S&#252;dwest bis West&#8221;</span> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;aus West bis S&#252;dost&#8221;</span>).</p>\n\n",
                "matched_terms": [
                    "german",
                    "→rightarrow"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Semantically incorrect outputs, true errors (French and German).</span>\nThe red-highlighted rows in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S5.T5\" title=\"Table 5 &#8227; &#8729; Semantic analysis. &#8227; 5.3 Qualitative and Semantic Error Analysis &#8227; 5 Results and Analysis &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> illustrate errors such as topic drift (predicting wind instead of temperature),\nincorrect locations (<span class=\"ltx_text ltx_font_italic\">&#8220;H&#246;henlagen S&#252;ddeutschlands&#8221;</span> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;K&#252;sten&#8221;</span>;\n<span class=\"ltx_text ltx_font_italic\">&#8220;sud-est&#8221;</span> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;nord&#8221;</span>),\nsystem inversions (<span class=\"ltx_text ltx_font_italic\">&#8220;Hoch&#8221;</span> <math alttext=\"\\leftrightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p4.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8596;</mo><annotation encoding=\"application/x-tex\">\\leftrightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;Tief&#8221;</span>; <span class=\"ltx_text ltx_font_italic\">&#8220;haut&#8221;</span> <math alttext=\"\\leftrightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p4.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8596;</mo><annotation encoding=\"application/x-tex\">\\leftrightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;profonde&#8221;</span>),\nhallucinated entities, or incorrect hazard categories\n(<span class=\"ltx_text ltx_font_italic\">&#8220;Risque d&#8217;inondation&#8221;</span> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p4.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;Avertissements m&#233;t&#233;orologiques violents&#8221;</span>).</p>\n\n",
                "matched_terms": [
                    "german",
                    "→rightarrow"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented a scalable SLT framework that breaks the traditional close dependency between sign and spoken languages in training data and system development.\nBy aligning sign language videos with multilingual, multimodal sentence embeddings from <span class=\"ltx_text ltx_font_smallcaps\">SONAR</span>, our approach yields a\nlanguage-agnostic semantic representation that generalizes across both sign languages and spoken targets.\nThis reduces reliance on language-model priors and prioritizes visual grounding and SLT-specific grammar over surface-level text patterns.</p>\n\n",
                "matched_terms": [
                    "our",
                    "language",
                    "videos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments show that language-agnostic supervision enables robust translation even under sign&#8211;target mismatches.\nMultilingual text augmentations, combined with visual augmentation, improves performance on PHOENIX-2014T despite\nlimited data. Ablations further confirm the advantages of this approach in preserving semantic adequacy.</p>\n\n",
                "matched_terms": [
                    "phoenix2014t",
                    "experiments"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our main limitation lies in the visual feature extractor rather than the model architecture itself. We used a pre-existing visual block to avoid evaluation bias, which restricted us to datasets with compatible video settings (CSL-Daily and PHOENIX-2014T) and excluded larger corpora such as How2Sign or YouTube-ASL. As a result, our approach focuses on preserving semantics rather than maximizing exact sentence matches.</p>\n\n",
                "matched_terms": [
                    "csldaily",
                    "phoenix2014t",
                    "datasets",
                    "our",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additional translation examples for CSL-Daily and PHOENIX-2014T are provided in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#A1.T6\" title=\"Table 6 &#8227; A.3 Additional Qualitative Results &#8227; Appendix A Appendix &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#A1.T7\" title=\"Table 7 &#8227; A.3 Additional Qualitative Results &#8227; Appendix A Appendix &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, respectively.</p>\n\n",
                "matched_terms": [
                    "csldaily",
                    "phoenix2014t"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision",
        "caption": "Table 2: Comparison of SONAR-SLT with other gloss-free models on PHOENIX-2014T and CSL-Daily (metrics: BLEU, BLEURT, ROUGE (RG)). Unreported metrics are left blank; SONAR-SLT sets the best reported BLEURT on PHOENIX-2014T and remains strongly competitive with several LLM-based baselines on both datasets.",
        "body": "Method\nPHOENIX-2014T\nCSL-Daily\n\n\nBLEU\nBLEURT\nRG\nBLEU\nBLEURT\nRG\n\n\nMonolingual\n\n\nCSGCR (Zhao et al., 2021)\n\n15.18\n–\n38.85\n–\n–\n–\n\n\nGFSLT-VLP (Zhou et al., 2023)\n\n21.44\n–\n42.29\n11.00\n–\n36.44\n\n\nFLa-LLM (Chen et al., 2024)\n\n23.09\n–\n45.27\n14.20\n–\n37.25\n\n\nSign2GPT (Wong et al., 2024)\n\n22.52\n–\n48.90\n15.40\n–\n42.36\n\n\nSignLLM (Gong et al., 2024)\n\n23.40\n–\n44.49\n15.75\n–\n39.91\n\n\nSEM-SLT (Hamidullah et al., 2024)\n\n24.10\n0.481\n–\n–\n–\n–\n\n\nMultilingual\n\n\nSign2(LID+Text) (Tan et al., 2025)\n\n24.23\n–\n50.60\n14.18\n–\n40.00\n\n\nSONAR-SLT (Ours)\n22.01\n0.545\n41.44\n16.23\n0.561\n42.29",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Method</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">PHOENIX-2014T</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">CSL-Daily</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">BLEU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">BLEURT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">RG</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">BLEU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">BLEURT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">RG</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"7\"><span class=\"ltx_text ltx_font_italic\">Monolingual</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">CSGCR <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib30\" title=\"\">2021</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">15.18</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">38.85</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">GFSLT-VLP <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib31\" title=\"\">2023</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">21.44</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">42.29</td>\n<td class=\"ltx_td ltx_align_center\">11.00</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">36.44</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">FLa-LLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib2\" title=\"\">2024</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">23.09</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">45.27</td>\n<td class=\"ltx_td ltx_align_center\">14.20</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">37.25</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Sign2GPT <cite class=\"ltx_cite ltx_citemacro_citep\">(Wong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib26\" title=\"\">2024</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">22.52</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">48.90</td>\n<td class=\"ltx_td ltx_align_center\">15.40</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">42.36</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">SignLLM <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib5\" title=\"\">2024</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">23.40</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">44.49</td>\n<td class=\"ltx_td ltx_align_center\">15.75</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">39.91</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">SEM-SLT <cite class=\"ltx_cite ltx_citemacro_citep\">(Hamidullah et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib11\" title=\"\">2024</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">24.10</td>\n<td class=\"ltx_td ltx_align_center\">0.481</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"7\"><span class=\"ltx_text ltx_font_italic\">Multilingual</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Sign2(LID+Text) <cite class=\"ltx_cite ltx_citemacro_citep\">(Tan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib22\" title=\"\">2025</a>)</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">24.23</span></td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">50.60</span></td>\n<td class=\"ltx_td ltx_align_center\">14.18</td>\n<td class=\"ltx_td ltx_align_center\">&#8211;</td>\n<td class=\"ltx_td ltx_align_center\">40.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">SONAR-SLT (Ours)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">22.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.545</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">41.44</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">16.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.561</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">42.29</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "baselines",
            "phoenix2014t",
            "remains",
            "both",
            "tan",
            "bleurt",
            "reported",
            "several",
            "rouge",
            "strongly",
            "datasets",
            "zhao",
            "multilingual",
            "gong",
            "gfsltvlp",
            "hamidullah",
            "blank",
            "signllm",
            "sign2gpt",
            "best",
            "glossfree",
            "competitive",
            "monolingual",
            "metrics",
            "flallm",
            "sets",
            "method",
            "comparison",
            "llmbased",
            "semslt",
            "unreported",
            "csldaily",
            "ours",
            "models",
            "sign2lidtext",
            "sonarslt",
            "wong",
            "bleu",
            "other",
            "left",
            "zhou",
            "csgcr",
            "chen"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We compare our approach with other gloss-free methods on both PHOENIX-2014T and CSL-Daily datasets in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S4.T2\" title=\"Table 2 &#8227; &#8729; Sentence embedding pooling. &#8227; 4.4 Implementation Details &#8227; 4 Experiments &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nOur method shows a clear advantage on the semantics-oriented BLEURT metric. It reaches a <span class=\"ltx_text ltx_font_bold\">BLEURT of 0.545</span>, outperforming the sentence-based supervision model using text-only sentence embedding (SEM-SLT). BLEURT uses a BERT-based scorer and is designed to capture meaning and fluency, unlike BLEU and ROUGE, which primarily measure <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram overlap. Moreover, our model outperforms previous monolingual and multilingual systems on CSL-Daily in terms of BLEU and achieves comparable results on PHOENIX-2014T.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Sign language translation (SLT) is typically trained with text in a single spoken language, which limits scalability and cross-language generalization. Earlier approaches have replaced gloss supervision with text-based sentence embeddings, but up to now, these remain tied to a specific language and modality. In contrast, here we employ language-agnostic, multimodal embeddings trained on text and speech from multiple languages to supervise SLT, enabling direct multilingual translation.\nTo address data scarcity, we propose a coupled augmentation method that combines multilingual target augmentations (i.e. translations into many languages) with video-level perturbations, improving model robustness. Experiments show consistent BLEURT gains over text-only sentence embedding supervision, with larger improvements in low-resource settings. Our results demonstrate that language-agnostic embedding supervision, combined with coupled augmentation, provides a scalable and semantically robust alternative to traditional SLT training.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We release the code, models, and features to facilitate further research. Github repository: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/DFKI-SignLanguage/sonar-slt.git\" title=\"\">https://github.com/DFKI-SignLanguage/sonar-slt.git</a>; Huggingface: <a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/mtmlt\" title=\"\">https://huggingface.co/mtmlt</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "models",
                    "method",
                    "multilingual",
                    "bleurt"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\n</span>\n</p>\n\n",
                "matched_terms": [
                    "sonarslt",
                    "multilingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Sign languages (SLs) are inherently visual and culturally embedded. Each SL has evolved independently and is closely tied to the communities and spoken languages of its region. As a result, most sign language translation (SLT) datasets are built around a <em class=\"ltx_emph ltx_font_italic\">single</em> sign&#8211;spoken language pair (e.g., DGS<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>German), which makes it difficult to scale models across languages or to combine datasets. Training a system for a new target language typically requires a separate model and fresh parallel data collection.</p>\n\n",
                "matched_terms": [
                    "models",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Historically, SLT systems have relied on manually provided <em class=\"ltx_emph ltx_font_italic\">gloss supervision</em> <cite class=\"ltx_cite ltx_citemacro_citep\">(Camgoz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib1\" title=\"\">2018</a>)</cite>, discrete word-like labels whose design and availability are language-, culture-, and region-specific. Even <span class=\"ltx_text ltx_font_italic\">gloss-free</span> SLT approaches assume that sign inputs should be supervised by text from the co-occurring spoken language, keeping the learning signal tied to a single language <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib5\" title=\"\">2024</a>; Wong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib26\" title=\"\">2024</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib2\" title=\"\">2024</a>; Hamidullah et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib10\" title=\"\">2022</a>)</cite> and limiting cross-dataset reuse and generalization.</p>\n\n",
                "matched_terms": [
                    "glossfree",
                    "wong",
                    "gong",
                    "hamidullah",
                    "chen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent work by <cite class=\"ltx_cite ltx_citemacro_citet\">Hamidullah et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib11\" title=\"\">2024</a>)</cite> has reduced the reliance on glosses by supervising SLT with <em class=\"ltx_emph ltx_font_italic\">text-based sentence embeddings</em>. This yields better semantic alignment, but the embeddings remain modality-specific and typically require dataset-specific fine-tuning. Furthermore, compared to large pre-trained models that exploit vast text corpora, these text-only embeddings show limited cross-lingual transfer and reduced robustness. This raises the key question: <em class=\"ltx_emph ltx_font_italic\">Can language-agnostic, multimodal sentence embedding supervision replace text-only alignment in SLT?</em> We hypothesize that <span class=\"ltx_text ltx_font_bold\">language-agnostic, multimodal sentence embeddings</span> can reduce the residual dependence on text. Concretely, we build on <span class=\"ltx_text ltx_font_smallcaps\">SONAR</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Duquenne et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib4\" title=\"\">2023</a>)</cite>, a pretrained multilingual and multimodal embedding space that jointly represents text and speech. SONAR embeddings are claimed to be language-agnostic.\nOur approach aligns sign representations directly with language-agnostic semantic vectors, thereby decoupling supervision from any specific spoken language and removing the need for glosses. Our model integrates multiple modalities and supports direct supervision across all 200 languages covered by <span class=\"ltx_text ltx_font_smallcaps\">SONAR</span> (see Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). In contrast to prior systems that relied on additional stages or separate models for multi-target translation, our method enables <em class=\"ltx_emph ltx_font_italic\">direct</em> translation into multiple languages within a single model.</p>\n\n",
                "matched_terms": [
                    "models",
                    "hamidullah",
                    "multilingual",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Traditional SLT systems rely on glosses &#8212;textual labels that represent signs&#8212; as an intermediate representation. MSKA-SLT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib6\" title=\"\">2025</a>)</cite> remains a strong baseline using glosses, reporting <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>29 BLEU on PHOENIX-2014T <cite class=\"ltx_cite ltx_citemacro_citep\">(Camgoz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib1\" title=\"\">2018</a>)</cite>. However, glosses are neither universal nor standardized: they are tightly coupled to specific languages, cultures, and regions. Moreover, producing gloss annotations is highly time-consuming, requiring expert linguistic knowledge <cite class=\"ltx_cite ltx_citemacro_citep\">(M&#252;ller et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib16\" title=\"\">2023b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "phoenix2014t",
                    "remains",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In parallel, gloss-free SLT has emerged, enabling training on weakly annotated datasets exceeding 1,000 hours for some sign languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Uthus et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib24\" title=\"\">2023</a>)</cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>A <em class=\"ltx_emph ltx_font_italic\">weakly annotated dataset</em> provides only coarse or noisy supervision. For instance, YouTube-ASL datasets are collected from online videos where annotations rely solely on automatically generated or the provided subtitles, without manual realignment,\nleading to potential inaccuracies and temporal misalignments.</span></span></span>\n<cite class=\"ltx_cite ltx_citemacro_citet\">Hamidullah et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib11\" title=\"\">2024</a>)</cite> aligns sign language videos with sentence-level text embeddings. This supervision avoids feeding long, fine-grained frame sequences to the decoder, thereby reducing redundancy in video features, lowering the need for aggressive masking, and encouraging learning at the sentence-semantic level. While intermediate supervision of visual blocks is common in multimodal models, compressing video into a sentence-level embedding before decoding improves semantic grounding and flexibility in target text generation. Nevertheless, current approaches <cite class=\"ltx_cite ltx_citemacro_citep\">(Hamidullah et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib11\" title=\"\">2024</a>; Gueuwou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib8\" title=\"\">2025b</a>)</cite> remain limited by their reliance on <em class=\"ltx_emph ltx_font_italic\">text-only</em> embedding spaces with restricted language coverage, constraining augmentation and cross-sign transfer.</p>\n\n",
                "matched_terms": [
                    "glossfree",
                    "models",
                    "hamidullah",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Complementary approaches leverage large language models (LLMs). SignLLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib5\" title=\"\">2024</a>)</cite> discretizes videos into tokens and prompts a frozen LLM; Sign2GPT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib26\" title=\"\">2024</a>)</cite> feeds pseudo-glosses to XGLM, reporting <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>22 BLEU on PHOENIX-2014T and <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>15 BLEU on CSL-Daily. SpaMo&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hwang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib12\" title=\"\">2025</a>)</cite> employs a straightforward approach that extracts spatial and motion features from sign language videos and utilizes a low-rank adapter to fine-tune an LLM for sign language translation. <cite class=\"ltx_cite ltx_citemacro_citet\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib2\" title=\"\">2024</a>)</cite> introduced FLa-LLM, a two-stage, gloss-free framework that first pre-trains the visual encoder and then fine-tunes a pre-trained LLM for the downstream SLT task. These methods inherit LLM fluency but are largely monolingual and require substantial tokenization and training overhead. In contrast, our PEFT-based SONAR adapters maintain multilinguality without retraining a large decoder on discretized video tokens. More recent work has explored large-scale pre-training to improve sign language understanding, with Uni-Sign <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib13\" title=\"\">2025</a>)</cite> proposing a unified generative framework that treats downstream tasks as SLT and incorporates prior-guided fusion.</p>\n\n",
                "matched_terms": [
                    "flallm",
                    "phoenix2014t",
                    "sign2gpt",
                    "csldaily",
                    "models",
                    "glossfree",
                    "wong",
                    "bleu",
                    "gong",
                    "monolingual",
                    "signllm",
                    "chen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, large-scale multilingual datasets <cite class=\"ltx_cite ltx_citemacro_citep\">(Uthus et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib24\" title=\"\">2023</a>; Yazdani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib28\" title=\"\">2025b</a>)</cite> remain scarce and noisy. Crawled web data increases coverage but introduces label and alignment errors that current models struggle to absorb, leading many studies to focus on a single language or a small set of cleaner corpora. Additionally, performance often varies widely even within the same language due to differences in feature pipelines and recording conditions.</p>\n\n",
                "matched_terms": [
                    "models",
                    "multilingual",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multilingual SLT models also remain in their early stages. MLSLT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib29\" title=\"\">2022</a>)</cite> covers ten European sign languages via a routing mechanism, while JWSign&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gueuwou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib9\" title=\"\">2023</a>)</cite> scales to 98 languages with language-ID tokens.\nMore recently, Sign2(LID+Text)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib22\" title=\"\">2025</a>)</cite> incorporated token-level language identification with a CTC loss, achieving competitive results. In addition, <cite class=\"ltx_cite ltx_citemacro_citet\">Yazdani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib27\" title=\"\">2025a</a>)</cite> explored continual learning for multilingual SLT.\nRecent work applies heavy pre-processing <cite class=\"ltx_cite ltx_citemacro_citep\">(Gueuwou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib8\" title=\"\">2025b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib7\" title=\"\">a</a>)</cite>, sometimes obscuring whether improvements arise from better SLT modeling or dataset-specific engineering. Both gloss-based and gloss-free methods perform best when signer distance, camera setup, and motion characteristics closely match training conditions.</p>\n\n",
                "matched_terms": [
                    "both",
                    "tan",
                    "best",
                    "models",
                    "sign2lidtext",
                    "glossfree",
                    "competitive",
                    "multilingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_bold\">SONAR-SLT</span>, a modular SLT framework that decouples\n<em class=\"ltx_emph ltx_font_italic\">semantic understanding</em> from <em class=\"ltx_emph ltx_font_italic\">text generation</em>.\nAs illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the system first maps an input sign language video into a multilingual, multimodal semantic space, and then (optionally) decodes from this space into a chosen spoken language.\nThis design allows training on heterogeneous sign language datasets, supports multilingual supervision, and removes the need for gloss annotations. A detailed architecture is presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#A1.SS1\" title=\"A.1 Detailed Architecture &#8227; Appendix A Appendix &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a> and summarized in the next subsections.</p>\n\n",
                "matched_terms": [
                    "sonarslt",
                    "multilingual",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we leverage the language-agnostic semantic space for dataset fusion.\nBecause supervision is defined independently of any specific spoken language, videos from different sign languages can be trained jointly with textual supervision in <em class=\"ltx_emph ltx_font_italic\">any</em> available language. For example, German sign language videos annotated in German can\nbe re-aligned with English, French, or Chinese translations via <math alttext=\"\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><annotation encoding=\"application/x-tex\">\\mathcal{E}</annotation></semantics></math>, allowing unified training across datasets such as PHOENIX-2014T and CSL-Daily.\nThis enables direct multi-target translation without glosses and facilitates fusion of heterogeneous sign-language corpora.</p>\n\n",
                "matched_terms": [
                    "csldaily",
                    "phoenix2014t",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CSL-Daily</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib32\" title=\"\">2021</a>)</cite>: A Chinese Sign Language (CSL) corpus tailored for sign-to-Chinese SLT, emphasizing interactions in daily communication contexts.</p>\n\n",
                "matched_terms": [
                    "csldaily",
                    "zhou"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Statistics of both datasets are summarizes in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S3.T1\" title=\"Table 1 &#8227; 3.7 Joint Training Objective &#8227; 3 Methodology &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
                "matched_terms": [
                    "both",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our method following <cite class=\"ltx_cite ltx_citemacro_citep\">(M&#252;ller et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib17\" title=\"\">2022</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib15\" title=\"\">2023a</a>)</cite>,\nusing BLEU<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><span class=\"ltx_text ltx_font_typewriter\">BLEU|nrefs:1|bs:1000|seed:16|case: mixed|eff:no|tok:13a|smooth:exp|version:2.4.0</span></span></span></span>\n(via SacreBLEU <cite class=\"ltx_cite ltx_citemacro_citep\">(Post, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib19\" title=\"\">2018</a>)</cite>) for lexical overlap,\nROUGE <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib14\" title=\"\">2004</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><span class=\"ltx_text ltx_font_typewriter\">ROUGE|L|nrefs:1|tok:13a|case:mixed|version:1.5.5</span></span></span></span>\nfor recall-oriented <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram overlap,\nand BLEURT <cite class=\"ltx_cite ltx_citemacro_citep\">(Sellam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib21\" title=\"\">2020</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>BLEURT v0.0.2 using checkpoint BLEURT-20.</span></span></span>\nfor semantic quality.</p>\n\n",
                "matched_terms": [
                    "rouge",
                    "method",
                    "bleurt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our method against several strong recent state-of-the-art systems within the gloss-free paradigm. CSGCR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib30\" title=\"\">2021</a>)</cite> improves SLT accuracy and fluency through three modules: word existence verification, conditional sentence generation, and cross-modal re-ranking for richer grammatical representations. GFSLT-VLP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib31\" title=\"\">2023</a>)</cite> leverages vision&#8211;language pretraining, while FLa-LLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib2\" title=\"\">2024</a>)</cite> adopts a two-stage gloss-free pipeline that first pre-trains the visual encoder and then fine-tunes a pre-trained LLM for SLT. Sign2GPT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib26\" title=\"\">2024</a>)</cite> maps visual inputs to pseudo-gloss sequences and decodes them with GPT-style language modeling, whereas SignLLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib5\" title=\"\">2024</a>)</cite> discretizes sign features into visual tokens to prompt a frozen LLM. SEM-SLT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hamidullah et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib11\" title=\"\">2024</a>)</cite> aligns sign language videos with sentence embeddings and serves as the foundation of our work. For multilingual settings, Sign2(LID+Text)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib22\" title=\"\">2025</a>)</cite> combines token-level sign language identification with a CTC objective to generate spoken text.</p>\n\n",
                "matched_terms": [
                    "flallm",
                    "zhou",
                    "sign2gpt",
                    "tan",
                    "method",
                    "sign2lidtext",
                    "several",
                    "zhao",
                    "glossfree",
                    "wong",
                    "csgcr",
                    "multilingual",
                    "semslt",
                    "gong",
                    "gfsltvlp",
                    "hamidullah",
                    "signllm",
                    "chen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt the best-performing visual representation strategies reported in prior work, noting that optimal choices vary across datasets. To ensure comparability in our multi&#8211;sign language experiments, we restrict evaluation to datasets with similar video settings and select the strongest corresponding model. The SpaMo Visual Block performs best with global, high-quality cues e.g., high-resolution videos with a moderate signer&#8211;camera distance (CSL-Daily), or lower-resolution videos where the signer is close and centered (PHOENIX-2014T). Consequently, we conduct multilingual experiments on CSL-Daily and PHOENIX-2014T.</p>\n\n",
                "matched_terms": [
                    "csldaily",
                    "phoenix2014t",
                    "best",
                    "datasets",
                    "reported",
                    "multilingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PHOENIX-2014-T (monolingual):</span> constant LR (we found it more stable)</p>\n\n",
                "matched_terms": [
                    "phoenix2014t",
                    "monolingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe a decrease in BLEU compared to the SEM-SLT system, which is expected since our model is not fine-tuned on sign-language text. Our language-agnostic, sentence embedding-based supervision preserves semantics without requiring fine-tuning on specific dataset: it goes beyond surface <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram matching to produce translations that are contextually accurate, grammatically correct, and cross-lingually robust. Part of the remaining gap stems from dataset capture conditions. Our feature extractor <cite class=\"ltx_cite ltx_citemacro_citep\">(Hwang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib12\" title=\"\">2025</a>)</cite> is tuned for global cues and can be less accurate in cases where fine-grained articulations, such as facial expressions and finger movements, are critical. Recent top systems address this with keypoint-based representations and extensive preprocessing <cite class=\"ltx_cite ltx_citemacro_citep\">(Gueuwou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib8\" title=\"\">2025b</a>)</cite>, which help preserve these fine-grained details.</p>\n\n",
                "matched_terms": [
                    "semslt",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate target-side augmentation, where language translations are included in training. Results for both low- and high-resource languages on PHOENIX-2014T are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S5.T3\" title=\"Table 3 &#8227; &#8729; Observed gaps. &#8227; 5.1 Comparative Analysis &#8227; 5 Results and Analysis &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. In our experiments, we augmented the target set in training with three high-resource languages&#8212;French, Spanish, and English&#8212;while the model was evaluated on other unseen languages. Using this augmented target set yields a modest improvement over training with a single target language. However, we observe a gap in performance between high- and low-resource languages, which primarily stems from lower reference translation quality in the low-resource languages.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Reference translations were obtained using <span class=\"ltx_text ltx_font_typewriter\">facebook/nllb-200-distilled-600M</span> model.</span></span></span> The narrow domain of PHOENIX-2014T can also introduce dataset-specific idiosyncrasies, complicating fair comparisons.</p>\n\n",
                "matched_terms": [
                    "phoenix2014t",
                    "both",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S5.T4\" title=\"Table 4 &#8227; &#8729; Observed gaps. &#8227; 5.1 Comparative Analysis &#8227; 5 Results and Analysis &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that pre-training on concatenated multi-sign corpora followed by monolingual fine-tuning proves most effective. In contrast, joint multi-sign fine-tuning risks resembling another full training run without yielding substantial gains. In our experiments, we first pre-train on the combined data and then fine-tune monolingually, consistent with&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hamidullah et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib11\" title=\"\">2024</a>)</cite>; post-fine-tuning performance remains largely unchanged (see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S5.T4\" title=\"Table 4 &#8227; &#8729; Observed gaps. &#8227; 5.1 Comparative Analysis &#8227; 5 Results and Analysis &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, mono vs. multilingual setup). Differences in dataset capture conditions still matter&#8212;for example, methods that rely solely on global visual features can underperform when fine-grained articulations, such as hand or facial details, are crucial. Pipelines that integrate keypoints with extensive preprocessing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gueuwou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib8\" title=\"\">2025b</a>)</cite> help mitigate such losses and achieve stronger results.</p>\n\n",
                "matched_terms": [
                    "remains",
                    "hamidullah",
                    "multilingual",
                    "monolingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, introducing the auto-encoding loss provides a second cross-entropy signal,\nwhich exerts a stronger influence on the Visual Block.\nHere, intermediate supervision continues to be beneficial, and the auto-encoding objective itself\naccelerates convergence.\nWe consistently observed this effect in CSL-Daily and in the augmented translation setup on PHOENIX-2014T.</p>\n\n",
                "matched_terms": [
                    "csldaily",
                    "phoenix2014t"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S5.T5\" title=\"Table 5 &#8227; &#8729; Semantic analysis. &#8227; 5.3 Qualitative and Semantic Error Analysis &#8227; 5 Results and Analysis &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows examples of two contrasting outcomes: cases where the model accurately captures the intended meaning and cases where it fails.\nWhen contextual understanding is incomplete, the decoder frequently compensates by generating fluent continuations via next-token prediction.\nThis behavior is characteristic of SLT systems that rely on pretrained language models as decoders: they can mask weaknesses in semantic grounding by producing outputs that are coherent but only partially faithful to the source.\nAs a result, improvements in BLEU may reflect the decoder&#8217;s ability to recover plausible sentences rather than true gains in sign-to-text comprehension.\nTherefore, exact sequence matching metrics such as BLEU are insufficient and in some cases misleading for evaluating translation quality in SLT.</p>\n\n",
                "matched_terms": [
                    "models",
                    "bleu",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Surface-form scores vs. meaning preservation.\nWe observe a systematic mismatch between surface-form metrics (e.g., BLEU) and semantic adequacy (BLEURT) across both German and French.\nOutputs with only moderate <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram overlap can still be semantically faithful, while some high-scoring predictions contain factual errors.</p>\n\n",
                "matched_terms": [
                    "both",
                    "bleu",
                    "metrics",
                    "bleurt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, as in other machine translation tasks, <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p5.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram metrics penalize near or even fully legitimate paraphrases and sometimes fail to capture serious factual errors.\nRobust SLT evaluation requires <em class=\"ltx_emph ltx_font_italic\">semantic</em> metrics that explicitly reward meaning preservation while penalizing distortions or hallucinations.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Evaluation and model development for multilingual SLT should be language-aware. In practice, one should combine semantics-focused metrics with targeted, language-specific checks (e.g., temporals and agreement in French; word order and compounding in German) to obtain fair comparisons and actionable diagnostics.</p>\n\n",
                "matched_terms": [
                    "multilingual",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented a scalable SLT framework that breaks the traditional close dependency between sign and spoken languages in training data and system development.\nBy aligning sign language videos with multilingual, multimodal sentence embeddings from <span class=\"ltx_text ltx_font_smallcaps\">SONAR</span>, our approach yields a\nlanguage-agnostic semantic representation that generalizes across both sign languages and spoken targets.\nThis reduces reliance on language-model priors and prioritizes visual grounding and SLT-specific grammar over surface-level text patterns.</p>\n\n",
                "matched_terms": [
                    "both",
                    "multilingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments show that language-agnostic supervision enables robust translation even under sign&#8211;target mismatches.\nMultilingual text augmentations, combined with visual augmentation, improves performance on PHOENIX-2014T despite\nlimited data. Ablations further confirm the advantages of this approach in preserving semantic adequacy.</p>\n\n",
                "matched_terms": [
                    "phoenix2014t",
                    "multilingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our main limitation lies in the visual feature extractor rather than the model architecture itself. We used a pre-existing visual block to avoid evaluation bias, which restricted us to datasets with compatible video settings (CSL-Daily and PHOENIX-2014T) and excluded larger corpora such as How2Sign or YouTube-ASL. As a result, our approach focuses on preserving semantics rather than maximizing exact sentence matches.</p>\n\n",
                "matched_terms": [
                    "csldaily",
                    "phoenix2014t",
                    "datasets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additional translation examples for CSL-Daily and PHOENIX-2014T are provided in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#A1.T6\" title=\"Table 6 &#8227; A.3 Additional Qualitative Results &#8227; Appendix A Appendix &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#A1.T7\" title=\"Table 7 &#8227; A.3 Additional Qualitative Results &#8227; Appendix A Appendix &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, respectively.</p>\n\n",
                "matched_terms": [
                    "csldaily",
                    "phoenix2014t"
                ]
            }
        ]
    },
    "S5.T3": {
        "source_file": "SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision",
        "caption": "Table 3: SONAR-SLT performance across target languages in both high- and low-resource settings on PHOENIX-2014T, reported using BLEU scores.",
        "body": "Resource\nLanguage\nBLEU\n\n\n\n\nHigh\nSpanish (es)\n22.3\n\n\nFrench (fr)\n22.6\n\n\nEnglish (en)\n21.6\n\n\nLow\nTurkish (tr)\n13.1\n\n\nMalagasy (mg)\n11.8\n\n\nPersian (fa)\n 8.7",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">Resource</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">Language</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:8.0pt;padding-right:8.0pt;\"><span class=\"ltx_text ltx_font_bold\">BLEU</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"3\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">High</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">Spanish (es)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">22.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">French (fr)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">22.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">English (en)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">21.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" rowspan=\"3\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">Low</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">Turkish (tr)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">13.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">Malagasy (mg)</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">11.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">Persian (fa)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:8.0pt;padding-right:8.0pt;\">&#160;8.7</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "phoenix2014t",
            "both",
            "reported",
            "high",
            "french",
            "turkish",
            "persian",
            "resource",
            "languages",
            "lowresource",
            "scores",
            "performance",
            "spanish",
            "malagasy",
            "language",
            "english",
            "across",
            "target",
            "settings",
            "sonarslt",
            "bleu",
            "low"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We evaluate target-side augmentation, where language translations are included in training. Results for both low- and high-resource languages on PHOENIX-2014T are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S5.T3\" title=\"Table 3 &#8227; &#8729; Observed gaps. &#8227; 5.1 Comparative Analysis &#8227; 5 Results and Analysis &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. In our experiments, we augmented the target set in training with three high-resource languages&#8212;French, Spanish, and English&#8212;while the model was evaluated on other unseen languages. Using this augmented target set yields a modest improvement over training with a single target language. However, we observe a gap in performance between high- and low-resource languages, which primarily stems from lower reference translation quality in the low-resource languages.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Reference translations were obtained using <span class=\"ltx_text ltx_font_typewriter\">facebook/nllb-200-distilled-600M</span> model.</span></span></span> The narrow domain of PHOENIX-2014T can also introduce dataset-specific idiosyncrasies, complicating fair comparisons.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Sign language translation (SLT) is typically trained with text in a single spoken language, which limits scalability and cross-language generalization. Earlier approaches have replaced gloss supervision with text-based sentence embeddings, but up to now, these remain tied to a specific language and modality. In contrast, here we employ language-agnostic, multimodal embeddings trained on text and speech from multiple languages to supervise SLT, enabling direct multilingual translation.\nTo address data scarcity, we propose a coupled augmentation method that combines multilingual target augmentations (i.e. translations into many languages) with video-level perturbations, improving model robustness. Experiments show consistent BLEURT gains over text-only sentence embedding supervision, with larger improvements in low-resource settings. Our results demonstrate that language-agnostic embedding supervision, combined with coupled augmentation, provides a scalable and semantically robust alternative to traditional SLT training.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We release the code, models, and features to facilitate further research. Github repository: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/DFKI-SignLanguage/sonar-slt.git\" title=\"\">https://github.com/DFKI-SignLanguage/sonar-slt.git</a>; Huggingface: <a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/mtmlt\" title=\"\">https://huggingface.co/mtmlt</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "language",
                    "target",
                    "settings",
                    "languages",
                    "lowresource"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\n</span>\n</p>\n\n",
                "matched_terms": [
                    "language",
                    "sonarslt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Sign languages (SLs) are inherently visual and culturally embedded. Each SL has evolved independently and is closely tied to the communities and spoken languages of its region. As a result, most sign language translation (SLT) datasets are built around a <em class=\"ltx_emph ltx_font_italic\">single</em> sign&#8211;spoken language pair (e.g., DGS<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>German), which makes it difficult to scale models across languages or to combine datasets. Training a system for a new target language typically requires a separate model and fresh parallel data collection.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "language",
                    "target",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent work by <cite class=\"ltx_cite ltx_citemacro_citet\">Hamidullah et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib11\" title=\"\">2024</a>)</cite> has reduced the reliance on glosses by supervising SLT with <em class=\"ltx_emph ltx_font_italic\">text-based sentence embeddings</em>. This yields better semantic alignment, but the embeddings remain modality-specific and typically require dataset-specific fine-tuning. Furthermore, compared to large pre-trained models that exploit vast text corpora, these text-only embeddings show limited cross-lingual transfer and reduced robustness. This raises the key question: <em class=\"ltx_emph ltx_font_italic\">Can language-agnostic, multimodal sentence embedding supervision replace text-only alignment in SLT?</em> We hypothesize that <span class=\"ltx_text ltx_font_bold\">language-agnostic, multimodal sentence embeddings</span> can reduce the residual dependence on text. Concretely, we build on <span class=\"ltx_text ltx_font_smallcaps\">SONAR</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Duquenne et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib4\" title=\"\">2023</a>)</cite>, a pretrained multilingual and multimodal embedding space that jointly represents text and speech. SONAR embeddings are claimed to be language-agnostic.\nOur approach aligns sign representations directly with language-agnostic semantic vectors, thereby decoupling supervision from any specific spoken language and removing the need for glosses. Our model integrates multiple modalities and supports direct supervision across all 200 languages covered by <span class=\"ltx_text ltx_font_smallcaps\">SONAR</span> (see Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). In contrast to prior systems that relied on additional stages or separate models for multi-target translation, our method enables <em class=\"ltx_emph ltx_font_italic\">direct</em> translation into multiple languages within a single model.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "language",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our <span class=\"ltx_text ltx_font_bold\">coupled multiple target language and video perturbation augmentation</span> strategy addresses these challenges by combining (i) <em class=\"ltx_emph ltx_font_italic\">target-language augmentation</em>, which pairs each sign sample with parallel sentences in multiple languages, and (ii) <em class=\"ltx_emph ltx_font_italic\">video augmentation</em>, which perturbs the visual stream through spatial, temporal, and photometric transformations. These augmentations are complementary: multiple target-language augmentation strengthens semantic supervision without requiring new sign recordings, while video augmentation improves the invariance of the sign encoder. Together, they yield a more robust SLT model and provide a scalable, semantically grounded alternative to traditional training, unifying supervision across languages and modalities while reducing dependence on language-, culture-, and region-specific annotations. In all, our contributions can be summarized as:</p>\n\n",
                "matched_terms": [
                    "languages",
                    "language",
                    "target",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Traditional SLT systems rely on glosses &#8212;textual labels that represent signs&#8212; as an intermediate representation. MSKA-SLT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib6\" title=\"\">2025</a>)</cite> remains a strong baseline using glosses, reporting <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>29 BLEU on PHOENIX-2014T <cite class=\"ltx_cite ltx_citemacro_citep\">(Camgoz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib1\" title=\"\">2018</a>)</cite>. However, glosses are neither universal nor standardized: they are tightly coupled to specific languages, cultures, and regions. Moreover, producing gloss annotations is highly time-consuming, requiring expert linguistic knowledge <cite class=\"ltx_cite ltx_citemacro_citep\">(M&#252;ller et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib16\" title=\"\">2023b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "phoenix2014t",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In parallel, gloss-free SLT has emerged, enabling training on weakly annotated datasets exceeding 1,000 hours for some sign languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Uthus et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib24\" title=\"\">2023</a>)</cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>A <em class=\"ltx_emph ltx_font_italic\">weakly annotated dataset</em> provides only coarse or noisy supervision. For instance, YouTube-ASL datasets are collected from online videos where annotations rely solely on automatically generated or the provided subtitles, without manual realignment,\nleading to potential inaccuracies and temporal misalignments.</span></span></span>\n<cite class=\"ltx_cite ltx_citemacro_citet\">Hamidullah et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib11\" title=\"\">2024</a>)</cite> aligns sign language videos with sentence-level text embeddings. This supervision avoids feeding long, fine-grained frame sequences to the decoder, thereby reducing redundancy in video features, lowering the need for aggressive masking, and encouraging learning at the sentence-semantic level. While intermediate supervision of visual blocks is common in multimodal models, compressing video into a sentence-level embedding before decoding improves semantic grounding and flexibility in target text generation. Nevertheless, current approaches <cite class=\"ltx_cite ltx_citemacro_citep\">(Hamidullah et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib11\" title=\"\">2024</a>; Gueuwou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib8\" title=\"\">2025b</a>)</cite> remain limited by their reliance on <em class=\"ltx_emph ltx_font_italic\">text-only</em> embedding spaces with restricted language coverage, constraining augmentation and cross-sign transfer.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "language",
                    "target"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Complementary approaches leverage large language models (LLMs). SignLLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib5\" title=\"\">2024</a>)</cite> discretizes videos into tokens and prompts a frozen LLM; Sign2GPT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib26\" title=\"\">2024</a>)</cite> feeds pseudo-glosses to XGLM, reporting <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>22 BLEU on PHOENIX-2014T and <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>15 BLEU on CSL-Daily. SpaMo&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hwang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib12\" title=\"\">2025</a>)</cite> employs a straightforward approach that extracts spatial and motion features from sign language videos and utilizes a low-rank adapter to fine-tune an LLM for sign language translation. <cite class=\"ltx_cite ltx_citemacro_citet\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib2\" title=\"\">2024</a>)</cite> introduced FLa-LLM, a two-stage, gloss-free framework that first pre-trains the visual encoder and then fine-tunes a pre-trained LLM for the downstream SLT task. These methods inherit LLM fluency but are largely monolingual and require substantial tokenization and training overhead. In contrast, our PEFT-based SONAR adapters maintain multilinguality without retraining a large decoder on discretized video tokens. More recent work has explored large-scale pre-training to improve sign language understanding, with Uni-Sign <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib13\" title=\"\">2025</a>)</cite> proposing a unified generative framework that treats downstream tasks as SLT and incorporates prior-guided fusion.</p>\n\n",
                "matched_terms": [
                    "phoenix2014t",
                    "bleu",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite these advances, large-scale multilingual datasets <cite class=\"ltx_cite ltx_citemacro_citep\">(Uthus et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib24\" title=\"\">2023</a>; Yazdani et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib28\" title=\"\">2025b</a>)</cite> remain scarce and noisy. Crawled web data increases coverage but introduces label and alignment errors that current models struggle to absorb, leading many studies to focus on a single language or a small set of cleaner corpora. Additionally, performance often varies widely even within the same language due to differences in feature pipelines and recording conditions.</p>\n\n",
                "matched_terms": [
                    "language",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multilingual SLT models also remain in their early stages. MLSLT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib29\" title=\"\">2022</a>)</cite> covers ten European sign languages via a routing mechanism, while JWSign&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gueuwou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib9\" title=\"\">2023</a>)</cite> scales to 98 languages with language-ID tokens.\nMore recently, Sign2(LID+Text)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib22\" title=\"\">2025</a>)</cite> incorporated token-level language identification with a CTC loss, achieving competitive results. In addition, <cite class=\"ltx_cite ltx_citemacro_citet\">Yazdani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib27\" title=\"\">2025a</a>)</cite> explored continual learning for multilingual SLT.\nRecent work applies heavy pre-processing <cite class=\"ltx_cite ltx_citemacro_citep\">(Gueuwou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib8\" title=\"\">2025b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib7\" title=\"\">a</a>)</cite>, sometimes obscuring whether improvements arise from better SLT modeling or dataset-specific engineering. Both gloss-based and gloss-free methods perform best when signer distance, camera setup, and motion characteristics closely match training conditions.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "language",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_bold\">SONAR-SLT</span>, a modular SLT framework that decouples\n<em class=\"ltx_emph ltx_font_italic\">semantic understanding</em> from <em class=\"ltx_emph ltx_font_italic\">text generation</em>.\nAs illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the system first maps an input sign language video into a multilingual, multimodal semantic space, and then (optionally) decodes from this space into a chosen spoken language.\nThis design allows training on heterogeneous sign language datasets, supports multilingual supervision, and removes the need for gloss annotations. A detailed architecture is presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#A1.SS1\" title=\"A.1 Detailed Architecture &#8227; Appendix A Appendix &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a> and summarized in the next subsections.</p>\n\n",
                "matched_terms": [
                    "language",
                    "sonarslt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then decode into natural language from the semantic embedding.\nA pretrained decoder <math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math> from <span class=\"ltx_text ltx_font_smallcaps\">SONAR</span> generates text from a semantic vector\nand a target language token <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>. Conditioned on the sign-derived and semantically text-aligned (Section &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S3.SS3\" title=\"3.3 Semantic Alignment &#8227; 3 Methodology &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>) embedding <math alttext=\"\\mathbf{z}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m3\" intent=\":literal\"><semantics><mi>&#119859;</mi><annotation encoding=\"application/x-tex\">\\mathbf{z}</annotation></semantics></math>,\nthe decoder is trained with teacher forcing:</p>\n\n",
                "matched_terms": [
                    "language",
                    "target"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we leverage the language-agnostic semantic space for dataset fusion.\nBecause supervision is defined independently of any specific spoken language, videos from different sign languages can be trained jointly with textual supervision in <em class=\"ltx_emph ltx_font_italic\">any</em> available language. For example, German sign language videos annotated in German can\nbe re-aligned with English, French, or Chinese translations via <math alttext=\"\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><annotation encoding=\"application/x-tex\">\\mathcal{E}</annotation></semantics></math>, allowing unified training across datasets such as PHOENIX-2014T and CSL-Daily.\nThis enables direct multi-target translation without glosses and facilitates fusion of heterogeneous sign-language corpora.</p>\n\n",
                "matched_terms": [
                    "phoenix2014t",
                    "language",
                    "english",
                    "across",
                    "french",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PHOENIX-2014T</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Camgoz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib1\" title=\"\">2018</a>)</cite>: German Sign Language (DGS) weather forecast videos with parallel German text.</p>\n\n",
                "matched_terms": [
                    "phoenix2014t",
                    "language"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our method against several strong recent state-of-the-art systems within the gloss-free paradigm. CSGCR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib30\" title=\"\">2021</a>)</cite> improves SLT accuracy and fluency through three modules: word existence verification, conditional sentence generation, and cross-modal re-ranking for richer grammatical representations. GFSLT-VLP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib31\" title=\"\">2023</a>)</cite> leverages vision&#8211;language pretraining, while FLa-LLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib2\" title=\"\">2024</a>)</cite> adopts a two-stage gloss-free pipeline that first pre-trains the visual encoder and then fine-tunes a pre-trained LLM for SLT. Sign2GPT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib26\" title=\"\">2024</a>)</cite> maps visual inputs to pseudo-gloss sequences and decodes them with GPT-style language modeling, whereas SignLLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib5\" title=\"\">2024</a>)</cite> discretizes sign features into visual tokens to prompt a frozen LLM. SEM-SLT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hamidullah et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib11\" title=\"\">2024</a>)</cite> aligns sign language videos with sentence embeddings and serves as the foundation of our work. For multilingual settings, Sign2(LID+Text)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib22\" title=\"\">2025</a>)</cite> combines token-level sign language identification with a CTC objective to generate spoken text.</p>\n\n",
                "matched_terms": [
                    "language",
                    "settings"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The original SONAR pools by running a shallow decoder: it feeds a special token (the EOS id in <span class=\"ltx_text ltx_font_typewriter\">M200M100</span>) as input and uses the encoder outputs as hidden states; the first decoder output is taken as the sentence embedding. During the Visual Block training, we adopt this approach with a shallow decoder initialized from the first three SONAR decoder layers and train it only for pooling. This supplies language context during pooling, while the incoming features themselves are language-agnostic (from another modality). Text generation is then conditioned on the target language.</p>\n\n",
                "matched_terms": [
                    "language",
                    "target"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt the best-performing visual representation strategies reported in prior work, noting that optimal choices vary across datasets. To ensure comparability in our multi&#8211;sign language experiments, we restrict evaluation to datasets with similar video settings and select the strongest corresponding model. The SpaMo Visual Block performs best with global, high-quality cues e.g., high-resolution videos with a moderate signer&#8211;camera distance (CSL-Daily), or lower-resolution videos where the signer is close and centered (PHOENIX-2014T). Consequently, we conduct multilingual experiments on CSL-Daily and PHOENIX-2014T.</p>\n\n",
                "matched_terms": [
                    "phoenix2014t",
                    "language",
                    "across",
                    "settings",
                    "reported"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text augmentation.</span> To expand the datasets using NLLB <cite class=\"ltx_cite ltx_citemacro_citep\">(NLLB Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib18\" title=\"\">2024</a>)</cite>, we machine-translate the target texts into three high-resource languages (English, French and Spanish) using the <span class=\"ltx_text ltx_font_typewriter\">facebook/nllb-200-distilled-600M</span> model.</p>\n\n",
                "matched_terms": [
                    "english",
                    "target",
                    "spanish",
                    "french",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare our approach with other gloss-free methods on both PHOENIX-2014T and CSL-Daily datasets in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S4.T2\" title=\"Table 2 &#8227; &#8729; Sentence embedding pooling. &#8227; 4.4 Implementation Details &#8227; 4 Experiments &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nOur method shows a clear advantage on the semantics-oriented BLEURT metric. It reaches a <span class=\"ltx_text ltx_font_bold\">BLEURT of 0.545</span>, outperforming the sentence-based supervision model using text-only sentence embedding (SEM-SLT). BLEURT uses a BERT-based scorer and is designed to capture meaning and fluency, unlike BLEU and ROUGE, which primarily measure <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram overlap. Moreover, our model outperforms previous monolingual and multilingual systems on CSL-Daily in terms of BLEU and achieves comparable results on PHOENIX-2014T.</p>\n\n",
                "matched_terms": [
                    "phoenix2014t",
                    "both",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S5.T5\" title=\"Table 5 &#8227; &#8729; Semantic analysis. &#8227; 5.3 Qualitative and Semantic Error Analysis &#8227; 5 Results and Analysis &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows examples of two contrasting outcomes: cases where the model accurately captures the intended meaning and cases where it fails.\nWhen contextual understanding is incomplete, the decoder frequently compensates by generating fluent continuations via next-token prediction.\nThis behavior is characteristic of SLT systems that rely on pretrained language models as decoders: they can mask weaknesses in semantic grounding by producing outputs that are coherent but only partially faithful to the source.\nAs a result, improvements in BLEU may reflect the decoder&#8217;s ability to recover plausible sentences rather than true gains in sign-to-text comprehension.\nTherefore, exact sequence matching metrics such as BLEU are insufficient and in some cases misleading for evaluating translation quality in SLT.</p>\n\n",
                "matched_terms": [
                    "language",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Language-specific tendencies.</span> We deep into the analysis of two languages: German, a language trained with original data and French, a language trained via machine translation augmentation.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "language",
                    "french"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Surface-form scores vs. meaning preservation.\nWe observe a systematic mismatch between surface-form metrics (e.g., BLEU) and semantic adequacy (BLEURT) across both German and French.\nOutputs with only moderate <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram overlap can still be semantically faithful, while some high-scoring predictions contain factual errors.</p>\n\n",
                "matched_terms": [
                    "both",
                    "across",
                    "french",
                    "bleu",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented a scalable SLT framework that breaks the traditional close dependency between sign and spoken languages in training data and system development.\nBy aligning sign language videos with multilingual, multimodal sentence embeddings from <span class=\"ltx_text ltx_font_smallcaps\">SONAR</span>, our approach yields a\nlanguage-agnostic semantic representation that generalizes across both sign languages and spoken targets.\nThis reduces reliance on language-model priors and prioritizes visual grounding and SLT-specific grammar over surface-level text patterns.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "language",
                    "both",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments show that language-agnostic supervision enables robust translation even under sign&#8211;target mismatches.\nMultilingual text augmentations, combined with visual augmentation, improves performance on PHOENIX-2014T despite\nlimited data. Ablations further confirm the advantages of this approach in preserving semantic adequacy.</p>\n\n",
                "matched_terms": [
                    "phoenix2014t",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Current evaluation practices often emphasize surface overlap rather than meaning.\nFuture work should develop metrics aligned with semantic similarity and extend supervision to\nlow-resource sign languages and continuous signing in the wild.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "lowresource"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our main limitation lies in the visual feature extractor rather than the model architecture itself. We used a pre-existing visual block to avoid evaluation bias, which restricted us to datasets with compatible video settings (CSL-Daily and PHOENIX-2014T) and excluded larger corpora such as How2Sign or YouTube-ASL. As a result, our approach focuses on preserving semantics rather than maximizing exact sentence matches.</p>\n\n",
                "matched_terms": [
                    "phoenix2014t",
                    "settings"
                ]
            }
        ]
    },
    "S5.T4": {
        "source_file": "SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision",
        "caption": "Table 4: SONAR-SLT results for the Visual Block (VB) variants under Multilingual and Monolingual settings on PHOENIX-2014T and CSL-Daily. Metrics include BLEURT, BLEU, and ROUGE (RG); best scores per dataset/metric are in bold.",
        "body": "PHOENIX-2014T\nCSL-Daily\n\n\nType\nVariant\nBLEURT\nBLEU\nRG\nBLEURT\nBLEU\nRG\n\n\n\n\n\n\nMulti\n\n\nVB pretrained\n0.5230.523\n21.5221.52\n41.1041.10\n0.5610.561\n16.2316.23\n42.2942.29\n\n\nVB scratch\n0.5080.508\n21.3821.38\n42.0342.03\n0.4720.472\n14.6814.68\n42.1242.12\n\n\nVB frozen\n0.5160.516\n21.5621.56\n41.3941.39\n0.5490.549\n16.0616.06\n41.9541.95\n\n\n\n\nMono\n\n\nVB pretrained\n0.5450.545\n22.0122.01\n40.5240.52\n0.5580.558\n16.0716.07\n42.1342.13\n\n\nVB scratch\n0.4900.490\n19.7919.79\n39.9539.95\n0.4470.447\n14.1414.14\n40.5940.59\n\n\nVB frozen\n0.5200.520\n21.5621.56\n41.4441.44\n0.5290.529\n15.7015.70\n41.7941.79",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\">PHOENIX-2014T</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\">CSL-Daily</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\">Type</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\">Variant</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\">BLEURT</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\">BLEU</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\">RG</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\">BLEURT</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\">BLEU</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><span class=\"ltx_text ltx_font_bold\">RG</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" rowspan=\"3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:6.9pt;height:24.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:24.2pt;transform:translate(-8.6pt,-8.6pt) rotate(-90deg) ;\">\n<p class=\"ltx_p\">Multi</p>\n</span></div>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">VB pretrained</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"0.523\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m1\" intent=\":literal\"><semantics><mn>0.523</mn><annotation encoding=\"application/x-tex\">0.523</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"21.52\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m2\" intent=\":literal\"><semantics><mn>21.52</mn><annotation encoding=\"application/x-tex\">21.52</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"41.10\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m3\" intent=\":literal\"><semantics><mn>41.10</mn><annotation encoding=\"application/x-tex\">41.10</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"0.561\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m4\" intent=\":literal\"><semantics><mn>0.561</mn><annotation encoding=\"application/x-tex\">0.561</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"16.23\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m5\" intent=\":literal\"><semantics><mn>16.23</mn><annotation encoding=\"application/x-tex\">16.23</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"42.29\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m6\" intent=\":literal\"><semantics><mn>42.29</mn><annotation encoding=\"application/x-tex\">42.29</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">VB scratch</th>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"0.508\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m7\" intent=\":literal\"><semantics><mn>0.508</mn><annotation encoding=\"application/x-tex\">0.508</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"21.38\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m8\" intent=\":literal\"><semantics><mn>21.38</mn><annotation encoding=\"application/x-tex\">21.38</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"42.03\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m9\" intent=\":literal\"><semantics><mn>42.03</mn><annotation encoding=\"application/x-tex\">42.03</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"0.472\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m10\" intent=\":literal\"><semantics><mn>0.472</mn><annotation encoding=\"application/x-tex\">0.472</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"14.68\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m11\" intent=\":literal\"><semantics><mn>14.68</mn><annotation encoding=\"application/x-tex\">14.68</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"42.12\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m12\" intent=\":literal\"><semantics><mn>42.12</mn><annotation encoding=\"application/x-tex\">42.12</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">VB frozen</th>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"0.516\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m13\" intent=\":literal\"><semantics><mn>0.516</mn><annotation encoding=\"application/x-tex\">0.516</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"21.56\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m14\" intent=\":literal\"><semantics><mn>21.56</mn><annotation encoding=\"application/x-tex\">21.56</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"41.39\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m15\" intent=\":literal\"><semantics><mn>41.39</mn><annotation encoding=\"application/x-tex\">41.39</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"0.549\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m16\" intent=\":literal\"><semantics><mn>0.549</mn><annotation encoding=\"application/x-tex\">0.549</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"16.06\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m17\" intent=\":literal\"><semantics><mn>16.06</mn><annotation encoding=\"application/x-tex\">16.06</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"41.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m18\" intent=\":literal\"><semantics><mn>41.95</mn><annotation encoding=\"application/x-tex\">41.95</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t\" rowspan=\"3\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:6.8pt;height:24.7pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"width:24.7pt;transform:translate(-8.9pt,-8.9pt) rotate(-90deg) ;\">\n<p class=\"ltx_p\">Mono</p>\n</span></div>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">VB pretrained</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"0.545\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m19\" intent=\":literal\"><semantics><mn>0.545</mn><annotation encoding=\"application/x-tex\">0.545</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"22.01\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m20\" intent=\":literal\"><semantics><mn>22.01</mn><annotation encoding=\"application/x-tex\">22.01</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"40.52\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m21\" intent=\":literal\"><semantics><mn>40.52</mn><annotation encoding=\"application/x-tex\">40.52</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"0.558\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m22\" intent=\":literal\"><semantics><mn>0.558</mn><annotation encoding=\"application/x-tex\">0.558</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"16.07\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m23\" intent=\":literal\"><semantics><mn>16.07</mn><annotation encoding=\"application/x-tex\">16.07</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"42.13\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m24\" intent=\":literal\"><semantics><mn>42.13</mn><annotation encoding=\"application/x-tex\">42.13</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">VB scratch</th>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"0.490\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m25\" intent=\":literal\"><semantics><mn>0.490</mn><annotation encoding=\"application/x-tex\">0.490</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"19.79\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m26\" intent=\":literal\"><semantics><mn>19.79</mn><annotation encoding=\"application/x-tex\">19.79</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"39.95\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m27\" intent=\":literal\"><semantics><mn>39.95</mn><annotation encoding=\"application/x-tex\">39.95</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"0.447\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m28\" intent=\":literal\"><semantics><mn>0.447</mn><annotation encoding=\"application/x-tex\">0.447</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"14.14\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m29\" intent=\":literal\"><semantics><mn>14.14</mn><annotation encoding=\"application/x-tex\">14.14</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"40.59\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m30\" intent=\":literal\"><semantics><mn>40.59</mn><annotation encoding=\"application/x-tex\">40.59</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\">VB frozen</th>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"0.520\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m31\" intent=\":literal\"><semantics><mn>0.520</mn><annotation encoding=\"application/x-tex\">0.520</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"21.56\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m32\" intent=\":literal\"><semantics><mn>21.56</mn><annotation encoding=\"application/x-tex\">21.56</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"41.44\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m33\" intent=\":literal\"><semantics><mn>41.44</mn><annotation encoding=\"application/x-tex\">41.44</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"0.529\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m34\" intent=\":literal\"><semantics><mn>0.529</mn><annotation encoding=\"application/x-tex\">0.529</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"15.70\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m35\" intent=\":literal\"><semantics><mn>15.70</mn><annotation encoding=\"application/x-tex\">15.70</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-top:0.75pt;padding-bottom:0.75pt;\"><math alttext=\"41.79\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m36\" intent=\":literal\"><semantics><mn>41.79</mn><annotation encoding=\"application/x-tex\">41.79</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "phoenix2014t",
            "bleurt",
            "visual",
            "rouge",
            "datasetmetric",
            "multilingual",
            "best",
            "block",
            "results",
            "pretrained",
            "frozen",
            "mono",
            "bold",
            "scores",
            "monolingual",
            "metrics",
            "scratch",
            "under",
            "multi",
            "variants",
            "csldaily",
            "include",
            "settings",
            "sonarslt",
            "bleu",
            "type",
            "variant"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S5.T4\" title=\"Table 4 &#8227; &#8729; Observed gaps. &#8227; 5.1 Comparative Analysis &#8227; 5 Results and Analysis &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that pre-training on concatenated multi-sign corpora followed by monolingual fine-tuning proves most effective. In contrast, joint multi-sign fine-tuning risks resembling another full training run without yielding substantial gains. In our experiments, we first pre-train on the combined data and then fine-tune monolingually, consistent with&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hamidullah et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib11\" title=\"\">2024</a>)</cite>; post-fine-tuning performance remains largely unchanged (see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S5.T4\" title=\"Table 4 &#8227; &#8729; Observed gaps. &#8227; 5.1 Comparative Analysis &#8227; 5 Results and Analysis &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, mono vs. multilingual setup). Differences in dataset capture conditions still matter&#8212;for example, methods that rely solely on global visual features can underperform when fine-grained articulations, such as hand or facial details, are crucial. Pipelines that integrate keypoints with extensive preprocessing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gueuwou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib8\" title=\"\">2025b</a>)</cite> help mitigate such losses and achieve stronger results.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Sign language translation (SLT) is typically trained with text in a single spoken language, which limits scalability and cross-language generalization. Earlier approaches have replaced gloss supervision with text-based sentence embeddings, but up to now, these remain tied to a specific language and modality. In contrast, here we employ language-agnostic, multimodal embeddings trained on text and speech from multiple languages to supervise SLT, enabling direct multilingual translation.\nTo address data scarcity, we propose a coupled augmentation method that combines multilingual target augmentations (i.e. translations into many languages) with video-level perturbations, improving model robustness. Experiments show consistent BLEURT gains over text-only sentence embedding supervision, with larger improvements in low-resource settings. Our results demonstrate that language-agnostic embedding supervision, combined with coupled augmentation, provides a scalable and semantically robust alternative to traditional SLT training.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We release the code, models, and features to facilitate further research. Github repository: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/DFKI-SignLanguage/sonar-slt.git\" title=\"\">https://github.com/DFKI-SignLanguage/sonar-slt.git</a>; Huggingface: <a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/mtmlt\" title=\"\">https://huggingface.co/mtmlt</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "results",
                    "settings",
                    "multilingual",
                    "bleurt"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_bold\">SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\n</span>\n</p>\n\n",
                "matched_terms": [
                    "sonarslt",
                    "multilingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent work by <cite class=\"ltx_cite ltx_citemacro_citet\">Hamidullah et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib11\" title=\"\">2024</a>)</cite> has reduced the reliance on glosses by supervising SLT with <em class=\"ltx_emph ltx_font_italic\">text-based sentence embeddings</em>. This yields better semantic alignment, but the embeddings remain modality-specific and typically require dataset-specific fine-tuning. Furthermore, compared to large pre-trained models that exploit vast text corpora, these text-only embeddings show limited cross-lingual transfer and reduced robustness. This raises the key question: <em class=\"ltx_emph ltx_font_italic\">Can language-agnostic, multimodal sentence embedding supervision replace text-only alignment in SLT?</em> We hypothesize that <span class=\"ltx_text ltx_font_bold\">language-agnostic, multimodal sentence embeddings</span> can reduce the residual dependence on text. Concretely, we build on <span class=\"ltx_text ltx_font_smallcaps\">SONAR</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Duquenne et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib4\" title=\"\">2023</a>)</cite>, a pretrained multilingual and multimodal embedding space that jointly represents text and speech. SONAR embeddings are claimed to be language-agnostic.\nOur approach aligns sign representations directly with language-agnostic semantic vectors, thereby decoupling supervision from any specific spoken language and removing the need for glosses. Our model integrates multiple modalities and supports direct supervision across all 200 languages covered by <span class=\"ltx_text ltx_font_smallcaps\">SONAR</span> (see Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). In contrast to prior systems that relied on additional stages or separate models for multi-target translation, our method enables <em class=\"ltx_emph ltx_font_italic\">direct</em> translation into multiple languages within a single model.</p>\n\n",
                "matched_terms": [
                    "multilingual",
                    "pretrained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Traditional SLT systems rely on glosses &#8212;textual labels that represent signs&#8212; as an intermediate representation. MSKA-SLT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib6\" title=\"\">2025</a>)</cite> remains a strong baseline using glosses, reporting <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>29 BLEU on PHOENIX-2014T <cite class=\"ltx_cite ltx_citemacro_citep\">(Camgoz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib1\" title=\"\">2018</a>)</cite>. However, glosses are neither universal nor standardized: they are tightly coupled to specific languages, cultures, and regions. Moreover, producing gloss annotations is highly time-consuming, requiring expert linguistic knowledge <cite class=\"ltx_cite ltx_citemacro_citep\">(M&#252;ller et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib16\" title=\"\">2023b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "phoenix2014t",
                    "bleu"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Complementary approaches leverage large language models (LLMs). SignLLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib5\" title=\"\">2024</a>)</cite> discretizes videos into tokens and prompts a frozen LLM; Sign2GPT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib26\" title=\"\">2024</a>)</cite> feeds pseudo-glosses to XGLM, reporting <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>22 BLEU on PHOENIX-2014T and <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>15 BLEU on CSL-Daily. SpaMo&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hwang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib12\" title=\"\">2025</a>)</cite> employs a straightforward approach that extracts spatial and motion features from sign language videos and utilizes a low-rank adapter to fine-tune an LLM for sign language translation. <cite class=\"ltx_cite ltx_citemacro_citet\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib2\" title=\"\">2024</a>)</cite> introduced FLa-LLM, a two-stage, gloss-free framework that first pre-trains the visual encoder and then fine-tunes a pre-trained LLM for the downstream SLT task. These methods inherit LLM fluency but are largely monolingual and require substantial tokenization and training overhead. In contrast, our PEFT-based SONAR adapters maintain multilinguality without retraining a large decoder on discretized video tokens. More recent work has explored large-scale pre-training to improve sign language understanding, with Uni-Sign <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib13\" title=\"\">2025</a>)</cite> proposing a unified generative framework that treats downstream tasks as SLT and incorporates prior-guided fusion.</p>\n\n",
                "matched_terms": [
                    "csldaily",
                    "phoenix2014t",
                    "visual",
                    "pretrained",
                    "bleu",
                    "monolingual",
                    "frozen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multilingual SLT models also remain in their early stages. MLSLT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yin et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib29\" title=\"\">2022</a>)</cite> covers ten European sign languages via a routing mechanism, while JWSign&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gueuwou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib9\" title=\"\">2023</a>)</cite> scales to 98 languages with language-ID tokens.\nMore recently, Sign2(LID+Text)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib22\" title=\"\">2025</a>)</cite> incorporated token-level language identification with a CTC loss, achieving competitive results. In addition, <cite class=\"ltx_cite ltx_citemacro_citet\">Yazdani et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib27\" title=\"\">2025a</a>)</cite> explored continual learning for multilingual SLT.\nRecent work applies heavy pre-processing <cite class=\"ltx_cite ltx_citemacro_citep\">(Gueuwou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib8\" title=\"\">2025b</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib7\" title=\"\">a</a>)</cite>, sometimes obscuring whether improvements arise from better SLT modeling or dataset-specific engineering. Both gloss-based and gloss-free methods perform best when signer distance, camera setup, and motion characteristics closely match training conditions.</p>\n\n",
                "matched_terms": [
                    "results",
                    "best",
                    "multilingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_bold\">SONAR-SLT</span>, a modular SLT framework that decouples\n<em class=\"ltx_emph ltx_font_italic\">semantic understanding</em> from <em class=\"ltx_emph ltx_font_italic\">text generation</em>.\nAs illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the system first maps an input sign language video into a multilingual, multimodal semantic space, and then (optionally) decodes from this space into a chosen spoken language.\nThis design allows training on heterogeneous sign language datasets, supports multilingual supervision, and removes the need for gloss annotations. A detailed architecture is presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#A1.SS1\" title=\"A.1 Detailed Architecture &#8227; Appendix A Appendix &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a> and summarized in the next subsections.</p>\n\n",
                "matched_terms": [
                    "sonarslt",
                    "multilingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first stage maps raw video frames into a compact visual embedding.\nLet <math alttext=\"x=(f_{1},\\dots,f_{T})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mi>x</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>f</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msub><mi>f</mi><mi>T</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x=(f_{1},\\dots,f_{T})</annotation></semantics></math> denote a sign language video of <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> frames.\nWe extract per-frame spatial features <math alttext=\"\\mathbf{s}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119852;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{s}_{t}</annotation></semantics></math> with ViT <cite class=\"ltx_cite ltx_citemacro_citep\">(Dosovitskiy et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib3\" title=\"\">2020</a>)</cite> and spatio-temporal\nmotion features <math alttext=\"\\mathbf{m}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119846;</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">\\mathbf{m}_{t}</annotation></semantics></math> with VideoMAE <cite class=\"ltx_cite ltx_citemacro_citep\">(Tong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib23\" title=\"\">2022</a>)</cite>. These are fused through a lightweight block (1D Conv followed by a multi-layer perceptron)\n<math alttext=\"\\mathcal{F}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8497;</mi><annotation encoding=\"application/x-tex\">\\mathcal{F}</annotation></semantics></math>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hwang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib12\" title=\"\">2025</a>)</cite>:</p>\n\n",
                "matched_terms": [
                    "visual",
                    "block"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Next, we align sign-derived embeddings with multilingual textual embeddings.\nWe adopt a pretrained multilingual, multimodal sentence encoder <math alttext=\"\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><annotation encoding=\"application/x-tex\">\\mathcal{E}</annotation></semantics></math>\n(i.e., <span class=\"ltx_text ltx_font_smallcaps\">SONAR</span>).\nGiven a reference sentence <math alttext=\"y\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.p1.m2\" intent=\":literal\"><semantics><mi>y</mi><annotation encoding=\"application/x-tex\">y</annotation></semantics></math>, we obtain its semantic embedding:</p>\n\n",
                "matched_terms": [
                    "multilingual",
                    "pretrained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we leverage the language-agnostic semantic space for dataset fusion.\nBecause supervision is defined independently of any specific spoken language, videos from different sign languages can be trained jointly with textual supervision in <em class=\"ltx_emph ltx_font_italic\">any</em> available language. For example, German sign language videos annotated in German can\nbe re-aligned with English, French, or Chinese translations via <math alttext=\"\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><annotation encoding=\"application/x-tex\">\\mathcal{E}</annotation></semantics></math>, allowing unified training across datasets such as PHOENIX-2014T and CSL-Daily.\nThis enables direct multi-target translation without glosses and facilitates fusion of heterogeneous sign-language corpora.</p>\n\n",
                "matched_terms": [
                    "csldaily",
                    "phoenix2014t"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our method following <cite class=\"ltx_cite ltx_citemacro_citep\">(M&#252;ller et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib17\" title=\"\">2022</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib15\" title=\"\">2023a</a>)</cite>,\nusing BLEU<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><span class=\"ltx_text ltx_font_typewriter\">BLEU|nrefs:1|bs:1000|seed:16|case: mixed|eff:no|tok:13a|smooth:exp|version:2.4.0</span></span></span></span>\n(via SacreBLEU <cite class=\"ltx_cite ltx_citemacro_citep\">(Post, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib19\" title=\"\">2018</a>)</cite>) for lexical overlap,\nROUGE <cite class=\"ltx_cite ltx_citemacro_citep\">(Lin, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib14\" title=\"\">2004</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><span class=\"ltx_text ltx_font_typewriter\">ROUGE|L|nrefs:1|tok:13a|case:mixed|version:1.5.5</span></span></span></span>\nfor recall-oriented <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram overlap,\nand BLEURT <cite class=\"ltx_cite ltx_citemacro_citep\">(Sellam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib21\" title=\"\">2020</a>)</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>BLEURT v0.0.2 using checkpoint BLEURT-20.</span></span></span>\nfor semantic quality.</p>\n\n",
                "matched_terms": [
                    "rouge",
                    "bleurt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our method against several strong recent state-of-the-art systems within the gloss-free paradigm. CSGCR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib30\" title=\"\">2021</a>)</cite> improves SLT accuracy and fluency through three modules: word existence verification, conditional sentence generation, and cross-modal re-ranking for richer grammatical representations. GFSLT-VLP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib31\" title=\"\">2023</a>)</cite> leverages vision&#8211;language pretraining, while FLa-LLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib2\" title=\"\">2024</a>)</cite> adopts a two-stage gloss-free pipeline that first pre-trains the visual encoder and then fine-tunes a pre-trained LLM for SLT. Sign2GPT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib26\" title=\"\">2024</a>)</cite> maps visual inputs to pseudo-gloss sequences and decodes them with GPT-style language modeling, whereas SignLLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib5\" title=\"\">2024</a>)</cite> discretizes sign features into visual tokens to prompt a frozen LLM. SEM-SLT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hamidullah et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib11\" title=\"\">2024</a>)</cite> aligns sign language videos with sentence embeddings and serves as the foundation of our work. For multilingual settings, Sign2(LID+Text)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib22\" title=\"\">2025</a>)</cite> combines token-level sign language identification with a CTC objective to generate spoken text.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "settings",
                    "pretrained",
                    "multilingual",
                    "frozen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These features are then fused via the visual fusion block <math alttext=\"\\mathcal{F}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8497;</mi><annotation encoding=\"application/x-tex\">\\mathcal{F}</annotation></semantics></math> from SpaMo to yield a joint representation <math alttext=\"h_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><msub><mi>h</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">h_{t}</annotation></semantics></math> for each timestep.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "block"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train the visual block using LoRA with:</p>\n\n",
                "matched_terms": [
                    "visual",
                    "block"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The original SONAR pools by running a shallow decoder: it feeds a special token (the EOS id in <span class=\"ltx_text ltx_font_typewriter\">M200M100</span>) as input and uses the encoder outputs as hidden states; the first decoder output is taken as the sentence embedding. During the Visual Block training, we adopt this approach with a shallow decoder initialized from the first three SONAR decoder layers and train it only for pooling. This supplies language context during pooling, while the incoming features themselves are language-agnostic (from another modality). Text generation is then conditioned on the target language.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "block"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt the best-performing visual representation strategies reported in prior work, noting that optimal choices vary across datasets. To ensure comparability in our multi&#8211;sign language experiments, we restrict evaluation to datasets with similar video settings and select the strongest corresponding model. The SpaMo Visual Block performs best with global, high-quality cues e.g., high-resolution videos with a moderate signer&#8211;camera distance (CSL-Daily), or lower-resolution videos where the signer is close and centered (PHOENIX-2014T). Consequently, we conduct multilingual experiments on CSL-Daily and PHOENIX-2014T.</p>\n\n",
                "matched_terms": [
                    "csldaily",
                    "phoenix2014t",
                    "visual",
                    "best",
                    "settings",
                    "block",
                    "multilingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train the end-to-end translation system (with the Visual Block or the fused spatial+motion features) using the <span class=\"ltx_text ltx_font_bold\">same LoRA configuration</span> as above.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "block"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PHOENIX-2014-T (monolingual):</span> constant LR (we found it more stable)</p>\n\n",
                "matched_terms": [
                    "phoenix2014t",
                    "monolingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Video augmentation.</span>\nCoupled with the target-language augmentation, we also perturb the input videos so that each training instance\nis presented with both linguistic and visual variability.\nAt each iteration, one augmented variant is sampled.\nIn this work we restrict ourselves to:</p>\n\n",
                "matched_terms": [
                    "visual",
                    "variant"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We compare our approach with other gloss-free methods on both PHOENIX-2014T and CSL-Daily datasets in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S4.T2\" title=\"Table 2 &#8227; &#8729; Sentence embedding pooling. &#8227; 4.4 Implementation Details &#8227; 4 Experiments &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nOur method shows a clear advantage on the semantics-oriented BLEURT metric. It reaches a <span class=\"ltx_text ltx_font_bold\">BLEURT of 0.545</span>, outperforming the sentence-based supervision model using text-only sentence embedding (SEM-SLT). BLEURT uses a BERT-based scorer and is designed to capture meaning and fluency, unlike BLEU and ROUGE, which primarily measure <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram overlap. Moreover, our model outperforms previous monolingual and multilingual systems on CSL-Daily in terms of BLEU and achieves comparable results on PHOENIX-2014T.</p>\n\n",
                "matched_terms": [
                    "csldaily",
                    "phoenix2014t",
                    "bleurt",
                    "rouge",
                    "results",
                    "bleu",
                    "multilingual",
                    "monolingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate target-side augmentation, where language translations are included in training. Results for both low- and high-resource languages on PHOENIX-2014T are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S5.T3\" title=\"Table 3 &#8227; &#8729; Observed gaps. &#8227; 5.1 Comparative Analysis &#8227; 5 Results and Analysis &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. In our experiments, we augmented the target set in training with three high-resource languages&#8212;French, Spanish, and English&#8212;while the model was evaluated on other unseen languages. Using this augmented target set yields a modest improvement over training with a single target language. However, we observe a gap in performance between high- and low-resource languages, which primarily stems from lower reference translation quality in the low-resource languages.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Reference translations were obtained using <span class=\"ltx_text ltx_font_typewriter\">facebook/nllb-200-distilled-600M</span> model.</span></span></span> The narrow domain of PHOENIX-2014T can also introduce dataset-specific idiosyncrasies, complicating fair comparisons.</p>\n\n",
                "matched_terms": [
                    "results",
                    "phoenix2014t"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The effect of sentence-embedding supervision is strongest when the Visual Block is still learning feature representations.\nOnce the block has converged&#8212;or is pretrained&#8212;the additional impact of cosine or MSE objectives diminishes.\nThis occurs because cross-entropy loss often remains relatively high (above <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>&#8211;<math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>),\nwhile MSE rapidly falls to <math alttext=\"\\mathcal{O}(10^{-5})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119978;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{O}(10^{-5})</annotation></semantics></math> and cosine similarity saturates around <math alttext=\"\\sim 0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">\\sim 0.3</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "block"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, introducing the auto-encoding loss provides a second cross-entropy signal,\nwhich exerts a stronger influence on the Visual Block.\nHere, intermediate supervision continues to be beneficial, and the auto-encoding objective itself\naccelerates convergence.\nWe consistently observed this effect in CSL-Daily and in the augmented translation setup on PHOENIX-2014T.</p>\n\n",
                "matched_terms": [
                    "csldaily",
                    "phoenix2014t",
                    "visual",
                    "block"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S5.T5\" title=\"Table 5 &#8227; &#8729; Semantic analysis. &#8227; 5.3 Qualitative and Semantic Error Analysis &#8227; 5 Results and Analysis &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows examples of two contrasting outcomes: cases where the model accurately captures the intended meaning and cases where it fails.\nWhen contextual understanding is incomplete, the decoder frequently compensates by generating fluent continuations via next-token prediction.\nThis behavior is characteristic of SLT systems that rely on pretrained language models as decoders: they can mask weaknesses in semantic grounding by producing outputs that are coherent but only partially faithful to the source.\nAs a result, improvements in BLEU may reflect the decoder&#8217;s ability to recover plausible sentences rather than true gains in sign-to-text comprehension.\nTherefore, exact sequence matching metrics such as BLEU are insufficient and in some cases misleading for evaluating translation quality in SLT.</p>\n\n",
                "matched_terms": [
                    "bleu",
                    "metrics",
                    "pretrained"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Surface-form scores vs. meaning preservation.\nWe observe a systematic mismatch between surface-form metrics (e.g., BLEU) and semantic adequacy (BLEURT) across both German and French.\nOutputs with only moderate <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram overlap can still be semantically faithful, while some high-scoring predictions contain factual errors.</p>\n\n",
                "matched_terms": [
                    "bleu",
                    "scores",
                    "metrics",
                    "bleurt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Evaluation and model development for multilingual SLT should be language-aware. In practice, one should combine semantics-focused metrics with targeted, language-specific checks (e.g., temporals and agreement in French; word order and compounding in German) to obtain fair comparisons and actionable diagnostics.</p>\n\n",
                "matched_terms": [
                    "multilingual",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented a scalable SLT framework that breaks the traditional close dependency between sign and spoken languages in training data and system development.\nBy aligning sign language videos with multilingual, multimodal sentence embeddings from <span class=\"ltx_text ltx_font_smallcaps\">SONAR</span>, our approach yields a\nlanguage-agnostic semantic representation that generalizes across both sign languages and spoken targets.\nThis reduces reliance on language-model priors and prioritizes visual grounding and SLT-specific grammar over surface-level text patterns.</p>\n\n",
                "matched_terms": [
                    "visual",
                    "multilingual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments show that language-agnostic supervision enables robust translation even under sign&#8211;target mismatches.\nMultilingual text augmentations, combined with visual augmentation, improves performance on PHOENIX-2014T despite\nlimited data. Ablations further confirm the advantages of this approach in preserving semantic adequacy.</p>\n\n",
                "matched_terms": [
                    "phoenix2014t",
                    "under",
                    "multilingual",
                    "visual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our main limitation lies in the visual feature extractor rather than the model architecture itself. We used a pre-existing visual block to avoid evaluation bias, which restricted us to datasets with compatible video settings (CSL-Daily and PHOENIX-2014T) and excluded larger corpora such as How2Sign or YouTube-ASL. As a result, our approach focuses on preserving semantics rather than maximizing exact sentence matches.</p>\n\n",
                "matched_terms": [
                    "csldaily",
                    "phoenix2014t",
                    "visual",
                    "settings",
                    "block"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Additional translation examples for CSL-Daily and PHOENIX-2014T are provided in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#A1.T6\" title=\"Table 6 &#8227; A.3 Additional Qualitative Results &#8227; Appendix A Appendix &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#A1.T7\" title=\"Table 7 &#8227; A.3 Additional Qualitative Results &#8227; Appendix A Appendix &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, respectively.</p>\n\n",
                "matched_terms": [
                    "csldaily",
                    "phoenix2014t"
                ]
            }
        ]
    },
    "S5.T5": {
        "source_file": "SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision",
        "caption": "Table 5: German and French examples —two semantically near correct paraphrases (green) and one semantically incorrect output (red), with English translations.",
        "body": "French (weather domain)\n\n\n\n\n\n\n\n\n\\rowcolorgreen!10 Ref (FR): vingt-huit août.\n\n\n\n\n\n\n\\rowcolorgreen!10 EN: Twenty-eighth of August.\n\n\n\n\n\n\n\\rowcolorgreen!10 Pred (FR): vingt-cinq novembre.\n\n\n\n\n\n\n\\rowcolorgreen!10 EN: Twenty-fifth of November.\n\n\n\n\n\n\n\\rowcolorgreen!10 Ref (FR): Des rafales orageuses de l’ouest.\n\n\n\n\n\n\n\\rowcolorgreen!10 EN: Stormy gusts from the west.\n\n\n\n\n\n\n\\rowcolorgreen!10 Pred (FR): Des rafales orageuses sont possibles.\n\n\n\n\n\n\n\\rowcolorgreen!10 EN: Stormy gusts are possible.\n\n\n\n\n\n\n\\rowcolorred!10 Ref (FR): Risque d’inondation.\n\n\n\n\n\n\n\\rowcolorred!10 EN: Risk of flooding.\n\n\n\n\n\n\n\\rowcolorred!10 Pred (FR): Avertissements météorologiques violents.\n\n\n\n\n\n\n\\rowcolorred!10 EN: Severe weather warnings.",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_text ltx_font_bold\">French (weather domain)</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>green!10 <span class=\"ltx_text ltx_font_bold\">Ref (FR):</span> vingt-huit ao&#251;t.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>green!10 <span class=\"ltx_text ltx_font_italic\">EN: Twenty-eighth of August.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>green!10 <span class=\"ltx_text ltx_font_bold\">Pred (FR):</span> vingt-cinq novembre.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>green!10 <span class=\"ltx_text ltx_font_italic\">EN: Twenty-fifth of November.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>green!10 <span class=\"ltx_text ltx_font_bold\">Ref (FR):</span> Des rafales orageuses de l&#8217;ouest.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>green!10 <span class=\"ltx_text ltx_font_italic\">EN: Stormy gusts from the west.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>green!10 <span class=\"ltx_text ltx_font_bold\">Pred (FR):</span> Des rafales orageuses sont possibles.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>green!10 <span class=\"ltx_text ltx_font_italic\">EN: Stormy gusts are possible.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>red!10 <span class=\"ltx_text ltx_font_bold\">Ref (FR):</span> Risque d&#8217;inondation.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>red!10 <span class=\"ltx_text ltx_font_italic\">EN: Risk of flooding.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>red!10 <span class=\"ltx_text ltx_font_bold\">Pred (FR):</span> Avertissements m&#233;t&#233;orologiques violents.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>red!10 <span class=\"ltx_text ltx_font_italic\">EN: Severe weather warnings.</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "possibles",
            "severe",
            "—two",
            "one",
            "twentyeighth",
            "pred",
            "french",
            "west",
            "l’ouest",
            "vingthuit",
            "violents",
            "translations",
            "vingtcinq",
            "warnings",
            "twentyfifth",
            "météorologiques",
            "rafales",
            "paraphrases",
            "domain",
            "green",
            "des",
            "risque",
            "august",
            "flooding",
            "correct",
            "novembre",
            "near",
            "gusts",
            "incorrect",
            "november",
            "rowcolorred10",
            "possible",
            "risk",
            "examples",
            "weather",
            "août",
            "orageuses",
            "from",
            "semantically",
            "avertissements",
            "english",
            "rowcolorgreen10",
            "german",
            "ref",
            "stormy",
            "sont",
            "output",
            "red",
            "d’inondation"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S5.T5\" title=\"Table 5 &#8227; &#8729; Semantic analysis. &#8227; 5.3 Qualitative and Semantic Error Analysis &#8227; 5 Results and Analysis &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows examples of two contrasting outcomes: cases where the model accurately captures the intended meaning and cases where it fails.\nWhen contextual understanding is incomplete, the decoder frequently compensates by generating fluent continuations via next-token prediction.\nThis behavior is characteristic of SLT systems that rely on pretrained language models as decoders: they can mask weaknesses in semantic grounding by producing outputs that are coherent but only partially faithful to the source.\nAs a result, improvements in BLEU may reflect the decoder&#8217;s ability to recover plausible sentences rather than true gains in sign-to-text comprehension.\nTherefore, exact sequence matching metrics such as BLEU are insufficient and in some cases misleading for evaluating translation quality in SLT.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Semantically near correct and correct paraphrases (German).</span>\nAs illustrated by the green-highlighted examples in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S5.T5\" title=\"Table 5 &#8227; &#8729; Semantic analysis. &#8227; 5.3 Qualitative and Semantic Error Analysis &#8227; 5 Results and Analysis &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, incorrect lexical or numeric substitutions leave most of the remaining meaning intact\n(e.g., date shifts: <span class=\"ltx_text ltx_font_italic\">&#8220;Sonntag, den neunzehnten Dezember&#8221;</span> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;Sonntag, den siebzehnten August&#8221;</span>;\ntemperature adjustments: <span class=\"ltx_text ltx_font_italic\">&#8220;sechs Grad an den Alpen&#8221;</span> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;neun Grad am Alpenrand&#8221;</span>).\nWe also observe benign stylistic reformulations (<span class=\"ltx_text ltx_font_italic\">&#8220;es gelten entsprechende Warnungen&#8221;</span> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;es bestehen Unwetterwarnungen&#8221;</span>)\nand word-order changes without semantic effect (<span class=\"ltx_text ltx_font_italic\">&#8220;aus S&#252;dwest bis West&#8221;</span> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;aus West bis S&#252;dost&#8221;</span>).</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Semantically incorrect outputs, true errors (French and German).</span>\nThe red-highlighted rows in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S5.T5\" title=\"Table 5 &#8227; &#8729; Semantic analysis. &#8227; 5.3 Qualitative and Semantic Error Analysis &#8227; 5 Results and Analysis &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> illustrate errors such as topic drift (predicting wind instead of temperature),\nincorrect locations (<span class=\"ltx_text ltx_font_italic\">&#8220;H&#246;henlagen S&#252;ddeutschlands&#8221;</span> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;K&#252;sten&#8221;</span>;\n<span class=\"ltx_text ltx_font_italic\">&#8220;sud-est&#8221;</span> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;nord&#8221;</span>),\nsystem inversions (<span class=\"ltx_text ltx_font_italic\">&#8220;Hoch&#8221;</span> <math alttext=\"\\leftrightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p4.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8596;</mo><annotation encoding=\"application/x-tex\">\\leftrightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;Tief&#8221;</span>; <span class=\"ltx_text ltx_font_italic\">&#8220;haut&#8221;</span> <math alttext=\"\\leftrightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p4.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8596;</mo><annotation encoding=\"application/x-tex\">\\leftrightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;profonde&#8221;</span>),\nhallucinated entities, or incorrect hazard categories\n(<span class=\"ltx_text ltx_font_italic\">&#8220;Risque d&#8217;inondation&#8221;</span> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p4.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;Avertissements m&#233;t&#233;orologiques violents&#8221;</span>).</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Sign language translation (SLT) is typically trained with text in a single spoken language, which limits scalability and cross-language generalization. Earlier approaches have replaced gloss supervision with text-based sentence embeddings, but up to now, these remain tied to a specific language and modality. In contrast, here we employ language-agnostic, multimodal embeddings trained on text and speech from multiple languages to supervise SLT, enabling direct multilingual translation.\nTo address data scarcity, we propose a coupled augmentation method that combines multilingual target augmentations (i.e. translations into many languages) with video-level perturbations, improving model robustness. Experiments show consistent BLEURT gains over text-only sentence embedding supervision, with larger improvements in low-resource settings. Our results demonstrate that language-agnostic embedding supervision, combined with coupled augmentation, provides a scalable and semantically robust alternative to traditional SLT training.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We release the code, models, and features to facilitate further research. Github repository: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/DFKI-SignLanguage/sonar-slt.git\" title=\"\">https://github.com/DFKI-SignLanguage/sonar-slt.git</a>; Huggingface: <a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/mtmlt\" title=\"\">https://huggingface.co/mtmlt</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "from",
                    "semantically",
                    "translations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enforce language-agnostic supervision, each reference sentence is paired with <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math>\ntranslations <math alttext=\"\\{y^{(k)}\\}_{k=1}^{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msup><mi>y</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><annotation encoding=\"application/x-tex\">\\{y^{(k)}\\}_{k=1}^{K}</annotation></semantics></math> (from the embedding decoder). At each iteration, one translation <math alttext=\"y^{(k)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msup><mi>y</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">y^{(k)}</annotation></semantics></math> is sampled\nand encoded as <math alttext=\"\\mathbf{s}=\\mathcal{E}_{txt}(y^{(k)})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#119852;</mi><mo>=</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>y</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{s}=\\mathcal{E}_{txt}(y^{(k)})</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "from",
                    "one",
                    "translations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then decode into natural language from the semantic embedding.\nA pretrained decoder <math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math> from <span class=\"ltx_text ltx_font_smallcaps\">SONAR</span> generates text from a semantic vector\nand a target language token <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>. Conditioned on the sign-derived and semantically text-aligned (Section &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S3.SS3\" title=\"3.3 Semantic Alignment &#8227; 3 Methodology &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>) embedding <math alttext=\"\\mathbf{z}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m3\" intent=\":literal\"><semantics><mi>&#119859;</mi><annotation encoding=\"application/x-tex\">\\mathbf{z}</annotation></semantics></math>,\nthe decoder is trained with teacher forcing:</p>\n\n",
                "matched_terms": [
                    "from",
                    "semantically"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we leverage the language-agnostic semantic space for dataset fusion.\nBecause supervision is defined independently of any specific spoken language, videos from different sign languages can be trained jointly with textual supervision in <em class=\"ltx_emph ltx_font_italic\">any</em> available language. For example, German sign language videos annotated in German can\nbe re-aligned with English, French, or Chinese translations via <math alttext=\"\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><annotation encoding=\"application/x-tex\">\\mathcal{E}</annotation></semantics></math>, allowing unified training across datasets such as PHOENIX-2014T and CSL-Daily.\nThis enables direct multi-target translation without glosses and facilitates fusion of heterogeneous sign-language corpora.</p>\n\n",
                "matched_terms": [
                    "english",
                    "german",
                    "french",
                    "translations",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PHOENIX-2014T</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Camgoz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib1\" title=\"\">2018</a>)</cite>: German Sign Language (DGS) weather forecast videos with parallel German text.</p>\n\n",
                "matched_terms": [
                    "weather",
                    "german"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The original SONAR pools by running a shallow decoder: it feeds a special token (the EOS id in <span class=\"ltx_text ltx_font_typewriter\">M200M100</span>) as input and uses the encoder outputs as hidden states; the first decoder output is taken as the sentence embedding. During the Visual Block training, we adopt this approach with a shallow decoder initialized from the first three SONAR decoder layers and train it only for pooling. This supplies language context during pooling, while the incoming features themselves are language-agnostic (from another modality). Text generation is then conditioned on the target language.</p>\n\n",
                "matched_terms": [
                    "from",
                    "output"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text augmentation.</span> To expand the datasets using NLLB <cite class=\"ltx_cite ltx_citemacro_citep\">(NLLB Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib18\" title=\"\">2024</a>)</cite>, we machine-translate the target texts into three high-resource languages (English, French and Spanish) using the <span class=\"ltx_text ltx_font_typewriter\">facebook/nllb-200-distilled-600M</span> model.</p>\n\n",
                "matched_terms": [
                    "english",
                    "french"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe a decrease in BLEU compared to the SEM-SLT system, which is expected since our model is not fine-tuned on sign-language text. Our language-agnostic, sentence embedding-based supervision preserves semantics without requiring fine-tuning on specific dataset: it goes beyond surface <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram matching to produce translations that are contextually accurate, grammatically correct, and cross-lingually robust. Part of the remaining gap stems from dataset capture conditions. Our feature extractor <cite class=\"ltx_cite ltx_citemacro_citep\">(Hwang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib12\" title=\"\">2025</a>)</cite> is tuned for global cues and can be less accurate in cases where fine-grained articulations, such as facial expressions and finger movements, are critical. Recent top systems address this with keypoint-based representations and extensive preprocessing <cite class=\"ltx_cite ltx_citemacro_citep\">(Gueuwou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib8\" title=\"\">2025b</a>)</cite>, which help preserve these fine-grained details.</p>\n\n",
                "matched_terms": [
                    "correct",
                    "from",
                    "translations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate target-side augmentation, where language translations are included in training. Results for both low- and high-resource languages on PHOENIX-2014T are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S5.T3\" title=\"Table 3 &#8227; &#8729; Observed gaps. &#8227; 5.1 Comparative Analysis &#8227; 5 Results and Analysis &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. In our experiments, we augmented the target set in training with three high-resource languages&#8212;French, Spanish, and English&#8212;while the model was evaluated on other unseen languages. Using this augmented target set yields a modest improvement over training with a single target language. However, we observe a gap in performance between high- and low-resource languages, which primarily stems from lower reference translation quality in the low-resource languages.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Reference translations were obtained using <span class=\"ltx_text ltx_font_typewriter\">facebook/nllb-200-distilled-600M</span> model.</span></span></span> The narrow domain of PHOENIX-2014T can also introduce dataset-specific idiosyncrasies, complicating fair comparisons.</p>\n\n",
                "matched_terms": [
                    "from",
                    "domain",
                    "translations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Language-specific tendencies.</span> We deep into the analysis of two languages: German, a language trained with original data and French, a language trained via machine translation augmentation.</p>\n\n",
                "matched_terms": [
                    "german",
                    "french"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">German:</span> Errors often arise from compound nouns, flexible word order, and embedded clauses, leading to partial omissions,\nattribute reordering, or unnatural compounds. When alignment is uncertain, the model may insert generic stock phrases or repetitions.</p>\n\n",
                "matched_terms": [
                    "from",
                    "german"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">French:</span> Our analysis shows more frequent noun substitutions, agreement mismatches, and text modality shifts (e.g., hedging with <span class=\"ltx_text ltx_font_italic\">&#8220;sont possibles&#8221;</span>).\nRegister differences from determiners or prepositions are also common. Incorrect date and numeric substitutions occur more frequently than in German,\nlikely due to segmentation differences in temporal expressions.</p>\n\n",
                "matched_terms": [
                    "from",
                    "incorrect",
                    "german",
                    "french"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Surface-form scores vs. meaning preservation.\nWe observe a systematic mismatch between surface-form metrics (e.g., BLEU) and semantic adequacy (BLEURT) across both German and French.\nOutputs with only moderate <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram overlap can still be semantically faithful, while some high-scoring predictions contain factual errors.</p>\n\n",
                "matched_terms": [
                    "german",
                    "semantically",
                    "french"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Semantically near correct and correct paraphrases (French).</span>\nSimilarly, the first green-highlighted examples show structural or modality shifts that preserve much of the remaining meaning, such as date substitutions\n(<span class=\"ltx_text ltx_font_italic\">&#8220;vingt-huit ao&#251;t&#8221;</span> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;vingt-cinq novembre&#8221;</span>),\nmodality changes (<span class=\"ltx_text ltx_font_italic\">&#8220;Des rafales orageuses de l&#8217;ouest&#8221;</span> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;Des rafales orageuses sont possibles&#8221;</span>),\nor expanded phrasing (<span class=\"ltx_text ltx_font_italic\">&#8220;&#201;galement orages sur la mer du Nord&#8221;</span> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;Il y a &#233;galement des orages sur la mer du Nord&#8221;</span>).</p>\n\n",
                "matched_terms": [
                    "rafales",
                    "paraphrases",
                    "near",
                    "des",
                    "french",
                    "examples",
                    "orageuses",
                    "sont",
                    "correct",
                    "semantically"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, as in other machine translation tasks, <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p5.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram metrics penalize near or even fully legitimate paraphrases and sometimes fail to capture serious factual errors.\nRobust SLT evaluation requires <em class=\"ltx_emph ltx_font_italic\">semantic</em> metrics that explicitly reward meaning preservation while penalizing distortions or hallucinations.</p>\n\n",
                "matched_terms": [
                    "paraphrases",
                    "near"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Evaluation and model development for multilingual SLT should be language-aware. In practice, one should combine semantics-focused metrics with targeted, language-specific checks (e.g., temporals and agreement in French; word order and compounding in German) to obtain fair comparisons and actionable diagnostics.</p>\n\n",
                "matched_terms": [
                    "one",
                    "german",
                    "french"
                ]
            }
        ]
    },
    "A1.T6": {
        "source_file": "SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision",
        "caption": "Table 6: CSL-Daily examples —good translations (green) and one bad translation (red), showing reference and prediction in Chinese, with English translations for clarity.",
        "body": "Text\n\n\n\n\n\n\n\n\n\\rowcolorgreen!10 Ref (ZH): 我们下午三点见面。\n\n\n\n\n\n\n\\rowcolorgreen!10 EN: We will meet at three in the afternoon.\n\n\n\n\n\n\n\\rowcolorgreen!10 Pred (ZH): 我们三点钟下午见。\n\n\n\n\n\n\n\\rowcolorgreen!10 EN: We meet at three o’clock in the afternoon.\n\n\n\n\n\n\n\\rowcolorgreen!10 Ref (ZH): 我早上吃面包和牛奶。\n\n\n\n\n\n\n\\rowcolorgreen!10 EN: I eat bread and milk in the morning.\n\n\n\n\n\n\n\\rowcolorgreen!10 Pred (ZH): 我早上吃了牛奶和面包。\n\n\n\n\n\n\n\\rowcolorgreen!10 EN: I had milk and bread in the morning.\n\n\n\n\n\n\n\\rowcolorred!10 Ref (ZH): 我们乘坐飞机去旅游，今天在酒店住宿。\n\n\n\n\n\n\n\\rowcolorred!10 EN: We took a plane to travel, and are staying in a hotel today.\n\n\n\n\n\n\n\\rowcolorred!10 Pred (ZH): 我们飞机去上海，今天喝酒睡觉。\n\n\n\n\n\n\n\\rowcolorred!10 EN: We took a plane to Shanghai, today we drink alcohol and sleep.",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:256.1pt;\"><span class=\"ltx_text ltx_font_bold\">Text</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:256.1pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>green!10 <span class=\"ltx_text ltx_font_bold\">Ref (ZH):</span> &#25105;&#20204;&#19979;&#21320;&#19977;&#28857;&#35265;&#38754;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:256.1pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>green!10 <span class=\"ltx_text ltx_font_italic\">EN: We will meet at three in the afternoon.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:256.1pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>green!10 <span class=\"ltx_text ltx_font_bold\">Pred (ZH):</span> &#25105;&#20204;&#19977;&#28857;&#38047;&#19979;&#21320;&#35265;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:256.1pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>green!10 <span class=\"ltx_text ltx_font_italic\">EN: We meet at three o&#8217;clock in the afternoon.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:256.1pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>green!10 <span class=\"ltx_text ltx_font_bold\">Ref (ZH):</span> &#25105;&#26089;&#19978;&#21507;&#38754;&#21253;&#21644;&#29275;&#22902;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:256.1pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>green!10 <span class=\"ltx_text ltx_font_italic\">EN: I eat bread and milk in the morning.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:256.1pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>green!10 <span class=\"ltx_text ltx_font_bold\">Pred (ZH):</span> &#25105;&#26089;&#19978;&#21507;&#20102;&#29275;&#22902;&#21644;&#38754;&#21253;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:256.1pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>green!10 <span class=\"ltx_text ltx_font_italic\">EN: I had milk and bread in the morning.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:256.1pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>red!10 <span class=\"ltx_text ltx_font_bold\">Ref (ZH):</span> &#25105;&#20204;&#20056;&#22352;&#39134;&#26426;&#21435;&#26053;&#28216;&#65292;&#20170;&#22825;&#22312;&#37202;&#24215;&#20303;&#23487;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:256.1pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>red!10 <span class=\"ltx_text ltx_font_italic\">EN: We took a plane to travel, and are staying in a hotel today.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:256.1pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>red!10 <span class=\"ltx_text ltx_font_bold\">Pred (ZH):</span> &#25105;&#20204;&#39134;&#26426;&#21435;&#19978;&#28023;&#65292;&#20170;&#22825;&#21917;&#37202;&#30561;&#35273;&#12290;</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:256.1pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>red!10 <span class=\"ltx_text ltx_font_italic\">EN: We took a plane to Shanghai, today we drink alcohol and sleep.</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "bad",
            "我早上吃面包和牛奶。",
            "one",
            "pred",
            "我早上吃了牛奶和面包。",
            "chinese",
            "translations",
            "reference",
            "morning",
            "will",
            "today",
            "text",
            "milk",
            "afternoon",
            "prediction",
            "green",
            "plane",
            "clarity",
            "drink",
            "我们三点钟下午见。",
            "took",
            "—good",
            "sleep",
            "rowcolorred10",
            "examples",
            "showing",
            "eat",
            "travel",
            "meet",
            "staying",
            "had",
            "我们飞机去上海，今天喝酒睡觉。",
            "我们乘坐飞机去旅游，今天在酒店住宿。",
            "alcohol",
            "csldaily",
            "english",
            "translation",
            "three",
            "shanghai",
            "rowcolorgreen10",
            "hotel",
            "bread",
            "ref",
            "o’clock",
            "我们下午三点见面。",
            "red"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Additional translation examples for CSL-Daily and PHOENIX-2014T are provided in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#A1.T6\" title=\"Table 6 &#8227; A.3 Additional Qualitative Results &#8227; Appendix A Appendix &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#A1.T7\" title=\"Table 7 &#8227; A.3 Additional Qualitative Results &#8227; Appendix A Appendix &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, respectively.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Sign language translation (SLT) is typically trained with text in a single spoken language, which limits scalability and cross-language generalization. Earlier approaches have replaced gloss supervision with text-based sentence embeddings, but up to now, these remain tied to a specific language and modality. In contrast, here we employ language-agnostic, multimodal embeddings trained on text and speech from multiple languages to supervise SLT, enabling direct multilingual translation.\nTo address data scarcity, we propose a coupled augmentation method that combines multilingual target augmentations (i.e. translations into many languages) with video-level perturbations, improving model robustness. Experiments show consistent BLEURT gains over text-only sentence embedding supervision, with larger improvements in low-resource settings. Our results demonstrate that language-agnostic embedding supervision, combined with coupled augmentation, provides a scalable and semantically robust alternative to traditional SLT training.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We release the code, models, and features to facilitate further research. Github repository: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/DFKI-SignLanguage/sonar-slt.git\" title=\"\">https://github.com/DFKI-SignLanguage/sonar-slt.git</a>; Huggingface: <a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/mtmlt\" title=\"\">https://huggingface.co/mtmlt</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "text",
                    "translation",
                    "translations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent work by <cite class=\"ltx_cite ltx_citemacro_citet\">Hamidullah et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib11\" title=\"\">2024</a>)</cite> has reduced the reliance on glosses by supervising SLT with <em class=\"ltx_emph ltx_font_italic\">text-based sentence embeddings</em>. This yields better semantic alignment, but the embeddings remain modality-specific and typically require dataset-specific fine-tuning. Furthermore, compared to large pre-trained models that exploit vast text corpora, these text-only embeddings show limited cross-lingual transfer and reduced robustness. This raises the key question: <em class=\"ltx_emph ltx_font_italic\">Can language-agnostic, multimodal sentence embedding supervision replace text-only alignment in SLT?</em> We hypothesize that <span class=\"ltx_text ltx_font_bold\">language-agnostic, multimodal sentence embeddings</span> can reduce the residual dependence on text. Concretely, we build on <span class=\"ltx_text ltx_font_smallcaps\">SONAR</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Duquenne et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib4\" title=\"\">2023</a>)</cite>, a pretrained multilingual and multimodal embedding space that jointly represents text and speech. SONAR embeddings are claimed to be language-agnostic.\nOur approach aligns sign representations directly with language-agnostic semantic vectors, thereby decoupling supervision from any specific spoken language and removing the need for glosses. Our model integrates multiple modalities and supports direct supervision across all 200 languages covered by <span class=\"ltx_text ltx_font_smallcaps\">SONAR</span> (see Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). In contrast to prior systems that relied on additional stages or separate models for multi-target translation, our method enables <em class=\"ltx_emph ltx_font_italic\">direct</em> translation into multiple languages within a single model.</p>\n\n",
                "matched_terms": [
                    "text",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Complementary approaches leverage large language models (LLMs). SignLLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib5\" title=\"\">2024</a>)</cite> discretizes videos into tokens and prompts a frozen LLM; Sign2GPT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib26\" title=\"\">2024</a>)</cite> feeds pseudo-glosses to XGLM, reporting <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>22 BLEU on PHOENIX-2014T and <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>15 BLEU on CSL-Daily. SpaMo&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hwang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib12\" title=\"\">2025</a>)</cite> employs a straightforward approach that extracts spatial and motion features from sign language videos and utilizes a low-rank adapter to fine-tune an LLM for sign language translation. <cite class=\"ltx_cite ltx_citemacro_citet\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib2\" title=\"\">2024</a>)</cite> introduced FLa-LLM, a two-stage, gloss-free framework that first pre-trains the visual encoder and then fine-tunes a pre-trained LLM for the downstream SLT task. These methods inherit LLM fluency but are largely monolingual and require substantial tokenization and training overhead. In contrast, our PEFT-based SONAR adapters maintain multilinguality without retraining a large decoder on discretized video tokens. More recent work has explored large-scale pre-training to improve sign language understanding, with Uni-Sign <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib13\" title=\"\">2025</a>)</cite> proposing a unified generative framework that treats downstream tasks as SLT and incorporates prior-guided fusion.</p>\n\n",
                "matched_terms": [
                    "csldaily",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enforce language-agnostic supervision, each reference sentence is paired with <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math>\ntranslations <math alttext=\"\\{y^{(k)}\\}_{k=1}^{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msup><mi>y</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><annotation encoding=\"application/x-tex\">\\{y^{(k)}\\}_{k=1}^{K}</annotation></semantics></math> (from the embedding decoder). At each iteration, one translation <math alttext=\"y^{(k)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msup><mi>y</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">y^{(k)}</annotation></semantics></math> is sampled\nand encoded as <math alttext=\"\\mathbf{s}=\\mathcal{E}_{txt}(y^{(k)})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#119852;</mi><mo>=</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>y</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{s}=\\mathcal{E}_{txt}(y^{(k)})</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "reference",
                    "one",
                    "translation",
                    "translations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we leverage the language-agnostic semantic space for dataset fusion.\nBecause supervision is defined independently of any specific spoken language, videos from different sign languages can be trained jointly with textual supervision in <em class=\"ltx_emph ltx_font_italic\">any</em> available language. For example, German sign language videos annotated in German can\nbe re-aligned with English, French, or Chinese translations via <math alttext=\"\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><annotation encoding=\"application/x-tex\">\\mathcal{E}</annotation></semantics></math>, allowing unified training across datasets such as PHOENIX-2014T and CSL-Daily.\nThis enables direct multi-target translation without glosses and facilitates fusion of heterogeneous sign-language corpora.</p>\n\n",
                "matched_terms": [
                    "csldaily",
                    "english",
                    "translation",
                    "chinese",
                    "translations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">CSL-Daily</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib32\" title=\"\">2021</a>)</cite>: A Chinese Sign Language (CSL) corpus tailored for sign-to-Chinese SLT, emphasizing interactions in daily communication contexts.</p>\n\n",
                "matched_terms": [
                    "csldaily",
                    "chinese"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate our method against several strong recent state-of-the-art systems within the gloss-free paradigm. CSGCR&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib30\" title=\"\">2021</a>)</cite> improves SLT accuracy and fluency through three modules: word existence verification, conditional sentence generation, and cross-modal re-ranking for richer grammatical representations. GFSLT-VLP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib31\" title=\"\">2023</a>)</cite> leverages vision&#8211;language pretraining, while FLa-LLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib2\" title=\"\">2024</a>)</cite> adopts a two-stage gloss-free pipeline that first pre-trains the visual encoder and then fine-tunes a pre-trained LLM for SLT. Sign2GPT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib26\" title=\"\">2024</a>)</cite> maps visual inputs to pseudo-gloss sequences and decodes them with GPT-style language modeling, whereas SignLLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib5\" title=\"\">2024</a>)</cite> discretizes sign features into visual tokens to prompt a frozen LLM. SEM-SLT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hamidullah et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib11\" title=\"\">2024</a>)</cite> aligns sign language videos with sentence embeddings and serves as the foundation of our work. For multilingual settings, Sign2(LID+Text)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib22\" title=\"\">2025</a>)</cite> combines token-level sign language identification with a CTC objective to generate spoken text.</p>\n\n",
                "matched_terms": [
                    "text",
                    "three"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The original SONAR pools by running a shallow decoder: it feeds a special token (the EOS id in <span class=\"ltx_text ltx_font_typewriter\">M200M100</span>) as input and uses the encoder outputs as hidden states; the first decoder output is taken as the sentence embedding. During the Visual Block training, we adopt this approach with a shallow decoder initialized from the first three SONAR decoder layers and train it only for pooling. This supplies language context during pooling, while the incoming features themselves are language-agnostic (from another modality). Text generation is then conditioned on the target language.</p>\n\n",
                "matched_terms": [
                    "text",
                    "three"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text augmentation.</span> To expand the datasets using NLLB <cite class=\"ltx_cite ltx_citemacro_citep\">(NLLB Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib18\" title=\"\">2024</a>)</cite>, we machine-translate the target texts into three high-resource languages (English, French and Spanish) using the <span class=\"ltx_text ltx_font_typewriter\">facebook/nllb-200-distilled-600M</span> model.</p>\n\n",
                "matched_terms": [
                    "text",
                    "english",
                    "three"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe a decrease in BLEU compared to the SEM-SLT system, which is expected since our model is not fine-tuned on sign-language text. Our language-agnostic, sentence embedding-based supervision preserves semantics without requiring fine-tuning on specific dataset: it goes beyond surface <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram matching to produce translations that are contextually accurate, grammatically correct, and cross-lingually robust. Part of the remaining gap stems from dataset capture conditions. Our feature extractor <cite class=\"ltx_cite ltx_citemacro_citep\">(Hwang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib12\" title=\"\">2025</a>)</cite> is tuned for global cues and can be less accurate in cases where fine-grained articulations, such as facial expressions and finger movements, are critical. Recent top systems address this with keypoint-based representations and extensive preprocessing <cite class=\"ltx_cite ltx_citemacro_citep\">(Gueuwou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib8\" title=\"\">2025b</a>)</cite>, which help preserve these fine-grained details.</p>\n\n",
                "matched_terms": [
                    "text",
                    "translations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate target-side augmentation, where language translations are included in training. Results for both low- and high-resource languages on PHOENIX-2014T are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S5.T3\" title=\"Table 3 &#8227; &#8729; Observed gaps. &#8227; 5.1 Comparative Analysis &#8227; 5 Results and Analysis &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. In our experiments, we augmented the target set in training with three high-resource languages&#8212;French, Spanish, and English&#8212;while the model was evaluated on other unseen languages. Using this augmented target set yields a modest improvement over training with a single target language. However, we observe a gap in performance between high- and low-resource languages, which primarily stems from lower reference translation quality in the low-resource languages.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Reference translations were obtained using <span class=\"ltx_text ltx_font_typewriter\">facebook/nllb-200-distilled-600M</span> model.</span></span></span> The narrow domain of PHOENIX-2014T can also introduce dataset-specific idiosyncrasies, complicating fair comparisons.</p>\n\n",
                "matched_terms": [
                    "reference",
                    "translation",
                    "three",
                    "translations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, introducing the auto-encoding loss provides a second cross-entropy signal,\nwhich exerts a stronger influence on the Visual Block.\nHere, intermediate supervision continues to be beneficial, and the auto-encoding objective itself\naccelerates convergence.\nWe consistently observed this effect in CSL-Daily and in the augmented translation setup on PHOENIX-2014T.</p>\n\n",
                "matched_terms": [
                    "csldaily",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S5.T5\" title=\"Table 5 &#8227; &#8729; Semantic analysis. &#8227; 5.3 Qualitative and Semantic Error Analysis &#8227; 5 Results and Analysis &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows examples of two contrasting outcomes: cases where the model accurately captures the intended meaning and cases where it fails.\nWhen contextual understanding is incomplete, the decoder frequently compensates by generating fluent continuations via next-token prediction.\nThis behavior is characteristic of SLT systems that rely on pretrained language models as decoders: they can mask weaknesses in semantic grounding by producing outputs that are coherent but only partially faithful to the source.\nAs a result, improvements in BLEU may reflect the decoder&#8217;s ability to recover plausible sentences rather than true gains in sign-to-text comprehension.\nTherefore, exact sequence matching metrics such as BLEU are insufficient and in some cases misleading for evaluating translation quality in SLT.</p>\n\n",
                "matched_terms": [
                    "examples",
                    "prediction",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments show that language-agnostic supervision enables robust translation even under sign&#8211;target mismatches.\nMultilingual text augmentations, combined with visual augmentation, improves performance on PHOENIX-2014T despite\nlimited data. Ablations further confirm the advantages of this approach in preserving semantic adequacy.</p>\n\n",
                "matched_terms": [
                    "text",
                    "translation"
                ]
            }
        ]
    },
    "A1.T7": {
        "source_file": "SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision",
        "caption": "Table 7: PHOENIX-2014T examples —two good translations (green) and one bad translation (red), showing reference (German), predictions (German and French), and English translations.",
        "body": "Text\n\n\n\n\n\n\n\n\n\\rowcolorgreen!10 Ref (DE): ich wünsche ihnen noch einen schönen abend.\n\n\n\n\n\n\n\\rowcolorgreen!10 EN: I wish you a pleasant evening.\n\n\n\n\n\n\n\\rowcolorgreen!10 Pred (DE): und jetzt wünsche ich ihnen noch einen schönen abend.\n\n\n\n\n\n\n\\rowcolorgreen!10 EN: And now, I wish you a pleasant evening.\n\n\n\n\n\n\n\\rowcolorgreen!10 Pred (FR): Et maintenant, je vous souhaite une bonne soirée.\n\n\n\n\n\n\n\\rowcolorgreen!10 EN: And now, I wish you a good evening.\n\n\n\n\n\n\n\\rowcolorgreen!10 Ref (DE): der wind aus süd bis west weht schwach bis mäßig.\n\n\n\n\n\n\n\\rowcolorgreen!10 EN: The wind from the south to west blows weakly to moderately.\n\n\n\n\n\n\n\\rowcolorgreen!10 Pred (DE): der wind weht meist schwach aus süd bis west.\n\n\n\n\n\n\n\\rowcolorgreen!10 EN: The wind generally blows weakly from south to west.\n\n\n\n\n\n\n\\rowcolorgreen!10 Pred (FR): Le vent souffle généralement faiblement du sud-ouest.\n\n\n\n\n\n\n\\rowcolorgreen!10 EN: The wind generally blows weakly from the southwest.\n\n\n\n\n\n\n\\rowcolorred!10 Ref (DE): in deutschland gibt es nur schwache luftdruckunterschiede.\n\n\n\n\n\n\n\\rowcolorred!10 EN: In Germany, there are only slight air pressure differences.\n\n\n\n\n\n\n\\rowcolorred!10 Pred (DE): im nordosten deutschlands sorgt das hoch für wenig unbeständiges wetter.\n\n\n\n\n\n\n\\rowcolorred!10 EN: In northeastern Germany, the high pressure system causes little unsettled weather.\n\n\n\n\n\n\n\\rowcolorred!10 Pred (FR): Dans certaines régions de l’Allemagne, la pression atmosphérique élevée n’est toujours pas atteinte.\n\n\n\n\n\n\n\\rowcolorred!10 EN: In some regions of Germany, high atmospheric pressure has still not been reached.",
        "html_code": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_text ltx_font_bold\">Text</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>green!10 <span class=\"ltx_text ltx_font_bold\">Ref (DE):</span> ich w&#252;nsche ihnen noch einen sch&#246;nen abend.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>green!10 <span class=\"ltx_text ltx_font_italic\">EN: I wish you a pleasant evening.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>green!10 <span class=\"ltx_text ltx_font_bold\">Pred (DE):</span> und jetzt w&#252;nsche ich ihnen noch einen sch&#246;nen abend.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>green!10 <span class=\"ltx_text ltx_font_italic\">EN: And now, I wish you a pleasant evening.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>green!10 <span class=\"ltx_text ltx_font_bold\">Pred (FR):</span> Et maintenant, je vous souhaite une bonne soir&#233;e.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>green!10 <span class=\"ltx_text ltx_font_italic\">EN: And now, I wish you a good evening.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>green!10 <span class=\"ltx_text ltx_font_bold\">Ref (DE):</span> der wind aus s&#252;d bis west weht schwach bis m&#228;&#223;ig.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>green!10 <span class=\"ltx_text ltx_font_italic\">EN: The wind from the south to west blows weakly to moderately.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>green!10 <span class=\"ltx_text ltx_font_bold\">Pred (DE):</span> der wind weht meist schwach aus s&#252;d bis west.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>green!10 <span class=\"ltx_text ltx_font_italic\">EN: The wind generally blows weakly from south to west.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>green!10 <span class=\"ltx_text ltx_font_bold\">Pred (FR):</span> Le vent souffle g&#233;n&#233;ralement faiblement du sud-ouest.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>green!10 <span class=\"ltx_text ltx_font_italic\">EN: The wind generally blows weakly from the southwest.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>red!10 <span class=\"ltx_text ltx_font_bold\">Ref (DE):</span> in deutschland gibt es nur schwache luftdruckunterschiede.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>red!10 <span class=\"ltx_text ltx_font_italic\">EN: In Germany, there are only slight air pressure differences.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>red!10 <span class=\"ltx_text ltx_font_bold\">Pred (DE):</span> im nordosten deutschlands sorgt das hoch f&#252;r wenig unbest&#228;ndiges wetter.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>red!10 <span class=\"ltx_text ltx_font_italic\">EN: In northeastern Germany, the high pressure system causes little unsettled weather.</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>red!10 <span class=\"ltx_text ltx_font_bold\">Pred (FR):</span> Dans certaines r&#233;gions de l&#8217;Allemagne, la pression atmosph&#233;rique &#233;lev&#233;e n&#8217;est toujours pas atteinte.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:284.5pt;\"><span class=\"ltx_ERROR undefined\">\\rowcolor</span>red!10 <span class=\"ltx_text ltx_font_italic\">EN: In some regions of Germany, high atmospheric pressure has still not been reached.</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "deutschlands",
            "wünsche",
            "unbeständiges",
            "generally",
            "n’est",
            "good",
            "meist",
            "luftdruckunterschiede",
            "vous",
            "wind",
            "blows",
            "gibt",
            "wish",
            "reached",
            "system",
            "pas",
            "weather",
            "bonne",
            "evening",
            "weakly",
            "differences",
            "causes",
            "nordosten",
            "german",
            "ref",
            "mäßig",
            "und",
            "bis",
            "régions",
            "phoenix2014t",
            "bad",
            "souhaite",
            "vent",
            "has",
            "high",
            "jetzt",
            "translations",
            "regions",
            "für",
            "green",
            "l’allemagne",
            "schönen",
            "schwach",
            "there",
            "moderately",
            "sorgt",
            "élevée",
            "pression",
            "sudouest",
            "hoch",
            "faiblement",
            "dans",
            "das",
            "pressure",
            "—two",
            "one",
            "west",
            "french",
            "wetter",
            "germany",
            "deutschland",
            "noch",
            "air",
            "south",
            "only",
            "wenig",
            "généralement",
            "some",
            "rowcolorred10",
            "certaines",
            "showing",
            "maintenant",
            "nur",
            "from",
            "english",
            "translation",
            "little",
            "toujours",
            "still",
            "atmosphérique",
            "atteinte",
            "süd",
            "pred",
            "predictions",
            "ihnen",
            "der",
            "southwest",
            "slight",
            "reference",
            "unsettled",
            "text",
            "ich",
            "weht",
            "souffle",
            "abend",
            "atmospheric",
            "you",
            "pleasant",
            "soirée",
            "examples",
            "une",
            "now",
            "schwache",
            "rowcolorgreen10",
            "einen",
            "aus",
            "northeastern",
            "red",
            "not"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Additional translation examples for CSL-Daily and PHOENIX-2014T are provided in Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#A1.T6\" title=\"Table 6 &#8227; A.3 Additional Qualitative Results &#8227; Appendix A Appendix &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#A1.T7\" title=\"Table 7 &#8227; A.3 Additional Qualitative Results &#8227; Appendix A Appendix &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, respectively.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Sign language translation (SLT) is typically trained with text in a single spoken language, which limits scalability and cross-language generalization. Earlier approaches have replaced gloss supervision with text-based sentence embeddings, but up to now, these remain tied to a specific language and modality. In contrast, here we employ language-agnostic, multimodal embeddings trained on text and speech from multiple languages to supervise SLT, enabling direct multilingual translation.\nTo address data scarcity, we propose a coupled augmentation method that combines multilingual target augmentations (i.e. translations into many languages) with video-level perturbations, improving model robustness. Experiments show consistent BLEURT gains over text-only sentence embedding supervision, with larger improvements in low-resource settings. Our results demonstrate that language-agnostic embedding supervision, combined with coupled augmentation, provides a scalable and semantically robust alternative to traditional SLT training.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>We release the code, models, and features to facilitate further research. Github repository: <a class=\"ltx_ref ltx_href\" href=\"https://github.com/DFKI-SignLanguage/sonar-slt.git\" title=\"\">https://github.com/DFKI-SignLanguage/sonar-slt.git</a>; Huggingface: <a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/mtmlt\" title=\"\">https://huggingface.co/mtmlt</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "text",
                    "translation",
                    "translations",
                    "from",
                    "now"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Sign languages (SLs) are inherently visual and culturally embedded. Each SL has evolved independently and is closely tied to the communities and spoken languages of its region. As a result, most sign language translation (SLT) datasets are built around a <em class=\"ltx_emph ltx_font_italic\">single</em> sign&#8211;spoken language pair (e.g., DGS<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math>German), which makes it difficult to scale models across languages or to combine datasets. Training a system for a new target language typically requires a separate model and fresh parallel data collection.</p>\n\n",
                "matched_terms": [
                    "system",
                    "has",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Historically, SLT systems have relied on manually provided <em class=\"ltx_emph ltx_font_italic\">gloss supervision</em> <cite class=\"ltx_cite ltx_citemacro_citep\">(Camgoz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib1\" title=\"\">2018</a>)</cite>, discrete word-like labels whose design and availability are language-, culture-, and region-specific. Even <span class=\"ltx_text ltx_font_italic\">gloss-free</span> SLT approaches assume that sign inputs should be supervised by text from the co-occurring spoken language, keeping the learning signal tied to a single language <cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib5\" title=\"\">2024</a>; Wong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib26\" title=\"\">2024</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib2\" title=\"\">2024</a>; Hamidullah et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib10\" title=\"\">2022</a>)</cite> and limiting cross-dataset reuse and generalization.</p>\n\n",
                "matched_terms": [
                    "text",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent work by <cite class=\"ltx_cite ltx_citemacro_citet\">Hamidullah et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib11\" title=\"\">2024</a>)</cite> has reduced the reliance on glosses by supervising SLT with <em class=\"ltx_emph ltx_font_italic\">text-based sentence embeddings</em>. This yields better semantic alignment, but the embeddings remain modality-specific and typically require dataset-specific fine-tuning. Furthermore, compared to large pre-trained models that exploit vast text corpora, these text-only embeddings show limited cross-lingual transfer and reduced robustness. This raises the key question: <em class=\"ltx_emph ltx_font_italic\">Can language-agnostic, multimodal sentence embedding supervision replace text-only alignment in SLT?</em> We hypothesize that <span class=\"ltx_text ltx_font_bold\">language-agnostic, multimodal sentence embeddings</span> can reduce the residual dependence on text. Concretely, we build on <span class=\"ltx_text ltx_font_smallcaps\">SONAR</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Duquenne et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib4\" title=\"\">2023</a>)</cite>, a pretrained multilingual and multimodal embedding space that jointly represents text and speech. SONAR embeddings are claimed to be language-agnostic.\nOur approach aligns sign representations directly with language-agnostic semantic vectors, thereby decoupling supervision from any specific spoken language and removing the need for glosses. Our model integrates multiple modalities and supports direct supervision across all 200 languages covered by <span class=\"ltx_text ltx_font_smallcaps\">SONAR</span> (see Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). In contrast to prior systems that relied on additional stages or separate models for multi-target translation, our method enables <em class=\"ltx_emph ltx_font_italic\">direct</em> translation into multiple languages within a single model.</p>\n\n",
                "matched_terms": [
                    "text",
                    "from",
                    "has",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A major obstacle for SLT is the scarcity of annotated data. Recent work on self-supervised pre-training from unannotated or anonymized data <cite class=\"ltx_cite ltx_citemacro_citep\">(Rust et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib20\" title=\"\">2024</a>)</cite> has shown promise in addressing this challenge. This motivates our second question: <em class=\"ltx_emph ltx_font_italic\">Can target-language augmentation further alleviate data scarcity and enhance robustness, particularly when combined with video augmentation?</em></p>\n\n",
                "matched_terms": [
                    "from",
                    "has"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Traditional SLT systems rely on glosses &#8212;textual labels that represent signs&#8212; as an intermediate representation. MSKA-SLT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Guan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib6\" title=\"\">2025</a>)</cite> remains a strong baseline using glosses, reporting <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>29 BLEU on PHOENIX-2014T <cite class=\"ltx_cite ltx_citemacro_citep\">(Camgoz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib1\" title=\"\">2018</a>)</cite>. However, glosses are neither universal nor standardized: they are tightly coupled to specific languages, cultures, and regions. Moreover, producing gloss annotations is highly time-consuming, requiring expert linguistic knowledge <cite class=\"ltx_cite ltx_citemacro_citep\">(M&#252;ller et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib16\" title=\"\">2023b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "phoenix2014t",
                    "regions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In parallel, gloss-free SLT has emerged, enabling training on weakly annotated datasets exceeding 1,000 hours for some sign languages <cite class=\"ltx_cite ltx_citemacro_citep\">(Uthus et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib24\" title=\"\">2023</a>)</cite>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>A <em class=\"ltx_emph ltx_font_italic\">weakly annotated dataset</em> provides only coarse or noisy supervision. For instance, YouTube-ASL datasets are collected from online videos where annotations rely solely on automatically generated or the provided subtitles, without manual realignment,\nleading to potential inaccuracies and temporal misalignments.</span></span></span>\n<cite class=\"ltx_cite ltx_citemacro_citet\">Hamidullah et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib11\" title=\"\">2024</a>)</cite> aligns sign language videos with sentence-level text embeddings. This supervision avoids feeding long, fine-grained frame sequences to the decoder, thereby reducing redundancy in video features, lowering the need for aggressive masking, and encouraging learning at the sentence-semantic level. While intermediate supervision of visual blocks is common in multimodal models, compressing video into a sentence-level embedding before decoding improves semantic grounding and flexibility in target text generation. Nevertheless, current approaches <cite class=\"ltx_cite ltx_citemacro_citep\">(Hamidullah et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib11\" title=\"\">2024</a>; Gueuwou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib8\" title=\"\">2025b</a>)</cite> remain limited by their reliance on <em class=\"ltx_emph ltx_font_italic\">text-only</em> embedding spaces with restricted language coverage, constraining augmentation and cross-sign transfer.</p>\n\n",
                "matched_terms": [
                    "text",
                    "weakly",
                    "only",
                    "has",
                    "some",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Complementary approaches leverage large language models (LLMs). SignLLM&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib5\" title=\"\">2024</a>)</cite> discretizes videos into tokens and prompts a frozen LLM; Sign2GPT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib26\" title=\"\">2024</a>)</cite> feeds pseudo-glosses to XGLM, reporting <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>22 BLEU on PHOENIX-2014T and <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>15 BLEU on CSL-Daily. SpaMo&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hwang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib12\" title=\"\">2025</a>)</cite> employs a straightforward approach that extracts spatial and motion features from sign language videos and utilizes a low-rank adapter to fine-tune an LLM for sign language translation. <cite class=\"ltx_cite ltx_citemacro_citet\">Chen et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib2\" title=\"\">2024</a>)</cite> introduced FLa-LLM, a two-stage, gloss-free framework that first pre-trains the visual encoder and then fine-tunes a pre-trained LLM for the downstream SLT task. These methods inherit LLM fluency but are largely monolingual and require substantial tokenization and training overhead. In contrast, our PEFT-based SONAR adapters maintain multilinguality without retraining a large decoder on discretized video tokens. More recent work has explored large-scale pre-training to improve sign language understanding, with Uni-Sign <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib13\" title=\"\">2025</a>)</cite> proposing a unified generative framework that treats downstream tasks as SLT and incorporates prior-guided fusion.</p>\n\n",
                "matched_terms": [
                    "phoenix2014t",
                    "from",
                    "has",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_bold\">SONAR-SLT</span>, a modular SLT framework that decouples\n<em class=\"ltx_emph ltx_font_italic\">semantic understanding</em> from <em class=\"ltx_emph ltx_font_italic\">text generation</em>.\nAs illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, the system first maps an input sign language video into a multilingual, multimodal semantic space, and then (optionally) decodes from this space into a chosen spoken language.\nThis design allows training on heterogeneous sign language datasets, supports multilingual supervision, and removes the need for gloss annotations. A detailed architecture is presented in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#A1.SS1\" title=\"A.1 Detailed Architecture &#8227; Appendix A Appendix &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">A.1</span></a> and summarized in the next subsections.</p>\n\n",
                "matched_terms": [
                    "text",
                    "from",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To enforce language-agnostic supervision, each reference sentence is paired with <math alttext=\"K\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>K</mi><annotation encoding=\"application/x-tex\">K</annotation></semantics></math>\ntranslations <math alttext=\"\\{y^{(k)}\\}_{k=1}^{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msup><mi>y</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></msubsup><annotation encoding=\"application/x-tex\">\\{y^{(k)}\\}_{k=1}^{K}</annotation></semantics></math> (from the embedding decoder). At each iteration, one translation <math alttext=\"y^{(k)}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><msup><mi>y</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msup><annotation encoding=\"application/x-tex\">y^{(k)}</annotation></semantics></math> is sampled\nand encoded as <math alttext=\"\\mathbf{s}=\\mathcal{E}_{txt}(y^{(k)})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS3.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi>&#119852;</mi><mo>=</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>x</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mi>y</mi><mrow><mo stretchy=\"false\">(</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{s}=\\mathcal{E}_{txt}(y^{(k)})</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "translation",
                    "one",
                    "from",
                    "translations",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then decode into natural language from the semantic embedding.\nA pretrained decoder <math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math> from <span class=\"ltx_text ltx_font_smallcaps\">SONAR</span> generates text from a semantic vector\nand a target language token <math alttext=\"\\ell\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#8467;</mi><annotation encoding=\"application/x-tex\">\\ell</annotation></semantics></math>. Conditioned on the sign-derived and semantically text-aligned (Section &#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S3.SS3\" title=\"3.3 Semantic Alignment &#8227; 3 Methodology &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">3.3</span></a>) embedding <math alttext=\"\\mathbf{z}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m3\" intent=\":literal\"><semantics><mi>&#119859;</mi><annotation encoding=\"application/x-tex\">\\mathbf{z}</annotation></semantics></math>,\nthe decoder is trained with teacher forcing:</p>\n\n",
                "matched_terms": [
                    "text",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, we leverage the language-agnostic semantic space for dataset fusion.\nBecause supervision is defined independently of any specific spoken language, videos from different sign languages can be trained jointly with textual supervision in <em class=\"ltx_emph ltx_font_italic\">any</em> available language. For example, German sign language videos annotated in German can\nbe re-aligned with English, French, or Chinese translations via <math alttext=\"\\mathcal{E}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS8.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8496;</mi><annotation encoding=\"application/x-tex\">\\mathcal{E}</annotation></semantics></math>, allowing unified training across datasets such as PHOENIX-2014T and CSL-Daily.\nThis enables direct multi-target translation without glosses and facilitates fusion of heterogeneous sign-language corpora.</p>\n\n",
                "matched_terms": [
                    "phoenix2014t",
                    "english",
                    "translation",
                    "german",
                    "french",
                    "translations",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">PHOENIX-2014T</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Camgoz et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib1\" title=\"\">2018</a>)</cite>: German Sign Language (DGS) weather forecast videos with parallel German text.</p>\n\n",
                "matched_terms": [
                    "text",
                    "phoenix2014t",
                    "weather",
                    "german"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Because our model operates on embedding vectors with small magnitudes, the MSE loss can rapidly fall to <math alttext=\"\\sim 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\sim 10^{-5}</annotation></semantics></math> even when cosine similarity remains suboptimal. Empirically, we observed that <span class=\"ltx_text ltx_font_bold\">cosine and MSE only begin to correlate at <math alttext=\"\\sim 10^{-6}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS4.SSS0.Px2.p2.m2\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>6</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\sim 10^{-6}</annotation></semantics></math></span>. Optimizing cosine alone often stalls, as MSE ceases to decrease, while optimizing MSE alone improves fidelity but does not guarantee angular alignment. To address this, we up-weight MSE to maintain shrinkage and retain a non-negligible cosine term to enforce directional consistency. We also experimented with InfoNCE, but under our effective batch size (with few hard negatives) it led to slower convergence and negligible improvements and we do not use it in our final experiments.</p>\n\n",
                "matched_terms": [
                    "only",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The original SONAR pools by running a shallow decoder: it feeds a special token (the EOS id in <span class=\"ltx_text ltx_font_typewriter\">M200M100</span>) as input and uses the encoder outputs as hidden states; the first decoder output is taken as the sentence embedding. During the Visual Block training, we adopt this approach with a shallow decoder initialized from the first three SONAR decoder layers and train it only for pooling. This supplies language context during pooling, while the incoming features themselves are language-agnostic (from another modality). Text generation is then conditioned on the target language.</p>\n\n",
                "matched_terms": [
                    "text",
                    "only",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We train the end-to-end translation system (with the Visual Block or the fused spatial+motion features) using the <span class=\"ltx_text ltx_font_bold\">same LoRA configuration</span> as above.</p>\n\n",
                "matched_terms": [
                    "system",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text augmentation.</span> To expand the datasets using NLLB <cite class=\"ltx_cite ltx_citemacro_citep\">(NLLB Team, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib18\" title=\"\">2024</a>)</cite>, we machine-translate the target texts into three high-resource languages (English, French and Spanish) using the <span class=\"ltx_text ltx_font_typewriter\">facebook/nllb-200-distilled-600M</span> model.</p>\n\n",
                "matched_terms": [
                    "text",
                    "english",
                    "french"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We observe a decrease in BLEU compared to the SEM-SLT system, which is expected since our model is not fine-tuned on sign-language text. Our language-agnostic, sentence embedding-based supervision preserves semantics without requiring fine-tuning on specific dataset: it goes beyond surface <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram matching to produce translations that are contextually accurate, grammatically correct, and cross-lingually robust. Part of the remaining gap stems from dataset capture conditions. Our feature extractor <cite class=\"ltx_cite ltx_citemacro_citep\">(Hwang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib12\" title=\"\">2025</a>)</cite> is tuned for global cues and can be less accurate in cases where fine-grained articulations, such as facial expressions and finger movements, are critical. Recent top systems address this with keypoint-based representations and extensive preprocessing <cite class=\"ltx_cite ltx_citemacro_citep\">(Gueuwou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib8\" title=\"\">2025b</a>)</cite>, which help preserve these fine-grained details.</p>\n\n",
                "matched_terms": [
                    "text",
                    "system",
                    "translations",
                    "from",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate target-side augmentation, where language translations are included in training. Results for both low- and high-resource languages on PHOENIX-2014T are presented in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S5.T3\" title=\"Table 3 &#8227; &#8729; Observed gaps. &#8227; 5.1 Comparative Analysis &#8227; 5 Results and Analysis &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>. In our experiments, we augmented the target set in training with three high-resource languages&#8212;French, Spanish, and English&#8212;while the model was evaluated on other unseen languages. Using this augmented target set yields a modest improvement over training with a single target language. However, we observe a gap in performance between high- and low-resource languages, which primarily stems from lower reference translation quality in the low-resource languages.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>Reference translations were obtained using <span class=\"ltx_text ltx_font_typewriter\">facebook/nllb-200-distilled-600M</span> model.</span></span></span> The narrow domain of PHOENIX-2014T can also introduce dataset-specific idiosyncrasies, complicating fair comparisons.</p>\n\n",
                "matched_terms": [
                    "phoenix2014t",
                    "translation",
                    "from",
                    "high",
                    "translations",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S5.T4\" title=\"Table 4 &#8227; &#8729; Observed gaps. &#8227; 5.1 Comparative Analysis &#8227; 5 Results and Analysis &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> shows that pre-training on concatenated multi-sign corpora followed by monolingual fine-tuning proves most effective. In contrast, joint multi-sign fine-tuning risks resembling another full training run without yielding substantial gains. In our experiments, we first pre-train on the combined data and then fine-tune monolingually, consistent with&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hamidullah et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib11\" title=\"\">2024</a>)</cite>; post-fine-tuning performance remains largely unchanged (see Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S5.T4\" title=\"Table 4 &#8227; &#8729; Observed gaps. &#8227; 5.1 Comparative Analysis &#8227; 5 Results and Analysis &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, mono vs. multilingual setup). Differences in dataset capture conditions still matter&#8212;for example, methods that rely solely on global visual features can underperform when fine-grained articulations, such as hand or facial details, are crucial. Pipelines that integrate keypoints with extensive preprocessing&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gueuwou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#bib.bib8\" title=\"\">2025b</a>)</cite> help mitigate such losses and achieve stronger results.</p>\n\n",
                "matched_terms": [
                    "still",
                    "differences"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The effect of sentence-embedding supervision is strongest when the Visual Block is still learning feature representations.\nOnce the block has converged&#8212;or is pretrained&#8212;the additional impact of cosine or MSE objectives diminishes.\nThis occurs because cross-entropy loss often remains relatively high (above <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>&#8211;<math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>),\nwhile MSE rapidly falls to <math alttext=\"\\mathcal{O}(10^{-5})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119978;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{O}(10^{-5})</annotation></semantics></math> and cosine similarity saturates around <math alttext=\"\\sim 0.3\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8764;</mo><mn>0.3</mn></mrow><annotation encoding=\"application/x-tex\">\\sim 0.3</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "still",
                    "has",
                    "high"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, introducing the auto-encoding loss provides a second cross-entropy signal,\nwhich exerts a stronger influence on the Visual Block.\nHere, intermediate supervision continues to be beneficial, and the auto-encoding objective itself\naccelerates convergence.\nWe consistently observed this effect in CSL-Daily and in the augmented translation setup on PHOENIX-2014T.</p>\n\n",
                "matched_terms": [
                    "phoenix2014t",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S5.T5\" title=\"Table 5 &#8227; &#8729; Semantic analysis. &#8227; 5.3 Qualitative and Semantic Error Analysis &#8227; 5 Results and Analysis &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> shows examples of two contrasting outcomes: cases where the model accurately captures the intended meaning and cases where it fails.\nWhen contextual understanding is incomplete, the decoder frequently compensates by generating fluent continuations via next-token prediction.\nThis behavior is characteristic of SLT systems that rely on pretrained language models as decoders: they can mask weaknesses in semantic grounding by producing outputs that are coherent but only partially faithful to the source.\nAs a result, improvements in BLEU may reflect the decoder&#8217;s ability to recover plausible sentences rather than true gains in sign-to-text comprehension.\nTherefore, exact sequence matching metrics such as BLEU are insufficient and in some cases misleading for evaluating translation quality in SLT.</p>\n\n",
                "matched_terms": [
                    "examples",
                    "only",
                    "translation",
                    "some"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Language-specific tendencies.</span> We deep into the analysis of two languages: German, a language trained with original data and French, a language trained via machine translation augmentation.</p>\n\n",
                "matched_terms": [
                    "german",
                    "translation",
                    "french"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">German:</span> Errors often arise from compound nouns, flexible word order, and embedded clauses, leading to partial omissions,\nattribute reordering, or unnatural compounds. When alignment is uncertain, the model may insert generic stock phrases or repetitions.</p>\n\n",
                "matched_terms": [
                    "from",
                    "german"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">French:</span> Our analysis shows more frequent noun substitutions, agreement mismatches, and text modality shifts (e.g., hedging with <span class=\"ltx_text ltx_font_italic\">&#8220;sont possibles&#8221;</span>).\nRegister differences from determiners or prepositions are also common. Incorrect date and numeric substitutions occur more frequently than in German,\nlikely due to segmentation differences in temporal expressions.</p>\n\n",
                "matched_terms": [
                    "text",
                    "differences",
                    "german",
                    "french",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Surface-form scores vs. meaning preservation.\nWe observe a systematic mismatch between surface-form metrics (e.g., BLEU) and semantic adequacy (BLEURT) across both German and French.\nOutputs with only moderate <math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math>-gram overlap can still be semantically faithful, while some high-scoring predictions contain factual errors.</p>\n\n",
                "matched_terms": [
                    "only",
                    "some",
                    "german",
                    "predictions",
                    "french",
                    "still"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Semantically near correct and correct paraphrases (German).</span>\nAs illustrated by the green-highlighted examples in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S5.T5\" title=\"Table 5 &#8227; &#8729; Semantic analysis. &#8227; 5.3 Qualitative and Semantic Error Analysis &#8227; 5 Results and Analysis &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, incorrect lexical or numeric substitutions leave most of the remaining meaning intact\n(e.g., date shifts: <span class=\"ltx_text ltx_font_italic\">&#8220;Sonntag, den neunzehnten Dezember&#8221;</span> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;Sonntag, den siebzehnten August&#8221;</span>;\ntemperature adjustments: <span class=\"ltx_text ltx_font_italic\">&#8220;sechs Grad an den Alpen&#8221;</span> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;neun Grad am Alpenrand&#8221;</span>).\nWe also observe benign stylistic reformulations (<span class=\"ltx_text ltx_font_italic\">&#8220;es gelten entsprechende Warnungen&#8221;</span> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;es bestehen Unwetterwarnungen&#8221;</span>)\nand word-order changes without semantic effect (<span class=\"ltx_text ltx_font_italic\">&#8220;aus S&#252;dwest bis West&#8221;</span> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;aus West bis S&#252;dost&#8221;</span>).</p>\n\n",
                "matched_terms": [
                    "examples",
                    "german",
                    "west",
                    "bis"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Semantically near correct and correct paraphrases (French).</span>\nSimilarly, the first green-highlighted examples show structural or modality shifts that preserve much of the remaining meaning, such as date substitutions\n(<span class=\"ltx_text ltx_font_italic\">&#8220;vingt-huit ao&#251;t&#8221;</span> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;vingt-cinq novembre&#8221;</span>),\nmodality changes (<span class=\"ltx_text ltx_font_italic\">&#8220;Des rafales orageuses de l&#8217;ouest&#8221;</span> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;Des rafales orageuses sont possibles&#8221;</span>),\nor expanded phrasing (<span class=\"ltx_text ltx_font_italic\">&#8220;&#201;galement orages sur la mer du Nord&#8221;</span> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p3.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;Il y a &#233;galement des orages sur la mer du Nord&#8221;</span>).</p>\n\n",
                "matched_terms": [
                    "examples",
                    "french"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Semantically incorrect outputs, true errors (French and German).</span>\nThe red-highlighted rows in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.19398v1#S5.T5\" title=\"Table 5 &#8227; &#8729; Semantic analysis. &#8227; 5.3 Qualitative and Semantic Error Analysis &#8227; 5 Results and Analysis &#8227; SONAR-SLT: Multilingual Sign Language Translation via Language-Agnostic Sentence Embedding Supervision\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> illustrate errors such as topic drift (predicting wind instead of temperature),\nincorrect locations (<span class=\"ltx_text ltx_font_italic\">&#8220;H&#246;henlagen S&#252;ddeutschlands&#8221;</span> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p4.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;K&#252;sten&#8221;</span>;\n<span class=\"ltx_text ltx_font_italic\">&#8220;sud-est&#8221;</span> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p4.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;nord&#8221;</span>),\nsystem inversions (<span class=\"ltx_text ltx_font_italic\">&#8220;Hoch&#8221;</span> <math alttext=\"\\leftrightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p4.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8596;</mo><annotation encoding=\"application/x-tex\">\\leftrightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;Tief&#8221;</span>; <span class=\"ltx_text ltx_font_italic\">&#8220;haut&#8221;</span> <math alttext=\"\\leftrightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p4.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8596;</mo><annotation encoding=\"application/x-tex\">\\leftrightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;profonde&#8221;</span>),\nhallucinated entities, or incorrect hazard categories\n(<span class=\"ltx_text ltx_font_italic\">&#8220;Risque d&#8217;inondation&#8221;</span> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS3.SSS0.Px2.p4.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_italic\">&#8220;Avertissements m&#233;t&#233;orologiques violents&#8221;</span>).</p>\n\n",
                "matched_terms": [
                    "system",
                    "wind",
                    "german",
                    "french"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Evaluation and model development for multilingual SLT should be language-aware. In practice, one should combine semantics-focused metrics with targeted, language-specific checks (e.g., temporals and agreement in French; word order and compounding in German) to obtain fair comparisons and actionable diagnostics.</p>\n\n",
                "matched_terms": [
                    "one",
                    "german",
                    "french"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented a scalable SLT framework that breaks the traditional close dependency between sign and spoken languages in training data and system development.\nBy aligning sign language videos with multilingual, multimodal sentence embeddings from <span class=\"ltx_text ltx_font_smallcaps\">SONAR</span>, our approach yields a\nlanguage-agnostic semantic representation that generalizes across both sign languages and spoken targets.\nThis reduces reliance on language-model priors and prioritizes visual grounding and SLT-specific grammar over surface-level text patterns.</p>\n\n",
                "matched_terms": [
                    "text",
                    "from",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Experiments show that language-agnostic supervision enables robust translation even under sign&#8211;target mismatches.\nMultilingual text augmentations, combined with visual augmentation, improves performance on PHOENIX-2014T despite\nlimited data. Ablations further confirm the advantages of this approach in preserving semantic adequacy.</p>\n\n",
                "matched_terms": [
                    "text",
                    "phoenix2014t",
                    "translation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">SONAR is officially supported in fairseq, but only its text encoder is available on Hugging Face. To enable full conditional generation, we ported both the encoder and decoder weights from the original SONAR checkpoints into <span class=\"ltx_text ltx_font_typewriter\">M200M100</span>, extending the earlier encoder-only port provided by the NLLB team. In particular, we transferred the decoder weights directly from fairseq, validated their functionality, and released the complete model for public use. The resulting <span class=\"ltx_text ltx_font_typewriter\">M200M100ForConditionalGeneration</span> can now be loaded end-to-end and fine-tuned directly.</p>\n\n",
                "matched_terms": [
                    "text",
                    "only",
                    "now",
                    "from"
                ]
            }
        ]
    }
}