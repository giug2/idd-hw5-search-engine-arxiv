{
    "S4.T1": {
        "caption": "Table 1. Statistics describing the messages (top part) and graphs (bottom part) of our dataset. Symbol ±{\\pm} denotes the standard deviation.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Average</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Minimum</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Maximum</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"4\"><span class=\"ltx_text ltx_font_italic\">Messages</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Number of words per message</th>\n<td class=\"ltx_td ltx_align_center\">7.22 <math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m3\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math> 5.06</td>\n<td class=\"ltx_td ltx_align_center\">1</td>\n<td class=\"ltx_td ltx_align_center\">94</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Number of characters per message</th>\n<td class=\"ltx_td ltx_align_center\">35.73 <math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m4\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math> 25.72</td>\n<td class=\"ltx_td ltx_align_center\">1</td>\n<td class=\"ltx_td ltx_align_center\">509</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Number of messages per conversation</th>\n<td class=\"ltx_td ltx_align_center\">86.62 <math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m5\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math> 50.09</td>\n<td class=\"ltx_td ltx_align_center\">42</td>\n<td class=\"ltx_td ltx_align_center\">375</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" colspan=\"4\"><span class=\"ltx_text ltx_font_italic\">Graphs</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Number of vertices per graph</th>\n<td class=\"ltx_td ltx_align_center\">47.8 <math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m6\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math> 20.3</td>\n<td class=\"ltx_td ltx_align_center\">2</td>\n<td class=\"ltx_td ltx_align_center\">214</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Graph density</th>\n<td class=\"ltx_td ltx_align_center\">0.54 <math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m7\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math> 0.15</td>\n<td class=\"ltx_td ltx_align_center\">0.12</td>\n<td class=\"ltx_td ltx_align_center\">1.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Number of negative edges per graph</th>\n<td class=\"ltx_td ltx_align_center\">189.7 <math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m8\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math> 38.4</td>\n<td class=\"ltx_td ltx_align_center\">3</td>\n<td class=\"ltx_td ltx_align_center\">1,800</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\">Number of positive edges per graph</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">272.3 <math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m9\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math> 45.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\">2,500</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "positive",
            "messages",
            "edges",
            "denotes",
            "words",
            "our",
            "maximum",
            "deviation",
            "average",
            "minimum",
            "statistics",
            "density",
            "top",
            "characters",
            "symbol",
            "bottom",
            "conversation",
            "describing",
            "number",
            "message",
            "negative",
            "graphs",
            "graph",
            "standard",
            "part",
            "±pm",
            "dataset",
            "vertices"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Our experiments use the <span class=\"ltx_text ltx_font_italic\">CyberAgressionAdo-Large</span> dataset<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Publicly available at: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymous.4open.science/r/CyberAgression-Large-C71C\" title=\"\">https://anonymous.4open.science/r/CyberAgression-Large-C71C</a></span></span></span>, an open-access resource of 36 French multi-party conversations collected in educational settings through structured role-playing games. The dataset simulates realistic scenarios of cyber aggression among adolescents in private chat environments, with participants engaging in behaviors ranging from teasing to overt verbal abuse. Each message is annotated for multiple tasks and enriched with metadata such as discursive roles and conversational context. The corpus covers four sensitive topics and six annotation layers, enabling multi-level analyses. Summary statistics of messages and graphs are reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#S4.T1\" title=\"Table 1 &#8227; 4. Experimental Setup &#8227; Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\n\n",
            "<p class=\"ltx_p\">To model conversational structure, we construct interaction graphs centered on a <span class=\"ltx_text ltx_font_italic\">target message</span> (the message to be classified). Graphs are directed and weighted, following the network extraction methodology of&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/access/CecillonLDA24, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib8\" title=\"\">8</a>)</cite>. Nodes represent participants, and edges encode co-occurrence of messages within a fixed-size sliding window of 21 surrounding messages. On average, graph sizes are reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#S4.T1\" title=\"Table 1 &#8227; 4. Experimental Setup &#8227; Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (<math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>48 vertices on average). We refer to that table for the exact range. Polarity is derived from sentiment annotations: messages labeled as positive, negative, or neutral are mapped to signed edges, with neutral treated as positive for experimental purposes. Because polarity is annotation-based rather than predicted, graph structures are inherently language-agnostic. For graph-level models, the full graph is used; for node-level models, only the embedding of the target author node is retained.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Antisocial behavior (ASB) on social media&#8212;including hate speech, harassment, and cyberbullying&#8212;poses growing risks to platform safety and societal well-being. Prior research has focused largely on networks such as X and Reddit, while <span class=\"ltx_text ltx_font_italic\">multi-party conversational settings</span> remain underexplored due to limited data. To address this gap, we use <span class=\"ltx_text ltx_font_italic\">CyberAgressionAdo-Large</span>, a French open-access dataset simulating ASB in multi-party conversations, and evaluate three tasks: <span class=\"ltx_text ltx_font_italic\">abuse detection</span>, <span class=\"ltx_text ltx_font_italic\">bullying behavior analysis</span>, and <span class=\"ltx_text ltx_font_italic\">bullying peer-group identification</span>. We benchmark six text-based and eight graph-based <span class=\"ltx_text ltx_font_italic\">representation-learning methods</span>, analyzing lexical cues, interactional dynamics, and their multimodal fusion. Results show that multimodal models outperform unimodal baselines. The late fusion model <span class=\"ltx_text ltx_font_typewriter\">mBERT + WD-SGCN</span> achieves the best overall results, with top performance on abuse detection (0.718) and competitive scores on peer-group identification (0.286) and bullying analysis (0.606). Error analysis highlights its effectiveness in handling nuanced ASB phenomena such as implicit aggression, role transitions, and context-dependent hostility.</p>\n\n",
                "matched_terms": [
                    "top",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we address these limitations by systematically evaluating state-of-the-art representation learning methods&#8212;spanning both lexical and graph-based approaches&#8212;for ASB detection in multi-party dialogues. Prior work has shown the benefit of combining textual and interactional features; we extend this approach by constructing embeddings that jointly capture semantic content and conversational structure&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/sncs/CecillonLDL21, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib9\" title=\"\">9</a>)</cite> and by testing multimodal fusion strategies that integrate both. Experiments are conducted on the <span class=\"ltx_text ltx_font_italic\">CyberAgressionAdo-Large</span> dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ollagnier2024CyberAgressionAdo, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib34\" title=\"\">34</a>)</cite>, a publicly available corpus of 36 simulated aggressive conversations in French, collected through role-play in educational settings<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymous.4open.science/r/CyberAgression-Large-C71C\" title=\"\">https://anonymous.4open.science/r/CyberAgression-Large-C71C</a></span></span></span>\n. Crucially, to move beyond the narrow subtasks that dominate prior work, we introduce two novel evaluation settings tailored to multi-party contexts: <span class=\"ltx_text ltx_font_italic\">bullying behavior analysis</span> (BBA), which classifies the discursive function of messages (bullying vs. non-abusive intent), and <span class=\"ltx_text ltx_font_italic\">bullying peer group identification</span> (BPI), which identifies participants&#8217; social alignment (e.g., bully vs. victim side). These tasks extend the scope of ASB detection from surface-level classification to pragmatic dimensions of language, capturing intent, stance, and role in dynamic interactions.\nOverall, this paper makes three main contributions: (1) a comprehensive benchmark of unimodal and multimodal representation learning methods for ASB detection in multi-party conversations, (2) the introduction of two pragmatics-oriented evaluation tasks (BBA and BPI) that address group-level and functional dimensions of cyberbullying, and (3) evidence that integrating linguistic and structural features provides a more robust account of the social dynamics underpinning online aggression.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "messages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early work on ASB detection treated messages in isolation, relying on shallow features such as n-grams and sentiment scores. These approaches were soon surpassed by deep learning models (CNNs, RNNs, Transformers) and, more recently, by pretrained language models like BERT and its variants, which now dominate benchmarks on tasks such as target identification and abuse type classification&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/aiccsa/Alrehili19, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib21\" title=\"\">21</a>)</cite>. Beyond message-level analysis, research has increasingly turned to modeling interactional structure. Graph-based methods capture conversational dynamics by representing participants as nodes and exchanges as edges&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/sncs/CecillonLDL21, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib31\" title=\"\">31</a>)</cite>. Traditional embeddings (node2vec, DeepWalk) exploit topological similarity, while Graph Neural Networks (e.g., GCNs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/isci/0004JWLJJH22, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib45\" title=\"\">45</a>)</cite>, GraphSAGE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/tnn/ChenGWWXLLWL22, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib11\" title=\"\">11</a>)</cite>) enable neighborhood-level aggregation. Signed graph learning further enriches these models by encoding the polarity of interactions (support vs. hostility), which is crucial for distinguishing roles such as aggressor and defender&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/access/CecillonLDA24, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib8\" title=\"\">8</a>)</cite>. Early signed embeddings (SNE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/ijcai/WangWZJ17, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib44\" title=\"\">44</a>)</cite>, SIDE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/sdm/KimE20, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib25\" title=\"\">25</a>)</cite>) captured attraction&#8211;repulsion patterns but lacked higher-order structure. Newer signed GNNs (SGCN&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/icdm/Derr0T18, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib18\" title=\"\">18</a>)</cite>, SiGAT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(wang2020, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib24\" title=\"\">24</a>)</cite>, SAT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(li2021signed, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib23\" title=\"\">23</a>)</cite>) address this by integrating polarity-sensitive message passing and balance-aware objectives. Despite these advances, most systems remain either textual or graph-based, with limited integration. Recent multimodal fusion approaches bridge this gap by combining textual embeddings with interactional graphs, improving robustness in scenarios where abusive behavior is distributed across participants or conveyed implicitly&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(cheng2020, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib7\" title=\"\">7</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "graphs",
                    "graph",
                    "messages",
                    "edges",
                    "message"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Bullying Behavior Analysis (BBA)</span>: Classifies each message as cyberbullying (<span class=\"ltx_text ltx_font_typewriter\">CBB</span>) or not (<span class=\"ltx_text ltx_font_typewriter\">NO-CBB</span>), based on predefined discursive roles and their associated intent. The role taxonomy follows prior work&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/coling/Ollagnier24, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib31\" title=\"\">31</a>)</cite>. Roles such as <span class=\"ltx_text ltx_font_typewriter\">attack</span>, <span class=\"ltx_text ltx_font_typewriter\">gaslighting</span>, and <span class=\"ltx_text ltx_font_typewriter\">instigating/abetting</span> are labeled <span class=\"ltx_text ltx_font_typewriter\">CBB</span>, while <span class=\"ltx_text ltx_font_typewriter\">empathy</span>, <span class=\"ltx_text ltx_font_typewriter\">counterspeech</span>, <span class=\"ltx_text ltx_font_typewriter\">conflict resolution</span>, and <span class=\"ltx_text ltx_font_typewriter\">defend</span> are <span class=\"ltx_text ltx_font_typewriter\">NO-CBB</span>. This task explicitly separates abusive behavior from reactive or supportive messages, addressing a key limitation of binary hate detection.</p>\n\n",
                "matched_terms": [
                    "messages",
                    "message"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Lexical embeddings</span>. To encode the semantic content of individual messages, we use six large language models. These include general-purpose systems&#8212;Gemini&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/corr/abs-2403-20327, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib29\" title=\"\">29</a>)</cite>, mBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/corr/abs-1810-04805, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib19\" title=\"\">19</a>)</cite>, GPT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(gpt4, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib36\" title=\"\">36</a>)</cite>&#8212;and French-specific models: CamemBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/acl/MartinMSDRCSS20, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib30\" title=\"\">30</a>)</cite>, FlauBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/lrec/LeVFSCLACBS20, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib28\" title=\"\">28</a>)</cite>, and CamemBERTa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/corr/abs-2411-08868, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib4\" title=\"\">4</a>)</cite>. Each model produces contextualized token embeddings, which are aggregated into a global message-level representation. Importantly, messages are encoded independently: no conversational history is aggregated, ensuring that the representation reflects only the local semantics of the message under analysis.</p>\n\n",
                "matched_terms": [
                    "messages",
                    "message"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Graph embeddings</span>. To capture conversational structure, we construct directed interaction graphs around each target message. A context window defines the set of surrounding messages, and a sliding window identifies potential recipients. Nodes represent participants, while edges encode message exchanges with attributes for polarity (positive/negative), weight (frequency), and direction (reply flow). We evaluate both node-level and whole-graph embeddings. WD-SGCN emphasizes high-weighted incoming connections, while FGSD and GraphWave capture global structural signatures. WalkLets and Node2Vec rely on random-walk strategies to model topological similarity, though they ignore polarity and weight information. SG2V and its weighted variant WD-SG2V extend this idea by incorporating signed edges, thereby encoding positive and negative interactions. Finally, NGNN merges subgraph representations to capture higher-order structural patterns. Together, these methods differ in how they exploit graph attributes such as directionality, polarity, and weight, offering complementary perspectives on conversational structure.</p>\n\n",
                "matched_terms": [
                    "graphs",
                    "positive",
                    "graph",
                    "messages",
                    "edges",
                    "message",
                    "negative"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents the dataset used to evaluate the three ASB tasks, the graph construction process, the evaluation protocol, and implementation details. Our study addresses two research questions:</p>\n\n",
                "matched_terms": [
                    "graph",
                    "dataset",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, the dataset is split into 70% training and 30% testing using stratified sampling to preserve class distributions. All models are assessed with 5-fold cross-validation, and performance is reported using the weighted F1-score, with standard deviation across folds to measure stability. Detailed class distributions are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#A2\" title=\"Appendix B Dataset Distribution &#8227; Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "standard",
                    "deviation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, the conversation-oriented nature of our dataset and methods limits direct comparison with existing ASB detection tools. Most prior approaches rely on proprietary conversational datasets, which prevents reproducibility&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/lrec/CecillonLDL20, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib10\" title=\"\">10</a>)</cite>, while publicly available resources are typically message-level and lack the multi-turn structure required here. Applying such datasets would discard conversational context, leading to incomplete comparisons. Furthermore, most existing tools are trained on English corpora, which hinders their portability to French, as previously noted in the literature&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/computing/CecillonLD25, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib7\" title=\"\">7</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, while our tasks span both binary (e.g., ABD, BBA) and multi-class (e.g., BPI) classification, mapping complex social behaviors to fixed labels can obscure key contextual nuances. For instance, teasing among peers or reactive aggression may be misclassified without considering interpersonal dynamics or conversation history&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(cowie2014understanding, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib16\" title=\"\">16</a>)</cite>. Additionally, our use of undersampling to mitigate class imbalance ensures balanced training but may distort the natural prevalence of abusive behaviors, which are typically rare in real-world settings. Future work should explore alternative approaches&#8212;such as oversampling, cost-sensitive learning, or data augmentation&#8212;to enhance model robustness and better reflect realistic distributions.</p>\n\n",
                "matched_terms": [
                    "conversation",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An additional limitation lies in the current binary design of signed conversational graphs, which model only positive and negative interactions between participants. While effective in capturing antagonistic or affiliative dynamics, this dichotomy overlooks the prevalence and pragmatic significance of neutral exchanges&#8212;such as factual statements, inquiries, or disengaged responses&#8212;that play a crucial role in structuring online conversations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/flairs/OllagnierCV23, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib32\" title=\"\">32</a>)</cite>. Our current experiments map neutral messages to positive edges for simplification, which may conflate passive or ambiguous behavior with active support. Future work should incorporate a third edge type to explicitly represent neutral interactions, thereby enhancing the expressiveness of conversational graphs and enabling a more nuanced modeling of participant roles, pragmatic stance, and socio-discursive functions in cyber aggression.</p>\n\n",
                "matched_terms": [
                    "our",
                    "graphs",
                    "positive",
                    "messages",
                    "edges",
                    "negative"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each dataset, we randomly sampled 70% of the exchanged messages for training and 30% for testing. To maintain consistency and readability, each binary classification scheme is represented using positive and negative classes. For the ABD task, the positive class consists of messages that perpetrate abusive behaviors, while the negative class includes non-abusive messages. For the BBA task, messages exhibiting cyberbullying behaviors are classified as negative, whereas messages that do not contain cyberbullying behaviors belong to the positive class. For the BPI task, each label represents a different role that participants play in cyberbullying interactions, as described in&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/lrec/OllagnierCVB22, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib35\" title=\"\">35</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "messages",
                    "positive",
                    "negative"
                ]
            }
        ]
    },
    "S4.T2": {
        "caption": "Table 2. Average weighted F-scores (mean ±\\pm std. from 5–fold cross-validation) with SVM classifier. Bold marks the best score per task; The row of best overall model is highlighted. Results from other classifiers are in Appendix C.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Embedding Type</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Dimension</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">ABD</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">BBA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">BPI</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Lexical embedding</span></th>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_typewriter\">Gemini004</span></th>\n<td class=\"ltx_td ltx_align_center\">1024</td>\n<td class=\"ltx_td ltx_align_center\">0.676<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02</td>\n<td class=\"ltx_td ltx_align_center\">0.608<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02</td>\n<td class=\"ltx_td ltx_align_center\">0.283<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_typewriter\">CamemBERT</span></th>\n<td class=\"ltx_td ltx_align_center\">768</td>\n<td class=\"ltx_td ltx_align_center\">0.697<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m6\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.01</td>\n<td class=\"ltx_td ltx_align_center\">0.587<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m7\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02</td>\n<td class=\"ltx_td ltx_align_center\">0.279<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m8\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_typewriter\">CamemBERTa</span></th>\n<td class=\"ltx_td ltx_align_center\">768</td>\n<td class=\"ltx_td ltx_align_center\">0.660<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m9\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.01</td>\n<td class=\"ltx_td ltx_align_center\">0.590<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m10\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.01</td>\n<td class=\"ltx_td ltx_align_center\">0.255<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m11\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.01</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_typewriter\">Gemini001</span></th>\n<td class=\"ltx_td ltx_align_center\">768</td>\n<td class=\"ltx_td ltx_align_center\">0.656<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m12\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.01</td>\n<td class=\"ltx_td ltx_align_center\">0.572<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m13\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02</td>\n<td class=\"ltx_td ltx_align_center\">0.267<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m14\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Graph embedding</span></th>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_typewriter\">WD-SGCN</span></th>\n<td class=\"ltx_td ltx_align_center\">128</td>\n<td class=\"ltx_td ltx_align_center\">0.566<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m15\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02</td>\n<td class=\"ltx_td ltx_align_center\">0.479<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m16\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.03</td>\n<td class=\"ltx_td ltx_align_center\">0.296<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m17\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.01</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_typewriter\">FGSD</span></th>\n<td class=\"ltx_td ltx_align_center\">200</td>\n<td class=\"ltx_td ltx_align_center\">0.529<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m18\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.00</td>\n<td class=\"ltx_td ltx_align_center\">0.533<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m19\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.00</td>\n<td class=\"ltx_td ltx_align_center\">0.236<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m20\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_typewriter\">WalkLets</span></th>\n<td class=\"ltx_td ltx_align_center\">32</td>\n<td class=\"ltx_td ltx_align_center\">0.488<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m21\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.00</td>\n<td class=\"ltx_td ltx_align_center\">0.516<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m22\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02</td>\n<td class=\"ltx_td ltx_align_center\">0.237<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m23\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_typewriter\">WD-SG2V</span></th>\n<td class=\"ltx_td ltx_align_center\">128</td>\n<td class=\"ltx_td ltx_align_center\">0.507<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m24\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.00</td>\n<td class=\"ltx_td ltx_align_center\">0.475<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m25\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02</td>\n<td class=\"ltx_td ltx_align_center\">0.211<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m26\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_typewriter\">NGNN</span></th>\n<td class=\"ltx_td ltx_align_center\">64</td>\n<td class=\"ltx_td ltx_align_center\">0.483<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m27\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.01</td>\n<td class=\"ltx_td ltx_align_center\">0.529<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m28\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.01</td>\n<td class=\"ltx_td ltx_align_center\">0.178<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m29\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.01</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_typewriter\">SG2V</span></th>\n<td class=\"ltx_td ltx_align_center\">128</td>\n<td class=\"ltx_td ltx_align_center\">0.481<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m30\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.00</td>\n<td class=\"ltx_td ltx_align_center\">0.444<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m31\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.01</td>\n<td class=\"ltx_td ltx_align_center\">0.231<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m32\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_typewriter\">Node2Vec</span></th>\n<td class=\"ltx_td ltx_align_center\">128</td>\n<td class=\"ltx_td ltx_align_center\">0.467<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m33\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.00</td>\n<td class=\"ltx_td ltx_align_center\">0.465<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m34\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02</td>\n<td class=\"ltx_td ltx_align_center\">0.224<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m35\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.01</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_typewriter\">GraphWave</span></th>\n<td class=\"ltx_td ltx_align_center\">200</td>\n<td class=\"ltx_td ltx_align_center\">0.473<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m36\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.00</td>\n<td class=\"ltx_td ltx_align_center\">0.438<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m37\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.00</td>\n<td class=\"ltx_td ltx_align_center\">0.237<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m38\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Fusion embedding</span></th>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\">Early Fusion</span></th>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_typewriter\">Gemini004 + GraphWave</span></th>\n<td class=\"ltx_td ltx_align_center\">1152</td>\n<td class=\"ltx_td ltx_align_center\">0.684<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m39\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02</td>\n<td class=\"ltx_td ltx_align_center\">0.616<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m40\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.01</td>\n<td class=\"ltx_td ltx_align_center\">0.299<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m41\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_typewriter\">mBERT + SG2V</span></th>\n<td class=\"ltx_td ltx_align_center\">896</td>\n<td class=\"ltx_td ltx_align_center\">0.703<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m42\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.01</td>\n<td class=\"ltx_td ltx_align_center\">0.590<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m43\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.301<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m44\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.01</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_typewriter\">Gemini004 + FGSD</span></th>\n<td class=\"ltx_td ltx_align_center\">776</td>\n<td class=\"ltx_td ltx_align_center\">0.674<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m45\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.01</td>\n<td class=\"ltx_td ltx_align_center\">0.617<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m46\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.01</td>\n<td class=\"ltx_td ltx_align_center\">0.297<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m47\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.03</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\">Late Fusion</span></th>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_typewriter\">Gemini004 + WD-SG2V</span></th>\n<td class=\"ltx_td ltx_align_center\">1152</td>\n<td class=\"ltx_td ltx_align_center\">0.715<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m48\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02</td>\n<td class=\"ltx_td ltx_align_center\">0.616<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m49\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.03</td>\n<td class=\"ltx_td ltx_align_center\">0.284<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m50\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_typewriter\">Gemini001 + WalkLets</span></th>\n<td class=\"ltx_td ltx_align_center\">1032</td>\n<td class=\"ltx_td ltx_align_center\">0.697<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m51\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.625<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m52\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.01</span></td>\n<td class=\"ltx_td ltx_align_center\">0.291<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m53\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_typewriter\">mBERT + WD-SGCN</span></th>\n<td class=\"ltx_td ltx_align_center\">776</td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">0.718<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m54\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" style=\"--ltx-bg-color:#E6E6E6;\">&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">0.606<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m55\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" style=\"--ltx-bg-color:#E6E6E6;\">&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.01</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text\" style=\"--ltx-bg-color:#E6E6E6;\">0.286<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m56\" intent=\":literal\"><semantics><mo mathbackground=\"#E6E6E6\" style=\"--ltx-bg-color:#E6E6E6;\">&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.00</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_bold\">Hybrid Fusion</span></th>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_typewriter\">mBERT + NGNN</span></th>\n<td class=\"ltx_td ltx_align_center\">1032</td>\n<td class=\"ltx_td ltx_align_center\">0.707<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m57\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02</td>\n<td class=\"ltx_td ltx_align_center\">0.613<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m58\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.01</td>\n<td class=\"ltx_td ltx_align_center\">0.286<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m59\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text ltx_font_typewriter\">Gemini004 + Node2Vec</span></th>\n<td class=\"ltx_td ltx_align_center\">896</td>\n<td class=\"ltx_td ltx_align_center\">0.696<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m60\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02</td>\n<td class=\"ltx_td ltx_align_center\">0.617<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m61\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.03</td>\n<td class=\"ltx_td ltx_align_center\">0.277<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m62\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span class=\"ltx_text ltx_font_typewriter\">CamemBERTa + WD-SGCN</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">776</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.698<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m63\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.604<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m64\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.267<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m65\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.00</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "wdsgcn",
            "0277±pm000",
            "wdsg2v",
            "0698±pm001",
            "bpi",
            "overall",
            "0656±pm001",
            "0516±pm002",
            "0684±pm002",
            "classifier",
            "from",
            "0473±pm000",
            "0438±pm000",
            "bold",
            "0284±pm000",
            "0488±pm000",
            "graphwave",
            "late",
            "results",
            "0529±pm001",
            "0237±pm002",
            "gemini004",
            "0529±pm000",
            "0608±pm002",
            "other",
            "0606±pm001",
            "svm",
            "fusion",
            "0465±pm002",
            "0590±pm002",
            "0286±pm000",
            "embedding",
            "abd",
            "type",
            "marks",
            "0483±pm001",
            "ngnn",
            "0707±pm002",
            "5–fold",
            "0572±pm002",
            "0297±pm003",
            "fscores",
            "0283±pm002",
            "0475±pm002",
            "camembert",
            "0291±pm000",
            "0301±pm001",
            "mean",
            "0507±pm000",
            "0237±pm000",
            "0231±pm000",
            "0296±pm001",
            "0674±pm001",
            "0255±pm001",
            "±pm",
            "0697±pm001",
            "std",
            "mbert",
            "0533±pm000",
            "hybrid",
            "0479±pm003",
            "0224±pm001",
            "0279±pm002",
            "classifiers",
            "0676±pm002",
            "0267±pm002",
            "crossvalidation",
            "0590±pm001",
            "average",
            "0299±pm002",
            "0617±pm001",
            "0696±pm002",
            "0703±pm001",
            "0267±pm000",
            "0616±pm003",
            "0660±pm001",
            "camemberta",
            "0616±pm001",
            "fgsd",
            "node2vec",
            "0481±pm000",
            "0718±pm002",
            "bba",
            "early",
            "score",
            "gemini001",
            "task",
            "model",
            "0236±pm002",
            "lexical",
            "0697±pm002",
            "row",
            "0613±pm001",
            "0566±pm002",
            "appendix",
            "0587±pm002",
            "0715±pm002",
            "0211±pm000",
            "0625±pm001",
            "highlighted",
            "0604±pm002",
            "sg2v",
            "0467±pm000",
            "walklets",
            "graph",
            "weighted",
            "best",
            "0617±pm003",
            "0444±pm001",
            "dimension",
            "0178±pm001"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To address <span class=\"ltx_text ltx_font_bold\">RQ1</span> and <span class=\"ltx_text ltx_font_bold\">RQ2</span>, we evaluate all representation learning methods on the <span class=\"ltx_text ltx_font_italic\">CyberAgressionAdo-Large</span> dataset. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#S4.T2\" title=\"Table 2 &#8227; 4.1. Performance Evaluation &#8227; 4. Experimental Setup &#8227; Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reports the top-performing models, while the full set of results is available in the KIDOS project repository<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://drive.google.com/drive/folders/1f21Pd1h-VDQAH4hK_bilIMvp4bUves_C?usp=sharing\" title=\"\">https://drive.google.com/drive/folders/1f21Pd1h-VDQAH4hK_bilIMvp4bUves_C?usp=sharing</a></span></span></span>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Antisocial behavior (ASB) on social media&#8212;including hate speech, harassment, and cyberbullying&#8212;poses growing risks to platform safety and societal well-being. Prior research has focused largely on networks such as X and Reddit, while <span class=\"ltx_text ltx_font_italic\">multi-party conversational settings</span> remain underexplored due to limited data. To address this gap, we use <span class=\"ltx_text ltx_font_italic\">CyberAgressionAdo-Large</span>, a French open-access dataset simulating ASB in multi-party conversations, and evaluate three tasks: <span class=\"ltx_text ltx_font_italic\">abuse detection</span>, <span class=\"ltx_text ltx_font_italic\">bullying behavior analysis</span>, and <span class=\"ltx_text ltx_font_italic\">bullying peer-group identification</span>. We benchmark six text-based and eight graph-based <span class=\"ltx_text ltx_font_italic\">representation-learning methods</span>, analyzing lexical cues, interactional dynamics, and their multimodal fusion. Results show that multimodal models outperform unimodal baselines. The late fusion model <span class=\"ltx_text ltx_font_typewriter\">mBERT + WD-SGCN</span> achieves the best overall results, with top performance on abuse detection (0.718) and competitive scores on peer-group identification (0.286) and bullying analysis (0.606). Error analysis highlights its effectiveness in handling nuanced ASB phenomena such as implicit aggression, role transitions, and context-dependent hostility.</p>\n\n",
                "matched_terms": [
                    "mbert",
                    "wdsgcn",
                    "model",
                    "overall",
                    "best",
                    "late",
                    "lexical",
                    "fusion",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we address these limitations by systematically evaluating state-of-the-art representation learning methods&#8212;spanning both lexical and graph-based approaches&#8212;for ASB detection in multi-party dialogues. Prior work has shown the benefit of combining textual and interactional features; we extend this approach by constructing embeddings that jointly capture semantic content and conversational structure&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/sncs/CecillonLDL21, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib9\" title=\"\">9</a>)</cite> and by testing multimodal fusion strategies that integrate both. Experiments are conducted on the <span class=\"ltx_text ltx_font_italic\">CyberAgressionAdo-Large</span> dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ollagnier2024CyberAgressionAdo, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib34\" title=\"\">34</a>)</cite>, a publicly available corpus of 36 simulated aggressive conversations in French, collected through role-play in educational settings<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymous.4open.science/r/CyberAgression-Large-C71C\" title=\"\">https://anonymous.4open.science/r/CyberAgression-Large-C71C</a></span></span></span>\n. Crucially, to move beyond the narrow subtasks that dominate prior work, we introduce two novel evaluation settings tailored to multi-party contexts: <span class=\"ltx_text ltx_font_italic\">bullying behavior analysis</span> (BBA), which classifies the discursive function of messages (bullying vs. non-abusive intent), and <span class=\"ltx_text ltx_font_italic\">bullying peer group identification</span> (BPI), which identifies participants&#8217; social alignment (e.g., bully vs. victim side). These tasks extend the scope of ASB detection from surface-level classification to pragmatic dimensions of language, capturing intent, stance, and role in dynamic interactions.\nOverall, this paper makes three main contributions: (1) a comprehensive benchmark of unimodal and multimodal representation learning methods for ASB detection in multi-party conversations, (2) the introduction of two pragmatics-oriented evaluation tasks (BBA and BPI) that address group-level and functional dimensions of cyberbullying, and (3) evidence that integrating linguistic and structural features provides a more robust account of the social dynamics underpinning online aggression.</p>\n\n",
                "matched_terms": [
                    "bba",
                    "overall",
                    "bpi",
                    "lexical",
                    "from",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early work on ASB detection treated messages in isolation, relying on shallow features such as n-grams and sentiment scores. These approaches were soon surpassed by deep learning models (CNNs, RNNs, Transformers) and, more recently, by pretrained language models like BERT and its variants, which now dominate benchmarks on tasks such as target identification and abuse type classification&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/aiccsa/Alrehili19, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib21\" title=\"\">21</a>)</cite>. Beyond message-level analysis, research has increasingly turned to modeling interactional structure. Graph-based methods capture conversational dynamics by representing participants as nodes and exchanges as edges&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/sncs/CecillonLDL21, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib31\" title=\"\">31</a>)</cite>. Traditional embeddings (node2vec, DeepWalk) exploit topological similarity, while Graph Neural Networks (e.g., GCNs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/isci/0004JWLJJH22, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib45\" title=\"\">45</a>)</cite>, GraphSAGE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/tnn/ChenGWWXLLWL22, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib11\" title=\"\">11</a>)</cite>) enable neighborhood-level aggregation. Signed graph learning further enriches these models by encoding the polarity of interactions (support vs. hostility), which is crucial for distinguishing roles such as aggressor and defender&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/access/CecillonLDA24, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib8\" title=\"\">8</a>)</cite>. Early signed embeddings (SNE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/ijcai/WangWZJ17, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib44\" title=\"\">44</a>)</cite>, SIDE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/sdm/KimE20, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib25\" title=\"\">25</a>)</cite>) captured attraction&#8211;repulsion patterns but lacked higher-order structure. Newer signed GNNs (SGCN&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/icdm/Derr0T18, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib18\" title=\"\">18</a>)</cite>, SiGAT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(wang2020, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib24\" title=\"\">24</a>)</cite>, SAT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(li2021signed, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib23\" title=\"\">23</a>)</cite>) address this by integrating polarity-sensitive message passing and balance-aware objectives. Despite these advances, most systems remain either textual or graph-based, with limited integration. Recent multimodal fusion approaches bridge this gap by combining textual embeddings with interactional graphs, improving robustness in scenarios where abusive behavior is distributed across participants or conveyed implicitly&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(cheng2020, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib7\" title=\"\">7</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "type",
                    "graph",
                    "node2vec",
                    "early",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Bullying Behavior Analysis (BBA)</span>: Classifies each message as cyberbullying (<span class=\"ltx_text ltx_font_typewriter\">CBB</span>) or not (<span class=\"ltx_text ltx_font_typewriter\">NO-CBB</span>), based on predefined discursive roles and their associated intent. The role taxonomy follows prior work&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/coling/Ollagnier24, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib31\" title=\"\">31</a>)</cite>. Roles such as <span class=\"ltx_text ltx_font_typewriter\">attack</span>, <span class=\"ltx_text ltx_font_typewriter\">gaslighting</span>, and <span class=\"ltx_text ltx_font_typewriter\">instigating/abetting</span> are labeled <span class=\"ltx_text ltx_font_typewriter\">CBB</span>, while <span class=\"ltx_text ltx_font_typewriter\">empathy</span>, <span class=\"ltx_text ltx_font_typewriter\">counterspeech</span>, <span class=\"ltx_text ltx_font_typewriter\">conflict resolution</span>, and <span class=\"ltx_text ltx_font_typewriter\">defend</span> are <span class=\"ltx_text ltx_font_typewriter\">NO-CBB</span>. This task explicitly separates abusive behavior from reactive or supportive messages, addressing a key limitation of binary hate detection.</p>\n\n",
                "matched_terms": [
                    "task",
                    "from",
                    "bba"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section outlines the representation learning methods employed to tackle the three core tasks: abuse detection (ABD), BBA, and BPI. ABD is framed as a binary classification task that determines whether a message contains abusive content, independent of the sender&#8217;s role or conversational context&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/coling/Ollagnier24, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib31\" title=\"\">31</a>)</cite>. To address these tasks, we benchmark a diverse set of text-based, graph-based, and fusion-based models.</p>\n\n",
                "matched_terms": [
                    "task",
                    "abd",
                    "bba",
                    "bpi"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Lexical embeddings</span>. To encode the semantic content of individual messages, we use six large language models. These include general-purpose systems&#8212;Gemini&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/corr/abs-2403-20327, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib29\" title=\"\">29</a>)</cite>, mBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/corr/abs-1810-04805, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib19\" title=\"\">19</a>)</cite>, GPT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(gpt4, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib36\" title=\"\">36</a>)</cite>&#8212;and French-specific models: CamemBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/acl/MartinMSDRCSS20, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib30\" title=\"\">30</a>)</cite>, FlauBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/lrec/LeVFSCLACBS20, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib28\" title=\"\">28</a>)</cite>, and CamemBERTa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/corr/abs-2411-08868, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib4\" title=\"\">4</a>)</cite>. Each model produces contextualized token embeddings, which are aggregated into a global message-level representation. Importantly, messages are encoded independently: no conversational history is aggregated, ensuring that the representation reflects only the local semantics of the message under analysis.</p>\n\n",
                "matched_terms": [
                    "mbert",
                    "camembert",
                    "model",
                    "lexical",
                    "camemberta"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Graph embeddings</span>. To capture conversational structure, we construct directed interaction graphs around each target message. A context window defines the set of surrounding messages, and a sliding window identifies potential recipients. Nodes represent participants, while edges encode message exchanges with attributes for polarity (positive/negative), weight (frequency), and direction (reply flow). We evaluate both node-level and whole-graph embeddings. WD-SGCN emphasizes high-weighted incoming connections, while FGSD and GraphWave capture global structural signatures. WalkLets and Node2Vec rely on random-walk strategies to model topological similarity, though they ignore polarity and weight information. SG2V and its weighted variant WD-SG2V extend this idea by incorporating signed edges, thereby encoding positive and negative interactions. Finally, NGNN merges subgraph representations to capture higher-order structural patterns. Together, these methods differ in how they exploit graph attributes such as directionality, polarity, and weight, offering complementary perspectives on conversational structure.</p>\n\n",
                "matched_terms": [
                    "ngnn",
                    "wdsgcn",
                    "walklets",
                    "graph",
                    "wdsg2v",
                    "fgsd",
                    "node2vec",
                    "model",
                    "weighted",
                    "graphwave",
                    "sg2v"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fusion strategies</span>. To integrate linguistic and structural signals, we implement three strategies, illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#acmlabel2\" title=\"Figure 2 &#8227; 3.2. Modeling Conversations &#8227; 3. Addressing Online Hate in Conversations &#8227; Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Each builds on representations and classifiers trained separately on text and graph modalities&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:phd/hal/Cecillon24, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib6\" title=\"\">6</a>)</cite>. In <span class=\"ltx_text ltx_font_italic\">early fusion</span>, embeddings from both modalities are concatenated into a unified feature vector for training a new classifier (e.g., SVM), allowing joint learning at the representation level and direct interaction between lexical and structural features. In <span class=\"ltx_text ltx_font_italic\">late fusion</span>, prediction scores from unimodal classifiers are fed into a meta-classifier, which combines their outputs and is particularly effective for integrating heterogeneous or noisy signals. Finally, <span class=\"ltx_text ltx_font_italic\">hybrid fusion</span> leverages both embedding-level and score-level information, enabling the final classifier to exploit fine-grained features as well as high-level decision patterns, thereby enhancing robustness and adaptability.</p>\n\n",
                "matched_terms": [
                    "graph",
                    "classifier",
                    "early",
                    "hybrid",
                    "lexical",
                    "from",
                    "late",
                    "fusion",
                    "svm",
                    "classifiers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RQ1</span>: How do different representation learning modalities (lexical, structural, and fusion-based) perform across the three ASB tasks (ABD, BBA, and BPI)?</p>\n\n",
                "matched_terms": [
                    "bpi",
                    "lexical",
                    "abd",
                    "bba"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RQ2</span>: To what extent do multimodal fusion strategies (early, late, and hybrid) improve the modeling of abusive dynamics and participant roles in multi-party conversations compared to unimodal modalities?</p>\n\n",
                "matched_terms": [
                    "early",
                    "hybrid",
                    "late",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To model conversational structure, we construct interaction graphs centered on a <span class=\"ltx_text ltx_font_italic\">target message</span> (the message to be classified). Graphs are directed and weighted, following the network extraction methodology of&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/access/CecillonLDA24, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib8\" title=\"\">8</a>)</cite>. Nodes represent participants, and edges encode co-occurrence of messages within a fixed-size sliding window of 21 surrounding messages. On average, graph sizes are reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#S4.T1\" title=\"Table 1 &#8227; 4. Experimental Setup &#8227; Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (<math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>48 vertices on average). We refer to that table for the exact range. Polarity is derived from sentiment annotations: messages labeled as positive, negative, or neutral are mapped to signed edges, with neutral treated as positive for experimental purposes. Because polarity is annotation-based rather than predicted, graph structures are inherently language-agnostic. For graph-level models, the full graph is used; for node-level models, only the embedding of the target author node is retained.</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "graph",
                    "average",
                    "model",
                    "weighted",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, the dataset is split into 70% training and 30% testing using stratified sampling to preserve class distributions. All models are assessed with 5-fold cross-validation, and performance is reported using the weighted F1-score, with standard deviation across folds to measure stability. Detailed class distributions are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#A2\" title=\"Appendix B Dataset Distribution &#8227; Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "weighted",
                    "appendix",
                    "crossvalidation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#S3.SS2\" title=\"3.2. Modeling Conversations &#8227; 3. Addressing Online Hate in Conversations &#8227; Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, we benchmark six lexical and eight graph-based representation learning methods. We also evaluate fusion strategies that combine top-performing lexical models (<span class=\"ltx_text ltx_font_typewriter\">CamemBERTa</span>, <span class=\"ltx_text ltx_font_typewriter\">Gemini</span>, and <span class=\"ltx_text ltx_font_typewriter\">mBERT</span>) with graph embeddings. All representations are classified using Support Vector Machines (SVM) implemented with cuML and scikit-learn. Experiments were conducted on a machine with an Intel&#174;Core&#8482;i7&#8211;1065G7 3.5 GHz CPU and an Nvidia A100 GPU. Hyperparameter settings are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#A3\" title=\"Appendix C Model Detailed Evaluation &#8227; Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "mbert",
                    "graph",
                    "appendix",
                    "lexical",
                    "fusion",
                    "svm",
                    "camemberta"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">RQ1</span>, clear differences emerge between lexical and graph-based encoders. Among lexical models, <span class=\"ltx_text ltx_font_typewriter\">CamemBERT</span> yields the strongest overall performance (0.697<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.01 on ABD, 0.587<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02 on BBA), closely followed by <span class=\"ltx_text ltx_font_typewriter\">Gemini004</span> with slightly better results on BBA (0.608<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m3\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02). By contrast, <span class=\"ltx_text ltx_font_typewriter\">CamemBERTa</span> and <span class=\"ltx_text ltx_font_typewriter\">Gemini001</span> consistently lag behind. Yet across all lexical models, performance on BPI remains modest (0.255&#8211;0.283), underscoring a fundamental limitation of content-only features: while effective for recognizing abusive language and intent, they fail to capture relational dynamics that define peer-group interactions.</p>\n\n",
                "matched_terms": [
                    "gemini001",
                    "abd",
                    "camembert",
                    "0587±pm002",
                    "gemini004",
                    "bpi",
                    "overall",
                    "bba",
                    "0608±pm002",
                    "lexical",
                    "0697±pm001",
                    "results",
                    "camemberta"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Graph embeddings reveal the opposite pattern. <span class=\"ltx_text ltx_font_typewriter\">WD-SGCN</span>, which integrates direction, polarity, and edge weights, achieves the highest BPI score among unimodal models (<math alttext=\"{0.296\\pm 0.01}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>0.296</mn><mo>&#177;</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">{0.296\\pm 0.01}</annotation></semantics></math>), highlighting the importance of modeling signed and weighted interactions for role attribution. In contrast, approaches neglecting edge semantics&#8212;such as <span class=\"ltx_text ltx_font_typewriter\">Node2Vec</span>, <span class=\"ltx_text ltx_font_typewriter\">SG2V</span>, or <span class=\"ltx_text ltx_font_typewriter\">GraphWave</span>&#8212;underperform substantially on BPI (<math alttext=\"\\leq\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mo>&#8804;</mo><annotation encoding=\"application/x-tex\">\\leq</annotation></semantics></math>0.237). Interestingly, models like <span class=\"ltx_text ltx_font_typewriter\">FGSD</span> and <span class=\"ltx_text ltx_font_typewriter\">NGNN</span> yield balanced but unremarkable results, suggesting some sensitivity to structural patterns but insufficient discriminatory power for complex role-based reasoning. Taken together, these results suggest that lexical models specialize in surface-level abuse detection, while graph-based methods are better suited for socially grounded classification, particularly when capturing implicit roles and alignments.</p>\n\n",
                "matched_terms": [
                    "score",
                    "ngnn",
                    "wdsgcn",
                    "graph",
                    "fgsd",
                    "node2vec",
                    "bpi",
                    "weighted",
                    "lexical",
                    "results",
                    "sg2v"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Early fusion</span>, which concatenates lexical and structural embeddings, proves most effective for BPI. The combination <span class=\"ltx_text ltx_font_typewriter\">mBERT + SG2V</span> achieves the overall best BPI score (0.301<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.01), while <span class=\"ltx_text ltx_font_typewriter\">Gemini004 + GraphWave</span> (0.299<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m2\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02) and <span class=\"ltx_text ltx_font_typewriter\">Gemini004 + FGSD</span> (0.297<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m3\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.03) also perform strongly. These results suggest that when peer-group identification requires fine-grained integration of semantic and relational cues, representation-level fusion is the most advantageous.</p>\n\n",
                "matched_terms": [
                    "mbert",
                    "score",
                    "fgsd",
                    "0299±pm002",
                    "0301±pm001",
                    "gemini004",
                    "bpi",
                    "overall",
                    "early",
                    "graphwave",
                    "lexical",
                    "best",
                    "fusion",
                    "results",
                    "sg2v",
                    "0297±pm003"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Late fusion</span>, which ensembles predictions from unimodal classifiers, delivers the highest scores on ABD and BBA. Notably, <span class=\"ltx_text ltx_font_typewriter\">mBERT + WD-SGCN</span> achieves the top ABD result overall (0.718<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02), while <span class=\"ltx_text ltx_font_typewriter\">Gemini001 + WalkLets</span> dominates BBA (0.625<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m2\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.01). Although slightly weaker on BPI <math alttext=\"\\leq\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m3\" intent=\":literal\"><semantics><mo>&#8804;</mo><annotation encoding=\"application/x-tex\">\\leq</annotation></semantics></math>0.291), late fusion provides the strongest and most balanced improvements on content- and behavior-oriented tasks, indicating that decision-level integration leverages modality-specific strengths without overfitting to noisy cues.\nWhile <span class=\"ltx_text ltx_font_typewriter\">mBERT + WD-SGCN</span> delivers the <em class=\"ltx_emph ltx_font_italic\">best overall</em> trade-off across tasks (top ABD and competitive BBA/BPI), the top BPI score is obtained by an early-fusion model (<span class=\"ltx_text ltx_font_typewriter\">mBERT + SG2V</span>, 0.301). We therefore distinguish per-task bests from the overall best compromise.</p>\n\n",
                "matched_terms": [
                    "mbert",
                    "gemini001",
                    "walklets",
                    "abd",
                    "wdsgcn",
                    "score",
                    "0625±pm001",
                    "model",
                    "0718±pm002",
                    "bpi",
                    "overall",
                    "bba",
                    "best",
                    "late",
                    "from",
                    "fusion",
                    "sg2v",
                    "classifiers"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Hybrid fusion</span>, which combines embeddings with prediction scores, does not reach the top F1-scores but yields consistently stable performance. For instance, <span class=\"ltx_text ltx_font_typewriter\">mBERT + NGNN</span> achieves 0.707<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p7.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02 on ABD, 0.613<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p7.m2\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.01 on BBA, and 0.286<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p7.m3\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.00 on BPI. The relatively low variance across folds suggests stronger robustness and adaptability&#8212;an important property in real-world applications where conversational dynamics and abuse strategies are highly variable.</p>\n\n",
                "matched_terms": [
                    "mbert",
                    "ngnn",
                    "abd",
                    "0707±pm002",
                    "0613±pm001",
                    "bpi",
                    "bba",
                    "hybrid",
                    "fusion",
                    "0286±pm000"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, lexical models perform best on content-oriented tasks (ABD, BBA), while graph-based encoders show relative advantages on socially grounded reasoning, particularly in BPI, where modeling interaction structure is essential. Yet absolute performance on BPI remains modest across models, reflecting the difficulty of this four-class, highly imbalanced task and pointing to the need for richer approaches to capture peer-group dynamics. Fusion strategies consistently outperform unimodal baselines, with early fusion proving most effective for relational inference, late fusion excelling at content-driven detection, and hybrid fusion offering robust generalization. Taken together, these results underscore the complementary strengths of lexical and structural cues and highlight the necessity of their integration to model the nuanced and evolving dynamics of abusive behavior in multi-party online conversations.</p>\n\n",
                "matched_terms": [
                    "abd",
                    "task",
                    "model",
                    "bpi",
                    "overall",
                    "bba",
                    "hybrid",
                    "late",
                    "early",
                    "fusion",
                    "results",
                    "lexical",
                    "best"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To deepen our understanding of how different representation learning strategies handle abusive dynamics, we conducted a systematic error analysis across the three classification tasks. Our goal was to evaluate not only overall accuracy but also each model&#8217;s capacity to capture participant roles, pragmatic cues, and evolving discourse structures in multi-party conversations. \n<br class=\"ltx_break\"/>All misclassification counts in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#S4.T3\" title=\"Table 3 &#8227; 4.2. Error Analysis &#8227; 4. Experimental Setup &#8227; Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> are aggregated over the five test splits in our cross-validation. For each model, we collect the misclassified instances from the test set of every split and sum them to obtain the total per class. The reported values therefore reflect cumulative errors across all folds, not results from a single split.</p>\n\n",
                "matched_terms": [
                    "crossvalidation",
                    "model",
                    "overall",
                    "from",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#S4.T3\" title=\"Table 3 &#8227; 4.2. Error Analysis &#8227; 4. Experimental Setup &#8227; Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows that text-based models misclassify a substantial portion of <em class=\"ltx_emph ltx_font_italic\">Abusive</em> messages (e.g., Gemini004: 22.45% / 229). In contrast, graph-based models yield higher error rates for both <em class=\"ltx_emph ltx_font_italic\">Abusive</em> messages (up to 37.65% / 384) and <em class=\"ltx_emph ltx_font_italic\">Non-abusive</em> cases (FGSD: 49.68% / 312), indicating a systematic over-detection bias.</p>\n\n",
                "matched_terms": [
                    "fgsd",
                    "gemini004"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Misclassified instances often involve implicit aggression, sarcasm, or connotative tone&#8212;features that are not structurally salient. For example, the message <em class=\"ltx_emph ltx_font_italic\">&#8220;elle est bonne la meuf&#8221;</em> (EN: <em class=\"ltx_emph ltx_font_italic\">&#8220;That girl is hot&#8221;</em>), labeled as abusive for its objectifying tone, was missed by GraphWave but correctly identified by Gemini004, highlighting that lexical cues remain critical for capturing implicit forms of abuse.</p>\n\n",
                "matched_terms": [
                    "gemini004",
                    "graphwave",
                    "lexical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Both text and fusion models accumulate high error rates on the <em class=\"ltx_emph ltx_font_italic\">Bullying</em> label (43&#8211;46% misclassified, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#S4.T3\" title=\"Table 3 &#8227; 4.2. Error Analysis &#8227; 4. Experimental Setup &#8227; Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), showing persistent difficulty in distinguishing bullying from non-bullying when cues are indirect. Graph-based models distribute errors more evenly but still misclassify about one third of both classes. For instance, the message <em class=\"ltx_emph ltx_font_italic\">&#8220;c&#8217;est toi qui commence a chercher les gens&#8221;</em> (EN: <em class=\"ltx_emph ltx_font_italic\">You&#8217;re the one who starts provoking people</em>) was misclassified as <span class=\"ltx_text ltx_font_smallcaps\">No-CBB</span> by GraphWave, while Gemini004 correctly inferred the accusatory stance, illustrating the lexical vs. structural contrast.</p>\n\n",
                "matched_terms": [
                    "gemini004",
                    "graphwave",
                    "lexical",
                    "from",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Errors are not uniform across roles. <em class=\"ltx_emph ltx_font_italic\">Victim Support</em> is systematically the most misclassified (CamemBERT: 24.65% / 211, GraphWave: 23.83% / 204, FGSD: 27.57% / 236). In contrast, explicit aggressor roles (<em class=\"ltx_emph ltx_font_italic\">Bully</em>, <em class=\"ltx_emph ltx_font_italic\">Bully Support</em>) show lower error shares (<math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSSx1.Px3.p1.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>15&#8211;22%), suggesting they are easier to detect. Example: the message <em class=\"ltx_emph ltx_font_italic\">&#8220;tu fais le mec intelligent pour impressionner les autres&#8221;</em> (EN: <em class=\"ltx_emph ltx_font_italic\">You&#8217;re pretending to be smart to impress others</em>) was correctly flagged as aggressive by Gemini004 but missed by GraphWave, confirming that subtle antagonism requires lexical sensitivity.</p>\n\n",
                "matched_terms": [
                    "camembert",
                    "fgsd",
                    "gemini004",
                    "graphwave",
                    "lexical"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across modalities, the hardest cases are implicit abuse (ABD, BBA) and ambiguous supportive roles (BPI). Graph encoders over-predict abuse, while text encoders miss subtler aggression.</p>\n\n",
                "matched_terms": [
                    "graph",
                    "abd",
                    "bba",
                    "bpi"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fusion reduces misclassifications on the <em class=\"ltx_emph ltx_font_italic\">Abusive</em> class (down to 19.12% / 195) compared to 22&#8211;38% with single modalities. However, this comes at the cost of slightly more false positives on <em class=\"ltx_emph ltx_font_italic\">Non-abusive</em> samples (<math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSSx2.Px1.p1.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>45% vs. 28% for mBERT alone).</p>\n\n",
                "matched_terms": [
                    "mbert",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fusion particularly reduces errors on aggressor roles. <em class=\"ltx_emph ltx_font_italic\">Bully</em> misclassifications drop from 22.49% (159, text) to 18.00% (126, fusion), and <em class=\"ltx_emph ltx_font_italic\">Bully Support</em> from 22.21% (157) to 17.14% (120). At the same time, <em class=\"ltx_emph ltx_font_italic\">Victim</em> and <em class=\"ltx_emph ltx_font_italic\">Victim Support</em> errors increase, reflecting a redistribution of mistakes rather than a global reduction.</p>\n\n",
                "matched_terms": [
                    "from",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fusion strategies shift rather than eliminate errors. They reduce false negatives for aggressor roles (ABD <em class=\"ltx_emph ltx_font_italic\">Abusive</em>, BPI <em class=\"ltx_emph ltx_font_italic\">Bully</em>) but increase confusion for neutral or supportive roles. This pattern highlights complementary error profiles between text and graph modalities and underscores the need for adaptive fusion mechanisms sensitive to role dynamics and pragmatic subtleties.</p>\n\n",
                "matched_terms": [
                    "graph",
                    "abd",
                    "fusion",
                    "bpi"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper presents representation learning methods for detecting hate speech in multi-party dialogues and introduces two new evaluation tasks: Bullying Behavior Analysis (BBA) and Bullying Peer Group Identification (BPI). These tasks provide a more granular understanding of how aggressive behaviors emerge and evolve within multi-participant conversations. We benchmark a broad spectrum of lexical and graph-based embedding models and assess fusion strategies that combine linguistic content with structural interaction patterns. Results show that multimodal approaches consistently outperform unimodal baselines&#8212;particularly on socially complex tasks like BPI, where understanding implicit roles and interactional dynamics is crucial. Lexical models excel in tasks driven by explicit textual cues (ABD, BBA), while graph-based models are better suited to relational inference. Fusion strategies, especially early fusion, capitalize on the complementary strengths of both modalities, yielding the most robust and context-aware representations for abuse detection.</p>\n\n",
                "matched_terms": [
                    "embedding",
                    "abd",
                    "bpi",
                    "bba",
                    "early",
                    "lexical",
                    "fusion",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In future work, more integrated representation learning could be explored by jointly encoding lexical and structural features within a unified model, rather than learning them separately. Additionally, while our experiments rely on structured role-play data for ethical and practical reasons, validating models on real-world conversations across different platforms and demographics will be critical for assessing generalizability. Incorporating richer context, such as temporal dynamics, user-level features, and conversational history, may further enhance the detection of subtle or evolving aggression patterns.</p>\n\n",
                "matched_terms": [
                    "lexical",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, while our tasks span both binary (e.g., ABD, BBA) and multi-class (e.g., BPI) classification, mapping complex social behaviors to fixed labels can obscure key contextual nuances. For instance, teasing among peers or reactive aggression may be misclassified without considering interpersonal dynamics or conversation history&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(cowie2014understanding, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib16\" title=\"\">16</a>)</cite>. Additionally, our use of undersampling to mitigate class imbalance ensures balanced training but may distort the natural prevalence of abusive behaviors, which are typically rare in real-world settings. Future work should explore alternative approaches&#8212;such as oversampling, cost-sensitive learning, or data augmentation&#8212;to enhance model robustness and better reflect realistic distributions.</p>\n\n",
                "matched_terms": [
                    "model",
                    "abd",
                    "bba",
                    "bpi"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An additional limitation lies in the current binary design of signed conversational graphs, which model only positive and negative interactions between participants. While effective in capturing antagonistic or affiliative dynamics, this dichotomy overlooks the prevalence and pragmatic significance of neutral exchanges&#8212;such as factual statements, inquiries, or disengaged responses&#8212;that play a crucial role in structuring online conversations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/flairs/OllagnierCV23, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib32\" title=\"\">32</a>)</cite>. Our current experiments map neutral messages to positive edges for simplification, which may conflate passive or ambiguous behavior with active support. Future work should incorporate a third edge type to explicitly represent neutral interactions, thereby enhancing the expressiveness of conversational graphs and enabling a more nuanced modeling of participant roles, pragmatic stance, and socio-discursive functions in cyber aggression.</p>\n\n",
                "matched_terms": [
                    "model",
                    "type"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The annotation process was similarly designed to minimize emotional burden and ensure labeling consistency, following best practices recommended in&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(vidgen-etal-2019-challenges, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib43\" title=\"\">43</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib26\" title=\"\">26</a>)</cite>. Annotators followed a structured training protocol that included reviewing detailed task guidelines and public definitions of antisocial behaviors, studying annotated examples&#8212;including edge cases such as implicit abuse or sarcasm&#8212;participating in trial labeling sessions with feedback, and engaging in regular calibration meetings to resolve disagreements and align interpretations. Institutional oversight, content warnings, and limited exposure protocols were also implemented to safeguard annotator well-being throughout the process; further details can be found in&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ollagnier2024CyberAgressionAdo, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib34\" title=\"\">34</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "task",
                    "best"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From a modeling perspective, inferring behavioral roles and intentions raises ethical and epistemological concerns, as assigned labels often rely on subjective interpretation and lack objective ground truth&#8212;even though the conversations in this dataset were collected through scenario-based role-play. Supporting subjectivity in NLP annotation is essential, as it allows models to reflect and explain the diverse perspectives of real individuals. In this context, the dataset provides annotations from three independent annotators, opening the door to perspectivist approaches&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/aaai/CabitzaCB23, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib5\" title=\"\">5</a>)</cite> that embrace disagreement and model a plurality of viewpoints.</p>\n\n",
                "matched_terms": [
                    "model",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each dataset, we randomly sampled 70% of the exchanged messages for training and 30% for testing. To maintain consistency and readability, each binary classification scheme is represented using positive and negative classes. For the ABD task, the positive class consists of messages that perpetrate abusive behaviors, while the negative class includes non-abusive messages. For the BBA task, messages exhibiting cyberbullying behaviors are classified as negative, whereas messages that do not contain cyberbullying behaviors belong to the positive class. For the BPI task, each label represents a different role that participants play in cyberbullying interactions, as described in&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/lrec/OllagnierCVB22, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib35\" title=\"\">35</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "task",
                    "abd",
                    "bba",
                    "bpi"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All models are trained using the same pipeline, with SVM hyperparameter configurations detailed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#A3.T5\" title=\"Table 5 &#8227; Appendix C Model Detailed Evaluation &#8227; Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. A grid search is conducted over predefined parameter values, and each configuration is evaluated using 5-fold cross-validation. The optimal hyperparameters are selected based on the highest average weighted F-score. Detailed configurations and results are provided in the KIDOS project folder<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://drive.google.com/drive/folders/1f21Pd1h-VDQAH4hK_bilIMvp4bUves_C?usp=sharing\" title=\"\">https://drive.google.com/drive/folders/1f21Pd1h-VDQAH4hK_bilIMvp4bUves_C?usp=sharing</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "crossvalidation",
                    "average",
                    "weighted",
                    "svm",
                    "results"
                ]
            }
        ]
    },
    "S4.T3": {
        "caption": "Table 3. Misclassified instance counts and percentages per label across tasks and models. Models are grouped by representation type: Text, Graph, and Fusion.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Task</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Label</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Text Models</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Graph Models</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\">Fusion Models</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_border_r\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Gemini004</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">CamemBERT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">mBERT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">GraphWave</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">SG2V</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FGSD</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Gemini004 + GW</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">CamemBERT + SG2V</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">mBERT + SG2V</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Gemini004 + FGSD</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">ABD</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Non-abusive</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">47.45% (298)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">47.77% (300)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">28.57% (104)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">40.13% (252)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">40.76% (256)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">49.68% (312)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">47.45% (298)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">45.38% (285)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">45.38% (285)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">49.68% (312)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Abusive</td>\n<td class=\"ltx_td ltx_align_center\">22.45% (229)</td>\n<td class=\"ltx_td ltx_align_center\">18.53% (189)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">34.60% (91)</td>\n<td class=\"ltx_td ltx_align_center\">37.65% (384)</td>\n<td class=\"ltx_td ltx_align_center\">36.47% (372)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">21.08% (215)</td>\n<td class=\"ltx_td ltx_align_center\">20.98% (214)</td>\n<td class=\"ltx_td ltx_align_center\">19.12% (195)</td>\n<td class=\"ltx_td ltx_align_center\">19.12% (195)</td>\n<td class=\"ltx_td ltx_align_center\">21.08% (215)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">BBA</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Non-bullying</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">37.14% (361)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">39.40% (383)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">30.32% (67)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">42.80% (416)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">42.39% (412)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">37.45% (364)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">35.49% (345)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">37.96% (369)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">37.96% (369)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">37.45% (364)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Bullying</td>\n<td class=\"ltx_td ltx_align_center\">42.75% (289)</td>\n<td class=\"ltx_td ltx_align_center\">44.67% (302)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">33.70% (92)</td>\n<td class=\"ltx_td ltx_align_center\">32.54% (220)</td>\n<td class=\"ltx_td ltx_align_center\">31.95% (216)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">40.09% (271)</td>\n<td class=\"ltx_td ltx_align_center\">43.05% (291)</td>\n<td class=\"ltx_td ltx_align_center\">45.86% (310)</td>\n<td class=\"ltx_td ltx_align_center\">45.86% (310)</td>\n<td class=\"ltx_td ltx_align_center\">40.09% (271)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"4\"><span class=\"ltx_text ltx_font_bold\">BPI</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Victim</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25.70% (165)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25.46% (180)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">25.46% (180)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25.79% (164)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25.63% (162)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">25.83% (202)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25.70% (165)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">29.86% (209)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">29.86% (209)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">28.63% (201)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Victim Support</td>\n<td class=\"ltx_td ltx_align_center\">32.55% (209)</td>\n<td class=\"ltx_td ltx_align_center\">29.84% (211)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">29.84% (211)</td>\n<td class=\"ltx_td ltx_align_center\">32.08% (204)</td>\n<td class=\"ltx_td ltx_align_center\">31.96% (202)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">30.18% (236)</td>\n<td class=\"ltx_td ltx_align_center\">32.55% (209)</td>\n<td class=\"ltx_td ltx_align_center\">35.00% (245)</td>\n<td class=\"ltx_td ltx_align_center\">35.00% (245)</td>\n<td class=\"ltx_td ltx_align_center\">33.76% (237)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Bully</td>\n<td class=\"ltx_td ltx_align_center\">21.03% (135)</td>\n<td class=\"ltx_td ltx_align_center\">22.49% (159)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">22.49% (159)</td>\n<td class=\"ltx_td ltx_align_center\">21.23% (135)</td>\n<td class=\"ltx_td ltx_align_center\">21.36% (135)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">22.12% (173)</td>\n<td class=\"ltx_td ltx_align_center\">21.03% (135)</td>\n<td class=\"ltx_td ltx_align_center\">18.00% (126)</td>\n<td class=\"ltx_td ltx_align_center\">18.00% (126)</td>\n<td class=\"ltx_td ltx_align_center\">19.09% (134)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">Bully Support</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">20.72% (133)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">22.21% (157)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">22.21% (157)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">20.91% (133)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">21.04% (133)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">21.87% (171)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">20.72% (133)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">17.14% (120)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">17.14% (120)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">18.52% (130)</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "mbert",
            "counts",
            "type",
            "abd",
            "victim",
            "label",
            "bullying",
            "nonabusive",
            "bpi",
            "misclassified",
            "percentages",
            "tasks",
            "abusive",
            "text",
            "across",
            "grouped",
            "camembert",
            "bully",
            "fgsd",
            "bba",
            "graphwave",
            "instance",
            "sg2v",
            "task",
            "graph",
            "gemini004",
            "representation",
            "support",
            "fusion",
            "nonbullying"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To deepen our understanding of how different representation learning strategies handle abusive dynamics, we conducted a systematic error analysis across the three classification tasks. Our goal was to evaluate not only overall accuracy but also each model&#8217;s capacity to capture participant roles, pragmatic cues, and evolving discourse structures in multi-party conversations. \n<br class=\"ltx_break\"/>All misclassification counts in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#S4.T3\" title=\"Table 3 &#8227; 4.2. Error Analysis &#8227; 4. Experimental Setup &#8227; Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> are aggregated over the five test splits in our cross-validation. For each model, we collect the misclassified instances from the test set of every split and sum them to obtain the total per class. The reported values therefore reflect cumulative errors across all folds, not results from a single split.</p>\n\n",
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#S4.T3\" title=\"Table 3 &#8227; 4.2. Error Analysis &#8227; 4. Experimental Setup &#8227; Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows that text-based models misclassify a substantial portion of <em class=\"ltx_emph ltx_font_italic\">Abusive</em> messages (e.g., Gemini004: 22.45% / 229). In contrast, graph-based models yield higher error rates for both <em class=\"ltx_emph ltx_font_italic\">Abusive</em> messages (up to 37.65% / 384) and <em class=\"ltx_emph ltx_font_italic\">Non-abusive</em> cases (FGSD: 49.68% / 312), indicating a systematic over-detection bias.</p>\n\n",
            "<p class=\"ltx_p\">Both text and fusion models accumulate high error rates on the <em class=\"ltx_emph ltx_font_italic\">Bullying</em> label (43&#8211;46% misclassified, Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#S4.T3\" title=\"Table 3 &#8227; 4.2. Error Analysis &#8227; 4. Experimental Setup &#8227; Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>), showing persistent difficulty in distinguishing bullying from non-bullying when cues are indirect. Graph-based models distribute errors more evenly but still misclassify about one third of both classes. For instance, the message <em class=\"ltx_emph ltx_font_italic\">&#8220;c&#8217;est toi qui commence a chercher les gens&#8221;</em> (EN: <em class=\"ltx_emph ltx_font_italic\">You&#8217;re the one who starts provoking people</em>) was misclassified as <span class=\"ltx_text ltx_font_smallcaps\">No-CBB</span> by GraphWave, while Gemini004 correctly inferred the accusatory stance, illustrating the lexical vs. structural contrast.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Antisocial behavior (ASB) on social media&#8212;including hate speech, harassment, and cyberbullying&#8212;poses growing risks to platform safety and societal well-being. Prior research has focused largely on networks such as X and Reddit, while <span class=\"ltx_text ltx_font_italic\">multi-party conversational settings</span> remain underexplored due to limited data. To address this gap, we use <span class=\"ltx_text ltx_font_italic\">CyberAgressionAdo-Large</span>, a French open-access dataset simulating ASB in multi-party conversations, and evaluate three tasks: <span class=\"ltx_text ltx_font_italic\">abuse detection</span>, <span class=\"ltx_text ltx_font_italic\">bullying behavior analysis</span>, and <span class=\"ltx_text ltx_font_italic\">bullying peer-group identification</span>. We benchmark six text-based and eight graph-based <span class=\"ltx_text ltx_font_italic\">representation-learning methods</span>, analyzing lexical cues, interactional dynamics, and their multimodal fusion. Results show that multimodal models outperform unimodal baselines. The late fusion model <span class=\"ltx_text ltx_font_typewriter\">mBERT + WD-SGCN</span> achieves the best overall results, with top performance on abuse detection (0.718) and competitive scores on peer-group identification (0.286) and bullying analysis (0.606). Error analysis highlights its effectiveness in handling nuanced ASB phenomena such as implicit aggression, role transitions, and context-dependent hostility.</p>\n\n",
                "matched_terms": [
                    "models",
                    "mbert",
                    "bullying",
                    "tasks",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In response, the automatic detection of ASB has become a central research focus in NLP, with numerous shared tasks advancing detection and classification methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/information/AlkomahM22, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib27\" title=\"\">27</a>)</cite>. Yet most existing work relies on public data from platforms such as Facebook, YouTube, or Instagram and targets relatively narrow subtasks (e.g., binary hate vs. non-hate classification, target identification, or hate-type categorization). At the same time, private messaging apps and chat rooms have become critical arenas for cyberbullying, particularly among adolescents&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(alhashmi2023taxonomy, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib1\" title=\"\">1</a>)</cite>. ASB in these settings remains underexplored, constrained by strict data access restrictions and the complexity of multi-party interactions. Recently released datasets simulating online aggression in private messaging contexts&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/acl-alw/SprugnoliMTOP18, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib40\" title=\"\">40</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib31\" title=\"\">31</a>)</cite> offer new opportunities to address this gap. As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, multi-party conversations are non-linear and interleaved, with utterances directed to multiple participants. These dynamics demand computational models that go beyond message-level detection to capture discourse structure, participant roles, and interactional patterns&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(ganesh-etal-2023-survey, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib21\" title=\"\">21</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we address these limitations by systematically evaluating state-of-the-art representation learning methods&#8212;spanning both lexical and graph-based approaches&#8212;for ASB detection in multi-party dialogues. Prior work has shown the benefit of combining textual and interactional features; we extend this approach by constructing embeddings that jointly capture semantic content and conversational structure&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/sncs/CecillonLDL21, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib9\" title=\"\">9</a>)</cite> and by testing multimodal fusion strategies that integrate both. Experiments are conducted on the <span class=\"ltx_text ltx_font_italic\">CyberAgressionAdo-Large</span> dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ollagnier2024CyberAgressionAdo, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib34\" title=\"\">34</a>)</cite>, a publicly available corpus of 36 simulated aggressive conversations in French, collected through role-play in educational settings<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymous.4open.science/r/CyberAgression-Large-C71C\" title=\"\">https://anonymous.4open.science/r/CyberAgression-Large-C71C</a></span></span></span>\n. Crucially, to move beyond the narrow subtasks that dominate prior work, we introduce two novel evaluation settings tailored to multi-party contexts: <span class=\"ltx_text ltx_font_italic\">bullying behavior analysis</span> (BBA), which classifies the discursive function of messages (bullying vs. non-abusive intent), and <span class=\"ltx_text ltx_font_italic\">bullying peer group identification</span> (BPI), which identifies participants&#8217; social alignment (e.g., bully vs. victim side). These tasks extend the scope of ASB detection from surface-level classification to pragmatic dimensions of language, capturing intent, stance, and role in dynamic interactions.\nOverall, this paper makes three main contributions: (1) a comprehensive benchmark of unimodal and multimodal representation learning methods for ASB detection in multi-party conversations, (2) the introduction of two pragmatics-oriented evaluation tasks (BBA and BPI) that address group-level and functional dimensions of cyberbullying, and (3) evidence that integrating linguistic and structural features provides a more robust account of the social dynamics underpinning online aggression.</p>\n\n",
                "matched_terms": [
                    "bully",
                    "victim",
                    "bullying",
                    "nonabusive",
                    "representation",
                    "bpi",
                    "bba",
                    "tasks",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early work on ASB detection treated messages in isolation, relying on shallow features such as n-grams and sentiment scores. These approaches were soon surpassed by deep learning models (CNNs, RNNs, Transformers) and, more recently, by pretrained language models like BERT and its variants, which now dominate benchmarks on tasks such as target identification and abuse type classification&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/aiccsa/Alrehili19, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib21\" title=\"\">21</a>)</cite>. Beyond message-level analysis, research has increasingly turned to modeling interactional structure. Graph-based methods capture conversational dynamics by representing participants as nodes and exchanges as edges&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/sncs/CecillonLDL21, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib31\" title=\"\">31</a>)</cite>. Traditional embeddings (node2vec, DeepWalk) exploit topological similarity, while Graph Neural Networks (e.g., GCNs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/isci/0004JWLJJH22, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib45\" title=\"\">45</a>)</cite>, GraphSAGE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/tnn/ChenGWWXLLWL22, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib11\" title=\"\">11</a>)</cite>) enable neighborhood-level aggregation. Signed graph learning further enriches these models by encoding the polarity of interactions (support vs. hostility), which is crucial for distinguishing roles such as aggressor and defender&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/access/CecillonLDA24, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib8\" title=\"\">8</a>)</cite>. Early signed embeddings (SNE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/ijcai/WangWZJ17, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib44\" title=\"\">44</a>)</cite>, SIDE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/sdm/KimE20, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib25\" title=\"\">25</a>)</cite>) captured attraction&#8211;repulsion patterns but lacked higher-order structure. Newer signed GNNs (SGCN&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/icdm/Derr0T18, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib18\" title=\"\">18</a>)</cite>, SiGAT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(wang2020, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib24\" title=\"\">24</a>)</cite>, SAT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(li2021signed, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib23\" title=\"\">23</a>)</cite>) address this by integrating polarity-sensitive message passing and balance-aware objectives. Despite these advances, most systems remain either textual or graph-based, with limited integration. Recent multimodal fusion approaches bridge this gap by combining textual embeddings with interactional graphs, improving robustness in scenarios where abusive behavior is distributed across participants or conveyed implicitly&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(cheng2020, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib7\" title=\"\">7</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "type",
                    "graph",
                    "support",
                    "tasks",
                    "abusive",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on this foundation, we leverage <span class=\"ltx_text ltx_font_italic\">CyberAgressionAdo-Large</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ollagnier2024CyberAgressionAdo, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib34\" title=\"\">34</a>)</cite>, the largest publicly available corpus of aggressive multi-party chats in French. This dataset enables detailed study of lexical content, interaction patterns, and participant roles. Unlike prior work focused on narrow subtasks, we propose a unified evaluation across three complementary objectives: abuse detection, bullying behavior analysis, and peer group identification, benchmarking a broad range of textual, graph-based, and fusion-based representation learning methods.</p>\n\n",
                "matched_terms": [
                    "across",
                    "bullying",
                    "representation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In conversational settings, the meaning and impact of a message depend heavily on the social dynamics it participates in. Aggressive language may reflect coercion when used by a perpetrator but serve a protective function when voiced by a victim. Likewise, mockery or profanity in peer exchanges may signal bonding rather than abuse. These examples highlight the need for models that go beyond surface-level toxicity detection to capture underlying intent, role asymmetries, and interaction patterns.</p>\n\n",
                "matched_terms": [
                    "victim",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Bullying Behavior Analysis (BBA)</span>: Classifies each message as cyberbullying (<span class=\"ltx_text ltx_font_typewriter\">CBB</span>) or not (<span class=\"ltx_text ltx_font_typewriter\">NO-CBB</span>), based on predefined discursive roles and their associated intent. The role taxonomy follows prior work&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/coling/Ollagnier24, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib31\" title=\"\">31</a>)</cite>. Roles such as <span class=\"ltx_text ltx_font_typewriter\">attack</span>, <span class=\"ltx_text ltx_font_typewriter\">gaslighting</span>, and <span class=\"ltx_text ltx_font_typewriter\">instigating/abetting</span> are labeled <span class=\"ltx_text ltx_font_typewriter\">CBB</span>, while <span class=\"ltx_text ltx_font_typewriter\">empathy</span>, <span class=\"ltx_text ltx_font_typewriter\">counterspeech</span>, <span class=\"ltx_text ltx_font_typewriter\">conflict resolution</span>, and <span class=\"ltx_text ltx_font_typewriter\">defend</span> are <span class=\"ltx_text ltx_font_typewriter\">NO-CBB</span>. This task explicitly separates abusive behavior from reactive or supportive messages, addressing a key limitation of binary hate detection.</p>\n\n",
                "matched_terms": [
                    "task",
                    "bullying",
                    "abusive",
                    "bba"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Bullying Peer Group Identification (BPI)</span>: Assigns each participant to one of four active social roles: <span class=\"ltx_text ltx_font_typewriter\">victim</span>, <span class=\"ltx_text ltx_font_typewriter\">victim support</span>, <span class=\"ltx_text ltx_font_typewriter\">bully</span>, and <span class=\"ltx_text ltx_font_typewriter\">bully support</span>. The role taxonomy follows prior work&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/coling/Ollagnier24, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib31\" title=\"\">31</a>)</cite>. The neutral <span class=\"ltx_text ltx_font_typewriter\">conciliator</span> role is excluded. Unlike binary classification tasks, BPI captures the peer-group nature of bullying by modeling power asymmetries and affiliations within multi-party interactions.</p>\n\n",
                "matched_terms": [
                    "bully",
                    "victim",
                    "bullying",
                    "support",
                    "bpi",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section outlines the representation learning methods employed to tackle the three core tasks: abuse detection (ABD), BBA, and BPI. ABD is framed as a binary classification task that determines whether a message contains abusive content, independent of the sender&#8217;s role or conversational context&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/coling/Ollagnier24, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib31\" title=\"\">31</a>)</cite>. To address these tasks, we benchmark a diverse set of text-based, graph-based, and fusion-based models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "abd",
                    "task",
                    "representation",
                    "bpi",
                    "bba",
                    "tasks",
                    "abusive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Lexical embeddings</span>. To encode the semantic content of individual messages, we use six large language models. These include general-purpose systems&#8212;Gemini&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/corr/abs-2403-20327, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib29\" title=\"\">29</a>)</cite>, mBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/corr/abs-1810-04805, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib19\" title=\"\">19</a>)</cite>, GPT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(gpt4, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib36\" title=\"\">36</a>)</cite>&#8212;and French-specific models: CamemBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/acl/MartinMSDRCSS20, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib30\" title=\"\">30</a>)</cite>, FlauBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/lrec/LeVFSCLACBS20, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib28\" title=\"\">28</a>)</cite>, and CamemBERTa&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/corr/abs-2411-08868, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib4\" title=\"\">4</a>)</cite>. Each model produces contextualized token embeddings, which are aggregated into a global message-level representation. Importantly, messages are encoded independently: no conversational history is aggregated, ensuring that the representation reflects only the local semantics of the message under analysis.</p>\n\n",
                "matched_terms": [
                    "camembert",
                    "models",
                    "representation",
                    "mbert"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Graph embeddings</span>. To capture conversational structure, we construct directed interaction graphs around each target message. A context window defines the set of surrounding messages, and a sliding window identifies potential recipients. Nodes represent participants, while edges encode message exchanges with attributes for polarity (positive/negative), weight (frequency), and direction (reply flow). We evaluate both node-level and whole-graph embeddings. WD-SGCN emphasizes high-weighted incoming connections, while FGSD and GraphWave capture global structural signatures. WalkLets and Node2Vec rely on random-walk strategies to model topological similarity, though they ignore polarity and weight information. SG2V and its weighted variant WD-SG2V extend this idea by incorporating signed edges, thereby encoding positive and negative interactions. Finally, NGNN merges subgraph representations to capture higher-order structural patterns. Together, these methods differ in how they exploit graph attributes such as directionality, polarity, and weight, offering complementary perspectives on conversational structure.</p>\n\n",
                "matched_terms": [
                    "sg2v",
                    "fgsd",
                    "graph",
                    "graphwave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fusion strategies</span>. To integrate linguistic and structural signals, we implement three strategies, illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#acmlabel2\" title=\"Figure 2 &#8227; 3.2. Modeling Conversations &#8227; 3. Addressing Online Hate in Conversations &#8227; Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. Each builds on representations and classifiers trained separately on text and graph modalities&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:phd/hal/Cecillon24, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib6\" title=\"\">6</a>)</cite>. In <span class=\"ltx_text ltx_font_italic\">early fusion</span>, embeddings from both modalities are concatenated into a unified feature vector for training a new classifier (e.g., SVM), allowing joint learning at the representation level and direct interaction between lexical and structural features. In <span class=\"ltx_text ltx_font_italic\">late fusion</span>, prediction scores from unimodal classifiers are fed into a meta-classifier, which combines their outputs and is particularly effective for integrating heterogeneous or noisy signals. Finally, <span class=\"ltx_text ltx_font_italic\">hybrid fusion</span> leverages both embedding-level and score-level information, enabling the final classifier to exploit fine-grained features as well as high-level decision patterns, thereby enhancing robustness and adaptability.</p>\n\n",
                "matched_terms": [
                    "representation",
                    "graph",
                    "text",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section presents the dataset used to evaluate the three ASB tasks, the graph construction process, the evaluation protocol, and implementation details. Our study addresses two research questions:</p>\n\n",
                "matched_terms": [
                    "graph",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RQ1</span>: How do different representation learning modalities (lexical, structural, and fusion-based) perform across the three ASB tasks (ABD, BBA, and BPI)?</p>\n\n",
                "matched_terms": [
                    "across",
                    "abd",
                    "representation",
                    "bpi",
                    "bba",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RQ2</span>: To what extent do multimodal fusion strategies (early, late, and hybrid) improve the modeling of abusive dynamics and participant roles in multi-party conversations compared to unimodal modalities?</p>\n\n",
                "matched_terms": [
                    "fusion",
                    "abusive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To model conversational structure, we construct interaction graphs centered on a <span class=\"ltx_text ltx_font_italic\">target message</span> (the message to be classified). Graphs are directed and weighted, following the network extraction methodology of&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/access/CecillonLDA24, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib8\" title=\"\">8</a>)</cite>. Nodes represent participants, and edges encode co-occurrence of messages within a fixed-size sliding window of 21 surrounding messages. On average, graph sizes are reported in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#S4.T1\" title=\"Table 1 &#8227; 4. Experimental Setup &#8227; Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (<math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>48 vertices on average). We refer to that table for the exact range. Polarity is derived from sentiment annotations: messages labeled as positive, negative, or neutral are mapped to signed edges, with neutral treated as positive for experimental purposes. Because polarity is annotation-based rather than predicted, graph structures are inherently language-agnostic. For graph-level models, the full graph is used; for node-level models, only the embedding of the target author node is retained.</p>\n\n",
                "matched_terms": [
                    "models",
                    "graph"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, the dataset is split into 70% training and 30% testing using stratified sampling to preserve class distributions. All models are assessed with 5-fold cross-validation, and performance is reported using the weighted F1-score, with standard deviation across folds to measure stability. Detailed class distributions are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#A2\" title=\"Appendix B Dataset Distribution &#8227; Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#S3.SS2\" title=\"3.2. Modeling Conversations &#8227; 3. Addressing Online Hate in Conversations &#8227; Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, we benchmark six lexical and eight graph-based representation learning methods. We also evaluate fusion strategies that combine top-performing lexical models (<span class=\"ltx_text ltx_font_typewriter\">CamemBERTa</span>, <span class=\"ltx_text ltx_font_typewriter\">Gemini</span>, and <span class=\"ltx_text ltx_font_typewriter\">mBERT</span>) with graph embeddings. All representations are classified using Support Vector Machines (SVM) implemented with cuML and scikit-learn. Experiments were conducted on a machine with an Intel&#174;Core&#8482;i7&#8211;1065G7 3.5 GHz CPU and an Nvidia A100 GPU. Hyperparameter settings are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#A3\" title=\"Appendix C Model Detailed Evaluation &#8227; Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "mbert",
                    "graph",
                    "representation",
                    "support",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address <span class=\"ltx_text ltx_font_bold\">RQ1</span> and <span class=\"ltx_text ltx_font_bold\">RQ2</span>, we evaluate all representation learning methods on the <span class=\"ltx_text ltx_font_italic\">CyberAgressionAdo-Large</span> dataset. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#S4.T2\" title=\"Table 2 &#8227; 4.1. Performance Evaluation &#8227; 4. Experimental Setup &#8227; Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reports the top-performing models, while the full set of results is available in the KIDOS project repository<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://drive.google.com/drive/folders/1f21Pd1h-VDQAH4hK_bilIMvp4bUves_C?usp=sharing\" title=\"\">https://drive.google.com/drive/folders/1f21Pd1h-VDQAH4hK_bilIMvp4bUves_C?usp=sharing</a></span></span></span>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "representation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">RQ1</span>, clear differences emerge between lexical and graph-based encoders. Among lexical models, <span class=\"ltx_text ltx_font_typewriter\">CamemBERT</span> yields the strongest overall performance (0.697<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.01 on ABD, 0.587<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02 on BBA), closely followed by <span class=\"ltx_text ltx_font_typewriter\">Gemini004</span> with slightly better results on BBA (0.608<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m3\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02). By contrast, <span class=\"ltx_text ltx_font_typewriter\">CamemBERTa</span> and <span class=\"ltx_text ltx_font_typewriter\">Gemini001</span> consistently lag behind. Yet across all lexical models, performance on BPI remains modest (0.255&#8211;0.283), underscoring a fundamental limitation of content-only features: while effective for recognizing abusive language and intent, they fail to capture relational dynamics that define peer-group interactions.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "abd",
                    "camembert",
                    "gemini004",
                    "bpi",
                    "bba",
                    "abusive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Graph embeddings reveal the opposite pattern. <span class=\"ltx_text ltx_font_typewriter\">WD-SGCN</span>, which integrates direction, polarity, and edge weights, achieves the highest BPI score among unimodal models (<math alttext=\"{0.296\\pm 0.01}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mn>0.296</mn><mo>&#177;</mo><mn>0.01</mn></mrow><annotation encoding=\"application/x-tex\">{0.296\\pm 0.01}</annotation></semantics></math>), highlighting the importance of modeling signed and weighted interactions for role attribution. In contrast, approaches neglecting edge semantics&#8212;such as <span class=\"ltx_text ltx_font_typewriter\">Node2Vec</span>, <span class=\"ltx_text ltx_font_typewriter\">SG2V</span>, or <span class=\"ltx_text ltx_font_typewriter\">GraphWave</span>&#8212;underperform substantially on BPI (<math alttext=\"\\leq\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p3.m2\" intent=\":literal\"><semantics><mo>&#8804;</mo><annotation encoding=\"application/x-tex\">\\leq</annotation></semantics></math>0.237). Interestingly, models like <span class=\"ltx_text ltx_font_typewriter\">FGSD</span> and <span class=\"ltx_text ltx_font_typewriter\">NGNN</span> yield balanced but unremarkable results, suggesting some sensitivity to structural patterns but insufficient discriminatory power for complex role-based reasoning. Taken together, these results suggest that lexical models specialize in surface-level abuse detection, while graph-based methods are better suited for socially grounded classification, particularly when capturing implicit roles and alignments.</p>\n\n",
                "matched_terms": [
                    "models",
                    "graph",
                    "fgsd",
                    "bpi",
                    "sg2v"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">RQ2</span>, multimodal fusion consistently outperforms unimodal baselines, though task-specific strengths emerge across strategies.</p>\n\n",
                "matched_terms": [
                    "across",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Early fusion</span>, which concatenates lexical and structural embeddings, proves most effective for BPI. The combination <span class=\"ltx_text ltx_font_typewriter\">mBERT + SG2V</span> achieves the overall best BPI score (0.301<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.01), while <span class=\"ltx_text ltx_font_typewriter\">Gemini004 + GraphWave</span> (0.299<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m2\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02) and <span class=\"ltx_text ltx_font_typewriter\">Gemini004 + FGSD</span> (0.297<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p5.m3\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.03) also perform strongly. These results suggest that when peer-group identification requires fine-grained integration of semantic and relational cues, representation-level fusion is the most advantageous.</p>\n\n",
                "matched_terms": [
                    "mbert",
                    "fgsd",
                    "gemini004",
                    "bpi",
                    "graphwave",
                    "fusion",
                    "sg2v"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Late fusion</span>, which ensembles predictions from unimodal classifiers, delivers the highest scores on ABD and BBA. Notably, <span class=\"ltx_text ltx_font_typewriter\">mBERT + WD-SGCN</span> achieves the top ABD result overall (0.718<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02), while <span class=\"ltx_text ltx_font_typewriter\">Gemini001 + WalkLets</span> dominates BBA (0.625<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m2\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.01). Although slightly weaker on BPI <math alttext=\"\\leq\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m3\" intent=\":literal\"><semantics><mo>&#8804;</mo><annotation encoding=\"application/x-tex\">\\leq</annotation></semantics></math>0.291), late fusion provides the strongest and most balanced improvements on content- and behavior-oriented tasks, indicating that decision-level integration leverages modality-specific strengths without overfitting to noisy cues.\nWhile <span class=\"ltx_text ltx_font_typewriter\">mBERT + WD-SGCN</span> delivers the <em class=\"ltx_emph ltx_font_italic\">best overall</em> trade-off across tasks (top ABD and competitive BBA/BPI), the top BPI score is obtained by an early-fusion model (<span class=\"ltx_text ltx_font_typewriter\">mBERT + SG2V</span>, 0.301). We therefore distinguish per-task bests from the overall best compromise.</p>\n\n",
                "matched_terms": [
                    "mbert",
                    "across",
                    "abd",
                    "bpi",
                    "bba",
                    "tasks",
                    "fusion",
                    "sg2v"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Hybrid fusion</span>, which combines embeddings with prediction scores, does not reach the top F1-scores but yields consistently stable performance. For instance, <span class=\"ltx_text ltx_font_typewriter\">mBERT + NGNN</span> achieves 0.707<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p7.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02 on ABD, 0.613<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p7.m2\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.01 on BBA, and 0.286<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p7.m3\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.00 on BPI. The relatively low variance across folds suggests stronger robustness and adaptability&#8212;an important property in real-world applications where conversational dynamics and abuse strategies are highly variable.</p>\n\n",
                "matched_terms": [
                    "mbert",
                    "across",
                    "abd",
                    "bba",
                    "bpi",
                    "instance",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, lexical models perform best on content-oriented tasks (ABD, BBA), while graph-based encoders show relative advantages on socially grounded reasoning, particularly in BPI, where modeling interaction structure is essential. Yet absolute performance on BPI remains modest across models, reflecting the difficulty of this four-class, highly imbalanced task and pointing to the need for richer approaches to capture peer-group dynamics. Fusion strategies consistently outperform unimodal baselines, with early fusion proving most effective for relational inference, late fusion excelling at content-driven detection, and hybrid fusion offering robust generalization. Taken together, these results underscore the complementary strengths of lexical and structural cues and highlight the necessity of their integration to model the nuanced and evolving dynamics of abusive behavior in multi-party online conversations.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "abd",
                    "task",
                    "bpi",
                    "bba",
                    "tasks",
                    "abusive",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Misclassified instances often involve implicit aggression, sarcasm, or connotative tone&#8212;features that are not structurally salient. For example, the message <em class=\"ltx_emph ltx_font_italic\">&#8220;elle est bonne la meuf&#8221;</em> (EN: <em class=\"ltx_emph ltx_font_italic\">&#8220;That girl is hot&#8221;</em>), labeled as abusive for its objectifying tone, was missed by GraphWave but correctly identified by Gemini004, highlighting that lexical cues remain critical for capturing implicit forms of abuse.</p>\n\n",
                "matched_terms": [
                    "misclassified",
                    "graphwave",
                    "gemini004",
                    "abusive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Errors are not uniform across roles. <em class=\"ltx_emph ltx_font_italic\">Victim Support</em> is systematically the most misclassified (CamemBERT: 24.65% / 211, GraphWave: 23.83% / 204, FGSD: 27.57% / 236). In contrast, explicit aggressor roles (<em class=\"ltx_emph ltx_font_italic\">Bully</em>, <em class=\"ltx_emph ltx_font_italic\">Bully Support</em>) show lower error shares (<math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSSx1.Px3.p1.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>15&#8211;22%), suggesting they are easier to detect. Example: the message <em class=\"ltx_emph ltx_font_italic\">&#8220;tu fais le mec intelligent pour impressionner les autres&#8221;</em> (EN: <em class=\"ltx_emph ltx_font_italic\">You&#8217;re pretending to be smart to impress others</em>) was correctly flagged as aggressive by Gemini004 but missed by GraphWave, confirming that subtle antagonism requires lexical sensitivity.</p>\n\n",
                "matched_terms": [
                    "across",
                    "bully",
                    "victim",
                    "misclassified",
                    "camembert",
                    "fgsd",
                    "gemini004",
                    "support",
                    "graphwave"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across modalities, the hardest cases are implicit abuse (ABD, BBA) and ambiguous supportive roles (BPI). Graph encoders over-predict abuse, while text encoders miss subtler aggression.</p>\n\n",
                "matched_terms": [
                    "across",
                    "abd",
                    "graph",
                    "bba",
                    "bpi",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fusion reduces misclassifications on the <em class=\"ltx_emph ltx_font_italic\">Abusive</em> class (down to 19.12% / 195) compared to 22&#8211;38% with single modalities. However, this comes at the cost of slightly more false positives on <em class=\"ltx_emph ltx_font_italic\">Non-abusive</em> samples (<math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSSx2.Px1.p1.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>45% vs. 28% for mBERT alone).</p>\n\n",
                "matched_terms": [
                    "mbert",
                    "fusion",
                    "nonabusive",
                    "abusive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fusion decreases errors on <em class=\"ltx_emph ltx_font_italic\">Non-bullying</em> (35.49% / 345 vs. 37&#8211;42% with single models), but errors on <em class=\"ltx_emph ltx_font_italic\">Bullying</em> remain high (<math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSSx2.Px2.p1.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>40&#8211;46%), indicating that the main source of confusion persists.</p>\n\n",
                "matched_terms": [
                    "models",
                    "bullying",
                    "fusion",
                    "nonbullying"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fusion particularly reduces errors on aggressor roles. <em class=\"ltx_emph ltx_font_italic\">Bully</em> misclassifications drop from 22.49% (159, text) to 18.00% (126, fusion), and <em class=\"ltx_emph ltx_font_italic\">Bully Support</em> from 22.21% (157) to 17.14% (120). At the same time, <em class=\"ltx_emph ltx_font_italic\">Victim</em> and <em class=\"ltx_emph ltx_font_italic\">Victim Support</em> errors increase, reflecting a redistribution of mistakes rather than a global reduction.</p>\n\n",
                "matched_terms": [
                    "bully",
                    "victim",
                    "support",
                    "fusion",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fusion strategies shift rather than eliminate errors. They reduce false negatives for aggressor roles (ABD <em class=\"ltx_emph ltx_font_italic\">Abusive</em>, BPI <em class=\"ltx_emph ltx_font_italic\">Bully</em>) but increase confusion for neutral or supportive roles. This pattern highlights complementary error profiles between text and graph modalities and underscores the need for adaptive fusion mechanisms sensitive to role dynamics and pragmatic subtleties.</p>\n\n",
                "matched_terms": [
                    "abd",
                    "bully",
                    "graph",
                    "bpi",
                    "abusive",
                    "fusion",
                    "text"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper presents representation learning methods for detecting hate speech in multi-party dialogues and introduces two new evaluation tasks: Bullying Behavior Analysis (BBA) and Bullying Peer Group Identification (BPI). These tasks provide a more granular understanding of how aggressive behaviors emerge and evolve within multi-participant conversations. We benchmark a broad spectrum of lexical and graph-based embedding models and assess fusion strategies that combine linguistic content with structural interaction patterns. Results show that multimodal approaches consistently outperform unimodal baselines&#8212;particularly on socially complex tasks like BPI, where understanding implicit roles and interactional dynamics is crucial. Lexical models excel in tasks driven by explicit textual cues (ABD, BBA), while graph-based models are better suited to relational inference. Fusion strategies, especially early fusion, capitalize on the complementary strengths of both modalities, yielding the most robust and context-aware representations for abuse detection.</p>\n\n",
                "matched_terms": [
                    "models",
                    "abd",
                    "bullying",
                    "representation",
                    "bpi",
                    "bba",
                    "tasks",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In future work, more integrated representation learning could be explored by jointly encoding lexical and structural features within a unified model, rather than learning them separately. Additionally, while our experiments rely on structured role-play data for ethical and practical reasons, validating models on real-world conversations across different platforms and demographics will be critical for assessing generalizability. Incorporating richer context, such as temporal dynamics, user-level features, and conversational history, may further enhance the detection of subtle or evolving aggression patterns.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "representation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our evaluation is based on <span class=\"ltx_text ltx_font_italic\">simulated</span> cyber aggression data generated through controlled role-play, this method has been shown to produce more naturalistic language than interviews, questionnaires, human&#8211;machine interactions, or reconstructed threads&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(tran2006naturalized, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib41\" title=\"\">41</a>)</cite>. However, as with any simulated resource, there may be differences from spontaneous online conversations, particularly in tone, fluency, or conversational dynamics. Moreover, cultural and linguistic features specific to French-speaking adolescents may shape language use and social behaviors, potentially limiting the generalizability of our findings. Future work should validate the models on naturally occurring data across more diverse linguistic and cultural settings to ensure cross-context robustness.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, while our tasks span both binary (e.g., ABD, BBA) and multi-class (e.g., BPI) classification, mapping complex social behaviors to fixed labels can obscure key contextual nuances. For instance, teasing among peers or reactive aggression may be misclassified without considering interpersonal dynamics or conversation history&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(cowie2014understanding, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib16\" title=\"\">16</a>)</cite>. Additionally, our use of undersampling to mitigate class imbalance ensures balanced training but may distort the natural prevalence of abusive behaviors, which are typically rare in real-world settings. Future work should explore alternative approaches&#8212;such as oversampling, cost-sensitive learning, or data augmentation&#8212;to enhance model robustness and better reflect realistic distributions.</p>\n\n",
                "matched_terms": [
                    "abd",
                    "misclassified",
                    "bpi",
                    "bba",
                    "tasks",
                    "abusive",
                    "instance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">An additional limitation lies in the current binary design of signed conversational graphs, which model only positive and negative interactions between participants. While effective in capturing antagonistic or affiliative dynamics, this dichotomy overlooks the prevalence and pragmatic significance of neutral exchanges&#8212;such as factual statements, inquiries, or disengaged responses&#8212;that play a crucial role in structuring online conversations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/flairs/OllagnierCV23, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib32\" title=\"\">32</a>)</cite>. Our current experiments map neutral messages to positive edges for simplification, which may conflate passive or ambiguous behavior with active support. Future work should incorporate a third edge type to explicitly represent neutral interactions, thereby enhancing the expressiveness of conversational graphs and enabling a more nuanced modeling of participant roles, pragmatic stance, and socio-discursive functions in cyber aggression.</p>\n\n",
                "matched_terms": [
                    "type",
                    "support"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite its benefits, role-play data may still expose both participants and annotators to emotionally charged or distressing situations. To mitigate these risks, we selected this dataset specifically because it was created following a strict experimental protocol for participants and adhered to established ethical and procedural standards for annotation. During data collection, all student participants under the age of 18 provided informed parental consent, and each school&#8217;s administration and ethics board approved the study. Psychological support was available throughout, and students participated in dedicated debriefing sessions focused on cyberbullying awareness and digital safety&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/lrec/OllagnierCVB22, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib35\" title=\"\">35</a>)</cite>. Importantly, no student was asked to play the role of a victim, and all sessions were conducted under the supervision of trained researchers ready to intervene if necessary.</p>\n\n",
                "matched_terms": [
                    "victim",
                    "support"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each dataset, we randomly sampled 70% of the exchanged messages for training and 30% for testing. To maintain consistency and readability, each binary classification scheme is represented using positive and negative classes. For the ABD task, the positive class consists of messages that perpetrate abusive behaviors, while the negative class includes non-abusive messages. For the BBA task, messages exhibiting cyberbullying behaviors are classified as negative, whereas messages that do not contain cyberbullying behaviors belong to the positive class. For the BPI task, each label represents a different role that participants play in cyberbullying interactions, as described in&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/lrec/OllagnierCVB22, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib35\" title=\"\">35</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "abd",
                    "task",
                    "label",
                    "nonabusive",
                    "bpi",
                    "bba",
                    "abusive"
                ]
            }
        ]
    },
    "A2.T4": {
        "caption": "Table 4. Distribution of data splits across tasks.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Task</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Label</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Split 1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Split 2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Split 3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Split 4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Split 5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<th class=\"ltx_td ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Tr</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Te</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Tr</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Te</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Tr</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Te</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Tr</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Te</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Tr</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Te</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">ABD</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Pos</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">445</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">204</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">445</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">204</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">444</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">204</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">445</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">204</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">445</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">204</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Neg</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">213</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">125</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">213</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">125</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">214</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">126</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">214</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">126</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">214</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">126</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"2\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">BBA</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Pos</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">372</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">194</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">373</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">194</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">373</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">195</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">373</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">195</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">373</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">194</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Neg</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">286</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">135</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">285</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">135</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">285</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">135</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">286</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">135</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">286</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">136</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" rowspan=\"5\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">BPI</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Victim</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">121</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">44</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">122</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">44</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">122</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">44</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">122</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">44</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">121</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">44</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Victim Supp.</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">170</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">78</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">170</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">79</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">170</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">79</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">171</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">78</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">171</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">78</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Bully</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">154</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">62</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">154</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">62</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">154</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">62</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">153</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">63</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">153</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">63</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Bully Supp.</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">167</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">96</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">167</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">96</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">167</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">96</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">167</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">96</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">168</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.0pt;padding-right:3.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">96</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "across",
            "abd",
            "neg",
            "pos",
            "task",
            "label",
            "victim",
            "bully",
            "bba",
            "bpi",
            "data",
            "splits",
            "tasks",
            "supp",
            "split",
            "distribution"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Antisocial behavior (ASB) on social media&#8212;including hate speech, harassment, and cyberbullying&#8212;poses growing risks to platform safety and societal well-being. Prior research has focused largely on networks such as X and Reddit, while <span class=\"ltx_text ltx_font_italic\">multi-party conversational settings</span> remain underexplored due to limited data. To address this gap, we use <span class=\"ltx_text ltx_font_italic\">CyberAgressionAdo-Large</span>, a French open-access dataset simulating ASB in multi-party conversations, and evaluate three tasks: <span class=\"ltx_text ltx_font_italic\">abuse detection</span>, <span class=\"ltx_text ltx_font_italic\">bullying behavior analysis</span>, and <span class=\"ltx_text ltx_font_italic\">bullying peer-group identification</span>. We benchmark six text-based and eight graph-based <span class=\"ltx_text ltx_font_italic\">representation-learning methods</span>, analyzing lexical cues, interactional dynamics, and their multimodal fusion. Results show that multimodal models outperform unimodal baselines. The late fusion model <span class=\"ltx_text ltx_font_typewriter\">mBERT + WD-SGCN</span> achieves the best overall results, with top performance on abuse detection (0.718) and competitive scores on peer-group identification (0.286) and bullying analysis (0.606). Error analysis highlights its effectiveness in handling nuanced ASB phenomena such as implicit aggression, role transitions, and context-dependent hostility.</p>\n\n",
                "matched_terms": [
                    "data",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In response, the automatic detection of ASB has become a central research focus in NLP, with numerous shared tasks advancing detection and classification methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/information/AlkomahM22, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib27\" title=\"\">27</a>)</cite>. Yet most existing work relies on public data from platforms such as Facebook, YouTube, or Instagram and targets relatively narrow subtasks (e.g., binary hate vs. non-hate classification, target identification, or hate-type categorization). At the same time, private messaging apps and chat rooms have become critical arenas for cyberbullying, particularly among adolescents&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(alhashmi2023taxonomy, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib1\" title=\"\">1</a>)</cite>. ASB in these settings remains underexplored, constrained by strict data access restrictions and the complexity of multi-party interactions. Recently released datasets simulating online aggression in private messaging contexts&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/acl-alw/SprugnoliMTOP18, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib40\" title=\"\">40</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib31\" title=\"\">31</a>)</cite> offer new opportunities to address this gap. As illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#acmlabel1\" title=\"Figure 1 &#8227; 1. Introduction &#8227; Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, multi-party conversations are non-linear and interleaved, with utterances directed to multiple participants. These dynamics demand computational models that go beyond message-level detection to capture discourse structure, participant roles, and interactional patterns&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(ganesh-etal-2023-survey, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib21\" title=\"\">21</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "data",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this paper, we address these limitations by systematically evaluating state-of-the-art representation learning methods&#8212;spanning both lexical and graph-based approaches&#8212;for ASB detection in multi-party dialogues. Prior work has shown the benefit of combining textual and interactional features; we extend this approach by constructing embeddings that jointly capture semantic content and conversational structure&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/sncs/CecillonLDL21, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib9\" title=\"\">9</a>)</cite> and by testing multimodal fusion strategies that integrate both. Experiments are conducted on the <span class=\"ltx_text ltx_font_italic\">CyberAgressionAdo-Large</span> dataset&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ollagnier2024CyberAgressionAdo, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib34\" title=\"\">34</a>)</cite>, a publicly available corpus of 36 simulated aggressive conversations in French, collected through role-play in educational settings<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://anonymous.4open.science/r/CyberAgression-Large-C71C\" title=\"\">https://anonymous.4open.science/r/CyberAgression-Large-C71C</a></span></span></span>\n. Crucially, to move beyond the narrow subtasks that dominate prior work, we introduce two novel evaluation settings tailored to multi-party contexts: <span class=\"ltx_text ltx_font_italic\">bullying behavior analysis</span> (BBA), which classifies the discursive function of messages (bullying vs. non-abusive intent), and <span class=\"ltx_text ltx_font_italic\">bullying peer group identification</span> (BPI), which identifies participants&#8217; social alignment (e.g., bully vs. victim side). These tasks extend the scope of ASB detection from surface-level classification to pragmatic dimensions of language, capturing intent, stance, and role in dynamic interactions.\nOverall, this paper makes three main contributions: (1) a comprehensive benchmark of unimodal and multimodal representation learning methods for ASB detection in multi-party conversations, (2) the introduction of two pragmatics-oriented evaluation tasks (BBA and BPI) that address group-level and functional dimensions of cyberbullying, and (3) evidence that integrating linguistic and structural features provides a more robust account of the social dynamics underpinning online aggression.</p>\n\n",
                "matched_terms": [
                    "bully",
                    "victim",
                    "bpi",
                    "bba",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early work on ASB detection treated messages in isolation, relying on shallow features such as n-grams and sentiment scores. These approaches were soon surpassed by deep learning models (CNNs, RNNs, Transformers) and, more recently, by pretrained language models like BERT and its variants, which now dominate benchmarks on tasks such as target identification and abuse type classification&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/aiccsa/Alrehili19, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib21\" title=\"\">21</a>)</cite>. Beyond message-level analysis, research has increasingly turned to modeling interactional structure. Graph-based methods capture conversational dynamics by representing participants as nodes and exchanges as edges&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/sncs/CecillonLDL21, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib9\" title=\"\">9</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib31\" title=\"\">31</a>)</cite>. Traditional embeddings (node2vec, DeepWalk) exploit topological similarity, while Graph Neural Networks (e.g., GCNs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/isci/0004JWLJJH22, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib45\" title=\"\">45</a>)</cite>, GraphSAGE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/tnn/ChenGWWXLLWL22, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib11\" title=\"\">11</a>)</cite>) enable neighborhood-level aggregation. Signed graph learning further enriches these models by encoding the polarity of interactions (support vs. hostility), which is crucial for distinguishing roles such as aggressor and defender&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:journals/access/CecillonLDA24, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib8\" title=\"\">8</a>)</cite>. Early signed embeddings (SNE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/ijcai/WangWZJ17, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib44\" title=\"\">44</a>)</cite>, SIDE&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/sdm/KimE20, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib25\" title=\"\">25</a>)</cite>) captured attraction&#8211;repulsion patterns but lacked higher-order structure. Newer signed GNNs (SGCN&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/icdm/Derr0T18, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib18\" title=\"\">18</a>)</cite>, SiGAT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(wang2020, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib24\" title=\"\">24</a>)</cite>, SAT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(li2021signed, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib23\" title=\"\">23</a>)</cite>) address this by integrating polarity-sensitive message passing and balance-aware objectives. Despite these advances, most systems remain either textual or graph-based, with limited integration. Recent multimodal fusion approaches bridge this gap by combining textual embeddings with interactional graphs, improving robustness in scenarios where abusive behavior is distributed across participants or conveyed implicitly&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(cheng2020, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib7\" title=\"\">7</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Bullying Behavior Analysis (BBA)</span>: Classifies each message as cyberbullying (<span class=\"ltx_text ltx_font_typewriter\">CBB</span>) or not (<span class=\"ltx_text ltx_font_typewriter\">NO-CBB</span>), based on predefined discursive roles and their associated intent. The role taxonomy follows prior work&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/coling/Ollagnier24, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib31\" title=\"\">31</a>)</cite>. Roles such as <span class=\"ltx_text ltx_font_typewriter\">attack</span>, <span class=\"ltx_text ltx_font_typewriter\">gaslighting</span>, and <span class=\"ltx_text ltx_font_typewriter\">instigating/abetting</span> are labeled <span class=\"ltx_text ltx_font_typewriter\">CBB</span>, while <span class=\"ltx_text ltx_font_typewriter\">empathy</span>, <span class=\"ltx_text ltx_font_typewriter\">counterspeech</span>, <span class=\"ltx_text ltx_font_typewriter\">conflict resolution</span>, and <span class=\"ltx_text ltx_font_typewriter\">defend</span> are <span class=\"ltx_text ltx_font_typewriter\">NO-CBB</span>. This task explicitly separates abusive behavior from reactive or supportive messages, addressing a key limitation of binary hate detection.</p>\n\n",
                "matched_terms": [
                    "task",
                    "bba"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Bullying Peer Group Identification (BPI)</span>: Assigns each participant to one of four active social roles: <span class=\"ltx_text ltx_font_typewriter\">victim</span>, <span class=\"ltx_text ltx_font_typewriter\">victim support</span>, <span class=\"ltx_text ltx_font_typewriter\">bully</span>, and <span class=\"ltx_text ltx_font_typewriter\">bully support</span>. The role taxonomy follows prior work&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/coling/Ollagnier24, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib31\" title=\"\">31</a>)</cite>. The neutral <span class=\"ltx_text ltx_font_typewriter\">conciliator</span> role is excluded. Unlike binary classification tasks, BPI captures the peer-group nature of bullying by modeling power asymmetries and affiliations within multi-party interactions.</p>\n\n",
                "matched_terms": [
                    "victim",
                    "bpi",
                    "tasks",
                    "bully"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section outlines the representation learning methods employed to tackle the three core tasks: abuse detection (ABD), BBA, and BPI. ABD is framed as a binary classification task that determines whether a message contains abusive content, independent of the sender&#8217;s role or conversational context&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/coling/Ollagnier24, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib31\" title=\"\">31</a>)</cite>. To address these tasks, we benchmark a diverse set of text-based, graph-based, and fusion-based models.</p>\n\n",
                "matched_terms": [
                    "abd",
                    "task",
                    "bpi",
                    "bba",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RQ1</span>: How do different representation learning modalities (lexical, structural, and fusion-based) perform across the three ASB tasks (ABD, BBA, and BPI)?</p>\n\n",
                "matched_terms": [
                    "across",
                    "abd",
                    "bpi",
                    "bba",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, the dataset is split into 70% training and 30% testing using stratified sampling to preserve class distributions. All models are assessed with 5-fold cross-validation, and performance is reported using the weighted F1-score, with standard deviation across folds to measure stability. Detailed class distributions are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#A2\" title=\"Appendix B Dataset Distribution &#8227; Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">B</span></a>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "split"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">RQ1</span>, clear differences emerge between lexical and graph-based encoders. Among lexical models, <span class=\"ltx_text ltx_font_typewriter\">CamemBERT</span> yields the strongest overall performance (0.697<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.01 on ABD, 0.587<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m2\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02 on BBA), closely followed by <span class=\"ltx_text ltx_font_typewriter\">Gemini004</span> with slightly better results on BBA (0.608<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p2.m3\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02). By contrast, <span class=\"ltx_text ltx_font_typewriter\">CamemBERTa</span> and <span class=\"ltx_text ltx_font_typewriter\">Gemini001</span> consistently lag behind. Yet across all lexical models, performance on BPI remains modest (0.255&#8211;0.283), underscoring a fundamental limitation of content-only features: while effective for recognizing abusive language and intent, they fail to capture relational dynamics that define peer-group interactions.</p>\n\n",
                "matched_terms": [
                    "bpi",
                    "across",
                    "abd",
                    "bba"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Late fusion</span>, which ensembles predictions from unimodal classifiers, delivers the highest scores on ABD and BBA. Notably, <span class=\"ltx_text ltx_font_typewriter\">mBERT + WD-SGCN</span> achieves the top ABD result overall (0.718<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02), while <span class=\"ltx_text ltx_font_typewriter\">Gemini001 + WalkLets</span> dominates BBA (0.625<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m2\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.01). Although slightly weaker on BPI <math alttext=\"\\leq\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p6.m3\" intent=\":literal\"><semantics><mo>&#8804;</mo><annotation encoding=\"application/x-tex\">\\leq</annotation></semantics></math>0.291), late fusion provides the strongest and most balanced improvements on content- and behavior-oriented tasks, indicating that decision-level integration leverages modality-specific strengths without overfitting to noisy cues.\nWhile <span class=\"ltx_text ltx_font_typewriter\">mBERT + WD-SGCN</span> delivers the <em class=\"ltx_emph ltx_font_italic\">best overall</em> trade-off across tasks (top ABD and competitive BBA/BPI), the top BPI score is obtained by an early-fusion model (<span class=\"ltx_text ltx_font_typewriter\">mBERT + SG2V</span>, 0.301). We therefore distinguish per-task bests from the overall best compromise.</p>\n\n",
                "matched_terms": [
                    "across",
                    "abd",
                    "bpi",
                    "bba",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Hybrid fusion</span>, which combines embeddings with prediction scores, does not reach the top F1-scores but yields consistently stable performance. For instance, <span class=\"ltx_text ltx_font_typewriter\">mBERT + NGNN</span> achieves 0.707<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p7.m1\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.02 on ABD, 0.613<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p7.m2\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.01 on BBA, and 0.286<math alttext=\"{\\pm}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p7.m3\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">{\\pm}</annotation></semantics></math>0.00 on BPI. The relatively low variance across folds suggests stronger robustness and adaptability&#8212;an important property in real-world applications where conversational dynamics and abuse strategies are highly variable.</p>\n\n",
                "matched_terms": [
                    "bpi",
                    "across",
                    "abd",
                    "bba"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, lexical models perform best on content-oriented tasks (ABD, BBA), while graph-based encoders show relative advantages on socially grounded reasoning, particularly in BPI, where modeling interaction structure is essential. Yet absolute performance on BPI remains modest across models, reflecting the difficulty of this four-class, highly imbalanced task and pointing to the need for richer approaches to capture peer-group dynamics. Fusion strategies consistently outperform unimodal baselines, with early fusion proving most effective for relational inference, late fusion excelling at content-driven detection, and hybrid fusion offering robust generalization. Taken together, these results underscore the complementary strengths of lexical and structural cues and highlight the necessity of their integration to model the nuanced and evolving dynamics of abusive behavior in multi-party online conversations.</p>\n\n",
                "matched_terms": [
                    "across",
                    "abd",
                    "task",
                    "bpi",
                    "bba",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To deepen our understanding of how different representation learning strategies handle abusive dynamics, we conducted a systematic error analysis across the three classification tasks. Our goal was to evaluate not only overall accuracy but also each model&#8217;s capacity to capture participant roles, pragmatic cues, and evolving discourse structures in multi-party conversations. \n<br class=\"ltx_break\"/>All misclassification counts in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#S4.T3\" title=\"Table 3 &#8227; 4.2. Error Analysis &#8227; 4. Experimental Setup &#8227; Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> are aggregated over the five test splits in our cross-validation. For each model, we collect the misclassified instances from the test set of every split and sum them to obtain the total per class. The reported values therefore reflect cumulative errors across all folds, not results from a single split.</p>\n\n",
                "matched_terms": [
                    "across",
                    "split",
                    "splits",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Errors are not uniform across roles. <em class=\"ltx_emph ltx_font_italic\">Victim Support</em> is systematically the most misclassified (CamemBERT: 24.65% / 211, GraphWave: 23.83% / 204, FGSD: 27.57% / 236). In contrast, explicit aggressor roles (<em class=\"ltx_emph ltx_font_italic\">Bully</em>, <em class=\"ltx_emph ltx_font_italic\">Bully Support</em>) show lower error shares (<math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.SSSx1.Px3.p1.m1\" intent=\":literal\"><semantics><mo>&#8776;</mo><annotation encoding=\"application/x-tex\">\\approx</annotation></semantics></math>15&#8211;22%), suggesting they are easier to detect. Example: the message <em class=\"ltx_emph ltx_font_italic\">&#8220;tu fais le mec intelligent pour impressionner les autres&#8221;</em> (EN: <em class=\"ltx_emph ltx_font_italic\">You&#8217;re pretending to be smart to impress others</em>) was correctly flagged as aggressive by Gemini004 but missed by GraphWave, confirming that subtle antagonism requires lexical sensitivity.</p>\n\n",
                "matched_terms": [
                    "victim",
                    "across",
                    "bully"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Across modalities, the hardest cases are implicit abuse (ABD, BBA) and ambiguous supportive roles (BPI). Graph encoders over-predict abuse, while text encoders miss subtler aggression.</p>\n\n",
                "matched_terms": [
                    "bpi",
                    "across",
                    "abd",
                    "bba"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fusion particularly reduces errors on aggressor roles. <em class=\"ltx_emph ltx_font_italic\">Bully</em> misclassifications drop from 22.49% (159, text) to 18.00% (126, fusion), and <em class=\"ltx_emph ltx_font_italic\">Bully Support</em> from 22.21% (157) to 17.14% (120). At the same time, <em class=\"ltx_emph ltx_font_italic\">Victim</em> and <em class=\"ltx_emph ltx_font_italic\">Victim Support</em> errors increase, reflecting a redistribution of mistakes rather than a global reduction.</p>\n\n",
                "matched_terms": [
                    "victim",
                    "bully"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fusion strategies shift rather than eliminate errors. They reduce false negatives for aggressor roles (ABD <em class=\"ltx_emph ltx_font_italic\">Abusive</em>, BPI <em class=\"ltx_emph ltx_font_italic\">Bully</em>) but increase confusion for neutral or supportive roles. This pattern highlights complementary error profiles between text and graph modalities and underscores the need for adaptive fusion mechanisms sensitive to role dynamics and pragmatic subtleties.</p>\n\n",
                "matched_terms": [
                    "bpi",
                    "abd",
                    "bully"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper presents representation learning methods for detecting hate speech in multi-party dialogues and introduces two new evaluation tasks: Bullying Behavior Analysis (BBA) and Bullying Peer Group Identification (BPI). These tasks provide a more granular understanding of how aggressive behaviors emerge and evolve within multi-participant conversations. We benchmark a broad spectrum of lexical and graph-based embedding models and assess fusion strategies that combine linguistic content with structural interaction patterns. Results show that multimodal approaches consistently outperform unimodal baselines&#8212;particularly on socially complex tasks like BPI, where understanding implicit roles and interactional dynamics is crucial. Lexical models excel in tasks driven by explicit textual cues (ABD, BBA), while graph-based models are better suited to relational inference. Fusion strategies, especially early fusion, capitalize on the complementary strengths of both modalities, yielding the most robust and context-aware representations for abuse detection.</p>\n\n",
                "matched_terms": [
                    "bpi",
                    "abd",
                    "tasks",
                    "bba"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In future work, more integrated representation learning could be explored by jointly encoding lexical and structural features within a unified model, rather than learning them separately. Additionally, while our experiments rely on structured role-play data for ethical and practical reasons, validating models on real-world conversations across different platforms and demographics will be critical for assessing generalizability. Incorporating richer context, such as temporal dynamics, user-level features, and conversational history, may further enhance the detection of subtle or evolving aggression patterns.</p>\n\n",
                "matched_terms": [
                    "across",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While our evaluation is based on <span class=\"ltx_text ltx_font_italic\">simulated</span> cyber aggression data generated through controlled role-play, this method has been shown to produce more naturalistic language than interviews, questionnaires, human&#8211;machine interactions, or reconstructed threads&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(tran2006naturalized, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib41\" title=\"\">41</a>)</cite>. However, as with any simulated resource, there may be differences from spontaneous online conversations, particularly in tone, fluency, or conversational dynamics. Moreover, cultural and linguistic features specific to French-speaking adolescents may shape language use and social behaviors, potentially limiting the generalizability of our findings. Future work should validate the models on naturally occurring data across more diverse linguistic and cultural settings to ensure cross-context robustness.</p>\n\n",
                "matched_terms": [
                    "across",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, while our tasks span both binary (e.g., ABD, BBA) and multi-class (e.g., BPI) classification, mapping complex social behaviors to fixed labels can obscure key contextual nuances. For instance, teasing among peers or reactive aggression may be misclassified without considering interpersonal dynamics or conversation history&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(cowie2014understanding, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib16\" title=\"\">16</a>)</cite>. Additionally, our use of undersampling to mitigate class imbalance ensures balanced training but may distort the natural prevalence of abusive behaviors, which are typically rare in real-world settings. Future work should explore alternative approaches&#8212;such as oversampling, cost-sensitive learning, or data augmentation&#8212;to enhance model robustness and better reflect realistic distributions.</p>\n\n",
                "matched_terms": [
                    "abd",
                    "bpi",
                    "bba",
                    "data",
                    "tasks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite its benefits, role-play data may still expose both participants and annotators to emotionally charged or distressing situations. To mitigate these risks, we selected this dataset specifically because it was created following a strict experimental protocol for participants and adhered to established ethical and procedural standards for annotation. During data collection, all student participants under the age of 18 provided informed parental consent, and each school&#8217;s administration and ethics board approved the study. Psychological support was available throughout, and students participated in dedicated debriefing sessions focused on cyberbullying awareness and digital safety&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/lrec/OllagnierCVB22, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib35\" title=\"\">35</a>)</cite>. Importantly, no student was asked to play the role of a victim, and all sessions were conducted under the supervision of trained researchers ready to intervene if necessary.</p>\n\n",
                "matched_terms": [
                    "victim",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For each dataset, we randomly sampled 70% of the exchanged messages for training and 30% for testing. To maintain consistency and readability, each binary classification scheme is represented using positive and negative classes. For the ABD task, the positive class consists of messages that perpetrate abusive behaviors, while the negative class includes non-abusive messages. For the BBA task, messages exhibiting cyberbullying behaviors are classified as negative, whereas messages that do not contain cyberbullying behaviors belong to the positive class. For the BPI task, each label represents a different role that participants play in cyberbullying interactions, as described in&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(DBLP:conf/lrec/OllagnierCVB22, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#bib.bib35\" title=\"\">35</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "abd",
                    "task",
                    "label",
                    "bpi",
                    "bba"
                ]
            }
        ]
    },
    "A3.T5": {
        "caption": "Table 5. SVM hyperparameter grid. Bold values mark the best-performing parameters.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Parameter</span></th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Values</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Kernel</span></th>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">{rbf, sigmoid, </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">poly</span><span class=\"ltx_text\" style=\"font-size:90%;\">, linear}</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">C</span></th>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">{0.01, </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.1</span><span class=\"ltx_text\" style=\"font-size:90%;\">, 1, 10, 100}</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Gamma</span></th>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">{scale, auto, 0.001, </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.01</span><span class=\"ltx_text\" style=\"font-size:90%;\">, 0.1, 1, 10}</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Degree</span></th>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">{2, </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3</span><span class=\"ltx_text\" style=\"font-size:90%;\">, 4, 5}</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Max Iterations</span></th>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">{100, 200, </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">500</span><span class=\"ltx_text\" style=\"font-size:90%;\">}</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "poly",
            "gamma",
            "iterations",
            "rbf",
            "parameter",
            "max",
            "mark",
            "hyperparameter",
            "scale",
            "bold",
            "degree",
            "grid",
            "sigmoid",
            "parameters",
            "values",
            "bestperforming",
            "kernel",
            "linear",
            "svm",
            "auto"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">All models are trained using the same pipeline, with SVM hyperparameter configurations detailed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#A3.T5\" title=\"Table 5 &#8227; Appendix C Model Detailed Evaluation &#8227; Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>. A grid search is conducted over predefined parameter values, and each configuration is evaluated using 5-fold cross-validation. The optimal hyperparameters are selected based on the highest average weighted F-score. Detailed configurations and results are provided in the KIDOS project folder<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://drive.google.com/drive/folders/1f21Pd1h-VDQAH4hK_bilIMvp4bUves_C?usp=sharing\" title=\"\">https://drive.google.com/drive/folders/1f21Pd1h-VDQAH4hK_bilIMvp4bUves_C?usp=sharing</a></span></span></span>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">As introduced in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#S3.SS2\" title=\"3.2. Modeling Conversations &#8227; 3. Addressing Online Hate in Conversations &#8227; Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>, we benchmark six lexical and eight graph-based representation learning methods. We also evaluate fusion strategies that combine top-performing lexical models (<span class=\"ltx_text ltx_font_typewriter\">CamemBERTa</span>, <span class=\"ltx_text ltx_font_typewriter\">Gemini</span>, and <span class=\"ltx_text ltx_font_typewriter\">mBERT</span>) with graph embeddings. All representations are classified using Support Vector Machines (SVM) implemented with cuML and scikit-learn. Experiments were conducted on a machine with an Intel&#174;Core&#8482;i7&#8211;1065G7 3.5 GHz CPU and an Nvidia A100 GPU. Hyperparameter settings are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.17289v1#A3\" title=\"Appendix C Model Detailed Evaluation &#8227; Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "hyperparameter",
                    "svm"
                ]
            }
        ]
    }
}