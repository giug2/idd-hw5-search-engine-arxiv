{
    "S2.T1": {
        "caption": "Table 1: \nObjective and subjective results on Tibetan multi-dialect TTS.\nThe highest metric is marked in bold.\n“*” denotes the proposed model.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\" rowspan=\"2\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Dialect</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\" rowspan=\"2\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"7\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Objective Metrics</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Subjective Metrics</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">STOI<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">PESQ<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SI-SDR(dB)<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">DCA(%)<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">DECS(%)<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">DNSMOS<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m6\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">RTF<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m7\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">nMOS<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m8\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">dMOC(%)<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m9\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math></span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" rowspan=\"4\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#220;-Tsang</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">SC-CNN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">80.40</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.62</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.24</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">40.37</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">65.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><math alttext=\"2.16\\pm 0.38\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m10\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">2.16</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.38</mn></mrow><annotation encoding=\"application/x-tex\">2.16\\pm 0.38</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.036</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.83</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">65.14</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">VITS2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">85.72</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.00</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.88</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">39.26</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">41.91</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><math alttext=\"2.53\\pm 0.35\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m11\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">2.53</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.35</mn></mrow><annotation encoding=\"application/x-tex\">2.53\\pm 0.35</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.020</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.18</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">69.15</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Matcha-TTS</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">93.84</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.43</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">12.32</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">65.80</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">65.20</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><math alttext=\"2.77\\pm 0.15\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m12\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">2.77</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.15</mn></mrow><annotation encoding=\"application/x-tex\">2.77\\pm 0.15</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.023</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.73</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">73.33</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">TMD-TTS</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">94.52</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.03</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">17.91</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">67.41</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">88.09</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><math alttext=\"\\textbf{2.78}\\pm\\textbf{0.29}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m13\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\" mathsize=\"0.900em\">2.78</mtext><mo mathsize=\"0.900em\">&#177;</mo><mtext class=\"ltx_mathvariant_bold\" mathsize=\"0.900em\">0.29</mtext></mrow><annotation encoding=\"application/x-tex\">\\textbf{2.78}\\pm\\textbf{0.29}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.032</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.83</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">76.64</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" rowspan=\"4\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Amdo</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">SC-CNN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">79.90</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.65</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.25</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">59.63</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">65.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><math alttext=\"2.16\\pm 0.38\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m14\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">2.16</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.38</mn></mrow><annotation encoding=\"application/x-tex\">2.16\\pm 0.38</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.036</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.82</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">63.07</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">VITS2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">89.13</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.98</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">11.28</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">39.26</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">41.91</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><math alttext=\"2.54\\pm 0.36\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m15\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">2.54</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.36</mn></mrow><annotation encoding=\"application/x-tex\">2.54\\pm 0.36</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.021</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.20</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">70.13</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Matcha-TTS</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">94.54</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.34</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">19.17</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">75.42</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">65.32</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><math alttext=\"\\textbf{2.79}\\pm\\textbf{0.13}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m16\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\" mathsize=\"0.900em\">2.79</mtext><mo mathsize=\"0.900em\">&#177;</mo><mtext class=\"ltx_mathvariant_bold\" mathsize=\"0.900em\">0.13</mtext></mrow><annotation encoding=\"application/x-tex\">\\textbf{2.79}\\pm\\textbf{0.13}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.023</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.75</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">74.56</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">TMD-TTS</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">94.92</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.13</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">21.32</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">87.78</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">79.17</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><math alttext=\"\\textbf{2.79}\\pm\\textbf{0.18}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m17\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\" mathsize=\"0.900em\">2.79</mtext><mo mathsize=\"0.900em\">&#177;</mo><mtext class=\"ltx_mathvariant_bold\" mathsize=\"0.900em\">0.18</mtext></mrow><annotation encoding=\"application/x-tex\">\\textbf{2.79}\\pm\\textbf{0.18}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.032</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.84</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">77.01</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" rowspan=\"4\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Kham</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">SC-CNN</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">76.09</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.47</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.69</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">38.52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">19.16</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><math alttext=\"2.01\\pm 0.30\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m18\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">2.01</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.30</mn></mrow><annotation encoding=\"application/x-tex\">2.01\\pm 0.30</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.034</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.67</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">65.14</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">VITS2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">82.25</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.87</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.06</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">44.81</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">46.01</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><math alttext=\"2.43\\pm 0.34\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m19\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">2.43</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.34</mn></mrow><annotation encoding=\"application/x-tex\">2.43\\pm 0.34</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.021</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.18</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">71.11</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Matcha-TTS</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">91.47</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.32</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">17.90</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">60.80</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">63.48</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><math alttext=\"2.74\\pm 0.20\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m20\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">2.74</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.20</mn></mrow><annotation encoding=\"application/x-tex\">2.74\\pm 0.20</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.022</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.73</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">72.16</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">TMD-TTS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">93.17</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.05</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">21.43</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">61.11</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">67.65</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><math alttext=\"\\textbf{2.77}\\pm\\textbf{0.17}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.T1.m21\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\" mathsize=\"0.900em\">2.77</mtext><mo mathsize=\"0.900em\">&#177;</mo><mtext class=\"ltx_mathvariant_bold\" mathsize=\"0.900em\">0.17</mtext></mrow><annotation encoding=\"application/x-tex\">\\textbf{2.77}\\pm\\textbf{0.17}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.031</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.86</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.5pt;padding-right:3.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">75.80</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "sccnn",
            "marked",
            "nmos↑uparrow",
            "denotes",
            "dmoc↑uparrow",
            "tmdtts",
            "tts",
            "rtf↓downarrow",
            "243±034243pm",
            "amdo",
            "277±017textbf277pmtextbf017",
            "sisdrdb↑uparrow",
            "ütsang",
            "objective",
            "253±035253pm",
            "kham",
            "dca↑uparrow",
            "matchatts",
            "278±029textbf278pmtextbf029",
            "274±020274pm",
            "decs↑uparrow",
            "stoi↑uparrow",
            "dnsmos↑uparrow",
            "279±013textbf279pmtextbf013",
            "metrics",
            "bold",
            "279±018textbf279pmtextbf018",
            "254±036254pm",
            "results",
            "metric",
            "tibetan",
            "multidialect",
            "277±015277pm",
            "proposed",
            "pesq↑uparrow",
            "216±038216pm",
            "201±030201pm",
            "dialect",
            "model",
            "highest",
            "vits2",
            "subjective"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#S2.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 2.1 TMD-TTS with DSDR-Net &#8227; 2 Methods &#8227; TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for &#220;-Tsang, Amdo and Kham Speech Dataset Generation\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, TMD-TTS consistently outperforms all baselines across both objective and subjective metrics. For speech quality, it achieves the best performance in all three dialects, e.g., &#220;-Tsang (94.52% STOI, 3.03 PESQ, 17.91&#160;dB SI-SDR, 2.78 DNSMOS), Amdo (94.92% STOI, 3.13 PESQ, 21.32&#160;dB SI-SDR, 2.79 DNSMOS), and Kham (93.17% STOI, 3.05 PESQ, 21.43&#160;dB SI-SDR, 2.77 DNSMOS), clearly surpassing Matcha-TTS and VITS2. In terms of dialect similarity, our model reaches up to 88.09% DECS and 87.78% DCA, showing clear advantages over baselines. Although its inference speed (RTF </span>\n  <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&#8776;</mo>\n      <annotation encoding=\"application/x-tex\">\\approx</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">0.031&#8211;0.032) is slightly slower than VITS2 (</span>\n  <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&#8776;</mo>\n      <annotation encoding=\"application/x-tex\">\\approx</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">0.020&#8211;0.021), it still meets the requirement for real-time synthesis. Subjective evaluations further confirm these findings, with TMD-TTS achieving the highest naturalness (nMOS 3.83/3.84/3.86) and dialect consistency (dMOC 76.64%/77.01%/75.80%) across &#220;-Tsang, Amdo, and Kham. Overall, these results demonstrate the superiority of TMD-TTS in both speech quality and dialectal fidelity.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Tibetan is a low-resource language with limited parallel speech corpora spanning its three major dialects (&#220;-Tsang, Amdo, and Kham), limiting progress in speech modeling. To address this issue, we propose TMD-TTS, a unified Tibetan multi-dialect text-to-speech (TTS) framework that synthesizes parallel dialectal speech from explicit dialect labels. Our method features a dialect fusion module and a Dialect-Specialized Dynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and linguistic variations across dialects. Extensive objective and subjective evaluations demonstrate that TMD-TTS significantly outperforms baselines in dialectal expressiveness. We further validate the quality and utility of the synthesized speech through a challenging Speech-to-Speech Dialect Conversion (S2SDC) task.</span>\n</p>\n\n",
                "matched_terms": [
                    "tibetan",
                    "tmdtts",
                    "multidialect",
                    "tts",
                    "amdo",
                    "dialect",
                    "ütsang",
                    "objective",
                    "kham",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"font-size:90%;\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;</span></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\nText-to-speech synthesis, multi-dialect Tibetan TTS, synthetic dataset</span>\n</p>\n\n",
                "matched_terms": [
                    "tibetan",
                    "multidialect",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Tibetan, spoken by over six million people across Tibet, neighboring Chinese provinces, and parts of South Asia, is a low-resource language with three major dialects&#8212;&#220;-Tsang, Amdo, and Kham&#8212;that differ substantially in phonology, lexicon, and syntax, often resulting in limited mutual intelligibility. To facilitate cross-dialect communication, speech-to-speech dialect conversion (S2SDC) is essential, yet its progress is constrained by the lack of large-scale, high-quality parallel data. Current resources remain scarce, with Zhuoma&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> providing only about 800 parallel samples, and most datasets relying on labor-intensive manual collection. This underscores the urgent need for a TTS-based approach to synthesize Tibetan multi-dialect data.</span>\n</p>\n\n",
                "matched_terms": [
                    "tibetan",
                    "multidialect",
                    "amdo",
                    "dialect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Modern TTS systems cover a spectrum of architectures. Fully end-to-end models such as VITS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib3\" title=\"\">3</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which combine cVAEs&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with normalizing flows&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, can achieve high-fidelity synthesis but often suffer from training instability and slow inference. Diffusion-based frameworks (e.g., NaturalSpeech&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib7\" title=\"\">7</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) and alignment-free designs (e.g., F5TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) also offer strong performance but at the cost of efficiency or simplicity. In contrast, semi-end-to-end approaches such as Tacotron2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and FastSpeech2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> generate mel-spectrograms that are subsequently converted to waveforms by vocoders like HiFi-GAN&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib11\" title=\"\">11</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, BigVGAN&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, BigVSAN&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib13\" title=\"\">13</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, or Vocos&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, striking a balance between stability and controllability. Recent systems such as Matcha-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and OT-CFM&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> further refine this paradigm.</span>\n</p>\n\n",
                "matched_terms": [
                    "matchatts",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Dialect modeling poses additional challenges. Research on Tibetan multi-dialect TTS remains limited: Xu et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> introduced a model that generates a shared mel-spectrogram and employs separate WaveNet-based vocoders for Amdo and &#220;-Tsang. However, this late-stage incorporation of dialect information and the reliance on multiple heavy vocoders restrict the model&#8217;s ability to capture fine-grained dialectal distinctions.</span>\n</p>\n\n",
                "matched_terms": [
                    "tibetan",
                    "multidialect",
                    "tts",
                    "amdo",
                    "dialect",
                    "model",
                    "ütsang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To this end, we propose TMD-TTS, a unified Tibetan multi-dialect TTS framework that integrates dialect representations into the TTS model. Dialect embeddings are first obtained through an embedding layer and fused via a linear layer. To capture nuanced phonetic distinctions and enable precise dialectal control, we design a Dialect-Specialized Dynamic Routing Network (DSDR-Net) to replace the conventional Feedforward Network (FFN) in the Transformer architecture. DSDR-Net incorporates a conditional computation mechanism that dynamically routes information to a dialect-specific sub-network based on the input dialect ID. This allows the model to learn distinct acoustic patterns such as rhythm and intonation, offering finer-grained dialect modeling compared to shared-parameter networks.</span>\n</p>\n\n",
                "matched_terms": [
                    "tibetan",
                    "tmdtts",
                    "multidialect",
                    "tts",
                    "dialect",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We propose TMD-TTS, the first Tibetan multi-dialect TTS framework with DSDR-Net, which significantly improves dialect consistency and captures fine-grained acoustic and linguistic variations across Tibetan dialects.</span>\n</p>\n\n",
                "matched_terms": [
                    "tibetan",
                    "tmdtts",
                    "multidialect",
                    "tts",
                    "dialect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We construct and release TMDD, a large-scale Tibetan multi-dialect speech dataset synthesized via TMD-TTS. This provides a reproducible pipeline for high-quality dialect-rich data generation, validated through its application to a challenging S2SDC task.</span>\n</p>\n\n",
                "matched_terms": [
                    "tibetan",
                    "tmdtts",
                    "multidialect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We develop and release a comprehensive evaluation toolkit for Tibetan dialect speech synthesis, enabling standardized assessment of audio quality and dialect similarity.</span>\n</p>\n\n",
                "matched_terms": [
                    "tibetan",
                    "dialect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we present TMD-TTS, a unified multi-dialect TTS model built upon Matcha-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, specifically designed for generating high-quality Tibetan dialect speech datasets. Our system addresses the limitations of prior approaches&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, such as the reliance on multi-output architectures and the need to train separate vocoders for each dialect.\nThe Dialect Fusion Module integrates dialect embeddings into both the text encoder and the flow prediction network, enabling dialect-aware speech synthesis. DSDR-Net introduces a conditional computation mechanism into the feed-forward network (FFN) of the Transformer architecture to provide fine-grained control over dialectal variation and accurately capture phonetic nuances across dialects.\nFurthermore, we propose a complete data generation pipeline based on TMD-TTS to facilitate the large-scale synthesis of Tibetan multi-dialect speech datasets.</span>\n</p>\n\n",
                "matched_terms": [
                    "tibetan",
                    "tmdtts",
                    "multidialect",
                    "tts",
                    "dialect",
                    "model",
                    "matchatts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for &#220;-Tsang, Amdo and Kham Speech Dataset Generation\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">(a) illustrates the overall architecture of TMD-TTS, which synthesizes multi-dialect Tibetan speech by integrating dialect representations into the TTS pipeline. The process begins with tokenization, converting Tibetan characters into long-format tensors, followed by a text encoder. A dialect fusion module conditions the hidden representations on dialect features. Dialect processing maps each dialect </span>\n  <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">d</mi>\n      <annotation encoding=\"application/x-tex\">d</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to a dialect ID </span>\n  <math alttext=\"did\\in{0,1,2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">d</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">i</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">d</mi>\n        </mrow>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">0</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">2</mn>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">did\\in{0,1,2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and generates a normalized embedding </span>\n  <math alttext=\"h_{\\mathrm{did}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">h</mi>\n        <mi mathsize=\"0.900em\">did</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">h_{\\mathrm{did}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">:</span>\n</p>\n\n",
                "matched_terms": [
                    "tibetan",
                    "tmdtts",
                    "multidialect",
                    "tts",
                    "dialect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To construct a high-quality parallel speech dataset covering the three major Tibetan dialects, we design a data generation pipeline, as illustrated in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for &#220;-Tsang, Amdo and Kham Speech Dataset Generation\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">(c). In this pipeline, text is sequentially sampled from a curated database, and for each selected entry, dialectal speech is synthesized using the TMD-TTS model.</span>\n</p>\n\n",
                "matched_terms": [
                    "tibetan",
                    "tmdtts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To ensure fidelity, both dialectal and perceptual quality assessments are applied. Dialect assessment is conducted by retaining only samples with a Dialect Embedding Cosine Similarity (DECS, see Section&#160;</span>\n  <span class=\"ltx_ref ltx_missing_label ltx_ref_self\" style=\"font-size:90%;\">LABEL:experiments</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) greater than 0.8, thereby ensuring accurate dialectal representation. For perceptual quality, PESQ and DNSMOS (see Section&#160;</span>\n  <span class=\"ltx_ref ltx_missing_label ltx_ref_self\" style=\"font-size:90%;\">LABEL:experiments</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) are employed as evaluation metrics; utterances with </span>\n  <math alttext=\"PESQ&lt;3\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">P</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">E</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">S</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">Q</mi>\n        </mrow>\n        <mo mathsize=\"0.900em\">&lt;</mo>\n        <mn mathsize=\"0.900em\">3</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">PESQ&lt;3</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> or </span>\n  <math alttext=\"DNSMOS&lt;2.7\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">D</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">N</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">S</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">M</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">O</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">S</mi>\n        </mrow>\n        <mo mathsize=\"0.900em\">&lt;</mo>\n        <mn mathsize=\"0.900em\">2.7</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">DNSMOS&lt;2.7</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are enhanced using MetricGAN+&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Finally, a manual screening step conducted by native speakers serves as the last stage to guarantee the reliability of the audio&#8211;text pairs, yielding the finalized dataset.</span>\n</p>\n\n",
                "matched_terms": [
                    "dialect",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Dataset</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We constructed a 179-hour multi-dialect Tibetan speech corpus, including 44h &#220;-Tsang, 45h Kham, and 90h Amdo from 1,500+ speakers. The training set contains 40k samples per dialect, with 300 samples each for validation and test.</span>\n</p>\n\n",
                "matched_terms": [
                    "tibetan",
                    "multidialect",
                    "amdo",
                    "dialect",
                    "ütsang",
                    "kham"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Setting</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> TMD-TTS was trained for 500k steps with Adam&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and the vocoder followed BigVGAN&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with AdamW&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and exponential decay. All experiments were conducted on two RTX 4090 GPUs. The model uses a 216-character vocabulary, 128-dim dialect embeddings, and a 192-dim DSDR-Net. For comparison, we re-implemented three baseline models, SC-CNN&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, VITS2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and Matcha-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and extended them with the Tibetan multi-dialect TTS design from Xu et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which includes multiple BigVGAN vocoders and Wylie transliteration, to enable multi-dialect synthesis.</span>\n</p>\n\n",
                "matched_terms": [
                    "tibetan",
                    "tmdtts",
                    "multidialect",
                    "tts",
                    "sccnn",
                    "dialect",
                    "model",
                    "matchatts",
                    "vits2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Metrics</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We evaluate using both subjective and objective metrics. Subjective metrics include the naturalness Mean Opinion Score (nMOS) and the dialect Mean Opinion Classification (dMOC, measured by F1 score between the predicted classifications and the labels), both evaluated by 20 native speakers. The objective evaluation includes STOI&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, PESQ&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, SI-SDR&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, DNSMOS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, dialect classification accuracy (DCA) obtained from a pre-trained dialect classifier inspired by metrics in Durflex-EVC&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, dialect embedding cosine similarity (DECS) for measuring dialectal consistency&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib27\" title=\"\">27</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and the real-time factor (RTF) for inference efficiency.</span>\n</p>\n\n",
                "matched_terms": [
                    "objective",
                    "dialect",
                    "metrics",
                    "subjective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate the contributions of dialect fusion module and the DSDR-Net in TMD-TTS (Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#S3.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 3.2 Main Result &#8227; 3 Experiments &#8227; TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for &#220;-Tsang, Amdo and Kham Speech Dataset Generation\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). Removing the dialect embedding (replacing it with zeros) reduces DCA from 80.25% to 74.15% and DECS from 78.3% to 72.8%, showing that explicit dialect information helps generate dialect-consistent speech. Replacing the DSDR-Net with a standard FFN drops DCA to 60.12% and DECS to 58.6%, highlighting its importance in modeling fine-grained dialectal variations. Removing both components further degrades performance (DCA 33.42%, DECS 32.2%), confirming their complementary contributions.</span>\n</p>\n\n",
                "matched_terms": [
                    "tmdtts",
                    "dialect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To further validate the effectiveness of our proposed method in modeling dialect representations, we conducted a comparative study against the VITS2 baseline. As shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#S3.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 3.3 Ablation Study &#8227; 3 Experiments &#8227; TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for &#220;-Tsang, Amdo and Kham Speech Dataset Generation\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we use the pre-trained dialect classifier (see Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#S3\" style=\"font-size:90%;\" title=\"3 Experiments &#8227; TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for &#220;-Tsang, Amdo and Kham Speech Dataset Generation\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) to extract softmax-based dialectal features from each synthesized utterance, averaged over &#220;-Tsang, Amdo, and Kham test samples. Compared to VITS2, our method produces speech with more salient target-dialect characteristics, demonstrating improved dialectal consistency and generalization.</span>\n</p>\n\n",
                "matched_terms": [
                    "proposed",
                    "amdo",
                    "dialect",
                    "ütsang",
                    "kham",
                    "vits2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Additionally, dialect embeddings were extracted using the pre-trained model for SECS computation and visualized via t-SNE (Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#S3.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 3.3 Ablation Study &#8227; 3 Experiments &#8227; TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for &#220;-Tsang, Amdo and Kham Speech Dataset Generation\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). Each point represents a dialect embedding, with cluster centers derived from dialect embeddings. Under VITS2, embeddings are less compact and exhibit notable overlap&#8212;especially between &#220;-Tsang and Amdo&#8212;indicating weaker dialect distinction. In contrast, our method achieves better cluster separation and closer alignment with the cluster centers, highlighting the effectiveness of the dialect fusion module and DSDR-Net.</span>\n</p>\n\n",
                "matched_terms": [
                    "vits2",
                    "dialect",
                    "model",
                    "ütsang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Objective evaluation using PESQ, SI-SDR, and DNSMOS shows that TMDD maintains high audio quality, with average PESQ </span>\n  <math alttext=\"3.06\\pm 0.38\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">3.06</mn>\n        <mo mathsize=\"0.900em\">&#177;</mo>\n        <mn mathsize=\"0.900em\">0.38</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">3.06\\pm 0.38</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (baseline: </span>\n  <math alttext=\"2.77\\pm 0.38\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">2.77</mn>\n        <mo mathsize=\"0.900em\">&#177;</mo>\n        <mn mathsize=\"0.900em\">0.38</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">2.77\\pm 0.38</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) and SI-SDR </span>\n  <math alttext=\"20.66\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">20.66</mn>\n      <annotation encoding=\"application/x-tex\">20.66</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dB (baseline: </span>\n  <math alttext=\"20.60\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">20.60</mn>\n      <annotation encoding=\"application/x-tex\">20.60</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dB). DNSMOS results also indicate consistent perceptual quality suitable for downstream tasks.</span>\n</p>\n\n",
                "matched_terms": [
                    "objective",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we propose TMD-TTS, a unified Tibetan multi-dialect TTS model that incorporates dialect representations for multi-dialect Tibetan speech synthesis. We design a dialect fusion module and introduce DSDR-Net to better control dialectal variations. Leveraging this model, we construct and release a large-scale, parallel Tibetan multi-dialect speech dataset, TMDD, facilitating broader research in Tibetan speech synthesis and conversion. To assess the quality and utility of the generated speech, we explore the S2SDC task and conduct subjective evaluations, including naturalness and dialectal consistency. Furthermore, we develop a comprehensive evaluation toolkit specifically tailored for Tibetan speech generation tasks.</span>\n</p>\n\n",
                "matched_terms": [
                    "tibetan",
                    "tmdtts",
                    "multidialect",
                    "tts",
                    "dialect",
                    "model",
                    "subjective"
                ]
            }
        ]
    },
    "S3.T2": {
        "caption": "Table 2: \nAblation study results. The highest metric is indicated in bold.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">DCA(%)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">DECS</span><span class=\"ltx_text\" style=\"font-size:90%;\">(%)</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">TMD-TTS</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">80.25</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">78.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8198;&#8195;w/o DSDR-Net</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">60.12</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">58.6</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8198;&#8195;w/o Dialect Fusion</span></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">74.15</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">72.8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8198;&#8195;w/o Dialect Fusion &amp; DSDR-Net</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">33.42</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">32.2</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "tmdtts",
            "dca",
            "study",
            "dialect",
            "model",
            "bold",
            "ablation",
            "highest",
            "indicated",
            "dsdrnet",
            "fusion",
            "results",
            "decs",
            "metric"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate the contributions of dialect fusion module and the DSDR-Net in TMD-TTS (Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#S3.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 3.2 Main Result &#8227; 3 Experiments &#8227; TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for &#220;-Tsang, Amdo and Kham Speech Dataset Generation\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). Removing the dialect embedding (replacing it with zeros) reduces DCA from 80.25% to 74.15% and DECS from 78.3% to 72.8%, showing that explicit dialect information helps generate dialect-consistent speech. Replacing the DSDR-Net with a standard FFN drops DCA to 60.12% and DECS to 58.6%, highlighting its importance in modeling fine-grained dialectal variations. Removing both components further degrades performance (DCA 33.42%, DECS 32.2%), confirming their complementary contributions.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Tibetan is a low-resource language with limited parallel speech corpora spanning its three major dialects (&#220;-Tsang, Amdo, and Kham), limiting progress in speech modeling. To address this issue, we propose TMD-TTS, a unified Tibetan multi-dialect text-to-speech (TTS) framework that synthesizes parallel dialectal speech from explicit dialect labels. Our method features a dialect fusion module and a Dialect-Specialized Dynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and linguistic variations across dialects. Extensive objective and subjective evaluations demonstrate that TMD-TTS significantly outperforms baselines in dialectal expressiveness. We further validate the quality and utility of the synthesized speech through a challenging Speech-to-Speech Dialect Conversion (S2SDC) task.</span>\n</p>\n\n",
                "matched_terms": [
                    "tmdtts",
                    "dialect",
                    "dsdrnet",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Dialect modeling poses additional challenges. Research on Tibetan multi-dialect TTS remains limited: Xu et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> introduced a model that generates a shared mel-spectrogram and employs separate WaveNet-based vocoders for Amdo and &#220;-Tsang. However, this late-stage incorporation of dialect information and the reliance on multiple heavy vocoders restrict the model&#8217;s ability to capture fine-grained dialectal distinctions.</span>\n</p>\n\n",
                "matched_terms": [
                    "dialect",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To this end, we propose TMD-TTS, a unified Tibetan multi-dialect TTS framework that integrates dialect representations into the TTS model. Dialect embeddings are first obtained through an embedding layer and fused via a linear layer. To capture nuanced phonetic distinctions and enable precise dialectal control, we design a Dialect-Specialized Dynamic Routing Network (DSDR-Net) to replace the conventional Feedforward Network (FFN) in the Transformer architecture. DSDR-Net incorporates a conditional computation mechanism that dynamically routes information to a dialect-specific sub-network based on the input dialect ID. This allows the model to learn distinct acoustic patterns such as rhythm and intonation, offering finer-grained dialect modeling compared to shared-parameter networks.</span>\n</p>\n\n",
                "matched_terms": [
                    "tmdtts",
                    "dialect",
                    "model",
                    "dsdrnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We propose TMD-TTS, the first Tibetan multi-dialect TTS framework with DSDR-Net, which significantly improves dialect consistency and captures fine-grained acoustic and linguistic variations across Tibetan dialects.</span>\n</p>\n\n",
                "matched_terms": [
                    "tmdtts",
                    "dialect",
                    "dsdrnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we present TMD-TTS, a unified multi-dialect TTS model built upon Matcha-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, specifically designed for generating high-quality Tibetan dialect speech datasets. Our system addresses the limitations of prior approaches&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, such as the reliance on multi-output architectures and the need to train separate vocoders for each dialect.\nThe Dialect Fusion Module integrates dialect embeddings into both the text encoder and the flow prediction network, enabling dialect-aware speech synthesis. DSDR-Net introduces a conditional computation mechanism into the feed-forward network (FFN) of the Transformer architecture to provide fine-grained control over dialectal variation and accurately capture phonetic nuances across dialects.\nFurthermore, we propose a complete data generation pipeline based on TMD-TTS to facilitate the large-scale synthesis of Tibetan multi-dialect speech datasets.</span>\n</p>\n\n",
                "matched_terms": [
                    "tmdtts",
                    "dialect",
                    "model",
                    "dsdrnet",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for &#220;-Tsang, Amdo and Kham Speech Dataset Generation\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">(a) illustrates the overall architecture of TMD-TTS, which synthesizes multi-dialect Tibetan speech by integrating dialect representations into the TTS pipeline. The process begins with tokenization, converting Tibetan characters into long-format tensors, followed by a text encoder. A dialect fusion module conditions the hidden representations on dialect features. Dialect processing maps each dialect </span>\n  <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">d</mi>\n      <annotation encoding=\"application/x-tex\">d</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to a dialect ID </span>\n  <math alttext=\"did\\in{0,1,2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">d</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">i</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">d</mi>\n        </mrow>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">0</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">2</mn>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">did\\in{0,1,2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and generates a normalized embedding </span>\n  <math alttext=\"h_{\\mathrm{did}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">h</mi>\n        <mi mathsize=\"0.900em\">did</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">h_{\\mathrm{did}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">:</span>\n</p>\n\n",
                "matched_terms": [
                    "tmdtts",
                    "dialect",
                    "fusion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To better capture dialectal characteristics, TMD-TTS incorporates a dialect-specialized dynamic routing network (DSDR-Net, Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for &#220;-Tsang, Amdo and Kham Speech Dataset Generation\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">(b)) that replaces the conventional feed-forward network (FFN) in the Transformer. Given hidden features </span>\n  <math alttext=\"h_{text}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">h</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">t</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">e</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">x</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">t</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">h_{text}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, a multi-head self-attention layer produces </span>\n  <math alttext=\"h_{\\text{attn}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">h</mi>\n        <mtext mathsize=\"0.900em\">attn</mtext>\n      </msub>\n      <annotation encoding=\"application/x-tex\">h_{\\text{attn}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">:</span>\n</p>\n\n",
                "matched_terms": [
                    "tmdtts",
                    "dsdrnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To construct a high-quality parallel speech dataset covering the three major Tibetan dialects, we design a data generation pipeline, as illustrated in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for &#220;-Tsang, Amdo and Kham Speech Dataset Generation\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">(c). In this pipeline, text is sequentially sampled from a curated database, and for each selected entry, dialectal speech is synthesized using the TMD-TTS model.</span>\n</p>\n\n",
                "matched_terms": [
                    "tmdtts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To ensure fidelity, both dialectal and perceptual quality assessments are applied. Dialect assessment is conducted by retaining only samples with a Dialect Embedding Cosine Similarity (DECS, see Section&#160;</span>\n  <span class=\"ltx_ref ltx_missing_label ltx_ref_self\" style=\"font-size:90%;\">LABEL:experiments</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) greater than 0.8, thereby ensuring accurate dialectal representation. For perceptual quality, PESQ and DNSMOS (see Section&#160;</span>\n  <span class=\"ltx_ref ltx_missing_label ltx_ref_self\" style=\"font-size:90%;\">LABEL:experiments</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) are employed as evaluation metrics; utterances with </span>\n  <math alttext=\"PESQ&lt;3\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">P</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">E</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">S</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">Q</mi>\n        </mrow>\n        <mo mathsize=\"0.900em\">&lt;</mo>\n        <mn mathsize=\"0.900em\">3</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">PESQ&lt;3</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> or </span>\n  <math alttext=\"DNSMOS&lt;2.7\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">D</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">N</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">S</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">M</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">O</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">S</mi>\n        </mrow>\n        <mo mathsize=\"0.900em\">&lt;</mo>\n        <mn mathsize=\"0.900em\">2.7</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">DNSMOS&lt;2.7</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are enhanced using MetricGAN+&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Finally, a manual screening step conducted by native speakers serves as the last stage to guarantee the reliability of the audio&#8211;text pairs, yielding the finalized dataset.</span>\n</p>\n\n",
                "matched_terms": [
                    "dialect",
                    "decs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Setting</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> TMD-TTS was trained for 500k steps with Adam&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and the vocoder followed BigVGAN&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with AdamW&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and exponential decay. All experiments were conducted on two RTX 4090 GPUs. The model uses a 216-character vocabulary, 128-dim dialect embeddings, and a 192-dim DSDR-Net. For comparison, we re-implemented three baseline models, SC-CNN&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, VITS2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and Matcha-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and extended them with the Tibetan multi-dialect TTS design from Xu et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which includes multiple BigVGAN vocoders and Wylie transliteration, to enable multi-dialect synthesis.</span>\n</p>\n\n",
                "matched_terms": [
                    "tmdtts",
                    "dialect",
                    "model",
                    "dsdrnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Metrics</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We evaluate using both subjective and objective metrics. Subjective metrics include the naturalness Mean Opinion Score (nMOS) and the dialect Mean Opinion Classification (dMOC, measured by F1 score between the predicted classifications and the labels), both evaluated by 20 native speakers. The objective evaluation includes STOI&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, PESQ&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, SI-SDR&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, DNSMOS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, dialect classification accuracy (DCA) obtained from a pre-trained dialect classifier inspired by metrics in Durflex-EVC&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, dialect embedding cosine similarity (DECS) for measuring dialectal consistency&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib27\" title=\"\">27</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and the real-time factor (RTF) for inference efficiency.</span>\n</p>\n\n",
                "matched_terms": [
                    "dialect",
                    "dca",
                    "decs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#S2.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 2.1 TMD-TTS with DSDR-Net &#8227; 2 Methods &#8227; TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for &#220;-Tsang, Amdo and Kham Speech Dataset Generation\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, TMD-TTS consistently outperforms all baselines across both objective and subjective metrics. For speech quality, it achieves the best performance in all three dialects, e.g., &#220;-Tsang (94.52% STOI, 3.03 PESQ, 17.91&#160;dB SI-SDR, 2.78 DNSMOS), Amdo (94.92% STOI, 3.13 PESQ, 21.32&#160;dB SI-SDR, 2.79 DNSMOS), and Kham (93.17% STOI, 3.05 PESQ, 21.43&#160;dB SI-SDR, 2.77 DNSMOS), clearly surpassing Matcha-TTS and VITS2. In terms of dialect similarity, our model reaches up to 88.09% DECS and 87.78% DCA, showing clear advantages over baselines. Although its inference speed (RTF </span>\n  <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&#8776;</mo>\n      <annotation encoding=\"application/x-tex\">\\approx</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">0.031&#8211;0.032) is slightly slower than VITS2 (</span>\n  <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&#8776;</mo>\n      <annotation encoding=\"application/x-tex\">\\approx</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">0.020&#8211;0.021), it still meets the requirement for real-time synthesis. Subjective evaluations further confirm these findings, with TMD-TTS achieving the highest naturalness (nMOS 3.83/3.84/3.86) and dialect consistency (dMOC 76.64%/77.01%/75.80%) across &#220;-Tsang, Amdo, and Kham. Overall, these results demonstrate the superiority of TMD-TTS in both speech quality and dialectal fidelity.</span>\n</p>\n\n",
                "matched_terms": [
                    "tmdtts",
                    "dca",
                    "dialect",
                    "model",
                    "highest",
                    "results",
                    "decs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To further validate the effectiveness of our proposed method in modeling dialect representations, we conducted a comparative study against the VITS2 baseline. As shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#S3.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 3.3 Ablation Study &#8227; 3 Experiments &#8227; TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for &#220;-Tsang, Amdo and Kham Speech Dataset Generation\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we use the pre-trained dialect classifier (see Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#S3\" style=\"font-size:90%;\" title=\"3 Experiments &#8227; TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for &#220;-Tsang, Amdo and Kham Speech Dataset Generation\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) to extract softmax-based dialectal features from each synthesized utterance, averaged over &#220;-Tsang, Amdo, and Kham test samples. Compared to VITS2, our method produces speech with more salient target-dialect characteristics, demonstrating improved dialectal consistency and generalization.</span>\n</p>\n\n",
                "matched_terms": [
                    "study",
                    "dialect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Additionally, dialect embeddings were extracted using the pre-trained model for SECS computation and visualized via t-SNE (Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#S3.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 3.3 Ablation Study &#8227; 3 Experiments &#8227; TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for &#220;-Tsang, Amdo and Kham Speech Dataset Generation\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). Each point represents a dialect embedding, with cluster centers derived from dialect embeddings. Under VITS2, embeddings are less compact and exhibit notable overlap&#8212;especially between &#220;-Tsang and Amdo&#8212;indicating weaker dialect distinction. In contrast, our method achieves better cluster separation and closer alignment with the cluster centers, highlighting the effectiveness of the dialect fusion module and DSDR-Net.</span>\n</p>\n\n",
                "matched_terms": [
                    "dialect",
                    "model",
                    "fusion",
                    "dsdrnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we propose TMD-TTS, a unified Tibetan multi-dialect TTS model that incorporates dialect representations for multi-dialect Tibetan speech synthesis. We design a dialect fusion module and introduce DSDR-Net to better control dialectal variations. Leveraging this model, we construct and release a large-scale, parallel Tibetan multi-dialect speech dataset, TMDD, facilitating broader research in Tibetan speech synthesis and conversion. To assess the quality and utility of the generated speech, we explore the S2SDC task and conduct subjective evaluations, including naturalness and dialectal consistency. Furthermore, we develop a comprehensive evaluation toolkit specifically tailored for Tibetan speech generation tasks.</span>\n</p>\n\n",
                "matched_terms": [
                    "tmdtts",
                    "dialect",
                    "model",
                    "dsdrnet",
                    "fusion"
                ]
            }
        ]
    },
    "S3.T3": {
        "caption": "Table 3:  Comparison of dialectal speech statistics between the TMDD and the baseline dataset. Abbreviations: Dur (Duration), Avg (Average)",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Dialect</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">File</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Size</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Dur.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Avg. Dur.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SI-SDR(dB)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">PESQ</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">DNSMOS</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#220;-Tsang</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">2,538</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.38G</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">4h34m</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.49s</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.86</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"2.54\\pm 0.34\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m1\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">2.54</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.34</mn></mrow><annotation encoding=\"application/x-tex\">2.54\\pm 0.34</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"3.55\\pm 0.06\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m2\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">3.55</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.06</mn></mrow><annotation encoding=\"application/x-tex\">3.55\\pm 0.06</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">baseline</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Amdo</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1,387</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.56G</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2h55m</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.57s</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">23.05</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"3.11\\pm 0.20\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m3\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">3.11</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.20</mn></mrow><annotation encoding=\"application/x-tex\">3.11\\pm 0.20</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"3.61\\pm 0.08\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m4\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">3.61</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.08</mn></mrow><annotation encoding=\"application/x-tex\">3.61\\pm 0.08</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Kham</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">838</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.15G</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2h05m</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.98s</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">17.90</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"2.67\\pm 0.60\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m5\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">2.67</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.60</mn></mrow><annotation encoding=\"application/x-tex\">2.67\\pm 0.60</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"3.34\\pm 0.12\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m6\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">3.34</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.12</mn></mrow><annotation encoding=\"application/x-tex\">3.34\\pm 0.12</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Total</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4,763</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">8.09G</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">9h34m</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">7.68s</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">20.60</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\textbf{2.77}\\pm\\textbf{0.38}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m7\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\" mathsize=\"0.900em\">2.77</mtext><mo mathsize=\"0.900em\">&#177;</mo><mtext class=\"ltx_mathvariant_bold\" mathsize=\"0.900em\">0.38</mtext></mrow><annotation encoding=\"application/x-tex\">\\textbf{2.77}\\pm\\textbf{0.38}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"\\textbf{3.50}\\pm\\textbf{0.08}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m8\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\" mathsize=\"0.900em\">3.50</mtext><mo mathsize=\"0.900em\">&#177;</mo><mtext class=\"ltx_mathvariant_bold\" mathsize=\"0.900em\">0.08</mtext></mrow><annotation encoding=\"application/x-tex\">\\textbf{3.50}\\pm\\textbf{0.08}</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r ltx_border_t\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#220;-Tsang</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">32,714</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.85G</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">31h55m</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.51s</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.92</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"3.05\\pm 0.44\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m9\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">3.05</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.44</mn></mrow><annotation encoding=\"application/x-tex\">3.05\\pm 0.44</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><math alttext=\"3.39\\pm 0.04\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m10\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">3.39</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.04</mn></mrow><annotation encoding=\"application/x-tex\">3.39\\pm 0.04</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">TMDD</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Amdo</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">32,714</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.79G</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">34h22m</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.78s</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">18.54</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"2.84\\pm 0.43\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m11\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">2.84</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.43</mn></mrow><annotation encoding=\"application/x-tex\">2.84\\pm 0.43</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"3.29\\pm 0.06\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m12\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">3.29</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.06</mn></mrow><annotation encoding=\"application/x-tex\">3.29\\pm 0.06</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Kham</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">32,714</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">7.37G</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">36h19m</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.00s</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">22.52</span></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"3.29\\pm 0.28\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m13\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">3.29</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.28</mn></mrow><annotation encoding=\"application/x-tex\">3.29\\pm 0.28</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center\"><math alttext=\"3.36\\pm 0.06\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m14\" intent=\":literal\"><semantics><mrow><mn mathsize=\"0.900em\">3.36</mn><mo mathsize=\"0.900em\">&#177;</mo><mn mathsize=\"0.900em\">0.06</mn></mrow><annotation encoding=\"application/x-tex\">3.36\\pm 0.06</annotation></semantics></math></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_bb ltx_border_r\"/>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Total</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">98,142</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">22.01G</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">102h36m</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.76s</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">20.66</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"\\textbf{3.06}\\pm\\textbf{0.38}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m15\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\" mathsize=\"0.900em\">3.06</mtext><mo mathsize=\"0.900em\">&#177;</mo><mtext class=\"ltx_mathvariant_bold\" mathsize=\"0.900em\">0.38</mtext></mrow><annotation encoding=\"application/x-tex\">\\textbf{3.06}\\pm\\textbf{0.38}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><math alttext=\"\\textbf{3.35}\\pm\\textbf{0.05}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m16\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_bold\" mathsize=\"0.900em\">3.35</mtext><mo mathsize=\"0.900em\">&#177;</mo><mtext class=\"ltx_mathvariant_bold\" mathsize=\"0.900em\">0.05</mtext></mrow><annotation encoding=\"application/x-tex\">\\textbf{3.35}\\pm\\textbf{0.05}</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "dur",
            "311±020311pm",
            "abbreviations",
            "685g",
            "339±004339pm",
            "pesq",
            "329±028329pm",
            "2h55m",
            "size",
            "334±012334pm",
            "tmdd",
            "329±006329pm",
            "335±005textbf335pmtextbf005",
            "350±008textbf350pmtextbf008",
            "comparison",
            "2h05m",
            "277±038textbf277pmtextbf038",
            "305±044305pm",
            "779g",
            "737g",
            "34h22m",
            "amdo",
            "average",
            "757s",
            "statistics",
            "ütsang",
            "809g",
            "kham",
            "351s",
            "36h19m",
            "400s",
            "between",
            "768s",
            "dialectal",
            "file",
            "avg",
            "355±006355pm",
            "376s",
            "31h55m",
            "256g",
            "306±038textbf306pmtextbf038",
            "898s",
            "sisdrdb",
            "649s",
            "9h34m",
            "speech",
            "102h36m",
            "254±034254pm",
            "4h34m",
            "dnsmos",
            "total",
            "dialect",
            "338g",
            "284±043284pm",
            "336±006336pm",
            "duration",
            "215g",
            "267±060267pm",
            "361±008361pm",
            "2201g",
            "dataset",
            "baseline",
            "378s"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Following the dataset generation pipeline, a total of 122,700 sentences were synthesized, and 32,714 high-quality samples were selected to construct the TMDD. Compared with the baseline dataset from Zhuoma et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (4,763 utterances, </span>\n  <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&#8764;</mo>\n      <annotation encoding=\"application/x-tex\">\\sim</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">9.5 hours), TMDD contains 98,142 utterances spanning over 102 hours, representing a 20-fold increase in sample count and an 11-fold increase in duration (Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.2 Main Result &#8227; 3 Experiments &#8227; TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for &#220;-Tsang, Amdo and Kham Speech Dataset Generation\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Tibetan is a low-resource language with limited parallel speech corpora spanning its three major dialects (&#220;-Tsang, Amdo, and Kham), limiting progress in speech modeling. To address this issue, we propose TMD-TTS, a unified Tibetan multi-dialect text-to-speech (TTS) framework that synthesizes parallel dialectal speech from explicit dialect labels. Our method features a dialect fusion module and a Dialect-Specialized Dynamic Routing Network (DSDR-Net) to capture fine-grained acoustic and linguistic variations across dialects. Extensive objective and subjective evaluations demonstrate that TMD-TTS significantly outperforms baselines in dialectal expressiveness. We further validate the quality and utility of the synthesized speech through a challenging Speech-to-Speech Dialect Conversion (S2SDC) task.</span>\n</p>\n\n",
                "matched_terms": [
                    "amdo",
                    "dialect",
                    "ütsang",
                    "kham",
                    "speech",
                    "dialectal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Tibetan, spoken by over six million people across Tibet, neighboring Chinese provinces, and parts of South Asia, is a low-resource language with three major dialects&#8212;&#220;-Tsang, Amdo, and Kham&#8212;that differ substantially in phonology, lexicon, and syntax, often resulting in limited mutual intelligibility. To facilitate cross-dialect communication, speech-to-speech dialect conversion (S2SDC) is essential, yet its progress is constrained by the lack of large-scale, high-quality parallel data. Current resources remain scarce, with Zhuoma&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> providing only about 800 parallel samples, and most datasets relying on labor-intensive manual collection. This underscores the urgent need for a TTS-based approach to synthesize Tibetan multi-dialect data.</span>\n</p>\n\n",
                "matched_terms": [
                    "amdo",
                    "dialect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Dialect modeling poses additional challenges. Research on Tibetan multi-dialect TTS remains limited: Xu et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> introduced a model that generates a shared mel-spectrogram and employs separate WaveNet-based vocoders for Amdo and &#220;-Tsang. However, this late-stage incorporation of dialect information and the reliance on multiple heavy vocoders restrict the model&#8217;s ability to capture fine-grained dialectal distinctions.</span>\n</p>\n\n",
                "matched_terms": [
                    "dialectal",
                    "amdo",
                    "dialect",
                    "ütsang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To this end, we propose TMD-TTS, a unified Tibetan multi-dialect TTS framework that integrates dialect representations into the TTS model. Dialect embeddings are first obtained through an embedding layer and fused via a linear layer. To capture nuanced phonetic distinctions and enable precise dialectal control, we design a Dialect-Specialized Dynamic Routing Network (DSDR-Net) to replace the conventional Feedforward Network (FFN) in the Transformer architecture. DSDR-Net incorporates a conditional computation mechanism that dynamically routes information to a dialect-specific sub-network based on the input dialect ID. This allows the model to learn distinct acoustic patterns such as rhythm and intonation, offering finer-grained dialect modeling compared to shared-parameter networks.</span>\n</p>\n\n",
                "matched_terms": [
                    "dialectal",
                    "dialect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We construct and release TMDD, a large-scale Tibetan multi-dialect speech dataset synthesized via TMD-TTS. This provides a reproducible pipeline for high-quality dialect-rich data generation, validated through its application to a challenging S2SDC task.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dataset",
                    "tmdd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We develop and release a comprehensive evaluation toolkit for Tibetan dialect speech synthesis, enabling standardized assessment of audio quality and dialect similarity.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dialect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we present TMD-TTS, a unified multi-dialect TTS model built upon Matcha-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, specifically designed for generating high-quality Tibetan dialect speech datasets. Our system addresses the limitations of prior approaches&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, such as the reliance on multi-output architectures and the need to train separate vocoders for each dialect.\nThe Dialect Fusion Module integrates dialect embeddings into both the text encoder and the flow prediction network, enabling dialect-aware speech synthesis. DSDR-Net introduces a conditional computation mechanism into the feed-forward network (FFN) of the Transformer architecture to provide fine-grained control over dialectal variation and accurately capture phonetic nuances across dialects.\nFurthermore, we propose a complete data generation pipeline based on TMD-TTS to facilitate the large-scale synthesis of Tibetan multi-dialect speech datasets.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dialectal",
                    "dialect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for &#220;-Tsang, Amdo and Kham Speech Dataset Generation\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">(a) illustrates the overall architecture of TMD-TTS, which synthesizes multi-dialect Tibetan speech by integrating dialect representations into the TTS pipeline. The process begins with tokenization, converting Tibetan characters into long-format tensors, followed by a text encoder. A dialect fusion module conditions the hidden representations on dialect features. Dialect processing maps each dialect </span>\n  <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mi mathsize=\"0.900em\">d</mi>\n      <annotation encoding=\"application/x-tex\">d</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to a dialect ID </span>\n  <math alttext=\"did\\in{0,1,2}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">d</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">i</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">d</mi>\n        </mrow>\n        <mo mathsize=\"0.900em\">&#8712;</mo>\n        <mrow>\n          <mn mathsize=\"0.900em\">0</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mn mathsize=\"0.900em\">2</mn>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">did\\in{0,1,2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and generates a normalized embedding </span>\n  <math alttext=\"h_{\\mathrm{did}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">h</mi>\n        <mi mathsize=\"0.900em\">did</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">h_{\\mathrm{did}}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">:</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dialect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The duration predictor then estimates phoneme durations, guiding upsampling, while the flow-prediction network and dialect representations synthesize mel-spectrograms, which are converted into waveforms by a pretrained vocoder.</span>\n</p>\n\n",
                "matched_terms": [
                    "dialect",
                    "duration"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To construct a high-quality parallel speech dataset covering the three major Tibetan dialects, we design a data generation pipeline, as illustrated in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for &#220;-Tsang, Amdo and Kham Speech Dataset Generation\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">(c). In this pipeline, text is sequentially sampled from a curated database, and for each selected entry, dialectal speech is synthesized using the TMD-TTS model.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dataset",
                    "dialectal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To ensure fidelity, both dialectal and perceptual quality assessments are applied. Dialect assessment is conducted by retaining only samples with a Dialect Embedding Cosine Similarity (DECS, see Section&#160;</span>\n  <span class=\"ltx_ref ltx_missing_label ltx_ref_self\" style=\"font-size:90%;\">LABEL:experiments</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) greater than 0.8, thereby ensuring accurate dialectal representation. For perceptual quality, PESQ and DNSMOS (see Section&#160;</span>\n  <span class=\"ltx_ref ltx_missing_label ltx_ref_self\" style=\"font-size:90%;\">LABEL:experiments</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) are employed as evaluation metrics; utterances with </span>\n  <math alttext=\"PESQ&lt;3\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">P</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">E</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">S</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">Q</mi>\n        </mrow>\n        <mo mathsize=\"0.900em\">&lt;</mo>\n        <mn mathsize=\"0.900em\">3</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">PESQ&lt;3</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> or </span>\n  <math alttext=\"DNSMOS&lt;2.7\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">D</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">N</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">S</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">M</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">O</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">S</mi>\n        </mrow>\n        <mo mathsize=\"0.900em\">&lt;</mo>\n        <mn mathsize=\"0.900em\">2.7</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">DNSMOS&lt;2.7</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> are enhanced using MetricGAN+&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib18\" title=\"\">18</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. Finally, a manual screening step conducted by native speakers serves as the last stage to guarantee the reliability of the audio&#8211;text pairs, yielding the finalized dataset.</span>\n</p>\n\n",
                "matched_terms": [
                    "pesq",
                    "dnsmos",
                    "dialect",
                    "dataset",
                    "dialectal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Dataset</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We constructed a 179-hour multi-dialect Tibetan speech corpus, including 44h &#220;-Tsang, 45h Kham, and 90h Amdo from 1,500+ speakers. The training set contains 40k samples per dialect, with 300 samples each for validation and test.</span>\n</p>\n\n",
                "matched_terms": [
                    "amdo",
                    "dialect",
                    "ütsang",
                    "kham",
                    "speech",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Setting</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> TMD-TTS was trained for 500k steps with Adam&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and the vocoder followed BigVGAN&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with AdamW&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and exponential decay. All experiments were conducted on two RTX 4090 GPUs. The model uses a 216-character vocabulary, 128-dim dialect embeddings, and a 192-dim DSDR-Net. For comparison, we re-implemented three baseline models, SC-CNN&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, VITS2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and Matcha-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and extended them with the Tibetan multi-dialect TTS design from Xu et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which includes multiple BigVGAN vocoders and Wylie transliteration, to enable multi-dialect synthesis.</span>\n</p>\n\n",
                "matched_terms": [
                    "dialect",
                    "baseline",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Metrics</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We evaluate using both subjective and objective metrics. Subjective metrics include the naturalness Mean Opinion Score (nMOS) and the dialect Mean Opinion Classification (dMOC, measured by F1 score between the predicted classifications and the labels), both evaluated by 20 native speakers. The objective evaluation includes STOI&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, PESQ&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, SI-SDR&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, DNSMOS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, dialect classification accuracy (DCA) obtained from a pre-trained dialect classifier inspired by metrics in Durflex-EVC&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, dialect embedding cosine similarity (DECS) for measuring dialectal consistency&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib27\" title=\"\">27</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and the real-time factor (RTF) for inference efficiency.</span>\n</p>\n\n",
                "matched_terms": [
                    "pesq",
                    "dnsmos",
                    "dialect",
                    "between",
                    "dialectal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#S2.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 2.1 TMD-TTS with DSDR-Net &#8227; 2 Methods &#8227; TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for &#220;-Tsang, Amdo and Kham Speech Dataset Generation\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, TMD-TTS consistently outperforms all baselines across both objective and subjective metrics. For speech quality, it achieves the best performance in all three dialects, e.g., &#220;-Tsang (94.52% STOI, 3.03 PESQ, 17.91&#160;dB SI-SDR, 2.78 DNSMOS), Amdo (94.92% STOI, 3.13 PESQ, 21.32&#160;dB SI-SDR, 2.79 DNSMOS), and Kham (93.17% STOI, 3.05 PESQ, 21.43&#160;dB SI-SDR, 2.77 DNSMOS), clearly surpassing Matcha-TTS and VITS2. In terms of dialect similarity, our model reaches up to 88.09% DECS and 87.78% DCA, showing clear advantages over baselines. Although its inference speed (RTF </span>\n  <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&#8776;</mo>\n      <annotation encoding=\"application/x-tex\">\\approx</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">0.031&#8211;0.032) is slightly slower than VITS2 (</span>\n  <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&#8776;</mo>\n      <annotation encoding=\"application/x-tex\">\\approx</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">0.020&#8211;0.021), it still meets the requirement for real-time synthesis. Subjective evaluations further confirm these findings, with TMD-TTS achieving the highest naturalness (nMOS 3.83/3.84/3.86) and dialect consistency (dMOC 76.64%/77.01%/75.80%) across &#220;-Tsang, Amdo, and Kham. Overall, these results demonstrate the superiority of TMD-TTS in both speech quality and dialectal fidelity.</span>\n</p>\n\n",
                "matched_terms": [
                    "pesq",
                    "dnsmos",
                    "amdo",
                    "dialect",
                    "ütsang",
                    "kham",
                    "speech",
                    "dialectal"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We evaluate the contributions of dialect fusion module and the DSDR-Net in TMD-TTS (Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#S3.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 3.2 Main Result &#8227; 3 Experiments &#8227; TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for &#220;-Tsang, Amdo and Kham Speech Dataset Generation\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). Removing the dialect embedding (replacing it with zeros) reduces DCA from 80.25% to 74.15% and DECS from 78.3% to 72.8%, showing that explicit dialect information helps generate dialect-consistent speech. Replacing the DSDR-Net with a standard FFN drops DCA to 60.12% and DECS to 58.6%, highlighting its importance in modeling fine-grained dialectal variations. Removing both components further degrades performance (DCA 33.42%, DECS 32.2%), confirming their complementary contributions.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dialectal",
                    "dialect"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To further validate the effectiveness of our proposed method in modeling dialect representations, we conducted a comparative study against the VITS2 baseline. As shown in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#S3.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 3.3 Ablation Study &#8227; 3 Experiments &#8227; TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for &#220;-Tsang, Amdo and Kham Speech Dataset Generation\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we use the pre-trained dialect classifier (see Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#S3\" style=\"font-size:90%;\" title=\"3 Experiments &#8227; TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for &#220;-Tsang, Amdo and Kham Speech Dataset Generation\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) to extract softmax-based dialectal features from each synthesized utterance, averaged over &#220;-Tsang, Amdo, and Kham test samples. Compared to VITS2, our method produces speech with more salient target-dialect characteristics, demonstrating improved dialectal consistency and generalization.</span>\n</p>\n\n",
                "matched_terms": [
                    "amdo",
                    "dialect",
                    "ütsang",
                    "kham",
                    "speech",
                    "dialectal",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Additionally, dialect embeddings were extracted using the pre-trained model for SECS computation and visualized via t-SNE (Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#S3.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 3.3 Ablation Study &#8227; 3 Experiments &#8227; TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for &#220;-Tsang, Amdo and Kham Speech Dataset Generation\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">). Each point represents a dialect embedding, with cluster centers derived from dialect embeddings. Under VITS2, embeddings are less compact and exhibit notable overlap&#8212;especially between &#220;-Tsang and Amdo&#8212;indicating weaker dialect distinction. In contrast, our method achieves better cluster separation and closer alignment with the cluster centers, highlighting the effectiveness of the dialect fusion module and DSDR-Net.</span>\n</p>\n\n",
                "matched_terms": [
                    "dialect",
                    "ütsang",
                    "between"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Objective evaluation using PESQ, SI-SDR, and DNSMOS shows that TMDD maintains high audio quality, with average PESQ </span>\n  <math alttext=\"3.06\\pm 0.38\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">3.06</mn>\n        <mo mathsize=\"0.900em\">&#177;</mo>\n        <mn mathsize=\"0.900em\">0.38</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">3.06\\pm 0.38</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (baseline: </span>\n  <math alttext=\"2.77\\pm 0.38\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">2.77</mn>\n        <mo mathsize=\"0.900em\">&#177;</mo>\n        <mn mathsize=\"0.900em\">0.38</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">2.77\\pm 0.38</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) and SI-SDR </span>\n  <math alttext=\"20.66\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">20.66</mn>\n      <annotation encoding=\"application/x-tex\">20.66</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dB (baseline: </span>\n  <math alttext=\"20.60\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">20.60</mn>\n      <annotation encoding=\"application/x-tex\">20.60</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dB). DNSMOS results also indicate consistent perceptual quality suitable for downstream tasks.</span>\n</p>\n\n",
                "matched_terms": [
                    "dnsmos",
                    "pesq",
                    "average",
                    "tmdd",
                    "baseline"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To validate its utility, we applied TMDD to a S2SDC task using DurFlex-EVC&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with BigVGAN&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> vocoding. As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#S3.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 3.5 Generation and Evaluation of TMDD &#8227; 3 Experiments &#8227; TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for &#220;-Tsang, Amdo and Kham Speech Dataset Generation\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, TMDD consistently outperforms the baseline, achieving a peak MOS of 3.63 at 22 kHz, demonstrating superior naturalness and overall quality for multi-dialect speech synthesis.</span>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "baseline",
                    "tmdd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we propose TMD-TTS, a unified Tibetan multi-dialect TTS model that incorporates dialect representations for multi-dialect Tibetan speech synthesis. We design a dialect fusion module and introduce DSDR-Net to better control dialectal variations. Leveraging this model, we construct and release a large-scale, parallel Tibetan multi-dialect speech dataset, TMDD, facilitating broader research in Tibetan speech synthesis and conversion. To assess the quality and utility of the generated speech, we explore the S2SDC task and conduct subjective evaluations, including naturalness and dialectal consistency. Furthermore, we develop a comprehensive evaluation toolkit specifically tailored for Tibetan speech generation tasks.</span>\n</p>\n\n",
                "matched_terms": [
                    "dialect",
                    "tmdd",
                    "speech",
                    "dataset",
                    "dialectal"
                ]
            }
        ]
    },
    "S3.T4": {
        "caption": "Table 4: \nSubjective result of DurFlex-EVC on baseline dataset and TMDD.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">MOS</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">baseline</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">DurFlex-EVC + BigVGAN 16K</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.07</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">baseline</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">DurFlex-EVC + BigVGAN 22K</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.54</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">TMDD</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">DurFlex-EVC + BigVGAN 16K</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.23</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">TMDD</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">DurFlex-EVC + BigVGAN 22K</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.63</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "mos",
            "result",
            "model",
            "bigvgan",
            "durflexevc",
            "16k",
            "22k",
            "tmdd",
            "dataset",
            "subjective",
            "baseline"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To validate its utility, we applied TMDD to a S2SDC task using DurFlex-EVC&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with BigVGAN&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> vocoding. As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#S3.T4\" style=\"font-size:90%;\" title=\"Table 4 &#8227; 3.5 Generation and Evaluation of TMDD &#8227; 3 Experiments &#8227; TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for &#220;-Tsang, Amdo and Kham Speech Dataset Generation\">\n    <span class=\"ltx_text ltx_ref_tag\">4</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, TMDD consistently outperforms the baseline, achieving a peak MOS of 3.63 at 22 kHz, demonstrating superior naturalness and overall quality for multi-dialect speech synthesis.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We construct and release TMDD, a large-scale Tibetan multi-dialect speech dataset synthesized via TMD-TTS. This provides a reproducible pipeline for high-quality dialect-rich data generation, validated through its application to a challenging S2SDC task.</span>\n</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "tmdd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">To construct a high-quality parallel speech dataset covering the three major Tibetan dialects, we design a data generation pipeline, as illustrated in Fig.&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#S1.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 1 Introduction &#8227; TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for &#220;-Tsang, Amdo and Kham Speech Dataset Generation\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">(c). In this pipeline, text is sequentially sampled from a curated database, and for each selected entry, dialectal speech is synthesized using the TMD-TTS model.</span>\n</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Setting</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> TMD-TTS was trained for 500k steps with Adam&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib19\" title=\"\">19</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and the vocoder followed BigVGAN&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib12\" title=\"\">12</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with AdamW&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib20\" title=\"\">20</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and exponential decay. All experiments were conducted on two RTX 4090 GPUs. The model uses a 216-character vocabulary, 128-dim dialect embeddings, and a 192-dim DSDR-Net. For comparison, we re-implemented three baseline models, SC-CNN&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib21\" title=\"\">21</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, VITS2&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and Matcha-TTS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and extended them with the Tibetan multi-dialect TTS design from Xu et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib17\" title=\"\">17</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which includes multiple BigVGAN vocoders and Wylie transliteration, to enable multi-dialect synthesis.</span>\n</p>\n\n",
                "matched_terms": [
                    "baseline",
                    "model",
                    "bigvgan"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Metrics</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> We evaluate using both subjective and objective metrics. Subjective metrics include the naturalness Mean Opinion Score (nMOS) and the dialect Mean Opinion Classification (dMOC, measured by F1 score between the predicted classifications and the labels), both evaluated by 20 native speakers. The objective evaluation includes STOI&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib22\" title=\"\">22</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, PESQ&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib23\" title=\"\">23</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, SI-SDR&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib24\" title=\"\">24</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, DNSMOS&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib25\" title=\"\">25</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, dialect classification accuracy (DCA) obtained from a pre-trained dialect classifier inspired by metrics in Durflex-EVC&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib26\" title=\"\">26</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, dialect embedding cosine similarity (DECS) for measuring dialectal consistency&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib27\" title=\"\">27</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and the real-time factor (RTF) for inference efficiency.</span>\n</p>\n\n",
                "matched_terms": [
                    "subjective",
                    "durflexevc"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">As shown in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#S2.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 2.1 TMD-TTS with DSDR-Net &#8227; 2 Methods &#8227; TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for &#220;-Tsang, Amdo and Kham Speech Dataset Generation\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, TMD-TTS consistently outperforms all baselines across both objective and subjective metrics. For speech quality, it achieves the best performance in all three dialects, e.g., &#220;-Tsang (94.52% STOI, 3.03 PESQ, 17.91&#160;dB SI-SDR, 2.78 DNSMOS), Amdo (94.92% STOI, 3.13 PESQ, 21.32&#160;dB SI-SDR, 2.79 DNSMOS), and Kham (93.17% STOI, 3.05 PESQ, 21.43&#160;dB SI-SDR, 2.77 DNSMOS), clearly surpassing Matcha-TTS and VITS2. In terms of dialect similarity, our model reaches up to 88.09% DECS and 87.78% DCA, showing clear advantages over baselines. Although its inference speed (RTF </span>\n  <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&#8776;</mo>\n      <annotation encoding=\"application/x-tex\">\\approx</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">0.031&#8211;0.032) is slightly slower than VITS2 (</span>\n  <math alttext=\"\\approx\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&#8776;</mo>\n      <annotation encoding=\"application/x-tex\">\\approx</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">0.020&#8211;0.021), it still meets the requirement for real-time synthesis. Subjective evaluations further confirm these findings, with TMD-TTS achieving the highest naturalness (nMOS 3.83/3.84/3.86) and dialect consistency (dMOC 76.64%/77.01%/75.80%) across &#220;-Tsang, Amdo, and Kham. Overall, these results demonstrate the superiority of TMD-TTS in both speech quality and dialectal fidelity.</span>\n</p>\n\n",
                "matched_terms": [
                    "subjective",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Following the dataset generation pipeline, a total of 122,700 sentences were synthesized, and 32,714 high-quality samples were selected to construct the TMDD. Compared with the baseline dataset from Zhuoma et al.&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (4,763 utterances, </span>\n  <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mo mathsize=\"0.900em\">&#8764;</mo>\n      <annotation encoding=\"application/x-tex\">\\sim</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">9.5 hours), TMDD contains 98,142 utterances spanning over 102 hours, representing a 20-fold increase in sample count and an 11-fold increase in duration (Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18060v1#S3.T3\" style=\"font-size:90%;\" title=\"Table 3 &#8227; 3.2 Main Result &#8227; 3 Experiments &#8227; TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for &#220;-Tsang, Amdo and Kham Speech Dataset Generation\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">).</span>\n</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "baseline",
                    "tmdd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Objective evaluation using PESQ, SI-SDR, and DNSMOS shows that TMDD maintains high audio quality, with average PESQ </span>\n  <math alttext=\"3.06\\pm 0.38\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">3.06</mn>\n        <mo mathsize=\"0.900em\">&#177;</mo>\n        <mn mathsize=\"0.900em\">0.38</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">3.06\\pm 0.38</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> (baseline: </span>\n  <math alttext=\"2.77\\pm 0.38\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">2.77</mn>\n        <mo mathsize=\"0.900em\">&#177;</mo>\n        <mn mathsize=\"0.900em\">0.38</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">2.77\\pm 0.38</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) and SI-SDR </span>\n  <math alttext=\"20.66\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m3\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">20.66</mn>\n      <annotation encoding=\"application/x-tex\">20.66</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dB (baseline: </span>\n  <math alttext=\"20.60\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS5.p2.m4\" intent=\":literal\">\n    <semantics>\n      <mn mathsize=\"0.900em\">20.60</mn>\n      <annotation encoding=\"application/x-tex\">20.60</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> dB). DNSMOS results also indicate consistent perceptual quality suitable for downstream tasks.</span>\n</p>\n\n",
                "matched_terms": [
                    "baseline",
                    "tmdd"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this work, we propose TMD-TTS, a unified Tibetan multi-dialect TTS model that incorporates dialect representations for multi-dialect Tibetan speech synthesis. We design a dialect fusion module and introduce DSDR-Net to better control dialectal variations. Leveraging this model, we construct and release a large-scale, parallel Tibetan multi-dialect speech dataset, TMDD, facilitating broader research in Tibetan speech synthesis and conversion. To assess the quality and utility of the generated speech, we explore the S2SDC task and conduct subjective evaluations, including naturalness and dialectal consistency. Furthermore, we develop a comprehensive evaluation toolkit specifically tailored for Tibetan speech generation tasks.</span>\n</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "subjective",
                    "model",
                    "tmdd"
                ]
            }
        ]
    }
}