{
    "S3.T1": {
        "caption": "Table 1: Objective comparison of our approach with other emotion-controllable TTS models in terms of emotion similarity, prosody similarity, WER, and UTMOS on ESD dataset.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">TTS Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Emo_SIM<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Prosody_SIM<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">UTMOS<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">FG-TT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib1\" title=\"\">1</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">93.91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">9.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.81</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">PromptTTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib2\" title=\"\">2</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">95.70</td>\n<td class=\"ltx_td ltx_align_center\">3.41</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">3.25</span></td>\n<td class=\"ltx_td ltx_align_center\">4.33</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Emospeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib3\" title=\"\">3</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">96.35</td>\n<td class=\"ltx_td ltx_align_center\">3.39</td>\n<td class=\"ltx_td ltx_align_center\">7.13</td>\n<td class=\"ltx_td ltx_align_center\">4.24</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">EmoDiff&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib4\" title=\"\">4</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">96.62</td>\n<td class=\"ltx_td ltx_align_center\">3.55</td>\n<td class=\"ltx_td ltx_align_center\">5.62</td>\n<td class=\"ltx_td ltx_align_center\">4.35</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib5\" title=\"\">5</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">97.07</td>\n<td class=\"ltx_td ltx_align_center\">3.64</td>\n<td class=\"ltx_td ltx_align_center\">4.32</td>\n<td class=\"ltx_td ltx_align_center\">4.41</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib6\" title=\"\">6</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">98.47</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.78</span></td>\n<td class=\"ltx_td ltx_align_center\">3.83</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">4.43</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">EmoVoice&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib7\" title=\"\">7</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">98.59</span></td>\n<td class=\"ltx_td ltx_align_center\">3.67</td>\n<td class=\"ltx_td ltx_align_center\">4.16</td>\n<td class=\"ltx_td ltx_align_center\">4.39</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Ours</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">99.15</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">3.89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">4.47</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "utmos",
            "wer↓downarrow",
            "wer",
            "emospeech",
            "emodiff",
            "comparison",
            "our",
            "similarity",
            "emotioncontrollable",
            "tts",
            "prosodysim↑uparrow",
            "objective",
            "terms",
            "prompttts",
            "emosim↑uparrow",
            "cosyvoice",
            "prosody",
            "esd",
            "cosyvoice2",
            "fgtt",
            "emotion",
            "ours",
            "model",
            "approach",
            "other",
            "emovoice",
            "dataset",
            "utmos↑uparrow"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To validate the effectiveness of our proposed emotionally preference-aligned method for TTS, we compare it with seven recent emotion-controllable baselines. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S3.T1\" title=\"Table 1 &#8227; 3.1 Datasets and Experimental Setup &#8227; 3 Experiments &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our method achieves superior performance across emotion similarity, prosody similarity, intelligibility (WER), and perceptual quality (UTMOS), with a notable 2.07% gain over CosyVoice in Emo_SIM. A similar trend is observed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S3.T2\" title=\"Table 2 &#8227; 3.1 Datasets and Experimental Setup &#8227; 3 Experiments &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, where our model consistently outperforms baselines in naturalness (MOS), emotional expressiveness (Emo_MOS), emotion consistency (MOS_EC), and emotion classification accuracy (Recall). These results demonstrate the effectiveness of our approach in generating emotionally aligned speech with enhanced coherence, while preserving natural prosody and speech quality. Demo page is available&#160;<a class=\"ltx_ref ltx_href\" href=\"https://jiachengqaq.github.io/emo-tts-demo/\" title=\"\">.</a></p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Emotional text-to-speech seeks to convey affect while preserving intelligibility and prosody, yet existing methods rely on coarse labels or proxy classifiers and receive only utterance-level feedback. We introduce Emotion-Aware Stepwise Preference Optimization (EASPO), a post-training framework that aligns diffusion TTS with fine-grained emotional preferences at intermediate denoising steps. Central to our approach is EASPM, a time-conditioned model that scores noisy intermediate speech states and enables automatic preference pair construction. EASPO optimizes generation to match these stepwise preferences, enabling controllable emotional shaping. Experiments show superior performance over existing methods in both expressiveness and naturalness.</p>\n\n",
                "matched_terms": [
                    "prosody",
                    "tts",
                    "model",
                    "approach",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early emotional TTS systems fused local style tokens with phoneme content via cross-attention for fine-grained control&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib1\" title=\"\">1</a>]</cite>, guided synthesis with semantic prompts&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib2\" title=\"\">2</a>]</cite>, and used variational decoders to model nuanced emotion&#8211;prosody relations&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib3\" title=\"\">3</a>]</cite>. Diffusion-based methods improved fidelity and control through emotion embeddings and noise-conditioned prosody&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib4\" title=\"\">4</a>]</cite>. Recent work adopts supervised speech tokens for semantic grounding and chunk-aware flow-based decoding&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib6\" title=\"\">6</a>]</cite>, and introduces parallel phoneme&#8211;audio branches in LLM-based generation to support fine-grained, freestyle emotional control&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib7\" title=\"\">7</a>]</cite>. Across these variants, two important gaps remain: supervision still targets proxy labels over preference-driven prompts, and feedback remains temporally sparse, limiting constraints on prosody&#8211;emotion dynamics.</p>\n\n",
                "matched_terms": [
                    "model",
                    "prosody",
                    "tts",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Direct Preference Optimization (DPO)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib8\" title=\"\">8</a>]</cite> aligns generative models to human choices via paired comparisons against a frozen reference policy. In diffusion settings<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib9\" title=\"\">9</a>]</cite>, a common practice is to assign preference at the final step and propagate it to intermediate latents, then optimize a DPO-style log-likelihood ratio at each step. This removes the need for an explicit reward model and is effective for global preferences, but offers limited guidance for temporally evolving signals.\nDPO has also been used to align large language model&#8211;based TTS. EmoDPO&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib10\" title=\"\">10</a>]</cite>, for instance, pairs utterances with identical text and marks the target emotion as preferred, achieving alignment without explicit reward modeling. However, attaching preference only at the endpoint yields sparse supervision for gradually varying cues, the assumption that all intermediate states on a preferred path are themselves preferred is often invalid, and curating domain-appropriate preference pairs is expensive. This leads to our central research question: <span class=\"ltx_text ltx_font_italic\">How can we provide dense, fine-grained supervision for emotionally expressive TTS generation, without relying on endpoint preferences or categorical labels?</span></p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "tts",
                    "model",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better align diffusion-based TTS systems with fine-grained emotional preferences, we propose <span class=\"ltx_text ltx_font_italic\">Emotion-Aware Stepwise Preference Optimization</span> (EASPO). At each denoising step starting from a latent representation <math alttext=\"x_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">x_{t}</annotation></semantics></math>, the model samples a small candidate set of <math alttext=\"x_{t-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">x_{t-1}</annotation></semantics></math> mel-spectrograms. An <span class=\"ltx_text ltx_font_italic\">Emotion-aware Stepwise Preference Model</span> (EASPM) scores their emotional expressiveness and selects a win&#8211;lose pair that differs subtly in prosody while preserving linguistic content. One candidate is then randomly chosen to continue generation. Since these candidates originate from the same latent and differ by only a single denoising step, their variations are localized and emotion-focused. EASPM captures these nuanced differences and steers the model toward producing more emotionally consistent speech.</p>\n\n",
                "matched_terms": [
                    "model",
                    "prosody",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contributions are: 1) We propose EASPO, a stepwise alignment framework that reformulates preference optimization as a local, time-conditioned task, replacing the flawed assumption that all intermediate states on a preferred trajectory are equally valid. By aligning win/lose candidates at each denoising step from a shared latent, it enables stepwise-controllable emotion shaping throughout the generation process. 2) We introduce EASPM, a time-aware reward model that directly scores emotional expressiveness and prosody on noisy intermediate states, enabling dense, temporally grounded preference learning and on-the-fly scoring.</p>\n\n",
                "matched_terms": [
                    "model",
                    "emotion",
                    "prosody",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a reinforcement learning framework for emotional TTS that fine-tunes diffusion models via stepwise preference supervision (Fig.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Starting from a pretrained Grad-TTS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib11\" title=\"\">11</a>]</cite>, our method introduces dense emotion-aligned rewards at each denoising step. At every latent state, multiple mel-spectrograms are sampled and ranked by a frozen emotion preference model(Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S2.SS1\" title=\"2.1 Emotion-Aware Stepwise Preference Model &#8227; 2 Method &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">2.1</span></a>). A preference pair is selected, and the model is optimized to favor the emotionally preferred sample by minimizing the gap between its advantage and the log-likelihood ratio(Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S2.SS2\" title=\"2.2 Objective Function of EASPO &#8227; 2 Method &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>). Our EASPO objective extends DPO with stepwise preference signals, enabling fine-grained emotional control during generation.</p>\n\n",
                "matched_terms": [
                    "our",
                    "models",
                    "tts",
                    "model",
                    "objective",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training.</span>\nEASPM is adapted from CLEP to handle noisy intermediate representations. Given a win&#8211;lose pair <math alttext=\"(x_{0}^{w},x_{0}^{l})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mn>0</mn><mi>w</mi></msubsup><mo>,</mo><msubsup><mi>x</mi><mn>0</mn><mi>l</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{0}^{w},x_{0}^{l})</annotation></semantics></math>, we sample a timestep <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m2\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> and apply the same forward diffusion to produce <math alttext=\"(x_{t}^{w},x_{t}^{l})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mi>t</mi><mi>w</mi></msubsup><mo>,</mo><msubsup><mi>x</mi><mi>t</mi><mi>l</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{t}^{w},x_{t}^{l})</annotation></semantics></math>. This tuple <math alttext=\"(x_{t}^{w},x_{t}^{l},t,c)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mi>t</mi><mi>w</mi></msubsup><mo>,</mo><msubsup><mi>x</mi><mi>t</mi><mi>l</mi></msubsup><mo>,</mo><mi>t</mi><mo>,</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{t}^{w},x_{t}^{l},t,c)</annotation></semantics></math> is used to optimize <math alttext=\"\\mathcal{L}_{\\text{pref}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>pref</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{pref}}</annotation></semantics></math>, encouraging the model to recover the correct preference at step <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m6\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>. A time-aware normalization layer is added to CLEP&#8217;s audio branch for timestep conditioning. To reduce mismatch with CLEP&#8217;s pretraining domain, we optionally estimate a pseudo-clean <math alttext=\"\\hat{x}_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m7\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\hat{x}_{0}</annotation></semantics></math> from <math alttext=\"x_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m8\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">x_{t}</annotation></semantics></math> via deterministic inversion followed by&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib14\" title=\"\">14</a>]</cite> . After training, EASPM is frozen and used solely as a stepwise scorer for the RL objective.</p>\n\n",
                "matched_terms": [
                    "objective",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tune EASPM on the English MSP-Podcast corpus&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib16\" title=\"\">16</a>]</cite> (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>55k utterances, <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>1,200 speakers), using textual prompts from emotion labels and acoustic descriptors (e.g., pitch, loudness, jitter, shimmer). Preference pairs are created by labeling target emotions (e.g., happy) as preferred over distractors (e.g., neutral) with the same text. For reinforcement learning, we use the English split of ESD (5 emotions &#215; 10 speakers, 350 utterances/emotion), with an 8:1:1 train/val/test split per speaker&#8211;emotion. We evaluate against seven emotion-controllable TTS baselines: FG&#8209;TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib1\" title=\"\">1</a>]</cite>, PromptTTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib2\" title=\"\">2</a>]</cite>, Emospeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib3\" title=\"\">3</a>]</cite>, EmoDiff&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib4\" title=\"\">4</a>]</cite>, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib5\" title=\"\">5</a>]</cite>, CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib6\" title=\"\">6</a>]</cite>, and EmoVoice&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib7\" title=\"\">7</a>]</cite>, using authors&#8217; code and checkpoints with default inference settings.</p>\n\n",
                "matched_terms": [
                    "emotioncontrollable",
                    "tts",
                    "emospeech",
                    "esd",
                    "cosyvoice2",
                    "emodiff",
                    "emovoice",
                    "prompttts",
                    "cosyvoice",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We initialize EASPM from CLEP. Audio is resampled to 16&#8201;kHz and cropped/padded to 5&#8201;s. The text encoder is frozen, while the audio encoder and projection head are trained using Adam (batch size 64, 80 epochs), with learning rates <math alttext=\"1\\!\\times\\!10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.052em\" rspace=\"0.052em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\!\\times\\!10^{-5}</annotation></semantics></math> and <math alttext=\"1\\!\\times\\!10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.052em\" rspace=\"0.052em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\!\\times\\!10^{-3}</annotation></semantics></math>, respectively. Preference supervision is derived from emotion-labeled recordings, marking the target emotion as preferred and another as dis-preferred. To make scoring step-aware, both waveforms in a pair are perturbed by identical diffusion noise at a sampled denoising step. Our base TTS model is Grad-TTS with 80-dim mel-spectrograms. We freeze the encoder and duration predictor and fine-tune only the decoder (score network), initialized from Grad-TTS pretraining settings (Adam, <math alttext=\"1\\!\\times\\!10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.052em\" rspace=\"0.052em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\!\\times\\!10^{-4}</annotation></semantics></math> LR, batch size 16, random 2&#8201;s mel segments). During EASPO, we apply step shuffling and candidate pooling. A denoising step <math alttext=\"\\tau\\sim\\mathcal{U}(1,T{-}\\kappa)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>&#8764;</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119984;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>,</mo><mrow><mi>T</mi><mo>&#8722;</mo><mi>&#954;</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau\\sim\\mathcal{U}(1,T{-}\\kappa)</annotation></semantics></math> is chosen (skipping noisy early steps), <math alttext=\"k{=}4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">k{=}4</annotation></semantics></math> candidates are sampled from <math alttext=\"p_{\\theta}(\\cdot\\mid x_{\\tau},c)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S3.SS1.p2.m6\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><msub><mi>x</mi><mi>&#964;</mi></msub><mo>,</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p_{\\theta}(\\cdot\\mid x_{\\tau},c)</annotation></semantics></math>, ranked by EASPM, and a win&#8211;lose pair is used to minimize the stepwise difference&#8211;matching loss between policy log-ratio difference and reward difference. Unless specified, we set <math alttext=\"\\kappa{=}0.25T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m7\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>=</mo><mrow><mn>0.25</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\kappa{=}0.25T</annotation></semantics></math>, batch size 32, and decoder learning rate <math alttext=\"1\\!\\times\\!10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m8\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.052em\" rspace=\"0.052em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\!\\times\\!10^{-5}</annotation></semantics></math>. Waveforms are synthesized using a pretrained HiFi-GAN vocoder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib17\" title=\"\">17</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "model",
                    "tts",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Objective metrics.</span>\n<span class=\"ltx_text ltx_font_italic\">Emo_SIM</span> quantifies emotional alignment as the average cosine similarity between emotion2vec-base embeddings of generated and reference utterances. <span class=\"ltx_text ltx_font_italic\">Prosod_SIM</span> is computed via AutoPCP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib18\" title=\"\">18</a>]</cite>, which compares utterance-level prosody (rhythm, stress, intonation). <span class=\"ltx_text ltx_font_italic\">WER</span> (word-error-rate) is calculated using Whisper Large-v3 transcripts.\nUTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib19\" title=\"\">19</a>]</cite> is employed to evaluate speech naturalness and perceptual quality.</p>\n\n",
                "matched_terms": [
                    "utmos",
                    "similarity",
                    "prosody",
                    "wer",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Comparison with other diffusion-based RL alignment methods.</span>\nWe compare EASPO to prior diffusion-based RL methods (DDPO, D3PO, Diff-DPO) in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S3.T8\" title=\"Table 8 &#8227; 3.1 Datasets and Experimental Setup &#8227; 3 Experiments &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, observing consistent metric gains that highlight EASPO&#8217;s strength in aligning fine-grained emotional and prosodic preferences.</p>\n\n",
                "matched_terms": [
                    "other",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present EASPO, a diffusion-based speech synthesis framework that introduces stepwise preference optimization for fine-grained emotional alignment. By leveraging an emotion-aware scoring model to compare candidate samples at each denoising step, EASPO progressively guides generation toward emotionally expressive and prosodically natural speech. This step-conditioned training strategy enables the model to capture subtle affective cues through contrastive supervision. Extensive experimental demonstrate the effectiveness across both objective and subjective evaluations.</p>\n\n",
                "matched_terms": [
                    "objective",
                    "model"
                ]
            }
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Subjective evaluation of naturalness, emotional expressiveness , emotion consistency, and emotion recall, evaluated by human raters.",
        "body": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">TTS Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">MOS<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Emo_MOS<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">MOS_EC<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Recall<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">PromptTTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib2\" title=\"\">2</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">74.12</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">EmoDiff&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib4\" title=\"\">4</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">3.28</td>\n<td class=\"ltx_td ltx_align_center\">3.36</td>\n<td class=\"ltx_td ltx_align_center\">3.40</td>\n<td class=\"ltx_td ltx_align_center\">78.59</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib6\" title=\"\">6</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.63</span></td>\n<td class=\"ltx_td ltx_align_center\">3.71</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.83</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">82.10</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">EmoVoice&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib7\" title=\"\">7</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\">3.56</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">3.79</span></td>\n<td class=\"ltx_td ltx_align_center\">3.64</td>\n<td class=\"ltx_td ltx_align_center\">80.36</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Ours</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">3.94</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">4.28</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">4.04</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">85.84</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "recall↑uparrow",
            "evaluated",
            "consistency",
            "emomos↑uparrow",
            "naturalness",
            "evaluation",
            "emodiff",
            "mosec↑uparrow",
            "tts",
            "emotional",
            "prompttts",
            "recall",
            "cosyvoice2",
            "raters",
            "emotion",
            "ours",
            "model",
            "human",
            "emovoice",
            "mos↑uparrow",
            "expressiveness",
            "subjective"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">To validate the effectiveness of our proposed emotionally preference-aligned method for TTS, we compare it with seven recent emotion-controllable baselines. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S3.T1\" title=\"Table 1 &#8227; 3.1 Datasets and Experimental Setup &#8227; 3 Experiments &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our method achieves superior performance across emotion similarity, prosody similarity, intelligibility (WER), and perceptual quality (UTMOS), with a notable 2.07% gain over CosyVoice in Emo_SIM. A similar trend is observed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S3.T2\" title=\"Table 2 &#8227; 3.1 Datasets and Experimental Setup &#8227; 3 Experiments &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, where our model consistently outperforms baselines in naturalness (MOS), emotional expressiveness (Emo_MOS), emotion consistency (MOS_EC), and emotion classification accuracy (Recall). These results demonstrate the effectiveness of our approach in generating emotionally aligned speech with enhanced coherence, while preserving natural prosody and speech quality. Demo page is available&#160;<a class=\"ltx_ref ltx_href\" href=\"https://jiachengqaq.github.io/emo-tts-demo/\" title=\"\">.</a></p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Emotional text-to-speech seeks to convey affect while preserving intelligibility and prosody, yet existing methods rely on coarse labels or proxy classifiers and receive only utterance-level feedback. We introduce Emotion-Aware Stepwise Preference Optimization (EASPO), a post-training framework that aligns diffusion TTS with fine-grained emotional preferences at intermediate denoising steps. Central to our approach is EASPM, a time-conditioned model that scores noisy intermediate speech states and enables automatic preference pair construction. EASPO optimizes generation to match these stepwise preferences, enabling controllable emotional shaping. Experiments show superior performance over existing methods in both expressiveness and naturalness.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "model",
                    "emotional",
                    "naturalness",
                    "expressiveness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;</span></span>\nSpeech Synthesis, Diffusion Model, Emotional Text-To-Speech (Emo-TTS).</p>\n\n",
                "matched_terms": [
                    "model",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Emotional text-to-speech (TTS) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib7\" title=\"\">7</a>]</cite> aims to generate speech that remains intelligible while conveying affect and prosodic nuance. Effective emotional control supports conversational agents, accessibility, and content creation. Yet fine-grained control without sacrificing naturalness remains challenging, as emotional cues unfold over time and interact with linguistic content and speaker variation.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "emotional",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early emotional TTS systems fused local style tokens with phoneme content via cross-attention for fine-grained control&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib1\" title=\"\">1</a>]</cite>, guided synthesis with semantic prompts&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib2\" title=\"\">2</a>]</cite>, and used variational decoders to model nuanced emotion&#8211;prosody relations&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib3\" title=\"\">3</a>]</cite>. Diffusion-based methods improved fidelity and control through emotion embeddings and noise-conditioned prosody&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib4\" title=\"\">4</a>]</cite>. Recent work adopts supervised speech tokens for semantic grounding and chunk-aware flow-based decoding&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib6\" title=\"\">6</a>]</cite>, and introduces parallel phoneme&#8211;audio branches in LLM-based generation to support fine-grained, freestyle emotional control&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib7\" title=\"\">7</a>]</cite>. Across these variants, two important gaps remain: supervision still targets proxy labels over preference-driven prompts, and feedback remains temporally sparse, limiting constraints on prosody&#8211;emotion dynamics.</p>\n\n",
                "matched_terms": [
                    "model",
                    "emotional",
                    "tts",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Direct Preference Optimization (DPO)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib8\" title=\"\">8</a>]</cite> aligns generative models to human choices via paired comparisons against a frozen reference policy. In diffusion settings<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib9\" title=\"\">9</a>]</cite>, a common practice is to assign preference at the final step and propagate it to intermediate latents, then optimize a DPO-style log-likelihood ratio at each step. This removes the need for an explicit reward model and is effective for global preferences, but offers limited guidance for temporally evolving signals.\nDPO has also been used to align large language model&#8211;based TTS. EmoDPO&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib10\" title=\"\">10</a>]</cite>, for instance, pairs utterances with identical text and marks the target emotion as preferred, achieving alignment without explicit reward modeling. However, attaching preference only at the endpoint yields sparse supervision for gradually varying cues, the assumption that all intermediate states on a preferred path are themselves preferred is often invalid, and curating domain-appropriate preference pairs is expensive. This leads to our central research question: <span class=\"ltx_text ltx_font_italic\">How can we provide dense, fine-grained supervision for emotionally expressive TTS generation, without relying on endpoint preferences or categorical labels?</span></p>\n\n",
                "matched_terms": [
                    "human",
                    "model",
                    "tts",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better align diffusion-based TTS systems with fine-grained emotional preferences, we propose <span class=\"ltx_text ltx_font_italic\">Emotion-Aware Stepwise Preference Optimization</span> (EASPO). At each denoising step starting from a latent representation <math alttext=\"x_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">x_{t}</annotation></semantics></math>, the model samples a small candidate set of <math alttext=\"x_{t-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">x_{t-1}</annotation></semantics></math> mel-spectrograms. An <span class=\"ltx_text ltx_font_italic\">Emotion-aware Stepwise Preference Model</span> (EASPM) scores their emotional expressiveness and selects a win&#8211;lose pair that differs subtly in prosody while preserving linguistic content. One candidate is then randomly chosen to continue generation. Since these candidates originate from the same latent and differ by only a single denoising step, their variations are localized and emotion-focused. EASPM captures these nuanced differences and steers the model toward producing more emotionally consistent speech.</p>\n\n",
                "matched_terms": [
                    "model",
                    "expressiveness",
                    "emotional",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contributions are: 1) We propose EASPO, a stepwise alignment framework that reformulates preference optimization as a local, time-conditioned task, replacing the flawed assumption that all intermediate states on a preferred trajectory are equally valid. By aligning win/lose candidates at each denoising step from a shared latent, it enables stepwise-controllable emotion shaping throughout the generation process. 2) We introduce EASPM, a time-aware reward model that directly scores emotional expressiveness and prosody on noisy intermediate states, enabling dense, temporally grounded preference learning and on-the-fly scoring.</p>\n\n",
                "matched_terms": [
                    "model",
                    "expressiveness",
                    "emotional",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a reinforcement learning framework for emotional TTS that fine-tunes diffusion models via stepwise preference supervision (Fig.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Starting from a pretrained Grad-TTS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib11\" title=\"\">11</a>]</cite>, our method introduces dense emotion-aligned rewards at each denoising step. At every latent state, multiple mel-spectrograms are sampled and ranked by a frozen emotion preference model(Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S2.SS1\" title=\"2.1 Emotion-Aware Stepwise Preference Model &#8227; 2 Method &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">2.1</span></a>). A preference pair is selected, and the model is optimized to favor the emotionally preferred sample by minimizing the gap between its advantage and the log-likelihood ratio(Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S2.SS2\" title=\"2.2 Objective Function of EASPO &#8227; 2 Method &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>). Our EASPO objective extends DPO with stepwise preference signals, enabling fine-grained emotional control during generation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "emotional",
                    "tts",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We fine-tune EASPM on the English MSP-Podcast corpus&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib16\" title=\"\">16</a>]</cite> (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>55k utterances, <math alttext=\"&gt;\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mo>&gt;</mo><annotation encoding=\"application/x-tex\">&gt;</annotation></semantics></math>1,200 speakers), using textual prompts from emotion labels and acoustic descriptors (e.g., pitch, loudness, jitter, shimmer). Preference pairs are created by labeling target emotions (e.g., happy) as preferred over distractors (e.g., neutral) with the same text. For reinforcement learning, we use the English split of ESD (5 emotions &#215; 10 speakers, 350 utterances/emotion), with an 8:1:1 train/val/test split per speaker&#8211;emotion. We evaluate against seven emotion-controllable TTS baselines: FG&#8209;TTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib1\" title=\"\">1</a>]</cite>, PromptTTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib2\" title=\"\">2</a>]</cite>, Emospeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib3\" title=\"\">3</a>]</cite>, EmoDiff&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib4\" title=\"\">4</a>]</cite>, CosyVoice&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib5\" title=\"\">5</a>]</cite>, CosyVoice2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib6\" title=\"\">6</a>]</cite>, and EmoVoice&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib7\" title=\"\">7</a>]</cite>, using authors&#8217; code and checkpoints with default inference settings.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "cosyvoice2",
                    "emodiff",
                    "emovoice",
                    "prompttts",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We initialize EASPM from CLEP. Audio is resampled to 16&#8201;kHz and cropped/padded to 5&#8201;s. The text encoder is frozen, while the audio encoder and projection head are trained using Adam (batch size 64, 80 epochs), with learning rates <math alttext=\"1\\!\\times\\!10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.052em\" rspace=\"0.052em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\!\\times\\!10^{-5}</annotation></semantics></math> and <math alttext=\"1\\!\\times\\!10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.052em\" rspace=\"0.052em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\!\\times\\!10^{-3}</annotation></semantics></math>, respectively. Preference supervision is derived from emotion-labeled recordings, marking the target emotion as preferred and another as dis-preferred. To make scoring step-aware, both waveforms in a pair are perturbed by identical diffusion noise at a sampled denoising step. Our base TTS model is Grad-TTS with 80-dim mel-spectrograms. We freeze the encoder and duration predictor and fine-tune only the decoder (score network), initialized from Grad-TTS pretraining settings (Adam, <math alttext=\"1\\!\\times\\!10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.052em\" rspace=\"0.052em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\!\\times\\!10^{-4}</annotation></semantics></math> LR, batch size 16, random 2&#8201;s mel segments). During EASPO, we apply step shuffling and candidate pooling. A denoising step <math alttext=\"\\tau\\sim\\mathcal{U}(1,T{-}\\kappa)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>&#8764;</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119984;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>,</mo><mrow><mi>T</mi><mo>&#8722;</mo><mi>&#954;</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau\\sim\\mathcal{U}(1,T{-}\\kappa)</annotation></semantics></math> is chosen (skipping noisy early steps), <math alttext=\"k{=}4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">k{=}4</annotation></semantics></math> candidates are sampled from <math alttext=\"p_{\\theta}(\\cdot\\mid x_{\\tau},c)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S3.SS1.p2.m6\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><msub><mi>x</mi><mi>&#964;</mi></msub><mo>,</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p_{\\theta}(\\cdot\\mid x_{\\tau},c)</annotation></semantics></math>, ranked by EASPM, and a win&#8211;lose pair is used to minimize the stepwise difference&#8211;matching loss between policy log-ratio difference and reward difference. Unless specified, we set <math alttext=\"\\kappa{=}0.25T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m7\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>=</mo><mrow><mn>0.25</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\kappa{=}0.25T</annotation></semantics></math>, batch size 32, and decoder learning rate <math alttext=\"1\\!\\times\\!10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m8\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.052em\" rspace=\"0.052em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\!\\times\\!10^{-5}</annotation></semantics></math>. Waveforms are synthesized using a pretrained HiFi-GAN vocoder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib17\" title=\"\">17</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "tts",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Objective metrics.</span>\n<span class=\"ltx_text ltx_font_italic\">Emo_SIM</span> quantifies emotional alignment as the average cosine similarity between emotion2vec-base embeddings of generated and reference utterances. <span class=\"ltx_text ltx_font_italic\">Prosod_SIM</span> is computed via AutoPCP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib18\" title=\"\">18</a>]</cite>, which compares utterance-level prosody (rhythm, stress, intonation). <span class=\"ltx_text ltx_font_italic\">WER</span> (word-error-rate) is calculated using Whisper Large-v3 transcripts.\nUTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib19\" title=\"\">19</a>]</cite> is employed to evaluate speech naturalness and perceptual quality.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Subjective metrics.</span>\nWe conduct listening tests with 20 raters. Each system is evaluated on 30 clips per rater (six per emotion across five emotions). <span class=\"ltx_text ltx_font_italic\">MOS</span> assesses naturalness, and <span class=\"ltx_text ltx_font_italic\">Emotion MOS</span> evaluates how well the target emotion is conveyed given the prompt, both on a 1&#8211;5 scale (0.5 increments). <span class=\"ltx_text ltx_font_italic\">MOS_EC</span> measures consistency between the generated audio and the instruction (emotion + text). For <span class=\"ltx_text ltx_font_italic\">Emotion Recall</span>, raters identify the perceived emotion from five choices; accuracy is averaged over both utterances and emotion classes.</p>\n\n",
                "matched_terms": [
                    "evaluated",
                    "consistency",
                    "naturalness",
                    "recall",
                    "raters",
                    "subjective",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Effectiveness of the Emotion-aware Stepwise Preference Model.</span> We ablate timestep conditioning and CLEP initialization to assess their roles in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S3.T8\" title=\"Table 8 &#8227; 3.1 Datasets and Experimental Setup &#8227; 3 Experiments &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. Removing either results in consistent performance drops, confirming that both components are essential for accurate step-aware emotional scoring.</p>\n\n",
                "matched_terms": [
                    "model",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present EASPO, a diffusion-based speech synthesis framework that introduces stepwise preference optimization for fine-grained emotional alignment. By leveraging an emotion-aware scoring model to compare candidate samples at each denoising step, EASPO progressively guides generation toward emotionally expressive and prosodically natural speech. This step-conditioned training strategy enables the model to capture subtle affective cues through contrastive supervision. Extensive experimental demonstrate the effectiveness across both objective and subjective evaluations.</p>\n\n",
                "matched_terms": [
                    "model",
                    "subjective",
                    "emotional"
                ]
            }
        ]
    },
    "S3.T8.fig1": {
        "caption": "Table 3: Comparing EASPM with variants: no time condition.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Prefer. model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">E-S</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">P-S</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">WER</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">UTMOS</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">EASPM</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">99.15</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">3.89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">3.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">4.47</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">w/o step con.</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">98.79</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.81</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.83</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.36</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">CLAP</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">95.84</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.36</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.96</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.05</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "time",
            "utmos",
            "comparing",
            "condition",
            "step",
            "wer",
            "variants",
            "clap",
            "model",
            "easpm",
            "prefer"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Emotional text-to-speech seeks to convey affect while preserving intelligibility and prosody, yet existing methods rely on coarse labels or proxy classifiers and receive only utterance-level feedback. We introduce Emotion-Aware Stepwise Preference Optimization (EASPO), a post-training framework that aligns diffusion TTS with fine-grained emotional preferences at intermediate denoising steps. Central to our approach is EASPM, a time-conditioned model that scores noisy intermediate speech states and enables automatic preference pair construction. EASPO optimizes generation to match these stepwise preferences, enabling controllable emotional shaping. Experiments show superior performance over existing methods in both expressiveness and naturalness.</p>\n\n",
                "matched_terms": [
                    "model",
                    "easpm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Early emotional TTS systems fused local style tokens with phoneme content via cross-attention for fine-grained control&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib1\" title=\"\">1</a>]</cite>, guided synthesis with semantic prompts&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib2\" title=\"\">2</a>]</cite>, and used variational decoders to model nuanced emotion&#8211;prosody relations&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib3\" title=\"\">3</a>]</cite>. Diffusion-based methods improved fidelity and control through emotion embeddings and noise-conditioned prosody&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib4\" title=\"\">4</a>]</cite>. Recent work adopts supervised speech tokens for semantic grounding and chunk-aware flow-based decoding&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib6\" title=\"\">6</a>]</cite>, and introduces parallel phoneme&#8211;audio branches in LLM-based generation to support fine-grained, freestyle emotional control&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib7\" title=\"\">7</a>]</cite>. Across these variants, two important gaps remain: supervision still targets proxy labels over preference-driven prompts, and feedback remains temporally sparse, limiting constraints on prosody&#8211;emotion dynamics.</p>\n\n",
                "matched_terms": [
                    "variants",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Direct Preference Optimization (DPO)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib8\" title=\"\">8</a>]</cite> aligns generative models to human choices via paired comparisons against a frozen reference policy. In diffusion settings<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib9\" title=\"\">9</a>]</cite>, a common practice is to assign preference at the final step and propagate it to intermediate latents, then optimize a DPO-style log-likelihood ratio at each step. This removes the need for an explicit reward model and is effective for global preferences, but offers limited guidance for temporally evolving signals.\nDPO has also been used to align large language model&#8211;based TTS. EmoDPO&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib10\" title=\"\">10</a>]</cite>, for instance, pairs utterances with identical text and marks the target emotion as preferred, achieving alignment without explicit reward modeling. However, attaching preference only at the endpoint yields sparse supervision for gradually varying cues, the assumption that all intermediate states on a preferred path are themselves preferred is often invalid, and curating domain-appropriate preference pairs is expensive. This leads to our central research question: <span class=\"ltx_text ltx_font_italic\">How can we provide dense, fine-grained supervision for emotionally expressive TTS generation, without relying on endpoint preferences or categorical labels?</span></p>\n\n",
                "matched_terms": [
                    "model",
                    "step"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better align diffusion-based TTS systems with fine-grained emotional preferences, we propose <span class=\"ltx_text ltx_font_italic\">Emotion-Aware Stepwise Preference Optimization</span> (EASPO). At each denoising step starting from a latent representation <math alttext=\"x_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">x_{t}</annotation></semantics></math>, the model samples a small candidate set of <math alttext=\"x_{t-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">x_{t-1}</annotation></semantics></math> mel-spectrograms. An <span class=\"ltx_text ltx_font_italic\">Emotion-aware Stepwise Preference Model</span> (EASPM) scores their emotional expressiveness and selects a win&#8211;lose pair that differs subtly in prosody while preserving linguistic content. One candidate is then randomly chosen to continue generation. Since these candidates originate from the same latent and differ by only a single denoising step, their variations are localized and emotion-focused. EASPM captures these nuanced differences and steers the model toward producing more emotionally consistent speech.</p>\n\n",
                "matched_terms": [
                    "model",
                    "step",
                    "easpm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contributions are: 1) We propose EASPO, a stepwise alignment framework that reformulates preference optimization as a local, time-conditioned task, replacing the flawed assumption that all intermediate states on a preferred trajectory are equally valid. By aligning win/lose candidates at each denoising step from a shared latent, it enables stepwise-controllable emotion shaping throughout the generation process. 2) We introduce EASPM, a time-aware reward model that directly scores emotional expressiveness and prosody on noisy intermediate states, enabling dense, temporally grounded preference learning and on-the-fly scoring.</p>\n\n",
                "matched_terms": [
                    "model",
                    "step",
                    "easpm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a reinforcement learning framework for emotional TTS that fine-tunes diffusion models via stepwise preference supervision (Fig.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Starting from a pretrained Grad-TTS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib11\" title=\"\">11</a>]</cite>, our method introduces dense emotion-aligned rewards at each denoising step. At every latent state, multiple mel-spectrograms are sampled and ranked by a frozen emotion preference model(Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S2.SS1\" title=\"2.1 Emotion-Aware Stepwise Preference Model &#8227; 2 Method &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">2.1</span></a>). A preference pair is selected, and the model is optimized to favor the emotionally preferred sample by minimizing the gap between its advantage and the log-likelihood ratio(Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S2.SS2\" title=\"2.2 Objective Function of EASPO &#8227; 2 Method &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>). Our EASPO objective extends DPO with stepwise preference signals, enabling fine-grained emotional control during generation.</p>\n\n",
                "matched_terms": [
                    "model",
                    "step"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall.</span>\nAt reverse step <math alttext=\"t-1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">t-1</annotation></semantics></math>, given the current latent <math alttext=\"x_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">x_{t}</annotation></semantics></math>, the generator draws <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> candidates\n<math alttext=\"\\{x_{t-1}^{1},\\ldots,x_{t-1}^{k}\\}\\sim p_{\\theta}(x_{t-1}\\mid x_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><mn>1</mn></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><mi>k</mi></msubsup><mo stretchy=\"false\">}</mo></mrow><mo>&#8764;</mo><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><mo>&#8739;</mo><msub><mi>x</mi><mi>t</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\{x_{t-1}^{1},\\ldots,x_{t-1}^{k}\\}\\sim p_{\\theta}(x_{t-1}\\mid x_{t})</annotation></semantics></math>. EASPM assigns each candidate a <em class=\"ltx_emph ltx_font_italic\">timestep-aware</em> emotion&#8211;prompt consistency score and induces a ranking; the highest and lowest scored items are taken as the win/lose pair.</p>\n\n",
                "matched_terms": [
                    "step",
                    "easpm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training.</span>\nEASPM is adapted from CLEP to handle noisy intermediate representations. Given a win&#8211;lose pair <math alttext=\"(x_{0}^{w},x_{0}^{l})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mn>0</mn><mi>w</mi></msubsup><mo>,</mo><msubsup><mi>x</mi><mn>0</mn><mi>l</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{0}^{w},x_{0}^{l})</annotation></semantics></math>, we sample a timestep <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m2\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> and apply the same forward diffusion to produce <math alttext=\"(x_{t}^{w},x_{t}^{l})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mi>t</mi><mi>w</mi></msubsup><mo>,</mo><msubsup><mi>x</mi><mi>t</mi><mi>l</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{t}^{w},x_{t}^{l})</annotation></semantics></math>. This tuple <math alttext=\"(x_{t}^{w},x_{t}^{l},t,c)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mi>t</mi><mi>w</mi></msubsup><mo>,</mo><msubsup><mi>x</mi><mi>t</mi><mi>l</mi></msubsup><mo>,</mo><mi>t</mi><mo>,</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{t}^{w},x_{t}^{l},t,c)</annotation></semantics></math> is used to optimize <math alttext=\"\\mathcal{L}_{\\text{pref}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>pref</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{pref}}</annotation></semantics></math>, encouraging the model to recover the correct preference at step <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m6\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>. A time-aware normalization layer is added to CLEP&#8217;s audio branch for timestep conditioning. To reduce mismatch with CLEP&#8217;s pretraining domain, we optionally estimate a pseudo-clean <math alttext=\"\\hat{x}_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m7\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\hat{x}_{0}</annotation></semantics></math> from <math alttext=\"x_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m8\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">x_{t}</annotation></semantics></math> via deterministic inversion followed by&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib14\" title=\"\">14</a>]</cite> . After training, EASPM is frozen and used solely as a stepwise scorer for the RL objective.</p>\n\n",
                "matched_terms": [
                    "model",
                    "step",
                    "easpm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We formulate denoising as a <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>-step MDP with state <math alttext=\"s_{t}=(c,x_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo>,</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">s_{t}=(c,x_{t})</annotation></semantics></math>, action <math alttext=\"a_{t}=x_{t-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>=</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">a_{t}=x_{t-1}</annotation></semantics></math>, and policy\n<math alttext=\"\\pi_{\\theta}(a_{t}\\mid s_{t})=p_{\\theta}(x_{t-1}\\mid x_{t},c)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>&#960;</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>&#8739;</mo><msub><mi>s</mi><mi>t</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><mo>&#8739;</mo><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><mi>c</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\pi_{\\theta}(a_{t}\\mid s_{t})=p_{\\theta}(x_{t-1}\\mid x_{t},c)</annotation></semantics></math>.\nAt each step <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, we sample <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> candidates <math alttext=\"\\{x_{t-1}^{i}\\}\\sim p_{\\theta}(\\cdot\\mid x_{t},c)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><mi>i</mi></msubsup><mo stretchy=\"false\">}</mo></mrow><mo>&#8764;</mo><msub><mi>p</mi><mi>&#952;</mi></msub><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\{x_{t-1}^{i}\\}\\sim p_{\\theta}(\\cdot\\mid x_{t},c)</annotation></semantics></math>\nand rank them via EASPM (Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S2.SS1\" title=\"2.1 Emotion-Aware Stepwise Preference Model &#8227; 2 Method &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">2.1</span></a>). Let <math alttext=\"x_{t-1}^{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m8\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><mi>w</mi></msubsup><annotation encoding=\"application/x-tex\">x_{t-1}^{w}</annotation></semantics></math> and <math alttext=\"x_{t-1}^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m9\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><mi>l</mi></msubsup><annotation encoding=\"application/x-tex\">x_{t-1}^{l}</annotation></semantics></math> denote the top and bottom-ranked samples from the <em class=\"ltx_emph ltx_font_italic\">same</em> latent <math alttext=\"x_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m10\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">x_{t}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "step",
                    "easpm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dense stepwise reward.</span>\nEASPM supplies a dense emotional reward at step <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p2.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "step",
                    "easpm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We initialize EASPM from CLEP. Audio is resampled to 16&#8201;kHz and cropped/padded to 5&#8201;s. The text encoder is frozen, while the audio encoder and projection head are trained using Adam (batch size 64, 80 epochs), with learning rates <math alttext=\"1\\!\\times\\!10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.052em\" rspace=\"0.052em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\!\\times\\!10^{-5}</annotation></semantics></math> and <math alttext=\"1\\!\\times\\!10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.052em\" rspace=\"0.052em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\!\\times\\!10^{-3}</annotation></semantics></math>, respectively. Preference supervision is derived from emotion-labeled recordings, marking the target emotion as preferred and another as dis-preferred. To make scoring step-aware, both waveforms in a pair are perturbed by identical diffusion noise at a sampled denoising step. Our base TTS model is Grad-TTS with 80-dim mel-spectrograms. We freeze the encoder and duration predictor and fine-tune only the decoder (score network), initialized from Grad-TTS pretraining settings (Adam, <math alttext=\"1\\!\\times\\!10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.052em\" rspace=\"0.052em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\!\\times\\!10^{-4}</annotation></semantics></math> LR, batch size 16, random 2&#8201;s mel segments). During EASPO, we apply step shuffling and candidate pooling. A denoising step <math alttext=\"\\tau\\sim\\mathcal{U}(1,T{-}\\kappa)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>&#8764;</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119984;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>,</mo><mrow><mi>T</mi><mo>&#8722;</mo><mi>&#954;</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau\\sim\\mathcal{U}(1,T{-}\\kappa)</annotation></semantics></math> is chosen (skipping noisy early steps), <math alttext=\"k{=}4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">k{=}4</annotation></semantics></math> candidates are sampled from <math alttext=\"p_{\\theta}(\\cdot\\mid x_{\\tau},c)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S3.SS1.p2.m6\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><msub><mi>x</mi><mi>&#964;</mi></msub><mo>,</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p_{\\theta}(\\cdot\\mid x_{\\tau},c)</annotation></semantics></math>, ranked by EASPM, and a win&#8211;lose pair is used to minimize the stepwise difference&#8211;matching loss between policy log-ratio difference and reward difference. Unless specified, we set <math alttext=\"\\kappa{=}0.25T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m7\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>=</mo><mrow><mn>0.25</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\kappa{=}0.25T</annotation></semantics></math>, batch size 32, and decoder learning rate <math alttext=\"1\\!\\times\\!10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m8\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.052em\" rspace=\"0.052em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\!\\times\\!10^{-5}</annotation></semantics></math>. Waveforms are synthesized using a pretrained HiFi-GAN vocoder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib17\" title=\"\">17</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "step",
                    "easpm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Objective metrics.</span>\n<span class=\"ltx_text ltx_font_italic\">Emo_SIM</span> quantifies emotional alignment as the average cosine similarity between emotion2vec-base embeddings of generated and reference utterances. <span class=\"ltx_text ltx_font_italic\">Prosod_SIM</span> is computed via AutoPCP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib18\" title=\"\">18</a>]</cite>, which compares utterance-level prosody (rhythm, stress, intonation). <span class=\"ltx_text ltx_font_italic\">WER</span> (word-error-rate) is calculated using Whisper Large-v3 transcripts.\nUTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib19\" title=\"\">19</a>]</cite> is employed to evaluate speech naturalness and perceptual quality.</p>\n\n",
                "matched_terms": [
                    "utmos",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate the effectiveness of our proposed emotionally preference-aligned method for TTS, we compare it with seven recent emotion-controllable baselines. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S3.T1\" title=\"Table 1 &#8227; 3.1 Datasets and Experimental Setup &#8227; 3 Experiments &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our method achieves superior performance across emotion similarity, prosody similarity, intelligibility (WER), and perceptual quality (UTMOS), with a notable 2.07% gain over CosyVoice in Emo_SIM. A similar trend is observed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S3.T2\" title=\"Table 2 &#8227; 3.1 Datasets and Experimental Setup &#8227; 3 Experiments &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, where our model consistently outperforms baselines in naturalness (MOS), emotional expressiveness (Emo_MOS), emotion consistency (MOS_EC), and emotion classification accuracy (Recall). These results demonstrate the effectiveness of our approach in generating emotionally aligned speech with enhanced coherence, while preserving natural prosody and speech quality. Demo page is available&#160;<a class=\"ltx_ref ltx_href\" href=\"https://jiachengqaq.github.io/emo-tts-demo/\" title=\"\">.</a></p>\n\n",
                "matched_terms": [
                    "utmos",
                    "model",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present EASPO, a diffusion-based speech synthesis framework that introduces stepwise preference optimization for fine-grained emotional alignment. By leveraging an emotion-aware scoring model to compare candidate samples at each denoising step, EASPO progressively guides generation toward emotionally expressive and prosodically natural speech. This step-conditioned training strategy enables the model to capture subtle affective cues through contrastive supervision. Extensive experimental demonstrate the effectiveness across both objective and subjective evaluations.</p>\n\n",
                "matched_terms": [
                    "model",
                    "step"
                ]
            }
        ]
    },
    "S3.T8.fig2": {
        "caption": "Table 4: Comparing random sampling with other sampling strategies.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:4.8pt;padding-right:4.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Initial.</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.8pt;padding-right:4.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">E-S</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.8pt;padding-right:4.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">P-S</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.8pt;padding-right:4.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">WER</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:4.8pt;padding-right:4.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">UTMOS</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:4.8pt;padding-right:4.8pt;\"><math alttext=\"\\boldsymbol{x}^{w}_{t-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T8.m1\" intent=\":literal\"><semantics><msubsup><mi mathsize=\"0.800em\">&#119961;</mi><mrow><mi mathsize=\"0.800em\">t</mi><mo mathsize=\"0.800em\">&#8722;</mo><mn mathsize=\"0.800em\">1</mn></mrow><mi mathsize=\"0.800em\">w</mi></msubsup><annotation encoding=\"application/x-tex\">\\boldsymbol{x}^{w}_{t-1}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.8pt;padding-right:4.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">97.78</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.8pt;padding-right:4.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.63</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.8pt;padding-right:4.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.81</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.8pt;padding-right:4.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.20</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:4.8pt;padding-right:4.8pt;\"><math alttext=\"\\boldsymbol{x}^{l}_{t-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T8.m2\" intent=\":literal\"><semantics><msubsup><mi mathsize=\"0.800em\">&#119961;</mi><mrow><mi mathsize=\"0.800em\">t</mi><mo mathsize=\"0.800em\">&#8722;</mo><mn mathsize=\"0.800em\">1</mn></mrow><mi mathsize=\"0.800em\">l</mi></msubsup><annotation encoding=\"application/x-tex\">\\boldsymbol{x}^{l}_{t-1}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.8pt;padding-right:4.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">98.39</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.8pt;padding-right:4.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.75</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.8pt;padding-right:4.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.79</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:4.8pt;padding-right:4.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.33</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:4.8pt;padding-right:4.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">random</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.8pt;padding-right:4.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">99.15</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.8pt;padding-right:4.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">3.89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.8pt;padding-right:4.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">3.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.8pt;padding-right:4.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">4.47</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "random",
            "utmos",
            "comparing",
            "wer",
            "sampling",
            "strategies",
            "initial",
            "𝒙t−1wboldsymbolxwt1",
            "other",
            "𝒙t−1lboldsymbolxlt1"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Random selection of the next state.</span>\nAfter EASPM ranks the candidate set <math alttext=\"\\{x_{t-1}^{1},\\ldots,x_{t-1}^{k}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><mn>1</mn></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><mi>k</mi></msubsup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{x_{t-1}^{1},\\ldots,x_{t-1}^{k}\\}</annotation></semantics></math> and selects a win&#8211;lose pair, we <em class=\"ltx_emph ltx_font_italic\">do not</em> continue with the top sample as shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. To avoid biased rollouts and degenerate paths, we uniformly sample the next state <math alttext=\"\\tilde{x}_{t-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m2\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">\\tilde{x}_{t-1}</annotation></semantics></math> from the pool and proceed with <math alttext=\"x_{t-2}\\sim p_{\\theta}(\\cdot\\mid\\tilde{x}_{t-1},c)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS1.p4.m3\" intent=\":literal\"><semantics><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>2</mn></mrow></msub><mo>&#8764;</mo><msub><mi>p</mi><mi>&#952;</mi></msub><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><mo>,</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x_{t-2}\\sim p_{\\theta}(\\cdot\\mid\\tilde{x}_{t-1},c)</annotation></semantics></math>. This ensures all preference pairs originate from the same latent <math alttext=\"x_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m4\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">x_{t}</annotation></semantics></math> while decoupling supervision from sampling. Candidate pooling is applied only when <math alttext=\"t\\leq\\kappa\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m5\" intent=\":literal\"><semantics><mrow><mi>t</mi><mo>&#8804;</mo><mi>&#954;</mi></mrow><annotation encoding=\"application/x-tex\">t\\leq\\kappa</annotation></semantics></math>; standard transitions are used for <math alttext=\"t&gt;\\kappa\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m6\" intent=\":literal\"><semantics><mrow><mi>t</mi><mo>&gt;</mo><mi>&#954;</mi></mrow><annotation encoding=\"application/x-tex\">t&gt;\\kappa</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "sampling",
                    "random"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Objective metrics.</span>\n<span class=\"ltx_text ltx_font_italic\">Emo_SIM</span> quantifies emotional alignment as the average cosine similarity between emotion2vec-base embeddings of generated and reference utterances. <span class=\"ltx_text ltx_font_italic\">Prosod_SIM</span> is computed via AutoPCP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib18\" title=\"\">18</a>]</cite>, which compares utterance-level prosody (rhythm, stress, intonation). <span class=\"ltx_text ltx_font_italic\">WER</span> (word-error-rate) is calculated using Whisper Large-v3 transcripts.\nUTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib19\" title=\"\">19</a>]</cite> is employed to evaluate speech naturalness and perceptual quality.</p>\n\n",
                "matched_terms": [
                    "utmos",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate the effectiveness of our proposed emotionally preference-aligned method for TTS, we compare it with seven recent emotion-controllable baselines. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S3.T1\" title=\"Table 1 &#8227; 3.1 Datasets and Experimental Setup &#8227; 3 Experiments &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our method achieves superior performance across emotion similarity, prosody similarity, intelligibility (WER), and perceptual quality (UTMOS), with a notable 2.07% gain over CosyVoice in Emo_SIM. A similar trend is observed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S3.T2\" title=\"Table 2 &#8227; 3.1 Datasets and Experimental Setup &#8227; 3 Experiments &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, where our model consistently outperforms baselines in naturalness (MOS), emotional expressiveness (Emo_MOS), emotion consistency (MOS_EC), and emotion classification accuracy (Recall). These results demonstrate the effectiveness of our approach in generating emotionally aligned speech with enhanced coherence, while preserving natural prosody and speech quality. Demo page is available&#160;<a class=\"ltx_ref ltx_href\" href=\"https://jiachengqaq.github.io/emo-tts-demo/\" title=\"\">.</a></p>\n\n",
                "matched_terms": [
                    "utmos",
                    "wer"
                ]
            }
        ]
    },
    "S3.T8.fig3": {
        "caption": "Table 5: Impact of number of sampled images kk at each step. We use k=4k=4.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:3.1pt;padding-right:3.1pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">#samples </span><math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T8.m7\" intent=\":literal\"><semantics><mi mathsize=\"0.800em\">k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">E-S</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">P-S</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">WER</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">UTMOS</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">2</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">98.31</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.76</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.78</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.23</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">99.15</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.89</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.74</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">4.47</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">8</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">98.84</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">3.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">3.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.27</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "impact",
            "utmos",
            "step",
            "images",
            "wer",
            "k4k4",
            "use",
            "samples",
            "sampled",
            "each",
            "number"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Direct Preference Optimization (DPO)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib8\" title=\"\">8</a>]</cite> aligns generative models to human choices via paired comparisons against a frozen reference policy. In diffusion settings<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib9\" title=\"\">9</a>]</cite>, a common practice is to assign preference at the final step and propagate it to intermediate latents, then optimize a DPO-style log-likelihood ratio at each step. This removes the need for an explicit reward model and is effective for global preferences, but offers limited guidance for temporally evolving signals.\nDPO has also been used to align large language model&#8211;based TTS. EmoDPO&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib10\" title=\"\">10</a>]</cite>, for instance, pairs utterances with identical text and marks the target emotion as preferred, achieving alignment without explicit reward modeling. However, attaching preference only at the endpoint yields sparse supervision for gradually varying cues, the assumption that all intermediate states on a preferred path are themselves preferred is often invalid, and curating domain-appropriate preference pairs is expensive. This leads to our central research question: <span class=\"ltx_text ltx_font_italic\">How can we provide dense, fine-grained supervision for emotionally expressive TTS generation, without relying on endpoint preferences or categorical labels?</span></p>\n\n",
                "matched_terms": [
                    "each",
                    "step"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better align diffusion-based TTS systems with fine-grained emotional preferences, we propose <span class=\"ltx_text ltx_font_italic\">Emotion-Aware Stepwise Preference Optimization</span> (EASPO). At each denoising step starting from a latent representation <math alttext=\"x_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">x_{t}</annotation></semantics></math>, the model samples a small candidate set of <math alttext=\"x_{t-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">x_{t-1}</annotation></semantics></math> mel-spectrograms. An <span class=\"ltx_text ltx_font_italic\">Emotion-aware Stepwise Preference Model</span> (EASPM) scores their emotional expressiveness and selects a win&#8211;lose pair that differs subtly in prosody while preserving linguistic content. One candidate is then randomly chosen to continue generation. Since these candidates originate from the same latent and differ by only a single denoising step, their variations are localized and emotion-focused. EASPM captures these nuanced differences and steers the model toward producing more emotionally consistent speech.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "each",
                    "step"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contributions are: 1) We propose EASPO, a stepwise alignment framework that reformulates preference optimization as a local, time-conditioned task, replacing the flawed assumption that all intermediate states on a preferred trajectory are equally valid. By aligning win/lose candidates at each denoising step from a shared latent, it enables stepwise-controllable emotion shaping throughout the generation process. 2) We introduce EASPM, a time-aware reward model that directly scores emotional expressiveness and prosody on noisy intermediate states, enabling dense, temporally grounded preference learning and on-the-fly scoring.</p>\n\n",
                "matched_terms": [
                    "each",
                    "step"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a reinforcement learning framework for emotional TTS that fine-tunes diffusion models via stepwise preference supervision (Fig.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Starting from a pretrained Grad-TTS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib11\" title=\"\">11</a>]</cite>, our method introduces dense emotion-aligned rewards at each denoising step. At every latent state, multiple mel-spectrograms are sampled and ranked by a frozen emotion preference model(Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S2.SS1\" title=\"2.1 Emotion-Aware Stepwise Preference Model &#8227; 2 Method &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">2.1</span></a>). A preference pair is selected, and the model is optimized to favor the emotionally preferred sample by minimizing the gap between its advantage and the log-likelihood ratio(Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S2.SS2\" title=\"2.2 Objective Function of EASPO &#8227; 2 Method &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>). Our EASPO objective extends DPO with stepwise preference signals, enabling fine-grained emotional control during generation.</p>\n\n",
                "matched_terms": [
                    "each",
                    "step",
                    "sampled"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall.</span>\nAt reverse step <math alttext=\"t-1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">t-1</annotation></semantics></math>, given the current latent <math alttext=\"x_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">x_{t}</annotation></semantics></math>, the generator draws <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> candidates\n<math alttext=\"\\{x_{t-1}^{1},\\ldots,x_{t-1}^{k}\\}\\sim p_{\\theta}(x_{t-1}\\mid x_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><mn>1</mn></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><mi>k</mi></msubsup><mo stretchy=\"false\">}</mo></mrow><mo>&#8764;</mo><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><mo>&#8739;</mo><msub><mi>x</mi><mi>t</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\{x_{t-1}^{1},\\ldots,x_{t-1}^{k}\\}\\sim p_{\\theta}(x_{t-1}\\mid x_{t})</annotation></semantics></math>. EASPM assigns each candidate a <em class=\"ltx_emph ltx_font_italic\">timestep-aware</em> emotion&#8211;prompt consistency score and induces a ranking; the highest and lowest scored items are taken as the win/lose pair.</p>\n\n",
                "matched_terms": [
                    "each",
                    "step"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We formulate denoising as a <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>-step MDP with state <math alttext=\"s_{t}=(c,x_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo>,</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">s_{t}=(c,x_{t})</annotation></semantics></math>, action <math alttext=\"a_{t}=x_{t-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>=</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">a_{t}=x_{t-1}</annotation></semantics></math>, and policy\n<math alttext=\"\\pi_{\\theta}(a_{t}\\mid s_{t})=p_{\\theta}(x_{t-1}\\mid x_{t},c)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>&#960;</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>&#8739;</mo><msub><mi>s</mi><mi>t</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><mo>&#8739;</mo><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><mi>c</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\pi_{\\theta}(a_{t}\\mid s_{t})=p_{\\theta}(x_{t-1}\\mid x_{t},c)</annotation></semantics></math>.\nAt each step <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, we sample <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> candidates <math alttext=\"\\{x_{t-1}^{i}\\}\\sim p_{\\theta}(\\cdot\\mid x_{t},c)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><mi>i</mi></msubsup><mo stretchy=\"false\">}</mo></mrow><mo>&#8764;</mo><msub><mi>p</mi><mi>&#952;</mi></msub><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\{x_{t-1}^{i}\\}\\sim p_{\\theta}(\\cdot\\mid x_{t},c)</annotation></semantics></math>\nand rank them via EASPM (Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S2.SS1\" title=\"2.1 Emotion-Aware Stepwise Preference Model &#8227; 2 Method &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">2.1</span></a>). Let <math alttext=\"x_{t-1}^{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m8\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><mi>w</mi></msubsup><annotation encoding=\"application/x-tex\">x_{t-1}^{w}</annotation></semantics></math> and <math alttext=\"x_{t-1}^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m9\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><mi>l</mi></msubsup><annotation encoding=\"application/x-tex\">x_{t-1}^{l}</annotation></semantics></math> denote the top and bottom-ranked samples from the <em class=\"ltx_emph ltx_font_italic\">same</em> latent <math alttext=\"x_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m10\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">x_{t}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "each",
                    "step"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We initialize EASPM from CLEP. Audio is resampled to 16&#8201;kHz and cropped/padded to 5&#8201;s. The text encoder is frozen, while the audio encoder and projection head are trained using Adam (batch size 64, 80 epochs), with learning rates <math alttext=\"1\\!\\times\\!10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.052em\" rspace=\"0.052em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\!\\times\\!10^{-5}</annotation></semantics></math> and <math alttext=\"1\\!\\times\\!10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.052em\" rspace=\"0.052em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\!\\times\\!10^{-3}</annotation></semantics></math>, respectively. Preference supervision is derived from emotion-labeled recordings, marking the target emotion as preferred and another as dis-preferred. To make scoring step-aware, both waveforms in a pair are perturbed by identical diffusion noise at a sampled denoising step. Our base TTS model is Grad-TTS with 80-dim mel-spectrograms. We freeze the encoder and duration predictor and fine-tune only the decoder (score network), initialized from Grad-TTS pretraining settings (Adam, <math alttext=\"1\\!\\times\\!10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.052em\" rspace=\"0.052em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\!\\times\\!10^{-4}</annotation></semantics></math> LR, batch size 16, random 2&#8201;s mel segments). During EASPO, we apply step shuffling and candidate pooling. A denoising step <math alttext=\"\\tau\\sim\\mathcal{U}(1,T{-}\\kappa)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>&#8764;</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119984;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>,</mo><mrow><mi>T</mi><mo>&#8722;</mo><mi>&#954;</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau\\sim\\mathcal{U}(1,T{-}\\kappa)</annotation></semantics></math> is chosen (skipping noisy early steps), <math alttext=\"k{=}4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">k{=}4</annotation></semantics></math> candidates are sampled from <math alttext=\"p_{\\theta}(\\cdot\\mid x_{\\tau},c)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S3.SS1.p2.m6\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><msub><mi>x</mi><mi>&#964;</mi></msub><mo>,</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p_{\\theta}(\\cdot\\mid x_{\\tau},c)</annotation></semantics></math>, ranked by EASPM, and a win&#8211;lose pair is used to minimize the stepwise difference&#8211;matching loss between policy log-ratio difference and reward difference. Unless specified, we set <math alttext=\"\\kappa{=}0.25T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m7\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>=</mo><mrow><mn>0.25</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\kappa{=}0.25T</annotation></semantics></math>, batch size 32, and decoder learning rate <math alttext=\"1\\!\\times\\!10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m8\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.052em\" rspace=\"0.052em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\!\\times\\!10^{-5}</annotation></semantics></math>. Waveforms are synthesized using a pretrained HiFi-GAN vocoder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib17\" title=\"\">17</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "k4k4",
                    "step",
                    "sampled"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Objective metrics.</span>\n<span class=\"ltx_text ltx_font_italic\">Emo_SIM</span> quantifies emotional alignment as the average cosine similarity between emotion2vec-base embeddings of generated and reference utterances. <span class=\"ltx_text ltx_font_italic\">Prosod_SIM</span> is computed via AutoPCP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib18\" title=\"\">18</a>]</cite>, which compares utterance-level prosody (rhythm, stress, intonation). <span class=\"ltx_text ltx_font_italic\">WER</span> (word-error-rate) is calculated using Whisper Large-v3 transcripts.\nUTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib19\" title=\"\">19</a>]</cite> is employed to evaluate speech naturalness and perceptual quality.</p>\n\n",
                "matched_terms": [
                    "utmos",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate the effectiveness of our proposed emotionally preference-aligned method for TTS, we compare it with seven recent emotion-controllable baselines. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S3.T1\" title=\"Table 1 &#8227; 3.1 Datasets and Experimental Setup &#8227; 3 Experiments &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our method achieves superior performance across emotion similarity, prosody similarity, intelligibility (WER), and perceptual quality (UTMOS), with a notable 2.07% gain over CosyVoice in Emo_SIM. A similar trend is observed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S3.T2\" title=\"Table 2 &#8227; 3.1 Datasets and Experimental Setup &#8227; 3 Experiments &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, where our model consistently outperforms baselines in naturalness (MOS), emotional expressiveness (Emo_MOS), emotion consistency (MOS_EC), and emotion classification accuracy (Recall). These results demonstrate the effectiveness of our approach in generating emotionally aligned speech with enhanced coherence, while preserving natural prosody and speech quality. Demo page is available&#160;<a class=\"ltx_ref ltx_href\" href=\"https://jiachengqaq.github.io/emo-tts-demo/\" title=\"\">.</a></p>\n\n",
                "matched_terms": [
                    "utmos",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Candidate Pool Size.</span>\nWe vary the number of candidates k at each step and observe its effect in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S3.T8\" title=\"Table 8 &#8227; 3.1 Datasets and Experimental Setup &#8227; 3 Experiments &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. A moderate k balances contrastive supervision and fidelity, improving the learning of emotional and prosodic preferences, while large k introduces artifacts that weaken supervision.</p>\n\n",
                "matched_terms": [
                    "impact",
                    "each",
                    "step",
                    "number"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Choice of Win/Lose Pairs.</span>\nWe compare selecting the top&#8211;bottom EASPM-scored candidates versus randomly sampled pairs from the same latent state in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S3.T8\" title=\"Table 8 &#8227; 3.1 Datasets and Experimental Setup &#8227; 3 Experiments &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. Using highest&#8211;lowest scoring samples ensures stronger emotional contrast while maintaining comparable content and noise levels, leading to more stable and informative supervision.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "sampled"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present EASPO, a diffusion-based speech synthesis framework that introduces stepwise preference optimization for fine-grained emotional alignment. By leveraging an emotion-aware scoring model to compare candidate samples at each denoising step, EASPO progressively guides generation toward emotionally expressive and prosodically natural speech. This step-conditioned training strategy enables the model to capture subtle affective cues through contrastive supervision. Extensive experimental demonstrate the effectiveness across both objective and subjective evaluations.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "each",
                    "step"
                ]
            }
        ]
    },
    "S3.T8.fig4": {
        "caption": "Table 6: Impact of timestep range.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Timestep Range</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">E-S</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">P-S</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">WER</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">UTMOS</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:80%;\">[0-250]</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">98.16</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.35</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.81</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.14</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:80%;\">[0-500]</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">98.79</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.62</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.76</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.39</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:80%;\">[0-750]</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">99.15</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">3.89</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.74</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">4.47</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:80%;\">[0-1000]</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">97.92</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.57</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">3.69</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.26</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:80%;\">[250-750]</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">98.85</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.81</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.71</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.42</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:80%;\">[500-750]</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">98.34</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.69</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.75</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.37</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text ltx_font_typewriter\" style=\"font-size:80%;\">[250-500]</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">98.68</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.77</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.73</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.4pt;padding-right:2.4pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.28</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "impact",
            "utmos",
            "wer",
            "timestep",
            "range"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Objective metrics.</span>\n<span class=\"ltx_text ltx_font_italic\">Emo_SIM</span> quantifies emotional alignment as the average cosine similarity between emotion2vec-base embeddings of generated and reference utterances. <span class=\"ltx_text ltx_font_italic\">Prosod_SIM</span> is computed via AutoPCP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib18\" title=\"\">18</a>]</cite>, which compares utterance-level prosody (rhythm, stress, intonation). <span class=\"ltx_text ltx_font_italic\">WER</span> (word-error-rate) is calculated using Whisper Large-v3 transcripts.\nUTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib19\" title=\"\">19</a>]</cite> is employed to evaluate speech naturalness and perceptual quality.</p>\n\n",
                "matched_terms": [
                    "utmos",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate the effectiveness of our proposed emotionally preference-aligned method for TTS, we compare it with seven recent emotion-controllable baselines. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S3.T1\" title=\"Table 1 &#8227; 3.1 Datasets and Experimental Setup &#8227; 3 Experiments &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our method achieves superior performance across emotion similarity, prosody similarity, intelligibility (WER), and perceptual quality (UTMOS), with a notable 2.07% gain over CosyVoice in Emo_SIM. A similar trend is observed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S3.T2\" title=\"Table 2 &#8227; 3.1 Datasets and Experimental Setup &#8227; 3 Experiments &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, where our model consistently outperforms baselines in naturalness (MOS), emotional expressiveness (Emo_MOS), emotion consistency (MOS_EC), and emotion classification accuracy (Recall). These results demonstrate the effectiveness of our approach in generating emotionally aligned speech with enhanced coherence, while preserving natural prosody and speech quality. Demo page is available&#160;<a class=\"ltx_ref ltx_href\" href=\"https://jiachengqaq.github.io/emo-tts-demo/\" title=\"\">.</a></p>\n\n",
                "matched_terms": [
                    "utmos",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Timestep Range.</span>\nWe apply EASPO on a subset of denoising steps <math alttext=\"[0,\\kappa]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mi>&#954;</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0,\\kappa]</annotation></semantics></math> in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S3.T8\" title=\"Table 8 &#8227; 3.1 Datasets and Experimental Setup &#8227; 3 Experiments &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> and find that skipping noisy early steps improves alignment due to limited speech structure, while omitting fine-grained late steps weakens prosodic refinement. A mid-range window (e.g., <math alttext=\"[0,750]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>750</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0,750]</annotation></semantics></math>) balances diversity and emotional clarity, yielding optimal performance.</p>\n\n",
                "matched_terms": [
                    "impact",
                    "range",
                    "timestep"
                ]
            }
        ]
    },
    "S3.T8.fig5": {
        "caption": "Table 7: Comparison with other diffusion based RL alignment methods.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Method</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">E-S</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">P-S</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">WER</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">UTMOS</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Vanilla-DM</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">96.62</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.55</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">5.62</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.35</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">DDPO</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">98.37</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.63</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.07</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.41</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">D3PO</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">97.51</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.59</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.41</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.40</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Diff.-DPO</span></th>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">97.85</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.67</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.82</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.37</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">EASPO</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">99.15</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">3.89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">3.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.8pt;padding-right:2.8pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">4.47</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "diffusion",
            "utmos",
            "alignment",
            "based",
            "wer",
            "vanilladm",
            "easpo",
            "d3po",
            "other",
            "ddpo",
            "method",
            "methods",
            "diffdpo",
            "comparison"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Emotional text-to-speech seeks to convey affect while preserving intelligibility and prosody, yet existing methods rely on coarse labels or proxy classifiers and receive only utterance-level feedback. We introduce Emotion-Aware Stepwise Preference Optimization (EASPO), a post-training framework that aligns diffusion TTS with fine-grained emotional preferences at intermediate denoising steps. Central to our approach is EASPM, a time-conditioned model that scores noisy intermediate speech states and enables automatic preference pair construction. EASPO optimizes generation to match these stepwise preferences, enabling controllable emotional shaping. Experiments show superior performance over existing methods in both expressiveness and naturalness.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "easpo",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Direct Preference Optimization (DPO)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib8\" title=\"\">8</a>]</cite> aligns generative models to human choices via paired comparisons against a frozen reference policy. In diffusion settings<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib9\" title=\"\">9</a>]</cite>, a common practice is to assign preference at the final step and propagate it to intermediate latents, then optimize a DPO-style log-likelihood ratio at each step. This removes the need for an explicit reward model and is effective for global preferences, but offers limited guidance for temporally evolving signals.\nDPO has also been used to align large language model&#8211;based TTS. EmoDPO&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib10\" title=\"\">10</a>]</cite>, for instance, pairs utterances with identical text and marks the target emotion as preferred, achieving alignment without explicit reward modeling. However, attaching preference only at the endpoint yields sparse supervision for gradually varying cues, the assumption that all intermediate states on a preferred path are themselves preferred is often invalid, and curating domain-appropriate preference pairs is expensive. This leads to our central research question: <span class=\"ltx_text ltx_font_italic\">How can we provide dense, fine-grained supervision for emotionally expressive TTS generation, without relying on endpoint preferences or categorical labels?</span></p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contributions are: 1) We propose EASPO, a stepwise alignment framework that reformulates preference optimization as a local, time-conditioned task, replacing the flawed assumption that all intermediate states on a preferred trajectory are equally valid. By aligning win/lose candidates at each denoising step from a shared latent, it enables stepwise-controllable emotion shaping throughout the generation process. 2) We introduce EASPM, a time-aware reward model that directly scores emotional expressiveness and prosody on noisy intermediate states, enabling dense, temporally grounded preference learning and on-the-fly scoring.</p>\n\n",
                "matched_terms": [
                    "easpo",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a reinforcement learning framework for emotional TTS that fine-tunes diffusion models via stepwise preference supervision (Fig.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Starting from a pretrained Grad-TTS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib11\" title=\"\">11</a>]</cite>, our method introduces dense emotion-aligned rewards at each denoising step. At every latent state, multiple mel-spectrograms are sampled and ranked by a frozen emotion preference model(Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S2.SS1\" title=\"2.1 Emotion-Aware Stepwise Preference Model &#8227; 2 Method &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">2.1</span></a>). A preference pair is selected, and the model is optimized to favor the emotionally preferred sample by minimizing the gap between its advantage and the log-likelihood ratio(Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S2.SS2\" title=\"2.2 Objective Function of EASPO &#8227; 2 Method &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>). Our EASPO objective extends DPO with stepwise preference signals, enabling fine-grained emotional control during generation.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "easpo",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Discussion.</span>\nEq.&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S2.E7\" title=\"In 2.2 Objective Function of EASPO &#8227; 2 Method &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>) integrates stepwise preference optimization with reward-difference learning:\nEASPM provides a <em class=\"ltx_emph ltx_font_italic\">dense emotional reward</em> (Eq.&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S2.E4\" title=\"In 2.2 Objective Function of EASPO &#8227; 2 Method &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>)),\nand the diffusion policy is updated so that its <em class=\"ltx_emph ltx_font_italic\">log-likelihood ratio difference</em> between win/lose transitions (Eq.&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S2.E5\" title=\"In 2.2 Objective Function of EASPO &#8227; 2 Method &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>)) matches that reward difference (Eq.&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S2.E6\" title=\"In 2.2 Objective Function of EASPO &#8227; 2 Method &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>)).\nEmpirical results from&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib15\" title=\"\">15</a>]</cite> show that using reward differences yields more stable reward optimization in diffusion models than standard policy gradient methods.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "methods"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We initialize EASPM from CLEP. Audio is resampled to 16&#8201;kHz and cropped/padded to 5&#8201;s. The text encoder is frozen, while the audio encoder and projection head are trained using Adam (batch size 64, 80 epochs), with learning rates <math alttext=\"1\\!\\times\\!10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.052em\" rspace=\"0.052em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\!\\times\\!10^{-5}</annotation></semantics></math> and <math alttext=\"1\\!\\times\\!10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.052em\" rspace=\"0.052em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\!\\times\\!10^{-3}</annotation></semantics></math>, respectively. Preference supervision is derived from emotion-labeled recordings, marking the target emotion as preferred and another as dis-preferred. To make scoring step-aware, both waveforms in a pair are perturbed by identical diffusion noise at a sampled denoising step. Our base TTS model is Grad-TTS with 80-dim mel-spectrograms. We freeze the encoder and duration predictor and fine-tune only the decoder (score network), initialized from Grad-TTS pretraining settings (Adam, <math alttext=\"1\\!\\times\\!10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.052em\" rspace=\"0.052em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\!\\times\\!10^{-4}</annotation></semantics></math> LR, batch size 16, random 2&#8201;s mel segments). During EASPO, we apply step shuffling and candidate pooling. A denoising step <math alttext=\"\\tau\\sim\\mathcal{U}(1,T{-}\\kappa)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>&#8764;</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119984;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>,</mo><mrow><mi>T</mi><mo>&#8722;</mo><mi>&#954;</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau\\sim\\mathcal{U}(1,T{-}\\kappa)</annotation></semantics></math> is chosen (skipping noisy early steps), <math alttext=\"k{=}4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">k{=}4</annotation></semantics></math> candidates are sampled from <math alttext=\"p_{\\theta}(\\cdot\\mid x_{\\tau},c)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S3.SS1.p2.m6\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><msub><mi>x</mi><mi>&#964;</mi></msub><mo>,</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p_{\\theta}(\\cdot\\mid x_{\\tau},c)</annotation></semantics></math>, ranked by EASPM, and a win&#8211;lose pair is used to minimize the stepwise difference&#8211;matching loss between policy log-ratio difference and reward difference. Unless specified, we set <math alttext=\"\\kappa{=}0.25T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m7\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>=</mo><mrow><mn>0.25</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\kappa{=}0.25T</annotation></semantics></math>, batch size 32, and decoder learning rate <math alttext=\"1\\!\\times\\!10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m8\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.052em\" rspace=\"0.052em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\!\\times\\!10^{-5}</annotation></semantics></math>. Waveforms are synthesized using a pretrained HiFi-GAN vocoder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib17\" title=\"\">17</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "diffusion",
                    "easpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Objective metrics.</span>\n<span class=\"ltx_text ltx_font_italic\">Emo_SIM</span> quantifies emotional alignment as the average cosine similarity between emotion2vec-base embeddings of generated and reference utterances. <span class=\"ltx_text ltx_font_italic\">Prosod_SIM</span> is computed via AutoPCP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib18\" title=\"\">18</a>]</cite>, which compares utterance-level prosody (rhythm, stress, intonation). <span class=\"ltx_text ltx_font_italic\">WER</span> (word-error-rate) is calculated using Whisper Large-v3 transcripts.\nUTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib19\" title=\"\">19</a>]</cite> is employed to evaluate speech naturalness and perceptual quality.</p>\n\n",
                "matched_terms": [
                    "utmos",
                    "alignment",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate the effectiveness of our proposed emotionally preference-aligned method for TTS, we compare it with seven recent emotion-controllable baselines. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S3.T1\" title=\"Table 1 &#8227; 3.1 Datasets and Experimental Setup &#8227; 3 Experiments &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our method achieves superior performance across emotion similarity, prosody similarity, intelligibility (WER), and perceptual quality (UTMOS), with a notable 2.07% gain over CosyVoice in Emo_SIM. A similar trend is observed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S3.T2\" title=\"Table 2 &#8227; 3.1 Datasets and Experimental Setup &#8227; 3 Experiments &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, where our model consistently outperforms baselines in naturalness (MOS), emotional expressiveness (Emo_MOS), emotion consistency (MOS_EC), and emotion classification accuracy (Recall). These results demonstrate the effectiveness of our approach in generating emotionally aligned speech with enhanced coherence, while preserving natural prosody and speech quality. Demo page is available&#160;<a class=\"ltx_ref ltx_href\" href=\"https://jiachengqaq.github.io/emo-tts-demo/\" title=\"\">.</a></p>\n\n",
                "matched_terms": [
                    "utmos",
                    "method",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Timestep Range.</span>\nWe apply EASPO on a subset of denoising steps <math alttext=\"[0,\\kappa]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mi>&#954;</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0,\\kappa]</annotation></semantics></math> in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S3.T8\" title=\"Table 8 &#8227; 3.1 Datasets and Experimental Setup &#8227; 3 Experiments &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> and find that skipping noisy early steps improves alignment due to limited speech structure, while omitting fine-grained late steps weakens prosodic refinement. A mid-range window (e.g., <math alttext=\"[0,750]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>750</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0,750]</annotation></semantics></math>) balances diversity and emotional clarity, yielding optimal performance.</p>\n\n",
                "matched_terms": [
                    "easpo",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Comparison with other diffusion-based RL alignment methods.</span>\nWe compare EASPO to prior diffusion-based RL methods (DDPO, D3PO, Diff-DPO) in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S3.T8\" title=\"Table 8 &#8227; 3.1 Datasets and Experimental Setup &#8227; 3 Experiments &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, observing consistent metric gains that highlight EASPO&#8217;s strength in aligning fine-grained emotional and prosodic preferences.</p>\n\n",
                "matched_terms": [
                    "easpo",
                    "alignment",
                    "d3po",
                    "ddpo",
                    "other",
                    "methods",
                    "diffdpo",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present EASPO, a diffusion-based speech synthesis framework that introduces stepwise preference optimization for fine-grained emotional alignment. By leveraging an emotion-aware scoring model to compare candidate samples at each denoising step, EASPO progressively guides generation toward emotionally expressive and prosodically natural speech. This step-conditioned training strategy enables the model to capture subtle affective cues through contrastive supervision. Extensive experimental demonstrate the effectiveness across both objective and subjective evaluations.</p>\n\n",
                "matched_terms": [
                    "easpo",
                    "alignment"
                ]
            }
        ]
    },
    "S3.T8.fig6": {
        "caption": "Table 8: Comparison of win–lose pair selection strategies. Using candidates with highest and lowest emotional scores yields stronger contrast and better alignment than random sampling.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">win-lose sample</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">E-S</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">P-S</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">WER</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">UTMOS</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">best &amp; worst SPO</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">99.15</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">3.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">3.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">4.47</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">random</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">98.82</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.89</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">3.95</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:3.1pt;padding-right:3.1pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">4.36</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "sample",
            "utmos",
            "wer",
            "candidates",
            "yields",
            "stronger",
            "comparison",
            "sampling",
            "strategies",
            "emotional",
            "worst",
            "spo",
            "lowest",
            "winlose",
            "selection",
            "scores",
            "win–lose",
            "alignment",
            "than",
            "pair",
            "contrast",
            "random",
            "better",
            "highest",
            "best"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Emotional text-to-speech seeks to convey affect while preserving intelligibility and prosody, yet existing methods rely on coarse labels or proxy classifiers and receive only utterance-level feedback. We introduce Emotion-Aware Stepwise Preference Optimization (EASPO), a post-training framework that aligns diffusion TTS with fine-grained emotional preferences at intermediate denoising steps. Central to our approach is EASPM, a time-conditioned model that scores noisy intermediate speech states and enables automatic preference pair construction. EASPO optimizes generation to match these stepwise preferences, enabling controllable emotional shaping. Experiments show superior performance over existing methods in both expressiveness and naturalness.</p>\n\n",
                "matched_terms": [
                    "pair",
                    "emotional",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Direct Preference Optimization (DPO)<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib8\" title=\"\">8</a>]</cite> aligns generative models to human choices via paired comparisons against a frozen reference policy. In diffusion settings<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib9\" title=\"\">9</a>]</cite>, a common practice is to assign preference at the final step and propagate it to intermediate latents, then optimize a DPO-style log-likelihood ratio at each step. This removes the need for an explicit reward model and is effective for global preferences, but offers limited guidance for temporally evolving signals.\nDPO has also been used to align large language model&#8211;based TTS. EmoDPO&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib10\" title=\"\">10</a>]</cite>, for instance, pairs utterances with identical text and marks the target emotion as preferred, achieving alignment without explicit reward modeling. However, attaching preference only at the endpoint yields sparse supervision for gradually varying cues, the assumption that all intermediate states on a preferred path are themselves preferred is often invalid, and curating domain-appropriate preference pairs is expensive. This leads to our central research question: <span class=\"ltx_text ltx_font_italic\">How can we provide dense, fine-grained supervision for emotionally expressive TTS generation, without relying on endpoint preferences or categorical labels?</span></p>\n\n",
                "matched_terms": [
                    "yields",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better align diffusion-based TTS systems with fine-grained emotional preferences, we propose <span class=\"ltx_text ltx_font_italic\">Emotion-Aware Stepwise Preference Optimization</span> (EASPO). At each denoising step starting from a latent representation <math alttext=\"x_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m1\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">x_{t}</annotation></semantics></math>, the model samples a small candidate set of <math alttext=\"x_{t-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p4.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">x_{t-1}</annotation></semantics></math> mel-spectrograms. An <span class=\"ltx_text ltx_font_italic\">Emotion-aware Stepwise Preference Model</span> (EASPM) scores their emotional expressiveness and selects a win&#8211;lose pair that differs subtly in prosody while preserving linguistic content. One candidate is then randomly chosen to continue generation. Since these candidates originate from the same latent and differ by only a single denoising step, their variations are localized and emotion-focused. EASPM captures these nuanced differences and steers the model toward producing more emotionally consistent speech.</p>\n\n",
                "matched_terms": [
                    "win–lose",
                    "pair",
                    "emotional",
                    "better",
                    "candidates",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our contributions are: 1) We propose EASPO, a stepwise alignment framework that reformulates preference optimization as a local, time-conditioned task, replacing the flawed assumption that all intermediate states on a preferred trajectory are equally valid. By aligning win/lose candidates at each denoising step from a shared latent, it enables stepwise-controllable emotion shaping throughout the generation process. 2) We introduce EASPM, a time-aware reward model that directly scores emotional expressiveness and prosody on noisy intermediate states, enabling dense, temporally grounded preference learning and on-the-fly scoring.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "emotional",
                    "candidates",
                    "winlose",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present a reinforcement learning framework for emotional TTS that fine-tunes diffusion models via stepwise preference supervision (Fig.<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). Starting from a pretrained Grad-TTS<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib11\" title=\"\">11</a>]</cite>, our method introduces dense emotion-aligned rewards at each denoising step. At every latent state, multiple mel-spectrograms are sampled and ranked by a frozen emotion preference model(Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S2.SS1\" title=\"2.1 Emotion-Aware Stepwise Preference Model &#8227; 2 Method &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">2.1</span></a>). A preference pair is selected, and the model is optimized to favor the emotionally preferred sample by minimizing the gap between its advantage and the log-likelihood ratio(Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S2.SS2\" title=\"2.2 Objective Function of EASPO &#8227; 2 Method &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">2.2</span></a>). Our EASPO objective extends DPO with stepwise preference signals, enabling fine-grained emotional control during generation.</p>\n\n",
                "matched_terms": [
                    "sample",
                    "pair",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall.</span>\nAt reverse step <math alttext=\"t-1\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">t-1</annotation></semantics></math>, given the current latent <math alttext=\"x_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">x_{t}</annotation></semantics></math>, the generator draws <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> candidates\n<math alttext=\"\\{x_{t-1}^{1},\\ldots,x_{t-1}^{k}\\}\\sim p_{\\theta}(x_{t-1}\\mid x_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m4\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><mn>1</mn></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><mi>k</mi></msubsup><mo stretchy=\"false\">}</mo></mrow><mo>&#8764;</mo><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><mo>&#8739;</mo><msub><mi>x</mi><mi>t</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\{x_{t-1}^{1},\\ldots,x_{t-1}^{k}\\}\\sim p_{\\theta}(x_{t-1}\\mid x_{t})</annotation></semantics></math>. EASPM assigns each candidate a <em class=\"ltx_emph ltx_font_italic\">timestep-aware</em> emotion&#8211;prompt consistency score and induces a ranking; the highest and lowest scored items are taken as the win/lose pair.</p>\n\n",
                "matched_terms": [
                    "pair",
                    "highest",
                    "lowest",
                    "candidates",
                    "winlose"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For a pair <math alttext=\"(x_{t-1}^{w},x_{t-1}^{l})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><mi>w</mi></msubsup><mo>,</mo><msubsup><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><mi>l</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{t-1}^{w},x_{t-1}^{l})</annotation></semantics></math> with scores <math alttext=\"s_{w},s_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m6\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>w</mi></msub><mo>,</mo><msub><mi>s</mi><mi>l</mi></msub></mrow><annotation encoding=\"application/x-tex\">s_{w},s_{l}</annotation></semantics></math>, define <math alttext=\"\\Delta_{t}=s_{w}-s_{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m7\" intent=\":literal\"><semantics><mrow><msub><mi mathvariant=\"normal\">&#916;</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>s</mi><mi>w</mi></msub><mo>&#8722;</mo><msub><mi>s</mi><mi>l</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">\\Delta_{t}=s_{w}-s_{l}</annotation></semantics></math>.\nThe probability that the win item is preferred is modeled by a pairwise logistic with temperature <math alttext=\"\\tau&gt;0\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p2.m8\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">\\tau&gt;0</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "pair",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training.</span>\nEASPM is adapted from CLEP to handle noisy intermediate representations. Given a win&#8211;lose pair <math alttext=\"(x_{0}^{w},x_{0}^{l})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mn>0</mn><mi>w</mi></msubsup><mo>,</mo><msubsup><mi>x</mi><mn>0</mn><mi>l</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{0}^{w},x_{0}^{l})</annotation></semantics></math>, we sample a timestep <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m2\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> and apply the same forward diffusion to produce <math alttext=\"(x_{t}^{w},x_{t}^{l})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mi>t</mi><mi>w</mi></msubsup><mo>,</mo><msubsup><mi>x</mi><mi>t</mi><mi>l</mi></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{t}^{w},x_{t}^{l})</annotation></semantics></math>. This tuple <math alttext=\"(x_{t}^{w},x_{t}^{l},t,c)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mi>t</mi><mi>w</mi></msubsup><mo>,</mo><msubsup><mi>x</mi><mi>t</mi><mi>l</mi></msubsup><mo>,</mo><mi>t</mi><mo>,</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(x_{t}^{w},x_{t}^{l},t,c)</annotation></semantics></math> is used to optimize <math alttext=\"\\mathcal{L}_{\\text{pref}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m5\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8466;</mi><mtext>pref</mtext></msub><annotation encoding=\"application/x-tex\">\\mathcal{L}_{\\text{pref}}</annotation></semantics></math>, encouraging the model to recover the correct preference at step <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m6\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>. A time-aware normalization layer is added to CLEP&#8217;s audio branch for timestep conditioning. To reduce mismatch with CLEP&#8217;s pretraining domain, we optionally estimate a pseudo-clean <math alttext=\"\\hat{x}_{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m7\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>x</mi><mo>^</mo></mover><mn>0</mn></msub><annotation encoding=\"application/x-tex\">\\hat{x}_{0}</annotation></semantics></math> from <math alttext=\"x_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p3.m8\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">x_{t}</annotation></semantics></math> via deterministic inversion followed by&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib14\" title=\"\">14</a>]</cite> . After training, EASPM is frozen and used solely as a stepwise scorer for the RL objective.</p>\n\n",
                "matched_terms": [
                    "sample",
                    "win–lose",
                    "pair"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Random selection of the next state.</span>\nAfter EASPM ranks the candidate set <math alttext=\"\\{x_{t-1}^{1},\\ldots,x_{t-1}^{k}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><mn>1</mn></msubsup><mo>,</mo><mi mathvariant=\"normal\">&#8230;</mi><mo>,</mo><msubsup><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><mi>k</mi></msubsup><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{x_{t-1}^{1},\\ldots,x_{t-1}^{k}\\}</annotation></semantics></math> and selects a win&#8211;lose pair, we <em class=\"ltx_emph ltx_font_italic\">do not</em> continue with the top sample as shown in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. To avoid biased rollouts and degenerate paths, we uniformly sample the next state <math alttext=\"\\tilde{x}_{t-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m2\" intent=\":literal\"><semantics><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><annotation encoding=\"application/x-tex\">\\tilde{x}_{t-1}</annotation></semantics></math> from the pool and proceed with <math alttext=\"x_{t-2}\\sim p_{\\theta}(\\cdot\\mid\\tilde{x}_{t-1},c)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS1.p4.m3\" intent=\":literal\"><semantics><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>2</mn></mrow></msub><mo>&#8764;</mo><msub><mi>p</mi><mi>&#952;</mi></msub><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><msub><mover accent=\"true\"><mi>x</mi><mo>~</mo></mover><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><mo>,</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">x_{t-2}\\sim p_{\\theta}(\\cdot\\mid\\tilde{x}_{t-1},c)</annotation></semantics></math>. This ensures all preference pairs originate from the same latent <math alttext=\"x_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m4\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">x_{t}</annotation></semantics></math> while decoupling supervision from sampling. Candidate pooling is applied only when <math alttext=\"t\\leq\\kappa\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m5\" intent=\":literal\"><semantics><mrow><mi>t</mi><mo>&#8804;</mo><mi>&#954;</mi></mrow><annotation encoding=\"application/x-tex\">t\\leq\\kappa</annotation></semantics></math>; standard transitions are used for <math alttext=\"t&gt;\\kappa\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p4.m6\" intent=\":literal\"><semantics><mrow><mi>t</mi><mo>&gt;</mo><mi>&#954;</mi></mrow><annotation encoding=\"application/x-tex\">t&gt;\\kappa</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "sample",
                    "random",
                    "win–lose",
                    "sampling",
                    "pair",
                    "selection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We formulate denoising as a <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math>-step MDP with state <math alttext=\"s_{t}=(c,x_{t})\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo>,</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">s_{t}=(c,x_{t})</annotation></semantics></math>, action <math alttext=\"a_{t}=x_{t-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>=</mo><msub><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">a_{t}=x_{t-1}</annotation></semantics></math>, and policy\n<math alttext=\"\\pi_{\\theta}(a_{t}\\mid s_{t})=p_{\\theta}(x_{t-1}\\mid x_{t},c)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\"><semantics><mrow><mrow><msub><mi>&#960;</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>&#8739;</mo><msub><mi>s</mi><mi>t</mi></msub></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub><mo>&#8739;</mo><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><mi>c</mi></mrow></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\pi_{\\theta}(a_{t}\\mid s_{t})=p_{\\theta}(x_{t-1}\\mid x_{t},c)</annotation></semantics></math>.\nAt each step <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, we sample <math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> candidates <math alttext=\"\\{x_{t-1}^{i}\\}\\sim p_{\\theta}(\\cdot\\mid x_{t},c)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\"><semantics><mrow><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><mi>i</mi></msubsup><mo stretchy=\"false\">}</mo></mrow><mo>&#8764;</mo><msub><mi>p</mi><mi>&#952;</mi></msub><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\{x_{t-1}^{i}\\}\\sim p_{\\theta}(\\cdot\\mid x_{t},c)</annotation></semantics></math>\nand rank them via EASPM (Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S2.SS1\" title=\"2.1 Emotion-Aware Stepwise Preference Model &#8227; 2 Method &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">2.1</span></a>). Let <math alttext=\"x_{t-1}^{w}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m8\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><mi>w</mi></msubsup><annotation encoding=\"application/x-tex\">x_{t-1}^{w}</annotation></semantics></math> and <math alttext=\"x_{t-1}^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m9\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><mi>l</mi></msubsup><annotation encoding=\"application/x-tex\">x_{t-1}^{l}</annotation></semantics></math> denote the top and bottom-ranked samples from the <em class=\"ltx_emph ltx_font_italic\">same</em> latent <math alttext=\"x_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m10\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">x_{t}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "sample",
                    "candidates"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Final EASPO loss.</span>\nTo improve sample efficiency, we optimize at a randomly shuffled step <math alttext=\"\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p5.m1\" intent=\":literal\"><semantics><mi>&#964;</mi><annotation encoding=\"application/x-tex\">\\tau</annotation></semantics></math> and skip the first <math alttext=\"\\kappa\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p5.m2\" intent=\":literal\"><semantics><mi>&#954;</mi><annotation encoding=\"application/x-tex\">\\kappa</annotation></semantics></math> high-noise steps.\nAveraging over prompts <math alttext=\"c\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p5.m3\" intent=\":literal\"><semantics><mi>c</mi><annotation encoding=\"application/x-tex\">c</annotation></semantics></math>, initial noises <math alttext=\"x_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p5.m4\" intent=\":literal\"><semantics><msub><mi>x</mi><mi>T</mi></msub><annotation encoding=\"application/x-tex\">x_{T}</annotation></semantics></math>, and win/lose pairs from the policy gives</p>\n\n",
                "matched_terms": [
                    "sample",
                    "winlose"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Discussion.</span>\nEq.&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S2.E7\" title=\"In 2.2 Objective Function of EASPO &#8227; 2 Method &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>) integrates stepwise preference optimization with reward-difference learning:\nEASPM provides a <em class=\"ltx_emph ltx_font_italic\">dense emotional reward</em> (Eq.&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S2.E4\" title=\"In 2.2 Objective Function of EASPO &#8227; 2 Method &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>)),\nand the diffusion policy is updated so that its <em class=\"ltx_emph ltx_font_italic\">log-likelihood ratio difference</em> between win/lose transitions (Eq.&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S2.E5\" title=\"In 2.2 Objective Function of EASPO &#8227; 2 Method &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>)) matches that reward difference (Eq.&#160;(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S2.E6\" title=\"In 2.2 Objective Function of EASPO &#8227; 2 Method &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>)).\nEmpirical results from&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib15\" title=\"\">15</a>]</cite> show that using reward differences yields more stable reward optimization in diffusion models than standard policy gradient methods.</p>\n\n",
                "matched_terms": [
                    "than",
                    "winlose",
                    "yields",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We initialize EASPM from CLEP. Audio is resampled to 16&#8201;kHz and cropped/padded to 5&#8201;s. The text encoder is frozen, while the audio encoder and projection head are trained using Adam (batch size 64, 80 epochs), with learning rates <math alttext=\"1\\!\\times\\!10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.052em\" rspace=\"0.052em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\!\\times\\!10^{-5}</annotation></semantics></math> and <math alttext=\"1\\!\\times\\!10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.052em\" rspace=\"0.052em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\!\\times\\!10^{-3}</annotation></semantics></math>, respectively. Preference supervision is derived from emotion-labeled recordings, marking the target emotion as preferred and another as dis-preferred. To make scoring step-aware, both waveforms in a pair are perturbed by identical diffusion noise at a sampled denoising step. Our base TTS model is Grad-TTS with 80-dim mel-spectrograms. We freeze the encoder and duration predictor and fine-tune only the decoder (score network), initialized from Grad-TTS pretraining settings (Adam, <math alttext=\"1\\!\\times\\!10^{-4}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m3\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.052em\" rspace=\"0.052em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>4</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\!\\times\\!10^{-4}</annotation></semantics></math> LR, batch size 16, random 2&#8201;s mel segments). During EASPO, we apply step shuffling and candidate pooling. A denoising step <math alttext=\"\\tau\\sim\\mathcal{U}(1,T{-}\\kappa)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m4\" intent=\":literal\"><semantics><mrow><mi>&#964;</mi><mo>&#8764;</mo><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119984;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo>,</mo><mrow><mi>T</mi><mo>&#8722;</mo><mi>&#954;</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\tau\\sim\\mathcal{U}(1,T{-}\\kappa)</annotation></semantics></math> is chosen (skipping noisy early steps), <math alttext=\"k{=}4\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><mi>k</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">k{=}4</annotation></semantics></math> candidates are sampled from <math alttext=\"p_{\\theta}(\\cdot\\mid x_{\\tau},c)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S3.SS1.p2.m6\" intent=\":literal\"><semantics><mrow><msub><mi>p</mi><mi>&#952;</mi></msub><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo lspace=\"0em\" rspace=\"0.167em\">&#8739;</mo><msub><mi>x</mi><mi>&#964;</mi></msub><mo>,</mo><mi>c</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">p_{\\theta}(\\cdot\\mid x_{\\tau},c)</annotation></semantics></math>, ranked by EASPM, and a win&#8211;lose pair is used to minimize the stepwise difference&#8211;matching loss between policy log-ratio difference and reward difference. Unless specified, we set <math alttext=\"\\kappa{=}0.25T\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m7\" intent=\":literal\"><semantics><mrow><mi>&#954;</mi><mo>=</mo><mrow><mn>0.25</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></mrow><annotation encoding=\"application/x-tex\">\\kappa{=}0.25T</annotation></semantics></math>, batch size 32, and decoder learning rate <math alttext=\"1\\!\\times\\!10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m8\" intent=\":literal\"><semantics><mrow><mn>1</mn><mo lspace=\"0.052em\" rspace=\"0.052em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">1\\!\\times\\!10^{-5}</annotation></semantics></math>. Waveforms are synthesized using a pretrained HiFi-GAN vocoder&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib17\" title=\"\">17</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "candidates",
                    "win–lose",
                    "random",
                    "pair"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Objective metrics.</span>\n<span class=\"ltx_text ltx_font_italic\">Emo_SIM</span> quantifies emotional alignment as the average cosine similarity between emotion2vec-base embeddings of generated and reference utterances. <span class=\"ltx_text ltx_font_italic\">Prosod_SIM</span> is computed via AutoPCP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib18\" title=\"\">18</a>]</cite>, which compares utterance-level prosody (rhythm, stress, intonation). <span class=\"ltx_text ltx_font_italic\">WER</span> (word-error-rate) is calculated using Whisper Large-v3 transcripts.\nUTMOS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#bib.bib19\" title=\"\">19</a>]</cite> is employed to evaluate speech naturalness and perceptual quality.</p>\n\n",
                "matched_terms": [
                    "utmos",
                    "emotional",
                    "alignment",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To validate the effectiveness of our proposed emotionally preference-aligned method for TTS, we compare it with seven recent emotion-controllable baselines. As shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S3.T1\" title=\"Table 1 &#8227; 3.1 Datasets and Experimental Setup &#8227; 3 Experiments &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our method achieves superior performance across emotion similarity, prosody similarity, intelligibility (WER), and perceptual quality (UTMOS), with a notable 2.07% gain over CosyVoice in Emo_SIM. A similar trend is observed in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S3.T2\" title=\"Table 2 &#8227; 3.1 Datasets and Experimental Setup &#8227; 3 Experiments &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, where our model consistently outperforms baselines in naturalness (MOS), emotional expressiveness (Emo_MOS), emotion consistency (MOS_EC), and emotion classification accuracy (Recall). These results demonstrate the effectiveness of our approach in generating emotionally aligned speech with enhanced coherence, while preserving natural prosody and speech quality. Demo page is available&#160;<a class=\"ltx_ref ltx_href\" href=\"https://jiachengqaq.github.io/emo-tts-demo/\" title=\"\">.</a></p>\n\n",
                "matched_terms": [
                    "utmos",
                    "emotional",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Random Selection for Next Iteration Initialization.</span>\nWe compare random selection from the candidate pool with reusing the previous lose sample <math alttext=\"x_{t-1}^{l}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p2.m1\" intent=\":literal\"><semantics><msubsup><mi>x</mi><mrow><mi>t</mi><mo>&#8722;</mo><mn>1</mn></mrow><mi>l</mi></msubsup><annotation encoding=\"application/x-tex\">x_{t-1}^{l}</annotation></semantics></math> to initialize the next denoising step in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S3.T8\" title=\"Table 8 &#8227; 3.1 Datasets and Experimental Setup &#8227; 3 Experiments &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. Random selection consistently improves overall performance by avoiding bias toward dispreferred regions and encouraging trajectory diversity.</p>\n\n",
                "matched_terms": [
                    "sample",
                    "random",
                    "selection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Candidate Pool Size.</span>\nWe vary the number of candidates k at each step and observe its effect in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S3.T8\" title=\"Table 8 &#8227; 3.1 Datasets and Experimental Setup &#8227; 3 Experiments &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. A moderate k balances contrastive supervision and fidelity, improving the learning of emotional and prosodic preferences, while large k introduces artifacts that weaken supervision.</p>\n\n",
                "matched_terms": [
                    "candidates",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Impact of Timestep Range.</span>\nWe apply EASPO on a subset of denoising steps <math alttext=\"[0,\\kappa]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mi>&#954;</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0,\\kappa]</annotation></semantics></math> in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S3.T8\" title=\"Table 8 &#8227; 3.1 Datasets and Experimental Setup &#8227; 3 Experiments &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> and find that skipping noisy early steps improves alignment due to limited speech structure, while omitting fine-grained late steps weakens prosodic refinement. A mid-range window (e.g., <math alttext=\"[0,750]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p4.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>750</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0,750]</annotation></semantics></math>) balances diversity and emotional clarity, yielding optimal performance.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Comparison with other diffusion-based RL alignment methods.</span>\nWe compare EASPO to prior diffusion-based RL methods (DDPO, D3PO, Diff-DPO) in Tab.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S3.T8\" title=\"Table 8 &#8227; 3.1 Datasets and Experimental Setup &#8227; 3 Experiments &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>, observing consistent metric gains that highlight EASPO&#8217;s strength in aligning fine-grained emotional and prosodic preferences.</p>\n\n",
                "matched_terms": [
                    "alignment",
                    "emotional",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Choice of Win/Lose Pairs.</span>\nWe compare selecting the top&#8211;bottom EASPM-scored candidates versus randomly sampled pairs from the same latent state in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25416v1#S3.T8\" title=\"Table 8 &#8227; 3.1 Datasets and Experimental Setup &#8227; 3 Experiments &#8227; Emotion-Aligned Generation in Diffusion Text to Speech Models via Preference-Guided Optimization\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>. Using highest&#8211;lowest scoring samples ensures stronger emotional contrast while maintaining comparable content and noise levels, leading to more stable and informative supervision.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "contrast",
                    "candidates",
                    "winlose",
                    "stronger"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present EASPO, a diffusion-based speech synthesis framework that introduces stepwise preference optimization for fine-grained emotional alignment. By leveraging an emotion-aware scoring model to compare candidate samples at each denoising step, EASPO progressively guides generation toward emotionally expressive and prosodically natural speech. This step-conditioned training strategy enables the model to capture subtle affective cues through contrastive supervision. Extensive experimental demonstrate the effectiveness across both objective and subjective evaluations.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "alignment"
                ]
            }
        ]
    }
}