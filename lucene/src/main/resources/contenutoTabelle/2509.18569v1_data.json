{
    "S6.T1": {
        "caption": "Table 1: Comparison for ASR models with different reward rules and data.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\"><math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m1\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\"><math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m2\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"3\"><span class=\"ltx_text\" style=\"font-size:90%;\">Short</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"3\"><span class=\"ltx_text\" style=\"font-size:90%;\">Long</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">WER</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">Ins</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">Del</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">WER</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">Ins</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">Del</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.25</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.51</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.25</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.35</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.72</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.87</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math alttext=\"R^{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m3\" intent=\":literal\"><semantics><msup><mi mathsize=\"0.900em\">R</mi><mn mathsize=\"0.900em\">1</mn></msup><annotation encoding=\"application/x-tex\">R^{1}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math alttext=\"D^{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m4\" intent=\":literal\"><semantics><msup><mi mathsize=\"0.900em\">D</mi><mn mathsize=\"0.900em\">0</mn></msup><annotation encoding=\"application/x-tex\">D^{0}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.17</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.61</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.04</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.63</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.05</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.87</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math alttext=\"R^{1,2}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m5\" intent=\":literal\"><semantics><msup><mi mathsize=\"0.900em\">R</mi><mrow><mn mathsize=\"0.900em\">1</mn><mo mathsize=\"0.900em\">,</mo><mn mathsize=\"0.900em\">2</mn></mrow></msup><annotation encoding=\"application/x-tex\">R^{1,2}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math alttext=\"D^{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m6\" intent=\":literal\"><semantics><msup><mi mathsize=\"0.900em\">D</mi><mn mathsize=\"0.900em\">0</mn></msup><annotation encoding=\"application/x-tex\">D^{0}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.14</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.63</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.96</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.54</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.01</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.84</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math alttext=\"R^{1,2}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m7\" intent=\":literal\"><semantics><msup><mi mathsize=\"0.900em\">R</mi><mrow><mn mathsize=\"0.900em\">1</mn><mo mathsize=\"0.900em\">,</mo><mn mathsize=\"0.900em\">2</mn></mrow></msup><annotation encoding=\"application/x-tex\">R^{1,2}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math alttext=\"D^{0,2}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m8\" intent=\":literal\"><semantics><msup><mi mathsize=\"0.900em\">D</mi><mrow><mn mathsize=\"0.900em\">0</mn><mo mathsize=\"0.900em\">,</mo><mn mathsize=\"0.900em\">2</mn></mrow></msup><annotation encoding=\"application/x-tex\">D^{0,2}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.08</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.60</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.08</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.35</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.86</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.69</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math alttext=\"R^{1,2}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m9\" intent=\":literal\"><semantics><msup><mi mathsize=\"0.900em\">R</mi><mrow><mn mathsize=\"0.900em\">1</mn><mo mathsize=\"0.900em\">,</mo><mn mathsize=\"0.900em\">2</mn></mrow></msup><annotation encoding=\"application/x-tex\">R^{1,2}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math alttext=\"D^{1,2}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m10\" intent=\":literal\"><semantics><msup><mi mathsize=\"0.900em\">D</mi><mrow><mn mathsize=\"0.900em\">1</mn><mo mathsize=\"0.900em\">,</mo><mn mathsize=\"0.900em\">2</mn></mrow></msup><annotation encoding=\"application/x-tex\">D^{1,2}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">10.01</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.67</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.87</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.25</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.69</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.82</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\"><math alttext=\"R^{all}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m11\" intent=\":literal\"><semantics><msup><mi mathsize=\"0.900em\">R</mi><mrow><mi mathsize=\"0.900em\">a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathsize=\"0.900em\">l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathsize=\"0.900em\">l</mi></mrow></msup><annotation encoding=\"application/x-tex\">R^{all}</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\"><math alttext=\"D^{all}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T1.m12\" intent=\":literal\"><semantics><msup><mi mathsize=\"0.900em\">D</mi><mrow><mi mathsize=\"0.900em\">a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathsize=\"0.900em\">l</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi mathsize=\"0.900em\">l</mi></mrow></msup><annotation encoding=\"application/x-tex\">D^{all}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.71</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.61</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.68</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.03</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.86</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.75</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "wer",
            "d12d12",
            "comparison",
            "del",
            "r12r12",
            "ra​l​lrall",
            "short",
            "long",
            "rules",
            "reward",
            "asr",
            "da​l​ldall",
            "ins",
            "d0d0",
            "data",
            "r1r1",
            "d02d02",
            "different"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We present the ASR experimental results in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#S6.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 6.2.2 Experiments Reults &#8227; 6.2 Experiments on ASR &#8227; 6 Experiments &#8227; Explore the Reinforcement Learning for the LLM based ASR and TTS system\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. From the table, we observe that RL achieves a relative improvement of 5.3% in WER on both short and long speech segments. Moreover, the design of training data and reward functions plays a crucial role in RL effectiveness, with distinct effects observed on the short and long evaluation sets.\nFor short audios, all data construction strategies and reward designs improve ASR performance, and combining them yields the best overall result.\nHowever, for long audios, we observe that even the baseline model exhibits significantly higher insertion (Ins) errors compared to deletion (Del) errors, indicating a tendency toward hallucination&#8212;generating words not present in the input. Simply including more long audio samples in training or adding hallucination detection mechanisms does not effectively address this issue. In contrast, using hard training samples </span>\n  <math alttext=\"D^{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">D</mi>\n        <mn mathsize=\"0.900em\">1</mn>\n      </msup>\n      <annotation encoding=\"application/x-tex\">D^{1}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> proves beneficial, suggesting that RL training data should be carefully constructed based on the failure patterns of the base model.\nFurthermore, keyword-related data and reward design are critical for mitigating hallucinations, as they impose stronger penalties than standard WER-based rewards when the model generates non-existent or irrelevant words.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In recent years, large language models (LLMs) have played an important role in automatic speech recognition (ASR) and text-to-speech (TTS) systems. While reinforcement learning (RL) has significantly enhanced LLM performance in text-based tasks, its application to ASR and TTS remains underexplored due to the complexity of training audio-based models.\nIn this study, we propose a lightweight RL framework tailored for audio-based LLMs that can process audio inputs and generate audio outputs. Based on this framework, we evaluate the effectiveness of reinforcement learning on both ASR and TTS tasks.\nFor the ASR task, we experiment with different rule-based reward functions within the Group Relative Policy Optimization (GRPO) framework and investigate the impact of RL data construction.\nFor the TTS task, we compare GRPO with Differentiable Reward Optimization (DiffRO) and further combine the two approaches to achieve improved performance.\nOur experiments demonstrate that RL can significantly enhance the performance of both ASR and TTS systems, even with limited training data and a small number of optimization steps.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "reward",
                    "data",
                    "asr",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Index Terms</span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Large language models, reinforcement learning, ASR, TTS</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The development of large language models (LLMs) has significantly influenced the speech domain, including both automatic speech recognition (ASR) and text-to-speech (TTS). Most recently published ASR and TTS systems are now based on LLMs, benefiting from scaling in data and model size. These systems demonstrate substantial advantages over traditional small neural network models.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "data",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In LLM training, reinforcement learning (RL) is a critical step that aligns models with human preferences and enhances their reasoning capabilities </span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#bib.bib1\" title=\"\">1</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. During RL, the model generates responses to a given input, and these responses are evaluated using either a learned reward model or handcrafted rules. The policy is then optimized to maximize the expected reward.\nCompared to supervised fine-tuning (SFT), RL directly shapes the model&#8217;s outputs to satisfy predefined rules or human preferences, rather than merely maximizing the posterior probability of the training labels.</span>\n</p>\n\n",
                "matched_terms": [
                    "rules",
                    "models",
                    "reward"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Several studies have applied RL to audio-based LLM systems, including ASR and TTS. Among recent audio LLM-based systems, Step-Audio2</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> investigates the effectiveness of RL by employing both PPO</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and GRPO</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to improve performance. For ASR, Seed-ASR</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> demonstrates that minimum word error rate (MWER)</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> training can slightly reduce WER and improve the F1 score on hard cases. Similarly, Seed-TTS</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows that REINFORCE, PPO, and DPO</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> can enhance TTS performance in terms of speaker similarity and WER. In CosyVoice3</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, differentiable reward optimization (DiffRO)</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is used to align the discrete tokens predicted by the TTS system with the preferences of a multi-task reward model, resulting in significant improvements in WER&#8212;indicating that RL is an essential component in TTS training. Moreover, some works have shown that RL can also improve a model&#8217;s ability to express emotion and follow instructions.</span>\n</p>\n\n",
                "matched_terms": [
                    "reward",
                    "asr",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">However, compared to text and video modalities, RL research for audio LLMs remains limited. Most existing studies only report which RL methods were used, often without detailed effectiveness analysis or ablation studies on key aspects such as reward rule design, reward model training, or RL data construction. Furthermore, compared to video LLMs, audio LLMs are more flexible and complex: they can process both discrete acoustic tokens and continuous embeddings as input, and generate either acoustic tokens or text tokens as output.</span>\n</p>\n\n",
                "matched_terms": [
                    "reward",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we conduct a detailed investigation into the effectiveness of RL in LLM-based ASR and TTS systems. We first design a lightweight RL framework for audio LLMs that can take audio embeddings as input and efficiently generate text tokens or synthesize waveforms as output. For ASR, we compare traditional MWER with GRPO under different reward functions and RL data construction strategies. For TTS, we compare GRPO with DiffRO and further combine the two methods to achieve additional performance gains.\nExperiments show that each RL method contributes to performance improvement, even with a small amount of training data and few optimization steps. Crucially, reward design and RL data construction significantly influence the model&#8217;s output preferences, ultimately affecting user experience.</span>\n</p>\n\n",
                "matched_terms": [
                    "reward",
                    "data",
                    "different",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">DiffRO is a recent published RL algorithm whch is desgined for the LLM based TTS system. Compare the other RL method, DiffRO can directly predict the reward from the generated token rather than the synthetic audio, and directly optimize the TTS system with the gradient back-propagated from the reward model.\nIt first use the GumbelSoftmax to sample the output token </span>\n  <math alttext=\"\\tilde{o}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">o</mi>\n          <mo mathsize=\"0.900em\">~</mo>\n        </mover>\n        <mi mathsize=\"0.900em\">t</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\tilde{o}_{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> predicted from the LLM, and then feed the sampled result into a token-based ASR system, then the ASR system calculate the posterior probability of the input text </span>\n  <math alttext=\"y_{1:N}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">y</mi>\n        <mrow>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n          <mi mathsize=\"0.900em\">N</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">y_{1:N}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and then use this probability </span>\n  <math alttext=\"P_{ASR}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">P</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">A</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">S</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">R</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">P_{ASR}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as reward to optimized the TTS model by back-propagation to maximin the posterior probability.</span>\n</p>\n\n",
                "matched_terms": [
                    "reward",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Besides the ASR reward, DiffRO can also use other downstream tasks to guide the TTS model training like speaker emotion and speech quality.</span>\n</p>\n\n",
                "matched_terms": [
                    "reward",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Unlike vanilla LLMs, audio LLMs typically include an audio encoder (for ASR) or decoder (for TTS). These frontend or backend modules add further complexity to the training pipeline and inter-process communication. To simplify the management of computational resources, we design a RL training framework that alternately allocates GPU resources among different components. The overall framework is illustrated in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#S2.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 2.2 Differentiable Reward Optimization (DiffRO) &#8227; 2 Related Work &#8227; Explore the Reinforcement Learning for the LLM based ASR and TTS system\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "different",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For ASR training, a vanilla PyTorch-based audio encoder first occupies the GPU. It processes all input audio samples in a single batch, extracting audio embeddings in parallel. Once completed, it releases the GPU resources and transfers the embeddings to main storage.\nNext, an SGLang-based LLM rollout engine takes control of the GPU to generate multiple groups of hypotheses based on the audio embeddings and instruction text tokens. Each hypothesis is assigned a reward according to predefined rules for advantage computation.\nFinally, an FSDP-based LLM policy model uses the audio embeddings and generated hypotheses to compute output probabilities and performs policy optimization. After each update, the updated policy is synchronized back to the rollout engine to ensure on-policy training.</span>\n</p>\n\n",
                "matched_terms": [
                    "rules",
                    "reward",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Although the LLM based ASR can realize a good results in most case, but it can could change the keyword or fall in hallucinations which serious impact on the user experience. To address these hard case samples, we design some rule based value functions </span>\n  <math alttext=\"\\{R^{k}(y_{i}^{*},y_{i})\\}_{k=1}^{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <mrow>\n            <msup>\n              <mi mathsize=\"0.900em\">R</mi>\n              <mi mathsize=\"0.900em\">k</mi>\n            </msup>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n              <msubsup>\n                <mi mathsize=\"0.900em\">y</mi>\n                <mi mathsize=\"0.900em\">i</mi>\n                <mo mathsize=\"0.900em\">&#8727;</mo>\n              </msubsup>\n              <mo mathsize=\"0.900em\">,</mo>\n              <msub>\n                <mi mathsize=\"0.900em\">y</mi>\n                <mi mathsize=\"0.900em\">i</mi>\n              </msub>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n            </mrow>\n          </mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">k</mi>\n          <mo mathsize=\"0.900em\">=</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n        </mrow>\n        <mi mathsize=\"0.900em\">K</mi>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\{R^{k}(y_{i}^{*},y_{i})\\}_{k=1}^{K}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for the GRPO based RL to improve the ASR performance more than reduce word error rate (WER).</span>\n</p>\n\n",
                "matched_terms": [
                    "asr",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <math alttext=\"R^{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mn mathsize=\"0.900em\">1</mn>\n      </msup>\n      <annotation encoding=\"application/x-tex\">R^{1}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: ASR Accuracy. To improve the ASR performance, we use the </span>\n  <math alttext=\"1-\\text{WER}(y^{*},y)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo mathsize=\"0.900em\">&#8722;</mo>\n        <mrow>\n          <mtext mathsize=\"0.900em\">WER</mtext>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <msup>\n              <mi mathsize=\"0.900em\">y</mi>\n              <mo mathsize=\"0.900em\">&#8727;</mo>\n            </msup>\n            <mo mathsize=\"0.900em\">,</mo>\n            <mi mathsize=\"0.900em\">y</mi>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1-\\text{WER}(y^{*},y)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as the basic value function.</span>\n</p>\n\n",
                "matched_terms": [
                    "r1r1",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mn mathsize=\"0.900em\">2</mn>\n      </msup>\n      <annotation encoding=\"application/x-tex\">R^{2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Hallucination Detection. Hallucination is a common problem of the LLM-based ASR system and will be more serious on noisy data; it could generate repeated, inexistent, and translated results. We detach these hallucination by some rules and it occurred, the final reward will be set to -1.</span>\n</p>\n\n",
                "matched_terms": [
                    "rules",
                    "reward",
                    "data",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Addressing practical issues in application scenarios, we build a small but high quality training dataset for the RL with the approach. We first collect the hard and hallucination-related samples </span>\n  <math alttext=\"D^{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">D</mi>\n        <mn mathsize=\"0.900em\">1</mn>\n      </msup>\n      <annotation encoding=\"application/x-tex\">D^{1}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> by compare transcription between different ASR systems, choosing the audios whose outputs are different or contain long repetitions. We also select audio segments longer than 20 seconds which are limited in the training dataset as </span>\n  <math alttext=\"D^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">D</mi>\n        <mn mathsize=\"0.900em\">2</mn>\n      </msup>\n      <annotation encoding=\"application/x-tex\">D^{2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For </span>\n  <math alttext=\"D^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">D</mi>\n        <mn mathsize=\"0.900em\">3</mn>\n      </msup>\n      <annotation encoding=\"application/x-tex\">D^{3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we choose the speech contains name, brand and other specialized vocabulary as the keywords samples. Finally, we also construct a random selection set </span>\n  <math alttext=\"D^{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">D</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msup>\n      <annotation encoding=\"application/x-tex\">D^{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as a control.</span>\n</p>\n\n",
                "matched_terms": [
                    "different",
                    "long",
                    "asr",
                    "d0d0"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Previous works has shown that the DiffRO can significant improve the pronunciation accuracy of the TTS system and the speech attribute control with different downstream task. However, it relay on a differentiable neutral network based reward model to compute the reward and optimize the TTS model. But for some subjective experience, it is difficult to train the reward model like the sound expressiveness sound, rhythm richness and audio duration.\nGRPO can measure a part of the subjective experience by some rules which means that the GRPO and the DiffRO can combine with each other for a better performance.</span>\n</p>\n\n",
                "matched_terms": [
                    "rules",
                    "reward",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Similarly to ASR, we also define some rules </span>\n  <math alttext=\"\\{R^{k}\\}_{k=1}^{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <msup>\n            <mi mathsize=\"0.900em\">R</mi>\n            <mi mathsize=\"0.900em\">k</mi>\n          </msup>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">k</mi>\n          <mo mathsize=\"0.900em\">=</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n        </mrow>\n        <mi mathsize=\"0.900em\">K</mi>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\{R^{k}\\}_{k=1}^{K}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to compute the rewards of the responses. Aseptically, some reward value can be computed according to the acoustic token, while some rules can be only applied on the waveform.</span>\n</p>\n\n",
                "matched_terms": [
                    "rules",
                    "reward",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <math alttext=\"R^{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.I1.i1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mn mathsize=\"0.900em\">1</mn>\n      </msup>\n      <annotation encoding=\"application/x-tex\">R^{1}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Recognition Accuracy of the ASR system. We can directly use the ASR recognition accuracy as the reward to prevent the error pronunciation. The accuracy can be calculated by a regular ASR from the synthesized audio or a token based ASR model like DiffRO.</span>\n</p>\n\n",
                "matched_terms": [
                    "reward",
                    "r1r1",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.I1.i2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mn mathsize=\"0.900em\">2</mn>\n      </msup>\n      <annotation encoding=\"application/x-tex\">R^{2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Audio Duration. Some works has find that during RL, the TTS system tend to slow the speech speed for better ASR result. To prevent this problem, we use the difference from median audio length in the response group as reward:</span>\n</p>\n\n",
                "matched_terms": [
                    "reward",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The ASR RL experiments are based on FunAudio-ASR. For training data construction, we collect the training set as described in Section&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#S4.SS2\" style=\"font-size:90%;\" title=\"4.2 Training Data Collection &#8227; 4 GRPO for ASR &#8227; Explore the Reinforcement Learning for the LLM based ASR and TTS system\">\n    <span class=\"ltx_text ltx_ref_tag\">4.2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, with each subset </span>\n  <math alttext=\"\\{D_{i}\\}_{i=0}^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">D</mi>\n            <mi mathsize=\"0.900em\">i</mi>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">i</mi>\n          <mo mathsize=\"0.900em\">=</mo>\n          <mn mathsize=\"0.900em\">0</mn>\n        </mrow>\n        <mn mathsize=\"0.900em\">3</mn>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\{D_{i}\\}_{i=0}^{3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> contributing 20k utterances.\nFor the training configuration, the batch size is set to 32 and the group sample size to 12. The LLM policy is trained online with a learning rate of 0.00001, and the KL divergence coefficient between the policy and the reference model is set to 0.1. With this setup, the training process can be completed within one day.\nFor evaluation, we use two in-house industrial test sets containing short (less than 10s) and long (longer than 20s) audio samples. To better analyze the hallucination problem, we report not only the overall WER but also insertion (Ins) and deletion (Del) error rates.</span>\n</p>\n\n",
                "matched_terms": [
                    "del",
                    "ins",
                    "wer",
                    "short",
                    "data",
                    "long",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For the TTS experiments, we use the open-source CosyVoice2-0.5B as the base model and construct the training set using text data from CommonVoice zh and en.\nWe compare DiffRO and GRPO under various reward settings and explore different strategies for combining them. The reward model follows the setup in&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and the KL divergence coefficient is set to 0.1.\nAll experiments are conducted with a batch size of 16. When GRPO is used, the group sample size is set to 8 and the sampling temperature is 1.0.\nEvaluation is performed on the CV3-Eval</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> test set.</span>\n</p>\n\n",
                "matched_terms": [
                    "reward",
                    "data",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We show the experiments results in Tabel </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#S6.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 6.3 Experiments on TTS &#8227; 6 Experiments &#8227; Explore the Reinforcement Learning for the LLM based ASR and TTS system\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and can find that most RL method can improvement the WER but slightly harm the speaker similarity (SS). This is reasonable as the CosyVoice2 only use the semantic information in the LLM and the RL reward also focus on the semantic and rhythm.\nAnd when only consider the WER, the DiffRO can be better than the GRPO, but the SS reduction is also larger. And if we directly combine them together, the result can become even worse. However, sample filter based combination can fix this problem and realize a much better result. Because we find that when sample a response group with high temperature, some response could contains many repeated token can not predict the stop token. Directly compute the differentiable reward loss on these bad sequence can be harmful. So filter these bad case can make GRPO and DiffRO more compatible.\nBased on the sample filter combination, we explore more reward function in the GRPO, and find that the audio duration reward can further improve the performance, and it can prevent the speech speed come slower. The diversity can not improve the objective WER and SS result, but for the subjective evaluation, it achieve the best result than others.</span>\n</p>\n\n",
                "matched_terms": [
                    "reward",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Besides the performance, we also compare the training stability between different RL method and show the results in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#S6.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 6.3 Experiments on TTS &#8227; 6 Experiments &#8227; Explore the Reinforcement Learning for the LLM based ASR and TTS system\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We can find that at the first 1000 steps, all of the RL models can achieve a better results than the baseline. But rapid deterioration will occur for the GRPO and the Combined w/o sample filter when the training step is larger than 1500. This means that the DiffRO can be more stable with longer training steps.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This study explore the effect of RL for audio-based LLM including both ASR and TTS.\nWe first propose an alternate GPU utilization framework which show great training efficiency based on audio input and output.\nFor ASR, the study explores various rule-based reward functions using GRPO and investigates the impact of RL data construction.\nIn TTS, it compares GRPO with DiffRO and demonstrates that combining both methods improves performance. Results show that RL significantly enhances both ASR and TTS systems, with reward design playing a crucial role in shaping model output preferences and user experience.\nThis work highlights the potential of RL in audio-domain LLMs and calls for better-supported RL frameworks for audio applications.</span>\n</p>\n\n",
                "matched_terms": [
                    "reward",
                    "data",
                    "asr"
                ]
            }
        ]
    },
    "S6.T2": {
        "caption": "Table 2: Accuracy comparison for Audios synthesized from Different TTS system.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">Method</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">Reward</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">zh</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">en</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">WER</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">SS</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">WER</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text\" style=\"font-size:90%;\">SS</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.280</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">77.64</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.074</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">70.87</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">DiffRO</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math alttext=\"R^{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m1\" intent=\":literal\"><semantics><msup><mi mathsize=\"0.900em\">R</mi><mn mathsize=\"0.900em\">1</mn></msup><annotation encoding=\"application/x-tex\">R^{1}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.418</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">77.00</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.690</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">70.01</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">GRPO</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math alttext=\"R^{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m2\" intent=\":literal\"><semantics><msup><mi mathsize=\"0.900em\">R</mi><mn mathsize=\"0.900em\">1</mn></msup><annotation encoding=\"application/x-tex\">R^{1}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.710</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">77.26</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.974</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">70.26</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">Combined</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math alttext=\"R^{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m3\" intent=\":literal\"><semantics><msup><mi mathsize=\"0.900em\">R</mi><mn mathsize=\"0.900em\">1</mn></msup><annotation encoding=\"application/x-tex\">R^{1}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.782</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">77.04</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.239</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">70.13</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">+ Filter</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math alttext=\"R^{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m4\" intent=\":literal\"><semantics><msup><mi mathsize=\"0.900em\">R</mi><mn mathsize=\"0.900em\">1</mn></msup><annotation encoding=\"application/x-tex\">R^{1}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.381</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">76.73</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.401</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">69.97</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span class=\"ltx_text\" style=\"font-size:90%;\">+ Filter</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><math alttext=\"R^{1,2}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m5\" intent=\":literal\"><semantics><msup><mi mathsize=\"0.900em\">R</mi><mrow><mn mathsize=\"0.900em\">1</mn><mo mathsize=\"0.900em\">,</mo><mn mathsize=\"0.900em\">2</mn></mrow></msup><annotation encoding=\"application/x-tex\">R^{1,2}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.330</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">77.37</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.279</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">70.41</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">+ Filter</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\"><math alttext=\"R^{1,2,3}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T2.m6\" intent=\":literal\"><semantics><msup><mi mathsize=\"0.900em\">R</mi><mrow><mn mathsize=\"0.900em\">1</mn><mo mathsize=\"0.900em\">,</mo><mn mathsize=\"0.900em\">2</mn><mo mathsize=\"0.900em\">,</mo><mn mathsize=\"0.900em\">3</mn></mrow></msup><annotation encoding=\"application/x-tex\">R^{1,2,3}</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.414</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">77.11</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.579</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text\" style=\"font-size:90%;\">70.05</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "combined",
            "filter",
            "tts",
            "wer",
            "reward",
            "audios",
            "synthesized",
            "diffro",
            "r12r12",
            "r123r123",
            "from",
            "r1r1",
            "method",
            "system",
            "accuracy",
            "different",
            "comparison",
            "grpo"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We show the experiments results in Tabel </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#S6.T2\" style=\"font-size:90%;\" title=\"Table 2 &#8227; 6.3 Experiments on TTS &#8227; 6 Experiments &#8227; Explore the Reinforcement Learning for the LLM based ASR and TTS system\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and can find that most RL method can improvement the WER but slightly harm the speaker similarity (SS). This is reasonable as the CosyVoice2 only use the semantic information in the LLM and the RL reward also focus on the semantic and rhythm.\nAnd when only consider the WER, the DiffRO can be better than the GRPO, but the SS reduction is also larger. And if we directly combine them together, the result can become even worse. However, sample filter based combination can fix this problem and realize a much better result. Because we find that when sample a response group with high temperature, some response could contains many repeated token can not predict the stop token. Directly compute the differentiable reward loss on these bad sequence can be harmful. So filter these bad case can make GRPO and DiffRO more compatible.\nBased on the sample filter combination, we explore more reward function in the GRPO, and find that the audio duration reward can further improve the performance, and it can prevent the speech speed come slower. The diversity can not improve the objective WER and SS result, but for the subjective evaluation, it achieve the best result than others.</span>\n</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In recent years, large language models (LLMs) have played an important role in automatic speech recognition (ASR) and text-to-speech (TTS) systems. While reinforcement learning (RL) has significantly enhanced LLM performance in text-based tasks, its application to ASR and TTS remains underexplored due to the complexity of training audio-based models.\nIn this study, we propose a lightweight RL framework tailored for audio-based LLMs that can process audio inputs and generate audio outputs. Based on this framework, we evaluate the effectiveness of reinforcement learning on both ASR and TTS tasks.\nFor the ASR task, we experiment with different rule-based reward functions within the Group Relative Policy Optimization (GRPO) framework and investigate the impact of RL data construction.\nFor the TTS task, we compare GRPO with Differentiable Reward Optimization (DiffRO) and further combine the two approaches to achieve improved performance.\nOur experiments demonstrate that RL can significantly enhance the performance of both ASR and TTS systems, even with limited training data and a small number of optimization steps.</span>\n</p>\n\n",
                "matched_terms": [
                    "tts",
                    "reward",
                    "diffro",
                    "different",
                    "grpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">The development of large language models (LLMs) has significantly influenced the speech domain, including both automatic speech recognition (ASR) and text-to-speech (TTS). Most recently published ASR and TTS systems are now based on LLMs, benefiting from scaling in data and model size. These systems demonstrate substantial advantages over traditional small neural network models.</span>\n</p>\n\n",
                "matched_terms": [
                    "from",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Several studies have applied RL to audio-based LLM systems, including ASR and TTS. Among recent audio LLM-based systems, Step-Audio2</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#bib.bib2\" title=\"\">2</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> investigates the effectiveness of RL by employing both PPO</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#bib.bib3\" title=\"\">3</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and GRPO</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#bib.bib4\" title=\"\">4</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> to improve performance. For ASR, Seed-ASR</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#bib.bib5\" title=\"\">5</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> demonstrates that minimum word error rate (MWER)</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#bib.bib6\" title=\"\">6</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> training can slightly reduce WER and improve the F1 score on hard cases. Similarly, Seed-TTS</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#bib.bib7\" title=\"\">7</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> shows that REINFORCE, PPO, and DPO</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#bib.bib8\" title=\"\">8</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> can enhance TTS performance in terms of speaker similarity and WER. In CosyVoice3</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, differentiable reward optimization (DiffRO)</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> is used to align the discrete tokens predicted by the TTS system with the preferences of a multi-task reward model, resulting in significant improvements in WER&#8212;indicating that RL is an essential component in TTS training. Moreover, some works have shown that RL can also improve a model&#8217;s ability to express emotion and follow instructions.</span>\n</p>\n\n",
                "matched_terms": [
                    "system",
                    "reward",
                    "tts",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">In this paper, we conduct a detailed investigation into the effectiveness of RL in LLM-based ASR and TTS systems. We first design a lightweight RL framework for audio LLMs that can take audio embeddings as input and efficiently generate text tokens or synthesize waveforms as output. For ASR, we compare traditional MWER with GRPO under different reward functions and RL data construction strategies. For TTS, we compare GRPO with DiffRO and further combine the two methods to achieve additional performance gains.\nExperiments show that each RL method contributes to performance improvement, even with a small amount of training data and few optimization steps. Crucially, reward design and RL data construction significantly influence the model&#8217;s output preferences, ultimately affecting user experience.</span>\n</p>\n\n",
                "matched_terms": [
                    "tts",
                    "reward",
                    "diffro",
                    "method",
                    "different",
                    "grpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">GRPO is a lightweight and effective policy-based method that has realized great success for the LLM post-training. Unlike other RL algorithms like PPO, GRPO eliminates a group of response </span>\n  <math alttext=\"\\{o_{i}\\}_{i=1}^{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">o</mi>\n            <mi mathsize=\"0.900em\">i</mi>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">i</mi>\n          <mo mathsize=\"0.900em\">=</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n        </mrow>\n        <mi mathsize=\"0.900em\">G</mi>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\{o_{i}\\}_{i=1}^{G}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> with some rule-based value function to get the reward </span>\n  <math alttext=\"\\{R_{i}\\}_{i=1}^{G}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <msub>\n            <mi mathsize=\"0.900em\">R</mi>\n            <mi mathsize=\"0.900em\">i</mi>\n          </msub>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">i</mi>\n          <mo mathsize=\"0.900em\">=</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n        </mrow>\n        <mi mathsize=\"0.900em\">G</mi>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\{R_{i}\\}_{i=1}^{G}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and then normalize the group-level rewards to get the advantage </span>\n  <math alttext=\"\\hat{A}_{i,t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS1.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">A</mi>\n          <mo mathsize=\"0.900em\">^</mo>\n        </mover>\n        <mrow>\n          <mi mathsize=\"0.900em\">i</mi>\n          <mo mathsize=\"0.900em\">,</mo>\n          <mi mathsize=\"0.900em\">t</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\hat{A}_{i,t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">:</span>\n</p>\n\n",
                "matched_terms": [
                    "reward",
                    "method",
                    "grpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">DiffRO is a recent published RL algorithm whch is desgined for the LLM based TTS system. Compare the other RL method, DiffRO can directly predict the reward from the generated token rather than the synthetic audio, and directly optimize the TTS system with the gradient back-propagated from the reward model.\nIt first use the GumbelSoftmax to sample the output token </span>\n  <math alttext=\"\\tilde{o}_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mover accent=\"true\">\n          <mi mathsize=\"0.900em\">o</mi>\n          <mo mathsize=\"0.900em\">~</mo>\n        </mover>\n        <mi mathsize=\"0.900em\">t</mi>\n      </msub>\n      <annotation encoding=\"application/x-tex\">\\tilde{o}_{t}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> predicted from the LLM, and then feed the sampled result into a token-based ASR system, then the ASR system calculate the posterior probability of the input text </span>\n  <math alttext=\"y_{1:N}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">y</mi>\n        <mrow>\n          <mn mathsize=\"0.900em\">1</mn>\n          <mo lspace=\"0.278em\" mathsize=\"0.900em\" rspace=\"0.278em\">:</mo>\n          <mi mathsize=\"0.900em\">N</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">y_{1:N}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and then use this probability </span>\n  <math alttext=\"P_{ASR}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msub>\n        <mi mathsize=\"0.900em\">P</mi>\n        <mrow>\n          <mi mathsize=\"0.900em\">A</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">S</mi>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mi mathsize=\"0.900em\">R</mi>\n        </mrow>\n      </msub>\n      <annotation encoding=\"application/x-tex\">P_{ASR}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as reward to optimized the TTS model by back-propagation to maximin the posterior probability.</span>\n</p>\n\n",
                "matched_terms": [
                    "tts",
                    "reward",
                    "diffro",
                    "from",
                    "method",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Besides the ASR reward, DiffRO can also use other downstream tasks to guide the TTS model training like speaker emotion and speech quality.</span>\n</p>\n\n",
                "matched_terms": [
                    "reward",
                    "diffro",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Unlike vanilla LLMs, audio LLMs typically include an audio encoder (for ASR) or decoder (for TTS). These frontend or backend modules add further complexity to the training pipeline and inter-process communication. To simplify the management of computational resources, we design a RL training framework that alternately allocates GPU resources among different components. The overall framework is illustrated in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#S2.F1\" style=\"font-size:90%;\" title=\"Figure 1 &#8227; 2.2 Differentiable Reward Optimization (DiffRO) &#8227; 2 Related Work &#8227; Explore the Reinforcement Learning for the LLM based ASR and TTS system\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "different",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For TTS training, the SGLang-based engine directly generates acoustic token sequences, which are then fed into a PyTorch-based flow-matching model and vocoder to produce Mel-spectrograms and waveforms. Subsequently, GPU resources are allocated to a PyTorch-based reward model (when used) to compute reward scores. The FSDP-based policy model then leverages these reward scores and the generated token sequences to perform reinforcement learning using either GRPO or DiffRO.</span>\n</p>\n\n",
                "matched_terms": [
                    "reward",
                    "diffro",
                    "tts",
                    "grpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Although the LLM based ASR can realize a good results in most case, but it can could change the keyword or fall in hallucinations which serious impact on the user experience. To address these hard case samples, we design some rule based value functions </span>\n  <math alttext=\"\\{R^{k}(y_{i}^{*},y_{i})\\}_{k=1}^{K}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msubsup>\n        <mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">{</mo>\n          <mrow>\n            <msup>\n              <mi mathsize=\"0.900em\">R</mi>\n              <mi mathsize=\"0.900em\">k</mi>\n            </msup>\n            <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n            <mrow>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n              <msubsup>\n                <mi mathsize=\"0.900em\">y</mi>\n                <mi mathsize=\"0.900em\">i</mi>\n                <mo mathsize=\"0.900em\">&#8727;</mo>\n              </msubsup>\n              <mo mathsize=\"0.900em\">,</mo>\n              <msub>\n                <mi mathsize=\"0.900em\">y</mi>\n                <mi mathsize=\"0.900em\">i</mi>\n              </msub>\n              <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n            </mrow>\n          </mrow>\n          <mo maxsize=\"0.900em\" minsize=\"0.900em\">}</mo>\n        </mrow>\n        <mrow>\n          <mi mathsize=\"0.900em\">k</mi>\n          <mo mathsize=\"0.900em\">=</mo>\n          <mn mathsize=\"0.900em\">1</mn>\n        </mrow>\n        <mi mathsize=\"0.900em\">K</mi>\n      </msubsup>\n      <annotation encoding=\"application/x-tex\">\\{R^{k}(y_{i}^{*},y_{i})\\}_{k=1}^{K}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> for the GRPO based RL to improve the ASR performance more than reduce word error rate (WER).</span>\n</p>\n\n",
                "matched_terms": [
                    "grpo",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <math alttext=\"R^{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mn mathsize=\"0.900em\">1</mn>\n      </msup>\n      <annotation encoding=\"application/x-tex\">R^{1}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: ASR Accuracy. To improve the ASR performance, we use the </span>\n  <math alttext=\"1-\\text{WER}(y^{*},y)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i1.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <mn mathsize=\"0.900em\">1</mn>\n        <mo mathsize=\"0.900em\">&#8722;</mo>\n        <mrow>\n          <mtext mathsize=\"0.900em\">WER</mtext>\n          <mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo>\n          <mrow>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo>\n            <msup>\n              <mi mathsize=\"0.900em\">y</mi>\n              <mo mathsize=\"0.900em\">&#8727;</mo>\n            </msup>\n            <mo mathsize=\"0.900em\">,</mo>\n            <mi mathsize=\"0.900em\">y</mi>\n            <mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo>\n          </mrow>\n        </mrow>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">1-\\text{WER}(y^{*},y)</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as the basic value function.</span>\n</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "r1r1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mn mathsize=\"0.900em\">2</mn>\n      </msup>\n      <annotation encoding=\"application/x-tex\">R^{2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Hallucination Detection. Hallucination is a common problem of the LLM-based ASR system and will be more serious on noisy data; it could generate repeated, inexistent, and translated results. We detach these hallucination by some rules and it occurred, the final reward will be set to -1.</span>\n</p>\n\n",
                "matched_terms": [
                    "reward",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <math alttext=\"R^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i3.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mn mathsize=\"0.900em\">3</mn>\n      </msup>\n      <annotation encoding=\"application/x-tex\">R^{3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Keyword Accuracy and Recall. As the keyword has more impact on the user experience, we use the average between the recall and precision rate for the keyword as reward.</span>\n</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "reward"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Addressing practical issues in application scenarios, we build a small but high quality training dataset for the RL with the approach. We first collect the hard and hallucination-related samples </span>\n  <math alttext=\"D^{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">D</mi>\n        <mn mathsize=\"0.900em\">1</mn>\n      </msup>\n      <annotation encoding=\"application/x-tex\">D^{1}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> by compare transcription between different ASR systems, choosing the audios whose outputs are different or contain long repetitions. We also select audio segments longer than 20 seconds which are limited in the training dataset as </span>\n  <math alttext=\"D^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">D</mi>\n        <mn mathsize=\"0.900em\">2</mn>\n      </msup>\n      <annotation encoding=\"application/x-tex\">D^{2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For </span>\n  <math alttext=\"D^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m3\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">D</mi>\n        <mn mathsize=\"0.900em\">3</mn>\n      </msup>\n      <annotation encoding=\"application/x-tex\">D^{3}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, we choose the speech contains name, brand and other specialized vocabulary as the keywords samples. Finally, we also construct a random selection set </span>\n  <math alttext=\"D^{0}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS2.p1.m4\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">D</mi>\n        <mn mathsize=\"0.900em\">0</mn>\n      </msup>\n      <annotation encoding=\"application/x-tex\">D^{0}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> as a control.</span>\n</p>\n\n",
                "matched_terms": [
                    "audios",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Previous works has shown that the DiffRO can significant improve the pronunciation accuracy of the TTS system and the speech attribute control with different downstream task. However, it relay on a differentiable neutral network based reward model to compute the reward and optimize the TTS model. But for some subjective experience, it is difficult to train the reward model like the sound expressiveness sound, rhythm richness and audio duration.\nGRPO can measure a part of the subjective experience by some rules which means that the GRPO and the DiffRO can combine with each other for a better performance.</span>\n</p>\n\n",
                "matched_terms": [
                    "tts",
                    "reward",
                    "diffro",
                    "system",
                    "accuracy",
                    "different",
                    "grpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <math alttext=\"R^{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.I1.i1.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mn mathsize=\"0.900em\">1</mn>\n      </msup>\n      <annotation encoding=\"application/x-tex\">R^{1}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Recognition Accuracy of the ASR system. We can directly use the ASR recognition accuracy as the reward to prevent the error pronunciation. The accuracy can be calculated by a regular ASR from the synthesized audio or a token based ASR model like DiffRO.</span>\n</p>\n\n",
                "matched_terms": [
                    "reward",
                    "synthesized",
                    "diffro",
                    "r1r1",
                    "from",
                    "system",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <math alttext=\"R^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.I1.i2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">R</mi>\n        <mn mathsize=\"0.900em\">2</mn>\n      </msup>\n      <annotation encoding=\"application/x-tex\">R^{2}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">: Audio Duration. Some works has find that during RL, the TTS system tend to slow the speech speed for better ASR result. To prevent this problem, we use the difference from median audio length in the response group as reward:</span>\n</p>\n\n",
                "matched_terms": [
                    "reward",
                    "from",
                    "tts",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">After get the GRPO reward, we can distinguish the response into positive (</span>\n  <math alttext=\"\\hat{A}_{i}&gt;0\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mover accent=\"true\">\n            <mi mathsize=\"0.900em\">A</mi>\n            <mo mathsize=\"0.900em\">^</mo>\n          </mover>\n          <mi mathsize=\"0.900em\">i</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&gt;</mo>\n        <mn mathsize=\"0.900em\">0</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\hat{A}_{i}&gt;0</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) and negative (</span>\n  <math alttext=\"\\hat{A}_{i}\\leq 0\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\">\n    <semantics>\n      <mrow>\n        <msub>\n          <mover accent=\"true\">\n            <mi mathsize=\"0.900em\">A</mi>\n            <mo mathsize=\"0.900em\">^</mo>\n          </mover>\n          <mi mathsize=\"0.900em\">i</mi>\n        </msub>\n        <mo mathsize=\"0.900em\">&#8804;</mo>\n        <mn mathsize=\"0.900em\">0</mn>\n      </mrow>\n      <annotation encoding=\"application/x-tex\">\\hat{A}_{i}\\leq 0</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">) samples according to the advantages.\nWith RL, the posterior probability of the positive responses should increase while the negative&#8217;s should be opposite. This optimization could be difficult and unstable. To address this problem, we also compute the DiffRO loss on the positive sample like eq </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#S2.E6\" style=\"font-size:90%;\" title=\"In 2.2 Differentiable Reward Optimization (DiffRO) &#8227; 2 Related Work &#8227; Explore the Reinforcement Learning for the LLM based ASR and TTS system\">\n    <span class=\"ltx_text ltx_ref_tag\">6</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. This means that the combined RL method can not only increase the probability of the positive response but also tell the model how to make them better, so the training process will be more effective and stable.</span>\n</p>\n\n",
                "matched_terms": [
                    "combined",
                    "reward",
                    "diffro",
                    "method",
                    "grpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We analyze the training efficiency of training frame works on 8 A100 GPUs and show the result in Figure&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#S6.F2\" style=\"font-size:90%;\" title=\"Figure 2 &#8227; 6.1 Training Efficiency Analysis &#8227; 6 Experiments &#8227; Explore the Reinforcement Learning for the LLM based ASR and TTS system\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. For the ASR, the experiments are based on the FunAudio-ASR</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#bib.bib14\" title=\"\">14</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, which contains a 0.7B encoder and a Qwen2.5-7B based LLM. The total duration of the input audio is about an hour, and one training step consumes about 54.6 seconds. So the real-time factor (RTF) is about 0.015. And for the TTS, the experiments are based on the CosyVoice2-0.5B</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#bib.bib15\" title=\"\">15</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We can see that the FM decoder will consume the most time and total duration for one training step is 16.73s with 128 batchsize. And for both ASR and TTS, the time consumption of the device switch and the parameters synchronization only take a small part. We further compare the RL some opensource RL frame work for Qwen2-Audio (R1-AQA)</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#bib.bib16\" title=\"\">16</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> and Cosyvoice2 (VeRL based)</span>\n  <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\">\n    <sup class=\"ltx_note_mark\">1</sup>\n    <span class=\"ltx_note_outer\">\n      <span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>https://github.com/nvidia-china-sae/mair-hub/tree/main/rl-tutorial/cosyvoice_llm</span>\n    </span>\n  </span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and find our frame work has a significant advantage on the training speed. As the training is based on same LLM backbone, we believe that our alternate GPU utilization framework is a high effective training method for audio LLM.</span>\n</p>\n\n",
                "matched_terms": [
                    "tts",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">We present the ASR experimental results in Table&#160;</span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#S6.T1\" style=\"font-size:90%;\" title=\"Table 1 &#8227; 6.2.2 Experiments Reults &#8227; 6.2 Experiments on ASR &#8227; 6 Experiments &#8227; Explore the Reinforcement Learning for the LLM based ASR and TTS system\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. From the table, we observe that RL achieves a relative improvement of 5.3% in WER on both short and long speech segments. Moreover, the design of training data and reward functions plays a crucial role in RL effectiveness, with distinct effects observed on the short and long evaluation sets.\nFor short audios, all data construction strategies and reward designs improve ASR performance, and combining them yields the best overall result.\nHowever, for long audios, we observe that even the baseline model exhibits significantly higher insertion (Ins) errors compared to deletion (Del) errors, indicating a tendency toward hallucination&#8212;generating words not present in the input. Simply including more long audio samples in training or adding hallucination detection mechanisms does not effectively address this issue. In contrast, using hard training samples </span>\n  <math alttext=\"D^{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S6.SS2.SSS2.p1.m1\" intent=\":literal\">\n    <semantics>\n      <msup>\n        <mi mathsize=\"0.900em\">D</mi>\n        <mn mathsize=\"0.900em\">1</mn>\n      </msup>\n      <annotation encoding=\"application/x-tex\">D^{1}</annotation>\n    </semantics>\n  </math>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> proves beneficial, suggesting that RL training data should be carefully constructed based on the failure patterns of the base model.\nFurthermore, keyword-related data and reward design are critical for mitigating hallucinations, as they impose stronger penalties than standard WER-based rewards when the model generates non-existent or irrelevant words.</span>\n</p>\n\n",
                "matched_terms": [
                    "reward",
                    "audios",
                    "from",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">For the TTS experiments, we use the open-source CosyVoice2-0.5B as the base model and construct the training set using text data from CommonVoice zh and en.\nWe compare DiffRO and GRPO under various reward settings and explore different strategies for combining them. The reward model follows the setup in&#160;</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#bib.bib10\" title=\"\">10</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">, and the KL divergence coefficient is set to 0.1.\nAll experiments are conducted with a batch size of 16. When GRPO is used, the group sample size is set to 8 and the sampling temperature is 1.0.\nEvaluation is performed on the CV3-Eval</span>\n  <cite class=\"ltx_cite ltx_citemacro_cite\">\n    <span class=\"ltx_text\" style=\"font-size:90%;\">[</span>\n    <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#bib.bib9\" title=\"\">9</a>\n    <span class=\"ltx_text\" style=\"font-size:90%;\">]</span>\n  </cite>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"> test set.</span>\n</p>\n\n",
                "matched_terms": [
                    "tts",
                    "reward",
                    "diffro",
                    "from",
                    "different",
                    "grpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">Besides the performance, we also compare the training stability between different RL method and show the results in Figure </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.18569v1#S6.F3\" style=\"font-size:90%;\" title=\"Figure 3 &#8227; 6.3 Experiments on TTS &#8227; 6 Experiments &#8227; Explore the Reinforcement Learning for the LLM based ASR and TTS system\">\n    <span class=\"ltx_text ltx_ref_tag\">3</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:90%;\">. We can find that at the first 1000 steps, all of the RL models can achieve a better results than the baseline. But rapid deterioration will occur for the GRPO and the Combined w/o sample filter when the training step is larger than 1500. This means that the DiffRO can be more stable with longer training steps.</span>\n</p>\n\n",
                "matched_terms": [
                    "combined",
                    "filter",
                    "diffro",
                    "method",
                    "different",
                    "grpo"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">This study explore the effect of RL for audio-based LLM including both ASR and TTS.\nWe first propose an alternate GPU utilization framework which show great training efficiency based on audio input and output.\nFor ASR, the study explores various rule-based reward functions using GRPO and investigates the impact of RL data construction.\nIn TTS, it compares GRPO with DiffRO and demonstrates that combining both methods improves performance. Results show that RL significantly enhances both ASR and TTS systems, with reward design playing a crucial role in shaping model output preferences and user experience.\nThis work highlights the potential of RL in audio-domain LLMs and calls for better-supported RL frameworks for audio applications.</span>\n</p>\n\n",
                "matched_terms": [
                    "reward",
                    "diffro",
                    "tts",
                    "grpo"
                ]
            }
        ]
    }
}