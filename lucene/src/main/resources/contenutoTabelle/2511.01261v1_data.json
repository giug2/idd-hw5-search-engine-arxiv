{
    "S4.T1": {
        "caption": "Table 1: Data summary of DRAME-EvalBench for archetype and realism evaluation. We provide a comparison of sources, evaluation dimensions (annotated speech role-play dimensions), supported languages, total samples, split details and scenarios for benchmarking.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">12,000 train</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">2,000 base test</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">1,000 real-recording test</td>\n</tr>\n</table>\n",
        "informative_terms_identified": [
            "roleplay",
            "drameevalbench",
            "scenarios",
            "evaluation",
            "details",
            "comparison",
            "sources",
            "realism",
            "provide",
            "base",
            "annotated",
            "benchmarking",
            "archetype",
            "test",
            "samples",
            "realrecording",
            "speech",
            "languages",
            "supported",
            "total",
            "train",
            "dimensions",
            "data",
            "split",
            "summary"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Archetype Data Collection &#8227; 4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents a summary of the curated <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> dataset, where we further elaborate the details of archetype and realism evaluation data collection in following subsections, respectively. For benchmark purpose, we provide an official train-test split for the human annotated dataset for <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> so that following researchers can use the dataset to train and evaluate their SEMs for speech role-play.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Role-play has become a key testbed for generative models, expanding from text-only dialogue to multimodal interaction. Extending role-play to speech captures prosody, emotion, and delivery, but also poses new evaluation challenges. Current pipelines often use audio large language models (ALLMs) as zero-shot judges, which miss paralinguistic cues, collapse multiple aspects into coarse scores, and rely on synthetic speech references that fail to reflect real-world roles. We present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework that contributes at three levels: (i) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-EvalBench</em>, an evaluation benchmark with bilingual human-annotated data and protocols for training and testing speech evaluation models&#160;(SEMs), (ii) <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, a fine-tuned evaluation model, which substantially outperforms zero-shot and few-shot ALLMs, and (iii) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-RoleBench</em>, a speech role-play benchmark that leverages <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> as an automatic judge to compare speech foundation models (SFMs). Speech-DRAME distinguishes between two complementary evaluation strategies: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>, a top-down approach measuring adherence to broad role archetypes, and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em>, a bottom-up approach grounded in real human speech that emphasizes nuanced role quality. Compared to zero-shot ALLM judges, <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves stronger agreement with human ratings (Pearson correlation from 0.480 to 0.629 in archetypes, and 0.390 to 0.625 in realism). By integrating transparent benchmark resources, modeling approaches, and system-level evaluation, Speech-DRAME provides the first comprehensive, reproducible foundation for assessing spoken role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "evaluation",
                    "data",
                    "archetype",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Evaluating speech role-play, however, remains a major challenge. Current pipelines often rely on ALLMs as judges, typically in a zero-shot setting&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>; Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>)</cite>. While this strategy can provide rough ratings, it falls short for speech role-play:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "roleplay",
                    "provide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">it collapses diverse aspects of performance into limited coarse dimensions, obscuring critical dimensions of role-play quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite>, and</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">it depends on synthetic speech references rather than real human speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>)</cite>, limiting its ability to reflect real-world diversity in speech role-play.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "roleplay"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome these limitations, we present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span> (Speech Detailed Role-play Assessment with Modeling and Evaluation), a unified framework that moves beyond synthetic or judge-only pipelines. Speech-DRAME is distinguished by three aspects: [i] a dual evaluation paradigm that balances archetype-driven generality with human-grounded realism (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS2\" title=\"3.2 Role-play Evaluation with Two Strategies &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), [ii] human annotations that provide perceptual grounding absent in prior resources (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4\" title=\"4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), and [iii] evaluation-model alignment that enables not only benchmarking of speech foundation models (SFMs) but also systematic analysis of the evaluation models (SEMs) themselves (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5\" title=\"5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). These aspects establish Speech-DRAME as a comprehensive and trustworthy framework for measuring and improving speech role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "provide",
                    "evaluation",
                    "benchmarking",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, Speech-DRAME contributes at three levels: (i) <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>, an evaluation benchmark consisting of human-annotated datasets for archetype and realism role-play; (ii) <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span>, a fine-tuned SEM trained on EvalBench that surpasses zero-shot and few-shot ALLMs; and (iii) <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span>, a speech role-play benchmark that leverages DRAME-Eval as an automatic judge to systematically compare SFMs. By combining dual evaluation strategies, human-anchored supervision, and evaluation-model alignment, Speech-DRAME delivers systematic insights into both role-play systems and evaluation models themselves.\nOur contributions are four-fold:</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "drameevalbench",
                    "evaluation",
                    "archetype",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a <span class=\"ltx_text ltx_font_bold\">formal task definition</span> of speech role-play and its evaluation via speech foundation models (SFMs) for generation and speech evaluation models (SEMs) for assessment, unifying linguistic and paralinguistic dimensions.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "roleplay",
                    "evaluation",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>, the first evaluation benchmark with human annotations under both archetype- and realism-based strategies, supporting the training and testing of SEMs.</p>\n\n",
                "matched_terms": [
                    "archetype",
                    "evaluation",
                    "drameevalbench"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We establish <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span>, a speech role-play benchmark that leverages DRAME-Eval as an automatic judge to systematically compare both end-to-end and cascaded SFMs across multiple role quality dimensions.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Relevant codes, pre-trained models, and datasets will be open-sourced at \n<br class=\"ltx_break\"/><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/Anuttacon/speech_drame\" title=\"\">https://github.com/Anuttacon/speech_drame</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "speech",
                    "roleplay"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through this work, we aim to move from acting in text to acting in speech, equipping the community with the models, evaluation strategies, and benchmarks needed to measure and improve speech role-play.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "roleplay",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-based role-play evaluation.</span>\nEarly research focused on text-only dialogue benchmarks. Datasets such as RoleLLM, CharacterBox, and RoleMRC <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib62\" title=\"\">2024</a>; Tu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib60\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib61\" title=\"\">2025a</a>; Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib53\" title=\"\">2023</a>; Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib41\" title=\"\">2025</a>)</cite> test whether models can sustain persona consistency and character fidelity, using LLM-as-judge pipelines for scalability. However, studies like PersonaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib81\" title=\"\">2025a</a>)</cite> have shown that LLM-as-judge approaches are far from perfect, struggling with tasks as basic as tracking speaker identity. This highlights broader concerns about reliability even in text settings, which become only more severe when extended to speech, where prosody, emotion, and vocal style play a central role.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "roleplay",
                    "evaluation",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-native benchmarks.</span>\nMore recently, a number of resources have been introduced to evaluate spoken role-play agents. OmniCharacter-10K <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>)</cite> emphasizes character voice traits, while others&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib39\" title=\"\">2025</a>; Hou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib27\" title=\"\">2025</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib15\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib7\" title=\"\">2024b</a>)</cite> extend evaluation toward vocal communication, assistant capability, and multi-turn realism. Omni models such as StepAudio and KimiAudio <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite> also include role-play scenarios in their evaluation. SpeechRole and VStyle <cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>; Zhan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib76\" title=\"\">2025</a>)</cite> are closer to our work, offering datasets that test interaction ability, expressiveness, and fidelity. However, these resources are primarily built from LLM-generated dialogues and TTS outputs, with limited human validation and a heavy reliance on proprietary ALLM-as-judge pipelines. As a result, they lack grounding in diverse human speech and typically apply only coarse rubrics.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "scenarios",
                    "evaluation",
                    "test",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Automatic judging with ALLMs.</span>\nIn parallel, advances in automatic judging highlight both opportunities and limitations. Audio-capable LLMs such as AudioJudge and Audio-Aware LLMs as Judges <cite class=\"ltx_cite ltx_citemacro_citep\">(Manakul et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib44\" title=\"\">2025</a>; Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>)</cite> can assess style, emotion, and delivery, while systems like Auto-ATT <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib65\" title=\"\">2025c</a>)</cite> scale this to Audio Turing Tests. Yet studies consistently reveal high prompt sensitivity and shallow capture of paralinguistic cues, suggesting that zero-shot judging remains insufficient for nuanced role-play evaluation.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Expressive TTS evaluation.</span>\nRecent work has explored evaluation of instruction-following and style-controlled TTS. InstructTTSEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite> explicitly includes role-play scenarios, while EmergentTTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">(Manku et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib45\" title=\"\">2025</a>)</cite> and discrete speech LM benchmarks <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang &amp; Sz&#233;kely, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib63\" title=\"\">2024</a>)</cite> emphasize prosody, spontaneity, and speaker consistency. These efforts push beyond intelligibility and naturalness, but still center on synthetic outputs rather than real human role-play.</p>\n\n",
                "matched_terms": [
                    "scenarios",
                    "roleplay",
                    "evaluation",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our distinction.</span>\nIn contrast, Speech-DRAME unifies these considerations through three elements: (i) a dual evaluation paradigm that combines archetype-driven generality with realism grounded in authentic human speech, (ii) human annotations that provide a reliable perceptual gold standard, and (iii) alignment of SEMs to human ratings, enabling systematic evaluation of both SFMs and the evaluation models themselves. This combination moves beyond synthetic benchmarks or zero-shot judging to provide a comprehensive and trustworthy framework for speech role-play assessment.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "provide",
                    "evaluation",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech role-play.</span>\nWe formalize <em class=\"ltx_emph ltx_font_italic\">speech role-play</em> as a conditional speech generation problem grounded in a character profile and scene description.\nGiven a dialogue context <math alttext=\"{\\bm{C}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{speech}}</annotation></semantics></math>, a character profile <math alttext=\"{\\bm{C}}_{\\text{profile}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>profile</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{profile}}</annotation></semantics></math>, and a scene specification <math alttext=\"{\\bm{C}}_{\\text{scene}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>scene</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{scene}}</annotation></semantics></math>, a <em class=\"ltx_emph ltx_font_italic\">speech foundation model</em> (SFM) generates a role-played speech response <math alttext=\"{\\bm{S}}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119930;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{S}}_{r}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "roleplay"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation.</span>\nIn principle, a <em class=\"ltx_emph ltx_font_italic\">speech evaluation model</em> (SEM) can assess a generated response using the same conditioning signals:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech role-play evaluation can be framed in multiple ways depending on the available data and the desired granularity of analysis.\nWithin Speech-DRAME, we formalize two distinct strategies inspired by sociology theories<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Please refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A2\" title=\"Appendix B Theoretical Motivation for Dual Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> for detailed discussion of the dual evaluation design.</span></span></span>, including a top-down, archetype-oriented option using synthetic speech and a bottom-up, realism-oriented option from real human speech.\nRather than merging these into a single rubric, we treat them as separate perspectives, each with unique advantages and limitations.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarizes the key contrasts, while further implementation details are provided in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>-&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "evaluation",
                    "data",
                    "details",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>). The first strategy follows a <em class=\"ltx_emph ltx_font_italic\">top-down</em> design, inspired by prior text-based role-play benchmarks.\nHere, role-play is judged with respect to <em class=\"ltx_emph ltx_font_italic\">stereotypical archetypes</em> (e.g., &#8220;firefighter&#8221; or &#8220;ER doctor&#8221;), using scene contexts (e.g., &#8220;comfort a child trapped in fire&#8221; or &#8220;ask for help in a serious operation&#8221;).\nThis approach provides accessible, general-purpose scoring that is scalable to a wide range of scenarios, making it suitable for large-scale benchmarking.\nHowever, its reliance on broad stereotypes and global impressions limits its ability to capture fine-grained aspects of delivery (e.g., subtle prosodic shifts, nuanced emotional control)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Holt &amp; Lotto, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib26\" title=\"\">2010</a>)</cite>.\nIn practice, this strategy mirrors simplified annotation settings and is particularly useful when annotation resources are limited&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib21\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "scenarios",
                    "roleplay",
                    "evaluation",
                    "benchmarking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS5\" title=\"F.5 Examples of Realism Role-play &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.5</span></a>). The second strategy adopts a <em class=\"ltx_emph ltx_font_italic\">bottom-up</em> design, motivated by real-world performance and real human speech.\nInstead of relying solely on stereotypes, this approach grounds evaluation in recordings from professional and non-professional speakers, drawn from realistic dialogue and media sources.\nJudgments in this setting are more fine-grained and multi-dimensional, addressing aspects such as prosodic dynamics, emotional fidelity, character consistency, and contextual fit.\nWhile this strategy requires more human annotation and careful curation, it provides benchmarks that reflect how role-play is perceived in practice.\nIt thus offers a richer framework for analyzing model outputs in settings where realism and subtlety are essential.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "evaluation",
                    "speech",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section details the <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> data curation process. Our data curation methodology follows a two&#8211;stage design: (i) <em class=\"ltx_emph ltx_font_italic\">task preparation</em>, which specifies the roles, scenes, and evaluation settings used to elicit speech; and (ii) <em class=\"ltx_emph ltx_font_italic\">human annotation</em>, which provides reliable ground-truth judgments on generated or real human speech.</p>\n\n",
                "matched_terms": [
                    "drameevalbench",
                    "evaluation",
                    "data",
                    "details",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human Annotation.</span>\nAll collected audio undergoes a standardized annotation protocol: (1) an initial semantically-based screening to exclude invalid clips; (2) rubric-based scoring across multiple perceptual dimensions; and (3) meta-annotations such as confidence or prompt-quality judgments. Full rubrics are provided in the appendices&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A5\" title=\"Appendix E Annotation Guidelines for Archetype Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">E</span></a> &amp; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>All data collection follows in high standard ethical consideration as outlined in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A9\" title=\"Appendix I Annotation Ethics &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Archetype evaluation adopts a top&#8211;down perspective from sociology and role theory&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goffman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib23\" title=\"\">1949</a>; Campbell, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib4\" title=\"\">2008</a>)</cite>, abstracting role-play into broad and recognizable categories.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Role and Scene Dimensions.</span>\nRoles are operationalized through seven categories: (1) functional occupations in daily life, (2) functional occupations in fictional or fantasy settings, (3) social identities, (4) named or specific characters, (5) personality traits, (6) relative relationship, (7) basic events. Scenes are anchored by everyday events or state changes that evoke stereotypical reactions. Details of template design and alternative prompting strategies are given in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>We note that some archetypes (e.g., firefighter) inevitably reflect cultural stereotypes. These are used solely for evaluation scalability and do not reflect normative judgments.</span></span></span></p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "evaluation",
                    "details"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Sources.</span> The data is collected from a range of speech foundation models, including both cascaded and end-to-end models. We limit that only audio prompts are used as the input but adopt different prompt strategies for best quality with regards to different models. Details about the selected models and prompting details are discussed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS4\" title=\"D.4 Candidate Models for Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data",
                    "details",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotation follows two stages: an initial semantic check (i.e., <span class=\"ltx_text ltx_font_italic\">Content Pass</span>) to filter out instruction non-compliance or audio defects, followed by rubric-based scoring on three dimensions: <em class=\"ltx_emph ltx_font_italic\">Audio Quality</em>, <em class=\"ltx_emph ltx_font_italic\">Human-likeness</em>, and <em class=\"ltx_emph ltx_font_italic\">Appropriateness</em>. Confidence judgments are also recorded. Detailed rubric scales are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A5\" title=\"Appendix E Annotation Guidelines for Archetype Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype Evaluation Data Summary.</span>\nThe resulting archetype evaluation dataset comprises 552 distinct scenarios for both Mandarin and English, evaluated across eight models,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>The model list is discussed in Appdenix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS4\" title=\"D.4 Candidate Models for Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.4</span></a>. Note we only include seven models for English due to the deactivation of ChatGPT-4O advanced voice model.</span></span></span> yielding 8,280 samples. In total, the dataset are split into 6,780 for training and 1,500 for testing. For benchmarking, an additional 1,250 scenarios spanning seven role/scene categories are prepared to facilitate archetype role-play benchmarking. More detailed statistics and analyses are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10\" title=\"Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">J</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "scenarios",
                    "total",
                    "samples",
                    "evaluation",
                    "data",
                    "benchmarking",
                    "archetype",
                    "split",
                    "summary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realism evaluation adopts a bottom&#8211;up perspective, emphasizing authentic delivery, prosody, and contextual fit.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Sources and Negative Samples.</span>\nWe curate speech from (i) ground-truth media segments, (ii) retrieved character profiles, and (iii) generated local scene descriptions. To stress-test evaluation models, we construct contrastive mismatches: (i) negative local scenes with varying degrees of mismatch, and (ii) negative character profiles created by re-synthesizing transcripts with TTS systems. Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6\" title=\"Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F</span></a> details the prompt design and TTS generation details.</p>\n\n",
                "matched_terms": [
                    "samples",
                    "evaluation",
                    "data",
                    "details",
                    "speech",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotators judge whether performances align with the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em>, using rubrics for <em class=\"ltx_emph ltx_font_italic\">Prosodic Dynamics (i.e., Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis)</em>, <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness (with progressive gating over three sub-dimensions: Emotion Accuracy, Emotion Intensity, Emotion Transition)</em>, <em class=\"ltx_emph ltx_font_italic\">Character Consistency (i.e., Voice Identity Matching and Traits Embodiment)</em>, <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance (i.e., Local Scene Fit and Global Story Coherence)</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>. A prompt quality check and confidence ratings are also collected. Full scales and gating details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "evaluation",
                    "details"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real-world Recoding.</span>\nAs noted in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S2\" title=\"2 Related Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, most prior efforts have relied heavily on synthetic data, mostly from TTS systems based on curated professional role-play voices. While such data can achieve high fidelity, it often lacks the diversity and spontaneity of real-world delivery, limiting the generalizability of evaluation models. To address this gap, we expand realism evaluation beyond synthetic and curated voices by incorporating newly recorded speech from both professional and amateur voice actors. This design introduces a broader spectrum of delivery styles, from polished professional acting to more natural and less controlled amateur performance.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "evaluation",
                    "data",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation Data Summary.</span>\nThe resulting realism evaluation dataset is constructed from a combination of curated media sources, including internal collections, public datasets (e.g., NCSSD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib40\" title=\"\">2024b</a>)</cite>), and original recordings collected for this project.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>All data were obtained under appropriate licenses and are legally available for research release.</span></span></span> We compile 15k annotated role-play samples: 9k in Mandarin and 5k in English. The dataset is divided into 12k samples for training, 2k mixed samples for the base test set, and 1k real-recording samples for the test set. The first test set supports general evaluation using our proposed pipeline with negative samples, while the second focuses exclusively on human speech performed by both professional and amateur voice actors. For benchmarking, 2,000 scenarios from the realism dataset are designated for realism role-play benchmarking. Detailed statistics and analyses are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11\" title=\"Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">K</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "scenarios",
                    "base",
                    "samples",
                    "annotated",
                    "realrecording",
                    "evaluation",
                    "data",
                    "benchmarking",
                    "test",
                    "speech",
                    "summary",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the two proposed datasets, we conduct experiments under three settings: zero-shot, few-shot, and finetuning, using pre-trained ALLMs. We formulate the task as a multi-metric estimation problem&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tjandra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib58\" title=\"\">2025</a>; Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib74\" title=\"\">2025</a>; Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib55\" title=\"\">2025b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib54\" title=\"\">a</a>)</cite>, where a single model is trained or prompted to predict multiple evaluation dimensions.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For few-shot experiments, we focus primarily on proprietary ALLMs, which demonstrate stronger instruction-following capabilities and have been widely used as judges in prior work. For each test query, we provide three additional annotated samples from the training set as in-context exemplars.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>Please find our detailed settings such as prompts, decoding algorithms in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS1\" title=\"L.1 Zeroshot/Fewshot Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.1</span></a> with additional ablation analysis in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS2\" title=\"L.2 Zeroshot/Fewshot Ablation Experiments &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.2</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "samples",
                    "test",
                    "annotated",
                    "provide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Finetuning Model: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>.</span>\nIn addition to zero-shot and few-shot evaluations, we fine-tune a dedicated evaluation model, denoted as <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> for archetype and realism role-play evaluation, respectively.\nWe initialize from <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio-7B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib11\" title=\"\">2024</a>)</cite>, which provides a strong foundation for multi-modal audio-text reasoning.\nTo better leverage the multiple human annotations available in our datasets, we adopt a sampling-based training strategy: at each step, one annotator label is randomly selected, following practices in label distribution learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Geng, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib22\" title=\"\">2016</a>; Wen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib67\" title=\"\">2023</a>)</cite>.\nThis stochastic supervision implicitly exposes the model to the empirical annotation distribution, while during inference we output the full distribution over discrete labels (e.g., <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score, consistent with recent decoding-based regression methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Song &amp; Bahri, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>; Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib70\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib64\" title=\"\">2025b</a>; Lukasik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>. We apply LoRA fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib28\" title=\"\">2022</a>)</cite>. All implementation details, including optimizer settings, training schedule, and sampling policy, are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS3\" title=\"L.3 Fine-tuning Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "evaluation",
                    "details",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics for Role-play Evaluation Model.</span> For most dimensions with 1-5 scale, we report Pearson correlation. For <span class=\"ltx_text ltx_font_italic\">Content Pass</span> estimation, we report accuracy.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "evaluation",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype Evaluation.</span>\nThe results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T2\" title=\"Table 2 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reveal clear distinctions across evaluation settings. In the zero-shot condition, proprietary ALLMs such as <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> substantially outperform public models, achieving strong <span class=\"ltx_text ltx_font_italic\">content pass</span> accuracy (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>90%) and moderate correlations with other dimensions(<math alttext=\"0.43\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mn>0.43</mn><annotation encoding=\"application/x-tex\">0.43</annotation></semantics></math>&#8211;<math alttext=\"0.48\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mn>0.48</mn><annotation encoding=\"application/x-tex\">0.48</annotation></semantics></math>). By contrast, public models exhibit limited predictive ability, with correlations often below 0.2. Few-shot prompting provides modest gains for proprietary models, improving both accuracy and correlation (e.g., <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> rises from 0.480 to 0.493 average correlation). The most significant improvement arises from finetuning: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves an average correlation of 0.629, marking a large margin over zero-shot/few-shot models and demonstrating the effectiveness of task-specific supervision.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T3\" title=\"Table 3 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> extends the analysis to fine-grained realism evaluation. Similar to archetype results, proprietary ALLMs lead the zero-shot setting, with <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> obtaining an average correlation of 0.390, compared to 0.284 for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>. Public models remain weak baselines. Few-shot prompting consistently improves performance: <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> gains +0.042 and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> gains +0.083 over their zero-shot averages, showing the benefit of in-context supervision. Once again, finetuning is most effective: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves 0.625 average correlation, higher than the best few-shot results. Importantly, the gains are consistent across all 10 realism dimensions, confirming that the sampling&#8211;expectation training strategy generalizes beyond simple classification to nuanced paralinguistic cues.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "evaluation",
                    "realism",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real Recording Test Set.</span>\nThe real-recording evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T4\" title=\"Table 4 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> is notably more challenging, with all models suffering substantial performance drops. Proprietary models in the zero-shot setting achieve only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.04&#8211;0.08 average correlation, and public models often fall below zero. Few-shot prompting yields small but noticeable improvements, especially for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> (0.226 avg.), suggesting that exemplars partially mitigate domain mismatch between synthetic and real human speech. Interestingly, while finetuning achieves the highest overall average (0.247), its relative advantage over few-shot prompting is narrower than in the synthetic setting, and some dimensions (e.g., <span class=\"ltx_text ltx_font_italic\">Character Identity</span>) remain weak. We emphasize that this observed gap between synthetic and real recordings is not merely an artifact but an intended feature of Speech-DRAME, designed to expose the mismatch and drive future research into bridging synthetic-to-real generalization.</p>\n\n",
                "matched_terms": [
                    "realrecording",
                    "evaluation",
                    "dimensions",
                    "test",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall.</span> Across both archetype and realism evaluations, we observe three consistent trends: (i) zero-shot performance is limited, particularly for open-source models, (ii) few-shot prompting offers moderate but consistent gains, especially for proprietary ALLMs, and (iii) finetuning with task-specific supervision (<span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>) delivers the largest improvements, achieving state-of-the-art correlations across nearly all dimensions. Nevertheless, results on the real-recording test set underscore the gap between synthetic and real human role-play scenarios, motivating future work in data diversification and domain adaptation.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "scenarios",
                    "archetype",
                    "realrecording",
                    "dimensions",
                    "data",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on fine-tuned <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, we establish the DRAME-RoleBench to evaluate SFMs&#8217; role play ability. For archetype evaluation, 1,250 scenarios are constructed following the same data construction pipeline as the <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>. For realism evaluation, we reuse 2,000 scenarios in the <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> test set. We follow the same prompt strategy as outlined in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "drameevalbench",
                    "scenarios",
                    "test",
                    "evaluation",
                    "data",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype evaluation.</span>\nOn archetype role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T5\" title=\"Table 5 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), cascaded pipelines consistently outperform end-to-end systems. The strongest cascaded systems, particularly <span class=\"ltx_text ltx_font_typewriter\">G-IndexTTS2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-IndexTTS2</span>, achieve both high content pass rates (above 94%) and balanced scores across audio quality, human likeness, and appropriateness. By contrast, end-to-end models such as <span class=\"ltx_text ltx_font_typewriter\">StepAudio2Mini</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> perform competitively in terms of content coverage (above 88%), but their expressive scores remain slightly lower than their cascaded counterparts. This gap suggests that separating reasoning (LLM) from speech rendering (TTS) remains advantageous when archetype fidelity is emphasized. Nevertheless, the fact that top end-to-end systems already surpass 3.0 in average rating indicates rapid closing of the performance gap.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "roleplay",
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism evaluation.</span>\nRealism role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T6\" title=\"Table 6 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) presents a different landscape. Here, performance differences between end-to-end and cascaded systems are less pronounced. Several end-to-end models, such as <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>, reach average scores of 3.24, matching or even exceeding the best cascaded pipelines. Importantly, realism scores are more evenly distributed across prosodic, emotional, character, and contextual dimensions, with most models clustering between 2.9&#8211;3.2. This convergence reflects the difficulty of realism-style evaluation: since the data preparation emphasized contrast with authentic human speech, models are judged under similar conditions, leading to smaller observed gaps.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "dimensions",
                    "evaluation",
                    "data",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human alignment.</span>\nWe further conducted small-scale human studies with 10% of the benchmark data to verify the reliability of the benchmark. Archetype evaluation achieves a Spearman correlation of 0.706 between human judgments and <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, validating that the automatic evaluator closely tracks human preferences in this setting. Realism evaluation, however, achieves a more modest Spearman correlation of 0.375. This lower alignment is expected, since realism data construction did not prioritize fine-grained model separation but instead emphasized distinguishing human versus synthetic speech.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>We view this as evidence that realism presents a harder, less-saturated frontier. Rather than a limitation of DRAME-Eval, this highlights the challenge of designing evaluators that capture nuanced human perception, which motivates future work.</span></span></span> As a result, all models occupy a relatively narrow performance band, reducing sensitivity to model-level differences. We view this as evidence that realism evaluation highlights a harder, less saturated frontier where both SFMs and SEMs require further advances.</p>\n\n",
                "matched_terms": [
                    "data",
                    "evaluation",
                    "realism",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, cascaded pipelines retain an edge in archetype scenarios, especially in content robustness and expressive fidelity, but end-to-end models are catching up quickly. In realism scenarios, the field remains unsolved: existing systems exhibit similar performance bands and evaluators show weaker alignment with human judgments, underscoring the need for improved realism-grounded data and evaluation. Together, these findings establish <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> as both a reliable testbed for archetype fidelity and a challenging benchmark for realism fidelity.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "scenarios",
                    "evaluation",
                    "data",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework for assessing spoken role-play through two complementary lenses: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em> (top-down) and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> (bottom-up, human-grounded). Our human-annotated <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> enables training and testing of SEMs, and our fine-tuned <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span> substantially outperforms zero-/few-shot ALLM judges. Building on this, <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> provides system-level comparison across various SFMs, revealing a consistent edge for cascaded pipelines on archetypes and a tighter spread under realism, where real human performance evaluation remain challenging. Together, these resources establish a reproducible, analyzable foundation for spoken role-play evaluation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>Limitations and future works are discussed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A13\" title=\"Appendix M Limitations and Future Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">M</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "drameevalbench",
                    "evaluation",
                    "archetype",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All data collection and annotation in this work followed strict ethical standards. Our bilingual datasets were constructed exclusively from legally accessible sources, including licensed media, open-source corpora, and original recordings conducted with informed consent. Annotators were recruited with appropriate compensation and provided with clear instructions, including the right to withdraw at any point. To minimize risks, no personally identifiable information was included in the released data, and all recordings were anonymized before annotation. We ensured cultural and linguistic diversity in annotator pools to mitigate potential bias across Mandarin and English role-play scenarios. While this study did not require formal IRB approval, we adhered to best practices in human-subject research, including transparency in task design and careful consideration of annotator well-being. Full details of our annotation protocol and ethical safeguards are presented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A9\" title=\"Appendix I Annotation Ethics &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "scenarios",
                    "data",
                    "details",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data generation:</span> LLMs were employed to expand prompts (e.g., character profiles, scene descriptions) and to produce negative samples in realism evaluation. All outputs were screened by human annotators to ensure validity and reduce bias.</p>\n\n",
                "matched_terms": [
                    "data",
                    "evaluation",
                    "realism",
                    "samples"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline evaluators:</span> State-of-the-art audio LLMs (e.g., <span class=\"ltx_text ltx_font_typewriter\">GPT4o</span>, <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>) were included as zero-shot judges in our evaluation benchmark, enabling direct comparison with human-aligned SEMs.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our distinction between <span class=\"ltx_text ltx_font_italic\">Archetype Evaluation</span> and <span class=\"ltx_text ltx_font_italic\">Realism Evaluation</span> is not merely pragmatic, but is grounded in established theories of social identity, communication, and performance. The two perspectives capture complementary dimensions of how humans perceive and evaluate roles in interaction.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "evaluation",
                    "realism",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism as Situated Performance</span>\nIn contrast, realism highlights the fine-grained enactment of roles in situated interaction. Goffman&#8217;s dramaturgical sociology emphasizes that everyday social life is itself a performance, where believability depends on subtle paralinguistic and pragmatic cues such as prosody, hesitation, and emotional shading&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goffman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib23\" title=\"\">1949</a>)</cite>. Similarly, interactional sociolinguistics has demonstrated how micro-level features like intonation, pacing, and turn-taking patterns shape the perception of authenticity in communication&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gumperz, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib25\" title=\"\">1982</a>)</cite>. Realism Evaluation thus anchors role-play assessment in bottom-up sensitivity to authentic delivery, beyond schematic expectations.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Complementarity of the Two Perspectives</span>\nPerformance theory provides a unifying lens for these dual dimensions. Schechner&#8217;s view of performance as both &#8220;restored behavior&#8221; (scripted, repeatable structures) and &#8220;living process&#8221; (situated enactment)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Schechner, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib52\" title=\"\">2017</a>)</cite> underscores the need to consider both archetypal scripts and enacted realism. Likewise, sociolinguistic theories of identity recognize both <span class=\"ltx_text ltx_font_italic\">ascribed categories</span> (archetypes imposed by social convention) and <span class=\"ltx_text ltx_font_italic\">performed identities</span> (emergent in interaction)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bucholtz &amp; Hall, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib2\" title=\"\">2005</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Accordingly, Archetype Evaluation and Realism Evaluation are best viewed as complementary:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Archetype Evaluation provides macro-level judgments grounded in socially shared schemas, enabling efficient and broad comparisons.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realism Evaluation captures micro-level cues of authenticity, ensuring sensitivity to nuanced delivery and interactional fit.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our main formulation (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS1\" title=\"3.1 Problem Formulation &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) focuses on the <em class=\"ltx_emph ltx_font_italic\">single-turn</em> setting, where each response <math alttext=\"{\\bm{S}}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119930;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{S}}_{r}</annotation></semantics></math> is generated and evaluated independently.\nThis simplification enables clearer analysis of speech role-play quality and aligns with our bilingual annotation protocol.\nNevertheless, the same formulation extends naturally to multi-turn dialogues.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "roleplay"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multi-turn evaluation.</span>\nEach generated speech segment is scored by an SEM:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Discussion.</span>\nWhile our released dataset and experiments concentrate on single-turn evaluation, the modular nature of the formulation ensures that extending to multi-turn is straightforward.\nFuture work can leverage <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p5.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> to study richer aspects of speech role-play, such as persona persistence, interaction dynamics, and role consistency across extended dialogues.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "roleplay",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This appendix first discusses the summary of task preparation steps for archetype evaluation. Then we provide concrete examples and extended discussion of the archetype evaluation annotation pipeline that are omitted in the main text for clarity.</p>\n\n",
                "matched_terms": [
                    "archetype",
                    "evaluation",
                    "summary",
                    "provide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Archetype Evaluation follows a top&#8211;down perspective rooted in sociology and role theory, where social behavior is interpreted through shared stereotypes and generalized expectations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goffman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib23\" title=\"\">1949</a>; Campbell, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib4\" title=\"\">2008</a>)</cite>.\nThis strategy abstracts role-play into broad and easily recognizable archetypes, enabling scalable evaluation of whether a generated response aligns with salient role features.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scene dimensions.</span>\nIn addition to roles, we incorporate contextual triggers that anchor role-play in stereotypical situations.\nA central focus is on <span class=\"ltx_text ltx_font_bold\">everyday external events or state changes</span> (e.g., receiving a promotion, being caught in gossip), which provide concrete scenarios for testing archetypical responses.</p>\n\n",
                "matched_terms": [
                    "scenarios",
                    "roleplay",
                    "dimensions",
                    "provide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Exclusion of fine-grained controls.</span>\nWe deliberately excluded explicit emotional states, prosodic instructions, or linguistic constraints.\nThese dimensions are better suited to realism-oriented or style-controlled evaluation, whereas archetype evaluation benefits from concrete, stereotype-level scenarios that sharpen judgments and improve human agreement.</p>\n\n",
                "matched_terms": [
                    "scenarios",
                    "dimensions",
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Pipeline overview.</span>\nAt a high level, archetype evaluation data are generated through a three-stage pipeline:\n(1) selecting keywords that specify role and scene;\n(2) expanding them into full prompts via structured templates; and\n(3) rendering prompts into speech with contemporary TTS systems.\nThis modular pipeline ensures both scalability and reproducibility, while keeping prompts vivid enough for reliable judgments. (4) Based on the generated data (paired scene and synthesized speech), expert human annotators are recruited for detailed annotation on semantic check (i.e., <span class=\"ltx_text ltx_font_italic\">Content Pass</span>) and three different spoken role-play dimensions (i.e., <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human-likeness</span>, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>).\nImplementation details, including template design, alternative prompting strategies, and annotation guidelines, are provided in the following subsections of this Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dimensions",
                    "evaluation",
                    "data",
                    "details",
                    "archetype",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, archetype evaluation emphasizes breadth and recognizability over subtlety.\nBy grounding role-play in socially recognizable categories and stereotypical situations, it provides a practical balance between scalability and interpretability, complementing the more fine-grained realism evaluation described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS2\" title=\"3.2 Role-play Evaluation with Two Strategies &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "evaluation",
                    "realism",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We constructed archetype evaluation prompts by combining role categories with contextual scenes.\nThe following illustrates the core categories with representative examples.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Excluded Dimensions.</span> We initially considered more fine-grained control dimensions, but excluded them to preserve the archetype-level focus. Examples of excluded dimensions include:\n<em class=\"ltx_emph ltx_font_italic\">Emotional State &amp; Expression, Single Emotion Prompts, Explicit User Input / Assistant Output structures, Emotional Transition / Fluctuation, Voice Style &amp; Prosody (pacing, intonation, vocal timbre, expressive style, accent/dialect), and Content / Interaction Patterns (task/topic controls, specific linguistic structures, or speech production challenges).</em></p>\n\n",
                "matched_terms": [
                    "speech",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech generation.</span>\nPrompts were rendered into audio with CosyVoice2-0.5B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib17\" title=\"\">2024b</a>)</cite>. The prompts are selected from human-verified high-quality samples, where the selection is conducted from open-source datasets, such as LibriSpeech/Gigaspeech for English, and WenetSpeech for Chinese&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib49\" title=\"\">2015</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib5\" title=\"\">2021</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib77\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "samples"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate a diverse set of speech foundation models spanning both English and Mandarin. In total, we include 8 English and 7 Mandarin models, covering open-source and proprietary systems, and balancing end-to-end and cascaded architectures. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.T7\" title=\"Table 7 &#8227; D.4 Candidate Models for Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> provides the full list of models.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "total"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Step 1: We first build a speech database (LibriSpeech and GigaSpeech for English, WenetSpeech for Mandarin&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib49\" title=\"\">2015</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib5\" title=\"\">2021</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib77\" title=\"\">2022</a>)</cite>). Each sample is annotated with Gemini-based speech captions describing salient acoustic attributes.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "annotated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Step 4: The retrieved samples guide the zero-shot TTS system, which synthesizes speech conditioned on both the transcript and matched acoustic profile.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "samples"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2\" title=\"Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the internal annotation interface used for <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>.\nThe platform provides an integrated environment for both playback and evaluation of model-generated role-play speech samples.\nAnnotators first interact with the audio player interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf1\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>), which visualizes waveforms and spectrograms for multiple versions of the same prompt, enabling side-by-side comparison under a shared role instruction (e.g., firefighter, teacher, or actor).\nThen, they proceed to the annotation interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf2\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a>), where they assign scores for multiple dimensions such as <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, along with reasoning and confidence inputs.\nThis structured interface supports high-consistency, fine-grained evaluation of role-play behaviors in speech models.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "samples",
                    "dimensions",
                    "evaluation",
                    "archetype",
                    "speech",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This appendix contains the full human annotation instructions used for archetype evaluation.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This appendix starts with a summary of the task preparation pipeline for our proposed realism evaluation. Then, we provides the detailed prompts and system setups for constructing the Realism Evaluation dataset described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4.SS2\" title=\"4.2 Realism Data Collection &#8227; 4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "summary",
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realism Evaluation adopts a bottom&#8211;up perspective grounded in real human speech data, with the goal of capturing the nuanced delivery, prosody, and contextual fit that synthetic archetype prompts cannot fully represent.\nRather than relying on stereotypes, this strategy leverages real-world recordings and carefully constructed contrasts, enabling fine-grained assessment of model performance under realistic conditions.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "evaluation",
                    "data",
                    "archetype",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data from real human speech sources.</span>\nWe construct the realism evaluation set primarily from natural speech material:</p>\n\n",
                "matched_terms": [
                    "realism",
                    "evaluation",
                    "data",
                    "speech",
                    "sources"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, real human ground-truth segments and systematically constructed negatives form a balanced dataset that supports realism-oriented evaluation.\nThe positives provide natural anchors of role-play performance, while the negatives introduce controlled mismatches that test whether evaluation models can distinguish real human speech from synthetic approximations.\nThis design ensures that realism evaluation emphasizes subtle paralinguistic and contextual cues, moving beyond intelligibility or content fidelity.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "provide",
                    "evaluation",
                    "test",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide consistent evaluation context, we generate local scene descriptions summarizing the situational background.\nThese prompts aim to capture the immediate narrative context without introducing extraneous detail.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "provide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To make the construction of the realism evaluation task concrete, we present a worked example that begins with a curated local scene and character profile, followed by the same transcript adapted into mismatched contexts. These mismatches appear either at the <em class=\"ltx_emph ltx_font_italic\">local-scene</em> level or at both the <em class=\"ltx_emph ltx_font_italic\">character profile and scene</em> level, thereby providing controlled degradations of realism.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the character Yunjin from <span class=\"ltx_text ltx_font_italic\">Genshin Impact</span> as a running case study, which is also included in our released dataset.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>As acknowledged, the use of real-world recordings is conducted under legal and ethical review. All curated data will be released as part of our open-source package.</span></span></span> This example illustrates how alignment between a speaker&#8217;s persona, local context, and delivery underpins realism, and how systematically breaking that alignment produces contrastive test cases.</p>\n\n",
                "matched_terms": [
                    "data",
                    "realism",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative Local Scenes.</span>\nTo stress realism evaluation, we insert the same transcript into alternative backgrounds. These reveal how context-shifted delivery degrades narrative coherence:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Level 2 (Severe Mismatch).</span>\n<em class=\"ltx_emph ltx_font_italic\">Fake Scene:</em> On a modern city street, a rebellious street performer in jeans and a rock-band T-shirt strums an electric guitar, suddenly shouting the verses in parody.\n<em class=\"ltx_emph ltx_font_italic\">Fake Profile:</em> Mark, an anti-traditionalist performer who mocks classical art and heroic ideals, favoring satire and irony.\n<em class=\"ltx_emph ltx_font_italic\">Reason:</em> The verses, originally meant to inspire reverence, become ironic and absurd when delivered by someone who rejects those values. The clash is extreme, generating intentional ridicule and total loss of realism.</p>\n\n",
                "matched_terms": [
                    "total",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Discussion: Realism Example vs. Archetype Examples.</span>\n</p>\n\n",
                "matched_terms": [
                    "realism",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Yunjin case study illustrates a fundamental difference between realism evaluation and archetype evaluation. Archetype prompts, as described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>, are constructed by pairing abstract role categories with synthetic contextual scenes. These cover a broad space of possible situations, ranging from modern occupations (e.g., firefighters) to fictional personas (e.g., cult leaders in martial arts fantasy) or social identities (e.g., overprotective parents, laid-back friends). While such prompts provide controlled and diverse coverage, they are ultimately template-based and rely on stereotypes to elicit role-play behaviors.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "provide",
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the Yunjin example is drawn from authentic performance, where the character&#8217;s persona (an opera singer) and local scene (a battlefield narration) are naturally aligned through artistic delivery. The realism task does not merely test whether a model can follow a prompt, but whether it can preserve coherence between a speaker&#8217;s global identity, immediate context, and expressive prosody. When Yunjin&#8217;s transcript is transplanted into mismatched settings or paired with incongruent speaker profiles, the breakdown in realism becomes apparent: the same words shift from inspiring to awkward, jarring, or even absurd.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This distinction highlights the complementary strengths of the two evaluation paradigms. Archetype evaluation systematically explores a wide combinatorial space of roles and situations, but may lack the paralinguistic and contextual richness of real speech. Realism evaluation, on the other hand, anchors assessment in authentic delivery and contextual fit, providing a finer-grained lens on coherence and naturalness. Together, they ensure that role-play evaluation spans both broad stereotypical coverage and nuanced real-world plausibility.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "evaluation",
                    "archetype",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed a diverse set of TTS systems to generate negative character profiles and resynthesized speech for Realism Evaluation.\nThese included both commercial and non-commercial models, with variation in style control, conditioning prompts, and reference usage.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.T8\" title=\"Table 8 &#8227; Same-Speaker Voice Clone. &#8227; F.6 TTS Systems and Settings &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> summarizes the systems and settings.\nBelow, we detail the major strategies used across systems:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A character profile was first converted into a descriptive voice specification by prompting <span class=\"ltx_text ltx_font_typewriter\">deepseek-v3</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib38\" title=\"\">2024a</a>)</cite>.\nThis description was submitted to the TTS service to generate candidate voices.\nWe then previewed samples via Gemini2.5Pro, selecting the voice that best matched the speaker style of the original recording before proceeding to generate speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "samples"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The annotation interface for <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> focuses on assessing contextual and narrative coherence in model-generated speech.\nUnlike the archetype-oriented evaluation, which emphasizes stylistic and emotional fidelity, this platform examines how well the voice performance aligns with a character&#8217;s personality, dialogue situation, and scene progression.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This appendix contains the full human annotation instructions used for realism evaluation.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dual settings: Archetype vs. Realism.</span>\nWe deliberately separate stereotype-driven <em class=\"ltx_emph ltx_font_italic\">Archetype</em> from bottom-up <em class=\"ltx_emph ltx_font_italic\">Realism</em> tasks.\nThis avoids averaging away nuance: archetype labels reflect schema-level fit; realism labels prioritize fine-grained, in-situ delivery from real-world or carefully constructed contrastive material.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Separation of concerns.</span> By isolating <em class=\"ltx_emph ltx_font_italic\">Archetype</em> vs. <em class=\"ltx_emph ltx_font_italic\">Realism</em> we (i) retain scalability where stereotypes are enough, and (ii) do fine-grained realism without stereotype leakage.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transparent evaluation substrate.</span> Ours produces per-dimension human rationales and confidence; judge-only pipelines (ALLM/Gemini) provide scores with limited per-sample interpretability&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "provide"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The data presented to annotators consisted of synthetic speech samples, anonymized human speech, or recordings from public corpora, and contained no personally identifiable or sensitive information. To mitigate potential risks, annotators were allowed to skip any item that they found uncomfortable or inappropriate.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "data",
                    "samples"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The archetype evaluation provides a detailed view of how speech foundation models (SFMs) perform when tasked with role-play scenarios grounded in stereotypical characters and situations. Human annotations cover four dimensions, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, and <span class=\"ltx_text ltx_font_italic\">Content Pass</span>, and the results reveal important trends, as well as challenges in interpreting scores across languages.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "languages",
                    "scenarios",
                    "dimensions",
                    "evaluation",
                    "archetype",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A critical observation is that annotations in Mandarin and English cannot be directly compared. The annotation groups for the two languages differed in their linguistic backgrounds and cultural interpretations, which leads to systematic differences in scoring. Our bilingual pilot further confirmed that annotators often interpret role-play appropriateness and expressiveness differently depending on their cultural frame of reference. For example, Mandarin raters tend to reward delivery styles that align with culturally expected prosody and politeness strategies, while English raters prioritize spontaneity and naturalness. These discrepancies parallel the findings of the singing voice conversion challenge (SVCC2023) from <cite class=\"ltx_cite ltx_citemacro_citet\">Huang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib31\" title=\"\">2023</a>)</cite>, where native and non-native listeners provided significantly different ratings for the same samples, particularly for naturalness and speaker similarity. Consequently, while absolute scores differ substantially across Mandarin and English, only within-language comparisons are meaningful. Cross-lingual ranking is not valid given the different annotation bases.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "samples",
                    "languages"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Within Mandarin from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, the relative rankings of models are clear. <span class=\"ltx_text ltx_font_typewriter\">Doubao</span> stands out as the strongest performer across all four dimensions, reaching near-ceiling levels in both <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> (4.37) and <span class=\"ltx_text ltx_font_italic\">Content Pass</span> (0.98). It also leads in role-related dimensions such as <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, indicating balanced competence across linguistic accuracy, delivery, and expressiveness. <span class=\"ltx_text ltx_font_typewriter\">G-CosyVoice2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-CosyVoice2</span> follow closely, especially in audio quality, though they lag behind in role appropriateness. <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> achieve mid-range results, reflecting higher variance across samples, while <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> shows competitive content understanding but lower naturalness and appropriateness. <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> consistently ranks the lowest, with large variance, underscoring instability and weaker alignment with archetypal role expectations.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "samples"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The picture shifts in English, elaborated in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Here, <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> emerges as the most effective model by a wide margin, achieving top scores in both <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> (3.46) and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> (3.49), as well as high content pass rates (0.97). This reflects its strong grounding in English discourse norms and role adherence. In contrast, <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> variants perform well in <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> but are weaker in role alignment, highlighting a trade-off between acoustic fidelity and role-play embodiment. Other models such as <span class=\"ltx_text ltx_font_typewriter\">Doubao</span>, <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span>, and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> achieve moderate results, while <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> trail far behind, showing limited robustness in English scenarios. Taken together, the English results reveal both a sharper separation across models and a wider variance in appropriateness scores compared to Mandarin.</p>\n\n",
                "matched_terms": [
                    "scenarios",
                    "roleplay"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T11\" title=\"Table 11 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> and Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.F5\" title=\"Figure 5 &#8227; English Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.F6\" title=\"Figure 6 &#8227; English Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> further highlight differences across domains. In Mandarin, performance is relatively consistent across categories, with means clustering around 3.1&#8211;3.3. The highest scores appear in <span class=\"ltx_text ltx_font_typewriter\">SocialIdentity</span> and <span class=\"ltx_text ltx_font_typewriter\">DailyOccupation</span>, suggesting that models handle culturally grounded archetypes more reliably. In English, however, domain variation is much greater. Categories such as <span class=\"ltx_text ltx_font_typewriter\">FantasyOccupation</span> and <span class=\"ltx_text ltx_font_typewriter\">NamedCharacters</span> receive notably lower scores (means below 2.0), reflecting the difficulty of producing expressive, role-appropriate speech in imaginative or culturally specific contexts. Occupational roles remain relatively easier for models in both languages, likely because these scenarios are closer to training data distributions.</p>\n\n",
                "matched_terms": [
                    "scenarios",
                    "languages",
                    "data",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although cross-lingual comparisons are invalid, this does not diminish the utility of archetype evaluation for role-play benchmarking. In fact, the observed inconsistencies reinforce a central point: successful role-play requires models to adapt to the cultural and linguistic norms of the target language community. A system that performs well in Mandarin but poorly in English cannot be considered universally capable, since end users will evaluate appropriateness and naturalness according to their native expectations. Thus, archetype evaluation highlights not only model-specific strengths and weaknesses but also the need for culturally grounded evaluation pipelines.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "evaluation",
                    "benchmarking",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In sum, archetype evaluation reveals three major findings:\n(1) Mandarin and English annotations cannot be directly compared, due to linguistic and cultural annotation inconsistencies, consistent with prior evidence from SVCC 2023.\n(2) Within each language, clear model rankings emerge, <span class=\"ltx_text ltx_font_typewriter\">Doubao</span> dominates in Mandarin, while <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> excels in English.\n(3) Domain sensitivity remains a challenge, particularly for imaginative or culturally rich roles, where scores are substantially lower and inter-model variance is higher.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although realism evaluation targets nuanced paralinguistic behaviors, human annotators achieve <em class=\"ltx_emph ltx_font_italic\">consistently high agreement</em> once the rubric disentangles the aspects to be judged.\nAs shown in the agreement summary in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F7\" title=\"Figure 7 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, most dimensions fall in the <math alttext=\"0.86\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>0.86</mn><annotation encoding=\"application/x-tex\">0.86</annotation></semantics></math>&#8211;<math alttext=\"0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mn>0.88</mn><annotation encoding=\"application/x-tex\">0.88</annotation></semantics></math> range, with the lowest agreement on <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> (<math alttext=\"\\approx 0.83\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.83</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.83</annotation></semantics></math>) and the highest on <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em> / <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> (<math alttext=\"\\approx 0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.88</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.88</annotation></semantics></math>).\nEmotion-related, progressive metrics (Intensity and Transition) are slightly lower (<math alttext=\"\\approx 0.85\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.85</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.85</annotation></semantics></math>), reflecting the added difficulty of judging graded expressivity.</p>\n\n",
                "matched_terms": [
                    "summary",
                    "dimensions",
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For both zeroshot and fewshot experiments, we adopt a streamlined version of the annotation guidelines to ensure consistency with the human annotation process. The exact rubric prompts for archetype and realism evaluation used are provided below:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.T12\" title=\"Table 12 &#8227; L.2 Zeroshot/Fewshot Ablation Experiments &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> shows how different prompting strategies affect realism evaluation performance with <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>. Several observations can be drawn:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the archetype test set, all fine-tuned variants achieve strong <span class=\"ltx_text ltx_font_italic\">Content Pass</span> accuracy above 92%. Differences appear mainly in correlation-based metrics. Here, <span class=\"ltx_text ltx_font_typewriter\">Base-FT</span> yields the highest score on <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> delivers the strongest performance in <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> and the best overall average (0.629). This suggests that instruction-tuned checkpoints paired with parameter-efficient adaptation can better capture higher-level perceptual traits.</p>\n\n",
                "matched_terms": [
                    "test",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For realism evaluation on the basic test set, <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> consistently achieves the strongest correlations across most dimensions, especially for <span class=\"ltx_text ltx_font_italic\">Character Identity</span> and <span class=\"ltx_text ltx_font_italic\">Traits</span>, leading to the best average correlation (0.668). <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> also performs competitively (0.630), while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> lags slightly behind (0.625). These results highlight that full fine-tuning may still offer an advantage when the evaluation set is relatively homogeneous and closer to the training distribution.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "evaluation",
                    "realism",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A different pattern emerges on the real recording test set. Both <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> and <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> struggle, yielding negative or near-zero correlations across most dimensions. In contrast, <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> shows clear improvements, achieving positive correlations in all prosodic and emotional dimensions and producing the highest overall average (0.247). This indicates that LoRA adaptation on top of an instruction-tuned backbone provides better generalization to out-of-distribution, real-world data.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "data",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> is competitive on in-domain evaluation, it suffers a substantial drop when applied to real recordings. By contrast, <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> offers a more robust balance: it maintains reasonable performance on archetype and basic sets, while significantly outperforming other settings on real-world data. Based on this robustness, we adopt <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> as our final configuration for subsequent experiments.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "data",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the comprehensive design of Speech-DRAME, several limitations remain.\nFirst, while our dual evaluation paradigm combines top-down archetype assessment with bottom-up realism evaluation, both strategies are constrained by the quality and scope of available data. Archetype evaluation inevitably relies on simplified stereotypes, which can overlook subtler paralinguistic and contextual cues. Realism evaluation, though richer, still struggles with domain mismatch between synthetic data and real human recordings, as evidenced by the performance drop observed in the real-recording test set. This highlights the persistent gap between benchmark-driven evaluation and real-world variability in speech role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "archetype",
                    "evaluation",
                    "realrecording",
                    "data",
                    "test",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Third, while DRAME-Eval substantially outperforms zero-shot and few-shot ALLMs, its reliance on fine-tuning from a strong base model raises questions of accessibility and scalability. Future work should explore lightweight adaptation techniques, semi-supervised training with partially annotated data, and leveraging emergent capabilities of open-source multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "base",
                    "data",
                    "annotated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, although we validated DRAME-Eval against human annotations, alignment under realism settings remains modest. This suggests that even human-annotated benchmarks are insufficiently fine-grained to capture the full complexity of human perception in speech role-play. Richer annotation protocols, more diverse speaker populations, and integration of perceptual psychology insights could further improve evaluation fidelity.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "roleplay",
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Looking forward, we see several promising research directions. We intentionally restrict evaluation to single-turn responses for clarity and tractability. However, multi-turn extensions that capture narrative progression, long-term consistency, and dynamic emotion shifts remain a natural next step. Incorporating multilingual and cross-cultural datasets will enhance generality and fairness. On the modeling side, exploring hybrid evaluation strategies that combine discrete rubrics with continuous perceptual signals (e.g., prosody embeddings or affective measures) may better bridge the gap between automatic and human judgments. Finally, integrating Speech-DRAME into reinforcement learning pipelines as a reward model could help drive the next generation of expressive, human-aligned speech role-play systems.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "roleplay",
                    "evaluation"
                ]
            }
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Archetype role-play evaluator performance. We report classification accuracy for Content Pass and Pearson correlation coefficients for other dimensions. The Avg. column is the average of all correlation coefficients.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\">Config.</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\">Model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">Content Pass Acc.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">Audio Quality</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">Human Likeness</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">Appropriateness</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Avg.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" rowspan=\"5\">Zeroshot</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">89.3%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.465</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.483</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.492</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.480</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">89.9%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.395</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.483</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.422</td>\n<td class=\"ltx_td ltx_align_center\">0.433</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">71.7%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.089</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.132</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.121</td>\n<td class=\"ltx_td ltx_align_center\">0.114</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">86.7%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.078</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.210</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.200</td>\n<td class=\"ltx_td ltx_align_center\">0.163</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">84.1%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.023</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.039</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.042</td>\n<td class=\"ltx_td ltx_align_center\">0.019</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" rowspan=\"2\">Fewshot</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">91.1%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.477</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.492</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.517</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.495</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">89.9%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.296</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.374</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.377</td>\n<td class=\"ltx_td ltx_align_center\">0.349</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\">Finetune</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">93.6%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.682</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.680</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.525</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.629</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "roleplay",
            "column",
            "content",
            "pearson",
            "fewshot",
            "config",
            "qwen2audio",
            "coefficients",
            "zeroshot",
            "evaluator",
            "audio",
            "average",
            "gemini25pro",
            "archetype",
            "accuracy",
            "performance",
            "gpt4oaudio",
            "avg",
            "pass",
            "finetune",
            "report",
            "classification",
            "kimiaudio",
            "qwen25omni",
            "correlation",
            "appropriateness",
            "model",
            "dimensions",
            "all",
            "other",
            "acc",
            "human",
            "drameeval",
            "likeness",
            "quality"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype Evaluation.</span>\nThe results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T2\" title=\"Table 2 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reveal clear distinctions across evaluation settings. In the zero-shot condition, proprietary ALLMs such as <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> substantially outperform public models, achieving strong <span class=\"ltx_text ltx_font_italic\">content pass</span> accuracy (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>90%) and moderate correlations with other dimensions(<math alttext=\"0.43\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mn>0.43</mn><annotation encoding=\"application/x-tex\">0.43</annotation></semantics></math>&#8211;<math alttext=\"0.48\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mn>0.48</mn><annotation encoding=\"application/x-tex\">0.48</annotation></semantics></math>). By contrast, public models exhibit limited predictive ability, with correlations often below 0.2. Few-shot prompting provides modest gains for proprietary models, improving both accuracy and correlation (e.g., <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> rises from 0.480 to 0.493 average correlation). The most significant improvement arises from finetuning: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves an average correlation of 0.629, marking a large margin over zero-shot/few-shot models and demonstrating the effectiveness of task-specific supervision.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Role-play has become a key testbed for generative models, expanding from text-only dialogue to multimodal interaction. Extending role-play to speech captures prosody, emotion, and delivery, but also poses new evaluation challenges. Current pipelines often use audio large language models (ALLMs) as zero-shot judges, which miss paralinguistic cues, collapse multiple aspects into coarse scores, and rely on synthetic speech references that fail to reflect real-world roles. We present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework that contributes at three levels: (i) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-EvalBench</em>, an evaluation benchmark with bilingual human-annotated data and protocols for training and testing speech evaluation models&#160;(SEMs), (ii) <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, a fine-tuned evaluation model, which substantially outperforms zero-shot and few-shot ALLMs, and (iii) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-RoleBench</em>, a speech role-play benchmark that leverages <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> as an automatic judge to compare speech foundation models (SFMs). Speech-DRAME distinguishes between two complementary evaluation strategies: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>, a top-down approach measuring adherence to broad role archetypes, and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em>, a bottom-up approach grounded in real human speech that emphasizes nuanced role quality. Compared to zero-shot ALLM judges, <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves stronger agreement with human ratings (Pearson correlation from 0.480 to 0.629 in archetypes, and 0.390 to 0.625 in realism). By integrating transparent benchmark resources, modeling approaches, and system-level evaluation, Speech-DRAME provides the first comprehensive, reproducible foundation for assessing spoken role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "correlation",
                    "zeroshot",
                    "audio",
                    "model",
                    "pearson",
                    "archetype",
                    "human",
                    "fewshot",
                    "drameeval",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Evaluating speech role-play, however, remains a major challenge. Current pipelines often rely on ALLMs as judges, typically in a zero-shot setting&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>; Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>)</cite>. While this strategy can provide rough ratings, it falls short for speech role-play:</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">it collapses diverse aspects of performance into limited coarse dimensions, obscuring critical dimensions of role-play quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite>, and</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dimensions",
                    "performance",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">it depends on synthetic speech references rather than real human speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>)</cite>, limiting its ability to reflect real-world diversity in speech role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome these limitations, we present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span> (Speech Detailed Role-play Assessment with Modeling and Evaluation), a unified framework that moves beyond synthetic or judge-only pipelines. Speech-DRAME is distinguished by three aspects: [i] a dual evaluation paradigm that balances archetype-driven generality with human-grounded realism (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS2\" title=\"3.2 Role-play Evaluation with Two Strategies &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), [ii] human annotations that provide perceptual grounding absent in prior resources (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4\" title=\"4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), and [iii] evaluation-model alignment that enables not only benchmarking of speech foundation models (SFMs) but also systematic analysis of the evaluation models (SEMs) themselves (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5\" title=\"5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). These aspects establish Speech-DRAME as a comprehensive and trustworthy framework for measuring and improving speech role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, Speech-DRAME contributes at three levels: (i) <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>, an evaluation benchmark consisting of human-annotated datasets for archetype and realism role-play; (ii) <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span>, a fine-tuned SEM trained on EvalBench that surpasses zero-shot and few-shot ALLMs; and (iii) <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span>, a speech role-play benchmark that leverages DRAME-Eval as an automatic judge to systematically compare SFMs. By combining dual evaluation strategies, human-anchored supervision, and evaluation-model alignment, Speech-DRAME delivers systematic insights into both role-play systems and evaluation models themselves.\nOur contributions are four-fold:</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "zeroshot",
                    "fewshot",
                    "archetype",
                    "drameeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a <span class=\"ltx_text ltx_font_bold\">formal task definition</span> of speech role-play and its evaluation via speech foundation models (SFMs) for generation and speech evaluation models (SEMs) for assessment, unifying linguistic and paralinguistic dimensions.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>, the first evaluation benchmark with human annotations under both archetype- and realism-based strategies, supporting the training and testing of SEMs.</p>\n\n",
                "matched_terms": [
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span>, a fine-tuned SEM trained on EvalBench that substantially outperforms zero-shot and few-shot ALLMs, demonstrating the importance of human supervision for evaluation models.</p>\n\n",
                "matched_terms": [
                    "drameeval",
                    "zeroshot",
                    "human",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We establish <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span>, a speech role-play benchmark that leverages DRAME-Eval as an automatic judge to systematically compare both end-to-end and cascaded SFMs across multiple role quality dimensions.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Relevant codes, pre-trained models, and datasets will be open-sourced at \n<br class=\"ltx_break\"/><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/Anuttacon/speech_drame\" title=\"\">https://github.com/Anuttacon/speech_drame</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "drameeval",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-native benchmarks.</span>\nMore recently, a number of resources have been introduced to evaluate spoken role-play agents. OmniCharacter-10K <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>)</cite> emphasizes character voice traits, while others&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib39\" title=\"\">2025</a>; Hou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib27\" title=\"\">2025</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib15\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib7\" title=\"\">2024b</a>)</cite> extend evaluation toward vocal communication, assistant capability, and multi-turn realism. Omni models such as StepAudio and KimiAudio <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite> also include role-play scenarios in their evaluation. SpeechRole and VStyle <cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>; Zhan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib76\" title=\"\">2025</a>)</cite> are closer to our work, offering datasets that test interaction ability, expressiveness, and fidelity. However, these resources are primarily built from LLM-generated dialogues and TTS outputs, with limited human validation and a heavy reliance on proprietary ALLM-as-judge pipelines. As a result, they lack grounding in diverse human speech and typically apply only coarse rubrics.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "kimiaudio",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Automatic judging with ALLMs.</span>\nIn parallel, advances in automatic judging highlight both opportunities and limitations. Audio-capable LLMs such as AudioJudge and Audio-Aware LLMs as Judges <cite class=\"ltx_cite ltx_citemacro_citep\">(Manakul et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib44\" title=\"\">2025</a>; Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>)</cite> can assess style, emotion, and delivery, while systems like Auto-ATT <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib65\" title=\"\">2025c</a>)</cite> scale this to Audio Turing Tests. Yet studies consistently reveal high prompt sensitivity and shallow capture of paralinguistic cues, suggesting that zero-shot judging remains insufficient for nuanced role-play evaluation.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "audio",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Expressive TTS evaluation.</span>\nRecent work has explored evaluation of instruction-following and style-controlled TTS. InstructTTSEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite> explicitly includes role-play scenarios, while EmergentTTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">(Manku et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib45\" title=\"\">2025</a>)</cite> and discrete speech LM benchmarks <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang &amp; Sz&#233;kely, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib63\" title=\"\">2024</a>)</cite> emphasize prosody, spontaneity, and speaker consistency. These efforts push beyond intelligibility and naturalness, but still center on synthetic outputs rather than real human role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our distinction.</span>\nIn contrast, Speech-DRAME unifies these considerations through three elements: (i) a dual evaluation paradigm that combines archetype-driven generality with realism grounded in authentic human speech, (ii) human annotations that provide a reliable perceptual gold standard, and (iii) alignment of SEMs to human ratings, enabling systematic evaluation of both SFMs and the evaluation models themselves. This combination moves beyond synthetic benchmarks or zero-shot judging to provide a comprehensive and trustworthy framework for speech role-play assessment.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "zeroshot",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech role-play.</span>\nWe formalize <em class=\"ltx_emph ltx_font_italic\">speech role-play</em> as a conditional speech generation problem grounded in a character profile and scene description.\nGiven a dialogue context <math alttext=\"{\\bm{C}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{speech}}</annotation></semantics></math>, a character profile <math alttext=\"{\\bm{C}}_{\\text{profile}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>profile</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{profile}}</annotation></semantics></math>, and a scene specification <math alttext=\"{\\bm{C}}_{\\text{scene}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>scene</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{scene}}</annotation></semantics></math>, a <em class=\"ltx_emph ltx_font_italic\">speech foundation model</em> (SFM) generates a role-played speech response <math alttext=\"{\\bm{S}}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119930;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{S}}_{r}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech role-play evaluation can be framed in multiple ways depending on the available data and the desired granularity of analysis.\nWithin Speech-DRAME, we formalize two distinct strategies inspired by sociology theories<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Please refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A2\" title=\"Appendix B Theoretical Motivation for Dual Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> for detailed discussion of the dual evaluation design.</span></span></span>, including a top-down, archetype-oriented option using synthetic speech and a bottom-up, realism-oriented option from real human speech.\nRather than merging these into a single rubric, we treat them as separate perspectives, each with unique advantages and limitations.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarizes the key contrasts, while further implementation details are provided in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>-&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS5\" title=\"F.5 Examples of Realism Role-play &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.5</span></a>). The second strategy adopts a <em class=\"ltx_emph ltx_font_italic\">bottom-up</em> design, motivated by real-world performance and real human speech.\nInstead of relying solely on stereotypes, this approach grounds evaluation in recordings from professional and non-professional speakers, drawn from realistic dialogue and media sources.\nJudgments in this setting are more fine-grained and multi-dimensional, addressing aspects such as prosodic dynamics, emotional fidelity, character consistency, and contextual fit.\nWhile this strategy requires more human annotation and careful curation, it provides benchmarks that reflect how role-play is perceived in practice.\nIt thus offers a richer framework for analyzing model outputs in settings where realism and subtlety are essential.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "model",
                    "human",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human Annotation.</span>\nAll collected audio undergoes a standardized annotation protocol: (1) an initial semantically-based screening to exclude invalid clips; (2) rubric-based scoring across multiple perceptual dimensions; and (3) meta-annotations such as confidence or prompt-quality judgments. Full rubrics are provided in the appendices&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A5\" title=\"Appendix E Annotation Guidelines for Archetype Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">E</span></a> &amp; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>All data collection follows in high standard ethical consideration as outlined in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A9\" title=\"Appendix I Annotation Ethics &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "audio",
                    "all",
                    "dimensions",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Archetype Data Collection &#8227; 4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents a summary of the curated <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> dataset, where we further elaborate the details of archetype and realism evaluation data collection in following subsections, respectively. For benchmark purpose, we provide an official train-test split for the human annotated dataset for <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> so that following researchers can use the dataset to train and evaluate their SEMs for speech role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Archetype evaluation adopts a top&#8211;down perspective from sociology and role theory&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goffman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib23\" title=\"\">1949</a>; Campbell, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib4\" title=\"\">2008</a>)</cite>, abstracting role-play into broad and recognizable categories.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Sources.</span> The data is collected from a range of speech foundation models, including both cascaded and end-to-end models. We limit that only audio prompts are used as the input but adopt different prompt strategies for best quality with regards to different models. Details about the selected models and prompting details are discussed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS4\" title=\"D.4 Candidate Models for Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotation follows two stages: an initial semantic check (i.e., <span class=\"ltx_text ltx_font_italic\">Content Pass</span>) to filter out instruction non-compliance or audio defects, followed by rubric-based scoring on three dimensions: <em class=\"ltx_emph ltx_font_italic\">Audio Quality</em>, <em class=\"ltx_emph ltx_font_italic\">Human-likeness</em>, and <em class=\"ltx_emph ltx_font_italic\">Appropriateness</em>. Confidence judgments are also recorded. Detailed rubric scales are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A5\" title=\"Appendix E Annotation Guidelines for Archetype Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</p>\n\n",
                "matched_terms": [
                    "appropriateness",
                    "pass",
                    "content",
                    "audio",
                    "dimensions",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype Evaluation Data Summary.</span>\nThe resulting archetype evaluation dataset comprises 552 distinct scenarios for both Mandarin and English, evaluated across eight models,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>The model list is discussed in Appdenix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS4\" title=\"D.4 Candidate Models for Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.4</span></a>. Note we only include seven models for English due to the deactivation of ChatGPT-4O advanced voice model.</span></span></span> yielding 8,280 samples. In total, the dataset are split into 6,780 for training and 1,500 for testing. For benchmarking, an additional 1,250 scenarios spanning seven role/scene categories are prepared to facilitate archetype role-play benchmarking. More detailed statistics and analyses are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10\" title=\"Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">J</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "model",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotators judge whether performances align with the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em>, using rubrics for <em class=\"ltx_emph ltx_font_italic\">Prosodic Dynamics (i.e., Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis)</em>, <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness (with progressive gating over three sub-dimensions: Emotion Accuracy, Emotion Intensity, Emotion Transition)</em>, <em class=\"ltx_emph ltx_font_italic\">Character Consistency (i.e., Voice Identity Matching and Traits Embodiment)</em>, <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance (i.e., Local Scene Fit and Global Story Coherence)</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>. A prompt quality check and confidence ratings are also collected. Full scales and gating details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "dimensions",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real-world Recoding.</span>\nAs noted in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S2\" title=\"2 Related Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, most prior efforts have relied heavily on synthetic data, mostly from TTS systems based on curated professional role-play voices. While such data can achieve high fidelity, it often lacks the diversity and spontaneity of real-world delivery, limiting the generalizability of evaluation models. To address this gap, we expand realism evaluation beyond synthetic and curated voices by incorporating newly recorded speech from both professional and amateur voice actors. This design introduces a broader spectrum of delivery styles, from polished professional acting to more natural and less controlled amateur performance.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation Data Summary.</span>\nThe resulting realism evaluation dataset is constructed from a combination of curated media sources, including internal collections, public datasets (e.g., NCSSD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib40\" title=\"\">2024b</a>)</cite>), and original recordings collected for this project.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>All data were obtained under appropriate licenses and are legally available for research release.</span></span></span> We compile 15k annotated role-play samples: 9k in Mandarin and 5k in English. The dataset is divided into 12k samples for training, 2k mixed samples for the base test set, and 1k real-recording samples for the test set. The first test set supports general evaluation using our proposed pipeline with negative samples, while the second focuses exclusively on human speech performed by both professional and amateur voice actors. For benchmarking, 2,000 scenarios from the realism dataset are designated for realism role-play benchmarking. Detailed statistics and analyses are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11\" title=\"Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">K</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the two proposed datasets, we conduct experiments under three settings: zero-shot, few-shot, and finetuning, using pre-trained ALLMs. We formulate the task as a multi-metric estimation problem&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tjandra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib58\" title=\"\">2025</a>; Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib74\" title=\"\">2025</a>; Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib55\" title=\"\">2025b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib54\" title=\"\">a</a>)</cite>, where a single model is trained or prompted to predict multiple evaluation dimensions.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dimensions",
                    "zeroshot",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-shot and Few-shot ALLMs.</span>\nWe first evaluate the zero-shot capabilities of both proprietary and public models, including <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>, <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>, <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio</span>, and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>. All evaluations follow a unified prompting template that provides audio input alongside scene and character context.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "gpt4oaudio",
                    "zeroshot",
                    "audio",
                    "gemini25pro",
                    "all",
                    "fewshot",
                    "kimiaudio",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For proprietary models, we report the average of five independent runs to reduce variance and obtain more stable predictions. For open-source models, we observed weaker instruction-following ability and limited adaptation to unfamiliar tasks. To mitigate this, we did not request simultaneous prediction of multiple metrics. Instead, we queried one metric at a time. Following <cite class=\"ltx_cite ltx_citemacro_citet\">Song &amp; Bahri (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>); Lukasik et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>, we interpret model outputs through token distribution over discrete labels (<span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score.</p>\n\n",
                "matched_terms": [
                    "report",
                    "average",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Finetuning Model: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>.</span>\nIn addition to zero-shot and few-shot evaluations, we fine-tune a dedicated evaluation model, denoted as <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> for archetype and realism role-play evaluation, respectively.\nWe initialize from <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio-7B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib11\" title=\"\">2024</a>)</cite>, which provides a strong foundation for multi-modal audio-text reasoning.\nTo better leverage the multiple human annotations available in our datasets, we adopt a sampling-based training strategy: at each step, one annotator label is randomly selected, following practices in label distribution learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Geng, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib22\" title=\"\">2016</a>; Wen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib67\" title=\"\">2023</a>)</cite>.\nThis stochastic supervision implicitly exposes the model to the empirical annotation distribution, while during inference we output the full distribution over discrete labels (e.g., <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score, consistent with recent decoding-based regression methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Song &amp; Bahri, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>; Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib70\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib64\" title=\"\">2025b</a>; Lukasik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>. We apply LoRA fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib28\" title=\"\">2022</a>)</cite>. All implementation details, including optimizer settings, training schedule, and sampling policy, are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS3\" title=\"L.3 Fine-tuning Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "finetune",
                    "zeroshot",
                    "model",
                    "fewshot",
                    "all",
                    "human",
                    "archetype",
                    "drameeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further conduct ablation studies in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS4\" title=\"L.4 Fine-tuning Ablation Experiments &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.4</span></a>, examining: the choice of different base audio LLMs and full-parameter fine-tuning versus LoRA adaptation.\nThese analyses highlight the robustness of our sampling&#8211;expectation training strategy and clarify the trade-offs in model design.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics for Role-play Evaluation Model.</span> For most dimensions with 1-5 scale, we report Pearson correlation. For <span class=\"ltx_text ltx_font_italic\">Content Pass</span> estimation, we report accuracy.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "correlation",
                    "pass",
                    "content",
                    "report",
                    "model",
                    "pearson",
                    "dimensions",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T3\" title=\"Table 3 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> extends the analysis to fine-grained realism evaluation. Similar to archetype results, proprietary ALLMs lead the zero-shot setting, with <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> obtaining an average correlation of 0.390, compared to 0.284 for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>. Public models remain weak baselines. Few-shot prompting consistently improves performance: <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> gains +0.042 and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> gains +0.083 over their zero-shot averages, showing the benefit of in-context supervision. Once again, finetuning is most effective: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves 0.625 average correlation, higher than the best few-shot results. Importantly, the gains are consistent across all 10 realism dimensions, confirming that the sampling&#8211;expectation training strategy generalizes beyond simple classification to nuanced paralinguistic cues.</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "correlation",
                    "zeroshot",
                    "average",
                    "gemini25pro",
                    "fewshot",
                    "dimensions",
                    "classification",
                    "all",
                    "archetype",
                    "drameeval",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real Recording Test Set.</span>\nThe real-recording evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T4\" title=\"Table 4 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> is notably more challenging, with all models suffering substantial performance drops. Proprietary models in the zero-shot setting achieve only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.04&#8211;0.08 average correlation, and public models often fall below zero. Few-shot prompting yields small but noticeable improvements, especially for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> (0.226 avg.), suggesting that exemplars partially mitigate domain mismatch between synthetic and real human speech. Interestingly, while finetuning achieves the highest overall average (0.247), its relative advantage over few-shot prompting is narrower than in the synthetic setting, and some dimensions (e.g., <span class=\"ltx_text ltx_font_italic\">Character Identity</span>) remain weak. We emphasize that this observed gap between synthetic and real recordings is not merely an artifact but an intended feature of Speech-DRAME, designed to expose the mismatch and drive future research into bridging synthetic-to-real generalization.</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "avg",
                    "zeroshot",
                    "correlation",
                    "average",
                    "dimensions",
                    "all",
                    "human",
                    "fewshot",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall.</span> Across both archetype and realism evaluations, we observe three consistent trends: (i) zero-shot performance is limited, particularly for open-source models, (ii) few-shot prompting offers moderate but consistent gains, especially for proprietary ALLMs, and (iii) finetuning with task-specific supervision (<span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>) delivers the largest improvements, achieving state-of-the-art correlations across nearly all dimensions. Nevertheless, results on the real-recording test set underscore the gap between synthetic and real human role-play scenarios, motivating future work in data diversification and domain adaptation.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "zeroshot",
                    "fewshot",
                    "dimensions",
                    "all",
                    "human",
                    "archetype",
                    "drameeval",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on fine-tuned <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, we establish the DRAME-RoleBench to evaluate SFMs&#8217; role play ability. For archetype evaluation, 1,250 scenarios are constructed following the same data construction pipeline as the <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>. For realism evaluation, we reuse 2,000 scenarios in the <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> test set. We follow the same prompt strategy as outlined in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "drameeval",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, we consider a comprehensive set of candidate models, including six end-to-end models: <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib47\" title=\"\">2025b</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib75\" title=\"\">2024</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib71\" title=\"\">2025a</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Qwen3Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib72\" title=\"\">2025b</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">StepAudio2Mini</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib68\" title=\"\">2025</a>)</cite>; and ten cascaded models by combining two widely-used textual LLMs (i.e., <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib12\" title=\"\">2025</a>)</cite> and <span class=\"ltx_text ltx_font_typewriter\">Qwen3-30B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib73\" title=\"\">2025</a>)</cite>) and five recent TTS models (i.e., <span class=\"ltx_text ltx_font_typewriter\">F5TTS</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib8\" title=\"\">2024c</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">IndexTTS2</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib82\" title=\"\">2025b</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">MaskGCT</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib66\" title=\"\">2025d</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Vevo1.5</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib79\" title=\"\">2025b</a>)</cite>, and <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib17\" title=\"\">2024b</a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "kimiaudio",
                    "qwen25omni",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype evaluation.</span>\nOn archetype role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T5\" title=\"Table 5 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), cascaded pipelines consistently outperform end-to-end systems. The strongest cascaded systems, particularly <span class=\"ltx_text ltx_font_typewriter\">G-IndexTTS2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-IndexTTS2</span>, achieve both high content pass rates (above 94%) and balanced scores across audio quality, human likeness, and appropriateness. By contrast, end-to-end models such as <span class=\"ltx_text ltx_font_typewriter\">StepAudio2Mini</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> perform competitively in terms of content coverage (above 88%), but their expressive scores remain slightly lower than their cascaded counterparts. This gap suggests that separating reasoning (LLM) from speech rendering (TTS) remains advantageous when archetype fidelity is emphasized. Nevertheless, the fact that top end-to-end systems already surpass 3.0 in average rating indicates rapid closing of the performance gap.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "appropriateness",
                    "quality",
                    "pass",
                    "content",
                    "audio",
                    "average",
                    "human",
                    "archetype",
                    "likeness",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism evaluation.</span>\nRealism role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T6\" title=\"Table 6 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) presents a different landscape. Here, performance differences between end-to-end and cascaded systems are less pronounced. Several end-to-end models, such as <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>, reach average scores of 3.24, matching or even exceeding the best cascaded pipelines. Importantly, realism scores are more evenly distributed across prosodic, emotional, character, and contextual dimensions, with most models clustering between 2.9&#8211;3.2. This convergence reflects the difficulty of realism-style evaluation: since the data preparation emphasized contrast with authentic human speech, models are judged under similar conditions, leading to smaller observed gaps.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "average",
                    "dimensions",
                    "performance",
                    "human",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human alignment.</span>\nWe further conducted small-scale human studies with 10% of the benchmark data to verify the reliability of the benchmark. Archetype evaluation achieves a Spearman correlation of 0.706 between human judgments and <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, validating that the automatic evaluator closely tracks human preferences in this setting. Realism evaluation, however, achieves a more modest Spearman correlation of 0.375. This lower alignment is expected, since realism data construction did not prioritize fine-grained model separation but instead emphasized distinguishing human versus synthetic speech.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>We view this as evidence that realism presents a harder, less-saturated frontier. Rather than a limitation of DRAME-Eval, this highlights the challenge of designing evaluators that capture nuanced human perception, which motivates future work.</span></span></span> As a result, all models occupy a relatively narrow performance band, reducing sensitivity to model-level differences. We view this as evidence that realism evaluation highlights a harder, less saturated frontier where both SFMs and SEMs require further advances.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "evaluator",
                    "model",
                    "all",
                    "human",
                    "archetype",
                    "drameeval",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, cascaded pipelines retain an edge in archetype scenarios, especially in content robustness and expressive fidelity, but end-to-end models are catching up quickly. In realism scenarios, the field remains unsolved: existing systems exhibit similar performance bands and evaluators show weaker alignment with human judgments, underscoring the need for improved realism-grounded data and evaluation. Together, these findings establish <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> as both a reliable testbed for archetype fidelity and a challenging benchmark for realism fidelity.</p>\n\n",
                "matched_terms": [
                    "content",
                    "human",
                    "performance",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework for assessing spoken role-play through two complementary lenses: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em> (top-down) and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> (bottom-up, human-grounded). Our human-annotated <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> enables training and testing of SEMs, and our fine-tuned <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span> substantially outperforms zero-/few-shot ALLM judges. Building on this, <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> provides system-level comparison across various SFMs, revealing a consistent edge for cascaded pipelines on archetypes and a tighter spread under realism, where real human performance evaluation remain challenging. Together, these resources establish a reproducible, analyzable foundation for spoken role-play evaluation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>Limitations and future works are discussed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A13\" title=\"Appendix M Limitations and Future Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">M</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "human",
                    "archetype",
                    "drameeval",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All data collection and annotation in this work followed strict ethical standards. Our bilingual datasets were constructed exclusively from legally accessible sources, including licensed media, open-source corpora, and original recordings conducted with informed consent. Annotators were recruited with appropriate compensation and provided with clear instructions, including the right to withdraw at any point. To minimize risks, no personally identifiable information was included in the released data, and all recordings were anonymized before annotation. We ensured cultural and linguistic diversity in annotator pools to mitigate potential bias across Mandarin and English role-play scenarios. While this study did not require formal IRB approval, we adhered to best practices in human-subject research, including transparency in task design and careful consideration of annotator well-being. Full details of our annotation protocol and ethical safeguards are presented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A9\" title=\"Appendix I Annotation Ethics &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data generation:</span> LLMs were employed to expand prompts (e.g., character profiles, scene descriptions) and to produce negative samples in realism evaluation. All outputs were screened by human annotators to ensure validity and reduce bias.</p>\n\n",
                "matched_terms": [
                    "all",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline evaluators:</span> State-of-the-art audio LLMs (e.g., <span class=\"ltx_text ltx_font_typewriter\">GPT4o</span>, <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>) were included as zero-shot judges in our evaluation benchmark, enabling direct comparison with human-aligned SEMs.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "zeroshot",
                    "qwen25omni",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation design and task construction:</span> <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> was used to assist with building instructions and summarizing story contexts. Drafted materials were subsequently refined by the authors and validated by human annotators.</p>\n\n",
                "matched_terms": [
                    "human",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our distinction between <span class=\"ltx_text ltx_font_italic\">Archetype Evaluation</span> and <span class=\"ltx_text ltx_font_italic\">Realism Evaluation</span> is not merely pragmatic, but is grounded in established theories of social identity, communication, and performance. The two perspectives capture complementary dimensions of how humans perceive and evaluate roles in interaction.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "performance",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism as Situated Performance</span>\nIn contrast, realism highlights the fine-grained enactment of roles in situated interaction. Goffman&#8217;s dramaturgical sociology emphasizes that everyday social life is itself a performance, where believability depends on subtle paralinguistic and pragmatic cues such as prosody, hesitation, and emotional shading&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goffman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib23\" title=\"\">1949</a>)</cite>. Similarly, interactional sociolinguistics has demonstrated how micro-level features like intonation, pacing, and turn-taking patterns shape the perception of authenticity in communication&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gumperz, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib25\" title=\"\">1982</a>)</cite>. Realism Evaluation thus anchors role-play assessment in bottom-up sensitivity to authentic delivery, beyond schematic expectations.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Complementarity of the Two Perspectives</span>\nPerformance theory provides a unifying lens for these dual dimensions. Schechner&#8217;s view of performance as both &#8220;restored behavior&#8221; (scripted, repeatable structures) and &#8220;living process&#8221; (situated enactment)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Schechner, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib52\" title=\"\">2017</a>)</cite> underscores the need to consider both archetypal scripts and enacted realism. Likewise, sociolinguistic theories of identity recognize both <span class=\"ltx_text ltx_font_italic\">ascribed categories</span> (archetypes imposed by social convention) and <span class=\"ltx_text ltx_font_italic\">performed identities</span> (emergent in interaction)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bucholtz &amp; Hall, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib2\" title=\"\">2005</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, they mirror the dual nature of human role assessment, balancing scalability and comparability with depth and nuance. This theoretical grounding reinforces the design of Speech-DRAME as a framework that does not privilege one perspective over the other, but instead integrates both to reflect the full spectrum of how roles are evaluated in speech-based interaction.</p>\n\n",
                "matched_terms": [
                    "other",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our main formulation (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS1\" title=\"3.1 Problem Formulation &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) focuses on the <em class=\"ltx_emph ltx_font_italic\">single-turn</em> setting, where each response <math alttext=\"{\\bm{S}}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119930;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{S}}_{r}</annotation></semantics></math> is generated and evaluated independently.\nThis simplification enables clearer analysis of speech role-play quality and aligns with our bilingual annotation protocol.\nNevertheless, the same formulation extends naturally to multi-turn dialogues.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p4.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> may be a simple mean, a recency-weighted average, or a learned temporal model that captures consistency across turns.\nThe choice of <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p4.m3\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> reflects the intended evaluation focus, e.g., stability of persona traits, adaptation to scene progression, or coherence across multiple speakers.</p>\n\n",
                "matched_terms": [
                    "average",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Archetype Evaluation follows a top&#8211;down perspective rooted in sociology and role theory, where social behavior is interpreted through shared stereotypes and generalized expectations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goffman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib23\" title=\"\">1949</a>; Campbell, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib4\" title=\"\">2008</a>)</cite>.\nThis strategy abstracts role-play into broad and easily recognizable archetypes, enabling scalable evaluation of whether a generated response aligns with salient role features.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scene dimensions.</span>\nIn addition to roles, we incorporate contextual triggers that anchor role-play in stereotypical situations.\nA central focus is on <span class=\"ltx_text ltx_font_bold\">everyday external events or state changes</span> (e.g., receiving a promotion, being caught in gossip), which provide concrete scenarios for testing archetypical responses.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Exclusion of fine-grained controls.</span>\nWe deliberately excluded explicit emotional states, prosodic instructions, or linguistic constraints.\nThese dimensions are better suited to realism-oriented or style-controlled evaluation, whereas archetype evaluation benefits from concrete, stereotype-level scenarios that sharpen judgments and improve human agreement.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Pipeline overview.</span>\nAt a high level, archetype evaluation data are generated through a three-stage pipeline:\n(1) selecting keywords that specify role and scene;\n(2) expanding them into full prompts via structured templates; and\n(3) rendering prompts into speech with contemporary TTS systems.\nThis modular pipeline ensures both scalability and reproducibility, while keeping prompts vivid enough for reliable judgments. (4) Based on the generated data (paired scene and synthesized speech), expert human annotators are recruited for detailed annotation on semantic check (i.e., <span class=\"ltx_text ltx_font_italic\">Content Pass</span>) and three different spoken role-play dimensions (i.e., <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human-likeness</span>, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>).\nImplementation details, including template design, alternative prompting strategies, and annotation guidelines, are provided in the following subsections of this Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "appropriateness",
                    "pass",
                    "content",
                    "audio",
                    "dimensions",
                    "human",
                    "archetype",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, archetype evaluation emphasizes breadth and recognizability over subtlety.\nBy grounding role-play in socially recognizable categories and stereotypical situations, it provides a practical balance between scalability and interpretability, complementing the more fine-grained realism evaluation described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS2\" title=\"3.2 Role-play Evaluation with Two Strategies &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Excluded Dimensions.</span> We initially considered more fine-grained control dimensions, but excluded them to preserve the archetype-level focus. Examples of excluded dimensions include:\n<em class=\"ltx_emph ltx_font_italic\">Emotional State &amp; Expression, Single Emotion Prompts, Explicit User Input / Assistant Output structures, Emotional Transition / Fluctuation, Voice Style &amp; Prosody (pacing, intonation, vocal timbre, expressive style, accent/dialect), and Content / Interaction Patterns (task/topic controls, specific linguistic structures, or speech production challenges).</em></p>\n\n",
                "matched_terms": [
                    "content",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These dimensions are more closely aligned with explicit instruction-following or style transfer rather than broad stereotype judgment, and they reduced clarity in human evaluations.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All steps are automatically processed by <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib12\" title=\"\">2025</a>)</cite>, but each output is manually verified to ensure high quality for downstream usage. We demonstrate an example for Roles: Occupation / Functional Role in Fictional or Fantasy as follows:</p>\n\n",
                "matched_terms": [
                    "all",
                    "quality",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Collection for Proprietary Models.</span> For proprietary systems, we adopt two collection strategies. For GPT-4o-realtime, responses are obtained directly via the <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-realtime</span> API. For Doubao and GPT-4o (Advanced Voice Mode), we collect outputs by recording from their official web interfaces. All recordings are captured through the interaction device itself, ensuring that no additional processing or distortion is introduced into the final audio.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">End-to-End Model Generation.</span>\nFor end-to-end systems, the audio prompt is directly used as input. The system prompt is set to default values to ensure consistent speech generation quality.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cascaded Model Generation.</span>\nFor cascaded systems, we only use Cosyvoice2 as the TTS model in our collection of data. We select <span class=\"ltx_text ltx_font_typewriter\">Qwen3-30B-Instruct</span> and <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> as the text LLM candidates&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib73\" title=\"\">2025</a>; Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib12\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2\" title=\"Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the internal annotation interface used for <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>.\nThe platform provides an integrated environment for both playback and evaluation of model-generated role-play speech samples.\nAnnotators first interact with the audio player interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf1\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>), which visualizes waveforms and spectrograms for multiple versions of the same prompt, enabling side-by-side comparison under a shared role instruction (e.g., firefighter, teacher, or actor).\nThen, they proceed to the annotation interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf2\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a>), where they assign scores for multiple dimensions such as <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, along with reasoning and confidence inputs.\nThis structured interface supports high-consistency, fine-grained evaluation of role-play behaviors in speech models.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "appropriateness",
                    "audio",
                    "dimensions",
                    "human",
                    "archetype",
                    "likeness",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This appendix contains the full human annotation instructions used for archetype evaluation.</p>\n\n",
                "matched_terms": [
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Quality (No Context):</span> independence from content, focus on distortions or synthesis artifacts.</p>\n\n",
                "matched_terms": [
                    "content",
                    "audio",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human Likeness (No Context):</span> perceived naturalness and plausibility of human delivery.</p>\n\n",
                "matched_terms": [
                    "likeness",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Appropriateness (Context Considered):</span> tonal fit of the vocal performance to the described role and scene.</p>\n\n",
                "matched_terms": [
                    "appropriateness",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comments are mandatory for Appropriateness (all scores).</p>\n\n",
                "matched_terms": [
                    "all",
                    "appropriateness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comments for Audio Quality and Human Likeness are only required for scores <math alttext=\"\\leq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I3.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8804;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\leq 3</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "likeness",
                    "human",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Foreign language content:</span> If <em class=\"ltx_emph ltx_font_italic\">any</em> part before 30s contains a foreign language (even a single word/short phrase) in otherwise English audio, reject the entire clip. \n<br class=\"ltx_break\"/><em class=\"ltx_emph ltx_font_italic\">Exception:</em> Extremely common/recognizable borrowed words (e.g., &#8220;croissant&#8221;) may pass. Niche items (e.g., &#8220;coq au vin&#8221;) should be rejected.</p>\n\n",
                "matched_terms": [
                    "content",
                    "audio",
                    "pass"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><em class=\"ltx_emph ltx_font_italic\">Example (pass):</em> Same prompt; audio says: &#8220;It looks like your stylist put you through a wood chipper.&#8221; <math alttext=\"\\Rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I5.i1.I0.i2.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8658;</mo><annotation encoding=\"application/x-tex\">\\Rightarrow</annotation></semantics></math> content pass = true.</p>\n\n",
                "matched_terms": [
                    "content",
                    "audio",
                    "pass"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><em class=\"ltx_emph ltx_font_italic\">Example (pass):</em> &#8220;Oh no. Are you hurt? Don&#8217;t worry. We&#8217;ll get you taken care of, little one.&#8221; <math alttext=\"\\Rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I5.i2.I0.i2.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8658;</mo><annotation encoding=\"application/x-tex\">\\Rightarrow</annotation></semantics></math> content pass = true.</p>\n\n",
                "matched_terms": [
                    "content",
                    "pass"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Important:</span> Once an audio receives content pass = false, <em class=\"ltx_emph ltx_font_italic\">do not</em> rate rubrics; proceed to the next clip.</p>\n\n",
                "matched_terms": [
                    "content",
                    "audio",
                    "pass"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rubric Rating Instructions (for content pass = true).</span>\nWe shift focus from <em class=\"ltx_emph ltx_font_italic\">what</em> was said to <em class=\"ltx_emph ltx_font_italic\">how</em> it was said (vocal performance). Use gut intuition guided by rubric definitions. Score each audio <em class=\"ltx_emph ltx_font_italic\">in isolation</em> (no ranking across clips).</p>\n\n",
                "matched_terms": [
                    "content",
                    "audio",
                    "pass",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">Audio Quality</span> &amp; <span class=\"ltx_text ltx_font_bold\">Human Likeness</span>: comment only if score <math alttext=\"\\leq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I6.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8804;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\leq 3</annotation></semantics></math>; if 4 or 5, write &#8220;N/A&#8221;.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "likeness",
                    "human",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">Appropriateness</span>: comment for <em class=\"ltx_emph ltx_font_italic\">all</em> scores 1&#8211;5. Explain <em class=\"ltx_emph ltx_font_italic\">why</em> the <em class=\"ltx_emph ltx_font_italic\">delivery</em> was/was not a good match to the in-context traits (role &amp; scene).</p>\n\n",
                "matched_terms": [
                    "all",
                    "appropriateness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Evaluate the <em class=\"ltx_emph ltx_font_italic\">performance</em>, not the script content.</p>\n\n",
                "matched_terms": [
                    "content",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Pronunciation errors: penalize only on <span class=\"ltx_text ltx_font_bold\">Human Likeness</span> if they significantly affect comprehension; do not affect <span class=\"ltx_text ltx_font_bold\">Appropriateness</span>.</p>\n\n",
                "matched_terms": [
                    "appropriateness",
                    "likeness",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Independently</span> rate technical clarity (ignore content and fit). After scoring Audio Quality, <em class=\"ltx_emph ltx_font_italic\">disregard</em> its issues for the other two dimensions.</p>\n\n",
                "matched_terms": [
                    "content",
                    "audio",
                    "dimensions",
                    "other",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Rate how likely an average listener would believe the clip is human-delivered (ignore Audio Quality issues and contextual fit).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "average",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Rate <em class=\"ltx_emph ltx_font_italic\">only</em> how well the <em class=\"ltx_emph ltx_font_italic\">delivery</em> fits the role/scene described in the prompt (tonal fit). Ignore Audio Quality and Human Likeness issues while scoring this dimension.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "likeness",
                    "human",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realism Evaluation adopts a bottom&#8211;up perspective grounded in real human speech data, with the goal of capturing the nuanced delivery, prosody, and contextual fit that synthetic archetype prompts cannot fully represent.\nRather than relying on stereotypes, this strategy leverages real-world recordings and carefully constructed contrasts, enabling fine-grained assessment of model performance under realistic conditions.</p>\n\n",
                "matched_terms": [
                    "human",
                    "model",
                    "performance",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, real human ground-truth segments and systematically constructed negatives form a balanced dataset that supports realism-oriented evaluation.\nThe positives provide natural anchors of role-play performance, while the negatives introduce controlled mismatches that test whether evaluation models can distinguish real human speech from synthetic approximations.\nThis design ensures that realism evaluation emphasizes subtle paralinguistic and contextual cues, moving beyond intelligibility or content fidelity.</p>\n\n",
                "matched_terms": [
                    "content",
                    "roleplay",
                    "human",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Yunjin case study illustrates a fundamental difference between realism evaluation and archetype evaluation. Archetype prompts, as described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>, are constructed by pairing abstract role categories with synthetic contextual scenes. These cover a broad space of possible situations, ranging from modern occupations (e.g., firefighters) to fictional personas (e.g., cult leaders in martial arts fantasy) or social identities (e.g., overprotective parents, laid-back friends). While such prompts provide controlled and diverse coverage, they are ultimately template-based and rely on stereotypes to elicit role-play behaviors.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the Yunjin example is drawn from authentic performance, where the character&#8217;s persona (an opera singer) and local scene (a battlefield narration) are naturally aligned through artistic delivery. The realism task does not merely test whether a model can follow a prompt, but whether it can preserve coherence between a speaker&#8217;s global identity, immediate context, and expressive prosody. When Yunjin&#8217;s transcript is transplanted into mismatched settings or paired with incongruent speaker profiles, the breakdown in realism becomes apparent: the same words shift from inspiring to awkward, jarring, or even absurd.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This distinction highlights the complementary strengths of the two evaluation paradigms. Archetype evaluation systematically explores a wide combinatorial space of roles and situations, but may lack the paralinguistic and contextual richness of real speech. Realism evaluation, on the other hand, anchors assessment in authentic delivery and contextual fit, providing a finer-grained lens on coherence and naturalness. Together, they ensure that role-play evaluation spans both broad stereotypical coverage and nuanced real-world plausibility.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "other",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For models using voice prompts, we use an inhouse enhancement model to first process the prompt audio so as to minimize the effect of audio events and background noise.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS1</span> (Phone&#8211;Cosy2 Pipeline).\nThe input is a phone sequence produced by a graphme-to-phoneme&#160;(G2P) front-end. A 0.4B GPT-2 model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib50\" title=\"\">2019</a>)</cite> predicts <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> S3-tokens, which are then passed through a flow-matching model to generate mel-spectrograms. A neural vocoder from <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> converts the mel features into audio. Both the GPT-2 and flow-matching modules are augmented with speaker embeddings for speaker control.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS3</span> (Embedding&#8211;AudioCode Pipeline).\nHere, the input is Qwen2.5 BPE embeddings, which a 0.4B GPT-2 model maps directly to 4,096-dimensional audio codes. These codes are decoded via a cascaded architecture: <span class=\"ltx_text ltx_font_typewriter\">RepCodec</span> decoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib32\" title=\"\">2024</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> content representation (with speaker embedding) <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DDIM</span> (denoising\ndiffusion implicit model) module&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jeong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib34\" title=\"\">2021</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DAC</span> latent representation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib37\" title=\"\">2023</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DAC</span> decoder. Conditioning is provided through prefix text embeddings (e.g., instructions expressing emotional states such as &#8220;angry, hateful: you betrayed me&#8230;&#8221;), along with speaker ID. The input format is <span class=\"ltx_text ltx_font_typewriter\">[spk_id, BPE embedding, audio codes]</span>.</p>\n\n",
                "matched_terms": [
                    "content",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS4</span> (Instruction-Conditioned AudioCode Pipeline).\nThis model also employs a 0.4B GPT-2 to predict 4,096-dimensional audio codes, but conditions generation on instruction embeddings. A <span class=\"ltx_text ltx_font_typewriter\">CLIPTextEncoder</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib51\" title=\"\">2021</a>)</cite> converts instruction text into embeddings, which serve as a prefix, combined with speaker ID and phone sequence. The input format is <span class=\"ltx_text ltx_font_typewriter\">[spk_id, instruction embedding sequence, phone sequence, audio codes]</span>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the audio&#8217;s speaker is <em class=\"ltx_emph ltx_font_italic\">clearly not</em> the intended target identity, assign a score of <span class=\"ltx_text ltx_font_bold\">1</span> to all Profile-dependent dimensions (e.g., Voice Identity Matching, Trait Embodiment) and proceed with other dimensions as applicable.</p>\n\n",
                "matched_terms": [
                    "all",
                    "other",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the prompt information is <span class=\"ltx_text ltx_font_bold\">contradictory</span> or <span class=\"ltx_text ltx_font_bold\">insufficient</span> to evaluate (e.g., audio too short, traits mutually incompatible), mark the prompt quality accordingly and <em class=\"ltx_emph ltx_font_italic\">do not</em> force granular ratings that cannot be judged.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Use both sources:</span> Play the audio and skim the transcript together to assess delivery quality, expression, and alignment with character intent (do not penalize transcript mismatches).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Focus only on human speech:</span> Do not factor in background music/soundtracks, environmental or ambient noise, or non-speech audio events; these may reflect the scene but should not influence scoring of the spoken performance.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "human",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt is insufficient to evaluate the voice sample.</span> Audio too short to gauge the dimensions; or the prompt lacks specific characteristics/is too vague or multi-directional to know which version of the character is present. (It is OK if audio does not match transcript exactly; if too much is missing to annotate, mark as insufficient.)</p>\n\n",
                "matched_terms": [
                    "audio",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 2.1 Emotion Accuracy (Required; check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Better for SEM training.</span> Rubric-aligned labels let us <em class=\"ltx_emph ltx_font_italic\">train</em> and <em class=\"ltx_emph ltx_font_italic\">audit</em> SEMs, measure human alignment, and analyze error modes; judge outputs are harder to trust long-term as they change with model versions and prompts.</p>\n\n",
                "matched_terms": [
                    "model",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All human annotation in this work was conducted in accordance with standard ethical practices for research involving human annotators. Annotators were recruited via public hiring for Mandarin and internal lab hiring for English. Prior to participation, all annotators were provided with a clear description of the task, the type of data they would encounter, and their rights as participants, including the ability to opt out at any time without penalty.</p>\n\n",
                "matched_terms": [
                    "all",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The archetype evaluation provides a detailed view of how speech foundation models (SFMs) perform when tasked with role-play scenarios grounded in stereotypical characters and situations. Human annotations cover four dimensions, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, and <span class=\"ltx_text ltx_font_italic\">Content Pass</span>, and the results reveal important trends, as well as challenges in interpreting scores across languages.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "appropriateness",
                    "pass",
                    "content",
                    "audio",
                    "dimensions",
                    "human",
                    "archetype",
                    "likeness",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A critical observation is that annotations in Mandarin and English cannot be directly compared. The annotation groups for the two languages differed in their linguistic backgrounds and cultural interpretations, which leads to systematic differences in scoring. Our bilingual pilot further confirmed that annotators often interpret role-play appropriateness and expressiveness differently depending on their cultural frame of reference. For example, Mandarin raters tend to reward delivery styles that align with culturally expected prosody and politeness strategies, while English raters prioritize spontaneity and naturalness. These discrepancies parallel the findings of the singing voice conversion challenge (SVCC2023) from <cite class=\"ltx_cite ltx_citemacro_citet\">Huang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib31\" title=\"\">2023</a>)</cite>, where native and non-native listeners provided significantly different ratings for the same samples, particularly for naturalness and speaker similarity. Consequently, while absolute scores differ substantially across Mandarin and English, only within-language comparisons are meaningful. Cross-lingual ranking is not valid given the different annotation bases.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "appropriateness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Within Mandarin from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, the relative rankings of models are clear. <span class=\"ltx_text ltx_font_typewriter\">Doubao</span> stands out as the strongest performer across all four dimensions, reaching near-ceiling levels in both <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> (4.37) and <span class=\"ltx_text ltx_font_italic\">Content Pass</span> (0.98). It also leads in role-related dimensions such as <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, indicating balanced competence across linguistic accuracy, delivery, and expressiveness. <span class=\"ltx_text ltx_font_typewriter\">G-CosyVoice2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-CosyVoice2</span> follow closely, especially in audio quality, though they lag behind in role appropriateness. <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> achieve mid-range results, reflecting higher variance across samples, while <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> shows competitive content understanding but lower naturalness and appropriateness. <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> consistently ranks the lowest, with large variance, underscoring instability and weaker alignment with archetypal role expectations.</p>\n\n",
                "matched_terms": [
                    "appropriateness",
                    "quality",
                    "pass",
                    "content",
                    "audio",
                    "dimensions",
                    "all",
                    "human",
                    "accuracy",
                    "likeness",
                    "kimiaudio",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The picture shifts in English, elaborated in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Here, <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> emerges as the most effective model by a wide margin, achieving top scores in both <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> (3.46) and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> (3.49), as well as high content pass rates (0.97). This reflects its strong grounding in English discourse norms and role adherence. In contrast, <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> variants perform well in <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> but are weaker in role alignment, highlighting a trade-off between acoustic fidelity and role-play embodiment. Other models such as <span class=\"ltx_text ltx_font_typewriter\">Doubao</span>, <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span>, and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> achieve moderate results, while <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> trail far behind, showing limited robustness in English scenarios. Taken together, the English results reveal both a sharper separation across models and a wider variance in appropriateness scores compared to Mandarin.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "appropriateness",
                    "quality",
                    "pass",
                    "content",
                    "audio",
                    "model",
                    "other",
                    "human",
                    "likeness",
                    "kimiaudio",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although cross-lingual comparisons are invalid, this does not diminish the utility of archetype evaluation for role-play benchmarking. In fact, the observed inconsistencies reinforce a central point: successful role-play requires models to adapt to the cultural and linguistic norms of the target language community. A system that performs well in Mandarin but poorly in English cannot be considered universally capable, since end users will evaluate appropriateness and naturalness according to their native expectations. Thus, archetype evaluation highlights not only model-specific strengths and weaknesses but also the need for culturally grounded evaluation pipelines.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "appropriateness",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another notable pattern is the trade-off between acoustic quality and role embodiment. CosyVoice2 variants achieve strong audio quality but weaker human likeness and appropriateness, whereas <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> prioritizes role coherence at the expense of audio fidelity. This suggests that future systems must better integrate high-quality synthesis with nuanced role adherence. Similarly, the consistently lower scores for imaginative and identity-driven roles underscore the limitations of current SFMs in capturing subtle sociocultural expectations and narrative context.</p>\n\n",
                "matched_terms": [
                    "appropriateness",
                    "audio",
                    "human",
                    "likeness",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In sum, archetype evaluation reveals three major findings:\n(1) Mandarin and English annotations cannot be directly compared, due to linguistic and cultural annotation inconsistencies, consistent with prior evidence from SVCC 2023.\n(2) Within each language, clear model rankings emerge, <span class=\"ltx_text ltx_font_typewriter\">Doubao</span> dominates in Mandarin, while <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> excels in English.\n(3) Domain sensitivity remains a challenge, particularly for imaginative or culturally rich roles, where scores are substantially lower and inter-model variance is higher.</p>\n\n",
                "matched_terms": [
                    "model",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although realism evaluation targets nuanced paralinguistic behaviors, human annotators achieve <em class=\"ltx_emph ltx_font_italic\">consistently high agreement</em> once the rubric disentangles the aspects to be judged.\nAs shown in the agreement summary in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F7\" title=\"Figure 7 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, most dimensions fall in the <math alttext=\"0.86\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>0.86</mn><annotation encoding=\"application/x-tex\">0.86</annotation></semantics></math>&#8211;<math alttext=\"0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mn>0.88</mn><annotation encoding=\"application/x-tex\">0.88</annotation></semantics></math> range, with the lowest agreement on <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> (<math alttext=\"\\approx 0.83\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.83</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.83</annotation></semantics></math>) and the highest on <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em> / <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> (<math alttext=\"\\approx 0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.88</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.88</annotation></semantics></math>).\nEmotion-related, progressive metrics (Intensity and Transition) are slightly lower (<math alttext=\"\\approx 0.85\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.85</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.85</annotation></semantics></math>), reflecting the added difficulty of judging graded expressivity.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F8\" title=\"Figure 8 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F9\" title=\"Figure 9 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> visualize the score distributions per dimension.\nWe highlight three patterns:\n(i) <span class=\"ltx_text ltx_font_bold\">Progressive/gated emotion metrics.</span> For <em class=\"ltx_emph ltx_font_italic\">Emotion Intensity</em> and <em class=\"ltx_emph ltx_font_italic\">Emotion Transition</em>, we assign <span class=\"ltx_text ltx_font_typewriter\">1</span> when the scene is marked <em class=\"ltx_emph ltx_font_italic\">N/A</em> (i.e., emotion accuracy is insufficient to meaningfully judge intensity/transition). This produces a visible mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, a sparse bin at <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>, and slightly lower overall agreement; downstream analyses conditioning on adequate <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> (e.g., mean&#160;<math alttext=\"\\geq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 3</annotation></semantics></math>) recover more bell-shaped distributions.\n(ii) <span class=\"ltx_text ltx_font_bold\">Traits Embodiment.</span> Scores tend to avoid the middle (<math alttext=\"\\sim\\!3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!3</annotation></semantics></math>), concentrating on low values when traits clearly mismatch and high values when traits are strongly realized. This yields the lowest inter-rater agreement among dimensions, reflecting the inherently compositional and role-specific nature of trait judgments.\n(iii) <span class=\"ltx_text ltx_font_bold\">Other dimensions.</span> <em class=\"ltx_emph ltx_font_italic\">Pitch Variation</em>, <em class=\"ltx_emph ltx_font_italic\">Rhythmic Naturalness</em>, <em class=\"ltx_emph ltx_font_italic\">Stress &amp; Emphasis</em>, <em class=\"ltx_emph ltx_font_italic\">Voice Identity Matching</em>, <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em>, <em class=\"ltx_emph ltx_font_italic\">Global Story Coherence</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> are approximately unimodal with modes at <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>&#8211;<math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math>, consistent with moderate&#8211;strong realization in the dataset and with the high agreement reported.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "other",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, the consistently high agreement confirms that the rubric isolates perceptually coherent factors, so learned evaluators can target them reliably.\nSecond, the gated behavior of <em class=\"ltx_emph ltx_font_italic\">Intensity/Transition</em> motivates a <em class=\"ltx_emph ltx_font_italic\">hierarchical</em> evaluator: verify <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> before scoring graded expressivity, or model the joint structure with conditional heads.\nThird, <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> remains challenging; models may benefit from explicit character-trait priors, long-term context aggregation, or contrastive supervision between matched and mismatched traits.\nFinally, prosody-focused objectives (e.g., pitch/rhythm/stress control) are likely to yield the largest quality gains for amateur-style speech.</p>\n\n",
                "matched_terms": [
                    "evaluator",
                    "quality",
                    "model",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For both zeroshot and fewshot experiments, we adopt a streamlined version of the annotation guidelines to ensure consistency with the human annotation process. The exact rubric prompts for archetype and realism evaluation used are provided below:</p>\n\n",
                "matched_terms": [
                    "archetype",
                    "zeroshot",
                    "human",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and fewshot settings, we condition the model with the rubric prompt and then append few-shot demonstration triplets (input, audio, gold JSON) to mirror the human annotation protocol. As shown in Listing&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#LST2\" title=\"Listing 2 &#8227; L.1 Zeroshot/Fewshot Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, We use <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> for these few-shot exemplars and require a minified JSON output consistent with the rubric schema.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "audio",
                    "model",
                    "gemini25pro",
                    "human",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and few-shot experiments on commercial models such as <span class=\"ltx_text ltx_font_typewriter\">Gemini</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o</span>, we report results as the average of five independent runs to reduce randomness and obtain more stable predictions. In contrast, for open-source models, we observed that their instruction-following ability is relatively weak. To mitigate this, we prompt them to predict one dimension at a time rather than all dimensions jointly. Specifically, for each metric, after showing the rubric prompt, we request completion in a fixed template and extract the probability distribution over the discrete tokens <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>. The final score is then computed from these probabilities, which we found makes open-source models usable to some extent under this setting.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "report",
                    "average",
                    "dimensions",
                    "all",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For few-shot experiments, we randomly sample three exemplars from the training set, with each exemplar chosen such that its human scores are close to the rounded average across all annotators. These exemplars are appended as demonstrations in the prompt. As with the zeroshot case, we run five trials for commercial models and report the averaged results for evaluation.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "report",
                    "average",
                    "all",
                    "human",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.T12\" title=\"Table 12 &#8227; L.2 Zeroshot/Fewshot Ablation Experiments &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> shows how different prompting strategies affect realism evaluation performance with <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>. Several observations can be drawn:</p>\n\n",
                "matched_terms": [
                    "performance",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Effect of repeated runs.\nWhen comparing single-run decoding (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot-1run</span>) against averaging over five runs (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot</span>), we see consistent gains across nearly all dimensions (average score improves from 0.353 to 0.390). This suggests that sampling variance plays a substantial role in the reliability of large language model (LLM) evaluators, and averaging multiple outputs reduces this variance, producing more stable correlation with human annotations.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "zeroshot",
                    "average",
                    "model",
                    "dimensions",
                    "all",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Impact of few-shot conditioning.\nIntroducing even three examples (<span class=\"ltx_text ltx_font_typewriter\">3-Shot</span>) yields a clear improvement over zeroshot (0.433 vs. 0.390). This indicates that providing explicit annotated demonstrations helps the model better align with the rubric. Interestingly, when using only a single run with few-shot demonstrations (<span class=\"ltx_text ltx_font_typewriter\">3-Shot-1run</span>), the performance is only marginally better than pure zeroshot, highlighting that both averaging and demonstrations are complementary: demonstrations improve grounding, while multiple runs mitigate stochasticity.</p>\n\n",
                "matched_terms": [
                    "model",
                    "zeroshot",
                    "performance",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Scaling the number of demonstrations.\nIncreasing the few-shot set size from three to six and nine further boosts performance (0.459 and 0.469, respectively). The improvements are most pronounced in prosodic and emotional categories (e.g., <em class=\"ltx_emph ltx_font_italic\">Pitch</em>, <em class=\"ltx_emph ltx_font_italic\">Accuracy</em>, <em class=\"ltx_emph ltx_font_italic\">Intensity</em>), suggesting that more examples provide stronger guidance in modeling subtle paralinguistic cues. Gains in <em class=\"ltx_emph ltx_font_italic\">Character Consistency</em> and <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance</em> are also noticeable, although smaller in magnitude, which may indicate that these higher-level judgments are less sensitive to prompt scaling once a few demonstrations are provided.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "performance",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In general, the results demonstrate two complementary strategies for stabilizing LLM-based evaluators: (i) averaging multiple runs to reduce randomness, and (ii) enriching prompts with more few-shot exemplars to improve adherence to rubric-based evaluation. Combining both leads to the strongest overall performance, with the nine-shot condition achieving the highest average correlation&#160;(0.469).</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "average",
                    "performance",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the archetype test set, all fine-tuned variants achieve strong <span class=\"ltx_text ltx_font_italic\">Content Pass</span> accuracy above 92%. Differences appear mainly in correlation-based metrics. Here, <span class=\"ltx_text ltx_font_typewriter\">Base-FT</span> yields the highest score on <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> delivers the strongest performance in <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> and the best overall average (0.629). This suggests that instruction-tuned checkpoints paired with parameter-efficient adaptation can better capture higher-level perceptual traits.</p>\n\n",
                "matched_terms": [
                    "appropriateness",
                    "quality",
                    "pass",
                    "content",
                    "audio",
                    "average",
                    "all",
                    "human",
                    "archetype",
                    "accuracy",
                    "likeness",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For realism evaluation on the basic test set, <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> consistently achieves the strongest correlations across most dimensions, especially for <span class=\"ltx_text ltx_font_italic\">Character Identity</span> and <span class=\"ltx_text ltx_font_italic\">Traits</span>, leading to the best average correlation (0.668). <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> also performs competitively (0.630), while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> lags slightly behind (0.625). These results highlight that full fine-tuning may still offer an advantage when the evaluation set is relatively homogeneous and closer to the training distribution.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "correlation",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A different pattern emerges on the real recording test set. Both <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> and <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> struggle, yielding negative or near-zero correlations across most dimensions. In contrast, <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> shows clear improvements, achieving positive correlations in all prosodic and emotional dimensions and producing the highest overall average (0.247). This indicates that LoRA adaptation on top of an instruction-tuned backbone provides better generalization to out-of-distribution, real-world data.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "average",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> is competitive on in-domain evaluation, it suffers a substantial drop when applied to real recordings. By contrast, <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> offers a more robust balance: it maintains reasonable performance on archetype and basic sets, while significantly outperforming other settings on real-world data. Based on this robustness, we adopt <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> as our final configuration for subsequent experiments.</p>\n\n",
                "matched_terms": [
                    "other",
                    "performance",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the comprehensive design of Speech-DRAME, several limitations remain.\nFirst, while our dual evaluation paradigm combines top-down archetype assessment with bottom-up realism evaluation, both strategies are constrained by the quality and scope of available data. Archetype evaluation inevitably relies on simplified stereotypes, which can overlook subtler paralinguistic and contextual cues. Realism evaluation, though richer, still struggles with domain mismatch between synthetic data and real human recordings, as evidenced by the performance drop observed in the real-recording test set. This highlights the persistent gap between benchmark-driven evaluation and real-world variability in speech role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "performance",
                    "human",
                    "archetype",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Third, while DRAME-Eval substantially outperforms zero-shot and few-shot ALLMs, its reliance on fine-tuning from a strong base model raises questions of accessibility and scalability. Future work should explore lightweight adaptation techniques, semi-supervised training with partially annotated data, and leveraging emergent capabilities of open-source multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "model",
                    "drameeval",
                    "zeroshot",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, although we validated DRAME-Eval against human annotations, alignment under realism settings remains modest. This suggests that even human-annotated benchmarks are insufficiently fine-grained to capture the full complexity of human perception in speech role-play. Richer annotation protocols, more diverse speaker populations, and integration of perceptual psychology insights could further improve evaluation fidelity.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "drameeval",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Looking forward, we see several promising research directions. We intentionally restrict evaluation to single-turn responses for clarity and tractability. However, multi-turn extensions that capture narrative progression, long-term consistency, and dynamic emotion shifts remain a natural next step. Incorporating multilingual and cross-cultural datasets will enhance generality and fairness. On the modeling side, exploring hybrid evaluation strategies that combine discrete rubrics with continuous perceptual signals (e.g., prosody embeddings or affective measures) may better bridge the gap between automatic and human judgments. Finally, integrating Speech-DRAME into reinforcement learning pipelines as a reward model could help drive the next generation of expressive, human-aligned speech role-play systems.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "model",
                    "human"
                ]
            }
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Realism role-play evaluator performance. We report Pearson correlation coefficients for all dimensions. The Avg. column is the average of all dimensions.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" rowspan=\"2\">Config.</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" rowspan=\"2\">Model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" colspan=\"3\">Prosodic Dynamics</td>\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" colspan=\"3\">Emotional Expressiveness</td>\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" colspan=\"2\">Character Consistency</td>\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" colspan=\"2\">Contextual Relevance</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\">Avg.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Pitch</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Rynthm</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Stress</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Accuracy</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Intensity</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Transition</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Identity</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Traits</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Local</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Global</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" rowspan=\"5\">Zeroshot</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.319</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.352</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.336</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.391</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.316</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.336</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.529</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.509</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.349</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.465</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.390</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.132</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.210</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.179</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.323</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.258</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.273</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.360</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.397</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.277</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.440</td>\n<td class=\"ltx_td ltx_align_center\">0.284</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.045</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.033</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.065</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.018</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.020</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.024</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.072</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.013</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.009</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.026</td>\n<td class=\"ltx_td ltx_align_center\">0.032</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.029</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.030</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.051</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.006</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.007</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.002</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.009</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.023</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.056</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.003</td>\n<td class=\"ltx_td ltx_align_center\">0.013</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.124</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.073</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.102</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.227</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.208</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.226</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.151</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.291</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.143</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.203</td>\n<td class=\"ltx_td ltx_align_center\">0.175</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" rowspan=\"2\">FewShot</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.380</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.415</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.406</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.418</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.354</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.354</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.596</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.524</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.376</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.497</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.432</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.236</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.264</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.230</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.417</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.344</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.412</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.422</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.514</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.332</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.498</td>\n<td class=\"ltx_td ltx_align_center\">0.367</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\">Finetune</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.621</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.627</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.631</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.632</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.596</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.601</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.660</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.655</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.563</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.668</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.625</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "roleplay",
            "column",
            "pitch",
            "identity",
            "consistency",
            "pearson",
            "prosodic",
            "fewshot",
            "dynamics",
            "config",
            "qwen2audio",
            "realism",
            "coefficients",
            "zeroshot",
            "evaluator",
            "average",
            "emotional",
            "character",
            "gemini25pro",
            "rynthm",
            "stress",
            "accuracy",
            "performance",
            "gpt4oaudio",
            "avg",
            "finetune",
            "relevance",
            "report",
            "intensity",
            "transition",
            "traits",
            "kimiaudio",
            "qwen25omni",
            "global",
            "correlation",
            "local",
            "contextual",
            "model",
            "all",
            "dimensions",
            "expressiveness",
            "drameeval"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T3\" title=\"Table 3 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> extends the analysis to fine-grained realism evaluation. Similar to archetype results, proprietary ALLMs lead the zero-shot setting, with <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> obtaining an average correlation of 0.390, compared to 0.284 for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>. Public models remain weak baselines. Few-shot prompting consistently improves performance: <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> gains +0.042 and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> gains +0.083 over their zero-shot averages, showing the benefit of in-context supervision. Once again, finetuning is most effective: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves 0.625 average correlation, higher than the best few-shot results. Importantly, the gains are consistent across all 10 realism dimensions, confirming that the sampling&#8211;expectation training strategy generalizes beyond simple classification to nuanced paralinguistic cues.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Role-play has become a key testbed for generative models, expanding from text-only dialogue to multimodal interaction. Extending role-play to speech captures prosody, emotion, and delivery, but also poses new evaluation challenges. Current pipelines often use audio large language models (ALLMs) as zero-shot judges, which miss paralinguistic cues, collapse multiple aspects into coarse scores, and rely on synthetic speech references that fail to reflect real-world roles. We present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework that contributes at three levels: (i) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-EvalBench</em>, an evaluation benchmark with bilingual human-annotated data and protocols for training and testing speech evaluation models&#160;(SEMs), (ii) <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, a fine-tuned evaluation model, which substantially outperforms zero-shot and few-shot ALLMs, and (iii) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-RoleBench</em>, a speech role-play benchmark that leverages <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> as an automatic judge to compare speech foundation models (SFMs). Speech-DRAME distinguishes between two complementary evaluation strategies: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>, a top-down approach measuring adherence to broad role archetypes, and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em>, a bottom-up approach grounded in real human speech that emphasizes nuanced role quality. Compared to zero-shot ALLM judges, <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves stronger agreement with human ratings (Pearson correlation from 0.480 to 0.629 in archetypes, and 0.390 to 0.625 in realism). By integrating transparent benchmark resources, modeling approaches, and system-level evaluation, Speech-DRAME provides the first comprehensive, reproducible foundation for assessing spoken role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "correlation",
                    "realism",
                    "zeroshot",
                    "model",
                    "pearson",
                    "fewshot",
                    "drameeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Role-play has been recently recognized a cornerstone of interactive systems, from initial text-based focuses to the growing ecosystem of multimodal dialogue agents&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib6\" title=\"\">2024a</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib59\" title=\"\">2024</a>; Dai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib13\" title=\"\">2025</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib43\" title=\"\">2024</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>)</cite>. With the rapid advancement of large language models (LLMs), role-play has evolved beyond scripted interactions into dynamic character-driven conversations, supporting applications in education, entertainment, and human&#8211;AI collaboration&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib69\" title=\"\">2024</a>; Agatsuma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib1\" title=\"\">2024</a>; Fung &amp; Laing, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib20\" title=\"\">2024</a>)</cite>. Extending role-play to speech-based interaction offers an even richer communication channel, capturing prosody, emotion, and character consistency in ways that text alone cannot&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "consistency",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Evaluating speech role-play, however, remains a major challenge. Current pipelines often rely on ALLMs as judges, typically in a zero-shot setting&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>; Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>)</cite>. While this strategy can provide rough ratings, it falls short for speech role-play:</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">it lacks sensitivity to paralinguistic cues such as intonation, pacing, and emotional dynamics that define role delivery&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>; Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib81\" title=\"\">2025a</a>)</cite>,</p>\n\n",
                "matched_terms": [
                    "dynamics",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">it collapses diverse aspects of performance into limited coarse dimensions, obscuring critical dimensions of role-play quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite>, and</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dimensions",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome these limitations, we present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span> (Speech Detailed Role-play Assessment with Modeling and Evaluation), a unified framework that moves beyond synthetic or judge-only pipelines. Speech-DRAME is distinguished by three aspects: [i] a dual evaluation paradigm that balances archetype-driven generality with human-grounded realism (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS2\" title=\"3.2 Role-play Evaluation with Two Strategies &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), [ii] human annotations that provide perceptual grounding absent in prior resources (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4\" title=\"4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), and [iii] evaluation-model alignment that enables not only benchmarking of speech foundation models (SFMs) but also systematic analysis of the evaluation models (SEMs) themselves (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5\" title=\"5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). These aspects establish Speech-DRAME as a comprehensive and trustworthy framework for measuring and improving speech role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, Speech-DRAME contributes at three levels: (i) <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>, an evaluation benchmark consisting of human-annotated datasets for archetype and realism role-play; (ii) <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span>, a fine-tuned SEM trained on EvalBench that surpasses zero-shot and few-shot ALLMs; and (iii) <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span>, a speech role-play benchmark that leverages DRAME-Eval as an automatic judge to systematically compare SFMs. By combining dual evaluation strategies, human-anchored supervision, and evaluation-model alignment, Speech-DRAME delivers systematic insights into both role-play systems and evaluation models themselves.\nOur contributions are four-fold:</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "zeroshot",
                    "fewshot",
                    "drameeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a <span class=\"ltx_text ltx_font_bold\">formal task definition</span> of speech role-play and its evaluation via speech foundation models (SFMs) for generation and speech evaluation models (SEMs) for assessment, unifying linguistic and paralinguistic dimensions.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span>, a fine-tuned SEM trained on EvalBench that substantially outperforms zero-shot and few-shot ALLMs, demonstrating the importance of human supervision for evaluation models.</p>\n\n",
                "matched_terms": [
                    "drameeval",
                    "zeroshot",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We establish <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span>, a speech role-play benchmark that leverages DRAME-Eval as an automatic judge to systematically compare both end-to-end and cascaded SFMs across multiple role quality dimensions.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Relevant codes, pre-trained models, and datasets will be open-sourced at \n<br class=\"ltx_break\"/><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/Anuttacon/speech_drame\" title=\"\">https://github.com/Anuttacon/speech_drame</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "drameeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-based role-play evaluation.</span>\nEarly research focused on text-only dialogue benchmarks. Datasets such as RoleLLM, CharacterBox, and RoleMRC <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib62\" title=\"\">2024</a>; Tu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib60\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib61\" title=\"\">2025a</a>; Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib53\" title=\"\">2023</a>; Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib41\" title=\"\">2025</a>)</cite> test whether models can sustain persona consistency and character fidelity, using LLM-as-judge pipelines for scalability. However, studies like PersonaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib81\" title=\"\">2025a</a>)</cite> have shown that LLM-as-judge approaches are far from perfect, struggling with tasks as basic as tracking speaker identity. This highlights broader concerns about reliability even in text settings, which become only more severe when extended to speech, where prosody, emotion, and vocal style play a central role.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "roleplay",
                    "consistency",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-native benchmarks.</span>\nMore recently, a number of resources have been introduced to evaluate spoken role-play agents. OmniCharacter-10K <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>)</cite> emphasizes character voice traits, while others&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib39\" title=\"\">2025</a>; Hou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib27\" title=\"\">2025</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib15\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib7\" title=\"\">2024b</a>)</cite> extend evaluation toward vocal communication, assistant capability, and multi-turn realism. Omni models such as StepAudio and KimiAudio <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite> also include role-play scenarios in their evaluation. SpeechRole and VStyle <cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>; Zhan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib76\" title=\"\">2025</a>)</cite> are closer to our work, offering datasets that test interaction ability, expressiveness, and fidelity. However, these resources are primarily built from LLM-generated dialogues and TTS outputs, with limited human validation and a heavy reliance on proprietary ALLM-as-judge pipelines. As a result, they lack grounding in diverse human speech and typically apply only coarse rubrics.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "character",
                    "expressiveness",
                    "traits",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Automatic judging with ALLMs.</span>\nIn parallel, advances in automatic judging highlight both opportunities and limitations. Audio-capable LLMs such as AudioJudge and Audio-Aware LLMs as Judges <cite class=\"ltx_cite ltx_citemacro_citep\">(Manakul et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib44\" title=\"\">2025</a>; Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>)</cite> can assess style, emotion, and delivery, while systems like Auto-ATT <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib65\" title=\"\">2025c</a>)</cite> scale this to Audio Turing Tests. Yet studies consistently reveal high prompt sensitivity and shallow capture of paralinguistic cues, suggesting that zero-shot judging remains insufficient for nuanced role-play evaluation.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Expressive TTS evaluation.</span>\nRecent work has explored evaluation of instruction-following and style-controlled TTS. InstructTTSEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite> explicitly includes role-play scenarios, while EmergentTTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">(Manku et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib45\" title=\"\">2025</a>)</cite> and discrete speech LM benchmarks <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang &amp; Sz&#233;kely, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib63\" title=\"\">2024</a>)</cite> emphasize prosody, spontaneity, and speaker consistency. These efforts push beyond intelligibility and naturalness, but still center on synthetic outputs rather than real human role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "consistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our distinction.</span>\nIn contrast, Speech-DRAME unifies these considerations through three elements: (i) a dual evaluation paradigm that combines archetype-driven generality with realism grounded in authentic human speech, (ii) human annotations that provide a reliable perceptual gold standard, and (iii) alignment of SEMs to human ratings, enabling systematic evaluation of both SFMs and the evaluation models themselves. This combination moves beyond synthetic benchmarks or zero-shot judging to provide a comprehensive and trustworthy framework for speech role-play assessment.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech role-play.</span>\nWe formalize <em class=\"ltx_emph ltx_font_italic\">speech role-play</em> as a conditional speech generation problem grounded in a character profile and scene description.\nGiven a dialogue context <math alttext=\"{\\bm{C}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{speech}}</annotation></semantics></math>, a character profile <math alttext=\"{\\bm{C}}_{\\text{profile}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>profile</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{profile}}</annotation></semantics></math>, and a scene specification <math alttext=\"{\\bm{C}}_{\\text{scene}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>scene</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{scene}}</annotation></semantics></math>, a <em class=\"ltx_emph ltx_font_italic\">speech foundation model</em> (SFM) generates a role-played speech response <math alttext=\"{\\bm{S}}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119930;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{S}}_{r}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "model",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>). The first strategy follows a <em class=\"ltx_emph ltx_font_italic\">top-down</em> design, inspired by prior text-based role-play benchmarks.\nHere, role-play is judged with respect to <em class=\"ltx_emph ltx_font_italic\">stereotypical archetypes</em> (e.g., &#8220;firefighter&#8221; or &#8220;ER doctor&#8221;), using scene contexts (e.g., &#8220;comfort a child trapped in fire&#8221; or &#8220;ask for help in a serious operation&#8221;).\nThis approach provides accessible, general-purpose scoring that is scalable to a wide range of scenarios, making it suitable for large-scale benchmarking.\nHowever, its reliance on broad stereotypes and global impressions limits its ability to capture fine-grained aspects of delivery (e.g., subtle prosodic shifts, nuanced emotional control)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Holt &amp; Lotto, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib26\" title=\"\">2010</a>)</cite>.\nIn practice, this strategy mirrors simplified annotation settings and is particularly useful when annotation resources are limited&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib21\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "roleplay",
                    "emotional",
                    "prosodic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS5\" title=\"F.5 Examples of Realism Role-play &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.5</span></a>). The second strategy adopts a <em class=\"ltx_emph ltx_font_italic\">bottom-up</em> design, motivated by real-world performance and real human speech.\nInstead of relying solely on stereotypes, this approach grounds evaluation in recordings from professional and non-professional speakers, drawn from realistic dialogue and media sources.\nJudgments in this setting are more fine-grained and multi-dimensional, addressing aspects such as prosodic dynamics, emotional fidelity, character consistency, and contextual fit.\nWhile this strategy requires more human annotation and careful curation, it provides benchmarks that reflect how role-play is perceived in practice.\nIt thus offers a richer framework for analyzing model outputs in settings where realism and subtlety are essential.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "contextual",
                    "consistency",
                    "emotional",
                    "model",
                    "character",
                    "prosodic",
                    "dynamics",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human Annotation.</span>\nAll collected audio undergoes a standardized annotation protocol: (1) an initial semantically-based screening to exclude invalid clips; (2) rubric-based scoring across multiple perceptual dimensions; and (3) meta-annotations such as confidence or prompt-quality judgments. Full rubrics are provided in the appendices&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A5\" title=\"Appendix E Annotation Guidelines for Archetype Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">E</span></a> &amp; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>All data collection follows in high standard ethical consideration as outlined in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A9\" title=\"Appendix I Annotation Ethics &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "all",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Archetype Data Collection &#8227; 4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents a summary of the curated <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> dataset, where we further elaborate the details of archetype and realism evaluation data collection in following subsections, respectively. For benchmark purpose, we provide an official train-test split for the human annotated dataset for <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> so that following researchers can use the dataset to train and evaluate their SEMs for speech role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Role and Scene Dimensions.</span>\nRoles are operationalized through seven categories: (1) functional occupations in daily life, (2) functional occupations in fictional or fantasy settings, (3) social identities, (4) named or specific characters, (5) personality traits, (6) relative relationship, (7) basic events. Scenes are anchored by everyday events or state changes that evoke stereotypical reactions. Details of template design and alternative prompting strategies are given in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>We note that some archetypes (e.g., firefighter) inevitably reflect cultural stereotypes. These are used solely for evaluation scalability and do not reflect normative judgments.</span></span></span></p>\n\n",
                "matched_terms": [
                    "traits",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype Evaluation Data Summary.</span>\nThe resulting archetype evaluation dataset comprises 552 distinct scenarios for both Mandarin and English, evaluated across eight models,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>The model list is discussed in Appdenix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS4\" title=\"D.4 Candidate Models for Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.4</span></a>. Note we only include seven models for English due to the deactivation of ChatGPT-4O advanced voice model.</span></span></span> yielding 8,280 samples. In total, the dataset are split into 6,780 for training and 1,500 for testing. For benchmarking, an additional 1,250 scenarios spanning seven role/scene categories are prepared to facilitate archetype role-play benchmarking. More detailed statistics and analyses are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10\" title=\"Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">J</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realism evaluation adopts a bottom&#8211;up perspective, emphasizing authentic delivery, prosody, and contextual fit.</p>\n\n",
                "matched_terms": [
                    "contextual",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Sources and Negative Samples.</span>\nWe curate speech from (i) ground-truth media segments, (ii) retrieved character profiles, and (iii) generated local scene descriptions. To stress-test evaluation models, we construct contrastive mismatches: (i) negative local scenes with varying degrees of mismatch, and (ii) negative character profiles created by re-synthesizing transcripts with TTS systems. Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6\" title=\"Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F</span></a> details the prompt design and TTS generation details.</p>\n\n",
                "matched_terms": [
                    "local",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotators judge whether performances align with the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em>, using rubrics for <em class=\"ltx_emph ltx_font_italic\">Prosodic Dynamics (i.e., Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis)</em>, <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness (with progressive gating over three sub-dimensions: Emotion Accuracy, Emotion Intensity, Emotion Transition)</em>, <em class=\"ltx_emph ltx_font_italic\">Character Consistency (i.e., Voice Identity Matching and Traits Embodiment)</em>, <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance (i.e., Local Scene Fit and Global Story Coherence)</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>. A prompt quality check and confidence ratings are also collected. Full scales and gating details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "pitch",
                    "identity",
                    "relevance",
                    "local",
                    "consistency",
                    "emotional",
                    "contextual",
                    "character",
                    "accuracy",
                    "prosodic",
                    "intensity",
                    "transition",
                    "stress",
                    "dimensions",
                    "expressiveness",
                    "traits",
                    "dynamics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real-world Recoding.</span>\nAs noted in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S2\" title=\"2 Related Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, most prior efforts have relied heavily on synthetic data, mostly from TTS systems based on curated professional role-play voices. While such data can achieve high fidelity, it often lacks the diversity and spontaneity of real-world delivery, limiting the generalizability of evaluation models. To address this gap, we expand realism evaluation beyond synthetic and curated voices by incorporating newly recorded speech from both professional and amateur voice actors. This design introduces a broader spectrum of delivery styles, from polished professional acting to more natural and less controlled amateur performance.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation Data Summary.</span>\nThe resulting realism evaluation dataset is constructed from a combination of curated media sources, including internal collections, public datasets (e.g., NCSSD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib40\" title=\"\">2024b</a>)</cite>), and original recordings collected for this project.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>All data were obtained under appropriate licenses and are legally available for research release.</span></span></span> We compile 15k annotated role-play samples: 9k in Mandarin and 5k in English. The dataset is divided into 12k samples for training, 2k mixed samples for the base test set, and 1k real-recording samples for the test set. The first test set supports general evaluation using our proposed pipeline with negative samples, while the second focuses exclusively on human speech performed by both professional and amateur voice actors. For benchmarking, 2,000 scenarios from the realism dataset are designated for realism role-play benchmarking. Detailed statistics and analyses are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11\" title=\"Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">K</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the two proposed datasets, we conduct experiments under three settings: zero-shot, few-shot, and finetuning, using pre-trained ALLMs. We formulate the task as a multi-metric estimation problem&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tjandra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib58\" title=\"\">2025</a>; Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib74\" title=\"\">2025</a>; Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib55\" title=\"\">2025b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib54\" title=\"\">a</a>)</cite>, where a single model is trained or prompted to predict multiple evaluation dimensions.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dimensions",
                    "zeroshot",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-shot and Few-shot ALLMs.</span>\nWe first evaluate the zero-shot capabilities of both proprietary and public models, including <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>, <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>, <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio</span>, and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>. All evaluations follow a unified prompting template that provides audio input alongside scene and character context.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "gpt4oaudio",
                    "zeroshot",
                    "character",
                    "gemini25pro",
                    "all",
                    "fewshot",
                    "kimiaudio",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For proprietary models, we report the average of five independent runs to reduce variance and obtain more stable predictions. For open-source models, we observed weaker instruction-following ability and limited adaptation to unfamiliar tasks. To mitigate this, we did not request simultaneous prediction of multiple metrics. Instead, we queried one metric at a time. Following <cite class=\"ltx_cite ltx_citemacro_citet\">Song &amp; Bahri (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>); Lukasik et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>, we interpret model outputs through token distribution over discrete labels (<span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score.</p>\n\n",
                "matched_terms": [
                    "report",
                    "average",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Finetuning Model: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>.</span>\nIn addition to zero-shot and few-shot evaluations, we fine-tune a dedicated evaluation model, denoted as <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> for archetype and realism role-play evaluation, respectively.\nWe initialize from <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio-7B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib11\" title=\"\">2024</a>)</cite>, which provides a strong foundation for multi-modal audio-text reasoning.\nTo better leverage the multiple human annotations available in our datasets, we adopt a sampling-based training strategy: at each step, one annotator label is randomly selected, following practices in label distribution learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Geng, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib22\" title=\"\">2016</a>; Wen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib67\" title=\"\">2023</a>)</cite>.\nThis stochastic supervision implicitly exposes the model to the empirical annotation distribution, while during inference we output the full distribution over discrete labels (e.g., <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score, consistent with recent decoding-based regression methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Song &amp; Bahri, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>; Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib70\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib64\" title=\"\">2025b</a>; Lukasik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>. We apply LoRA fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib28\" title=\"\">2022</a>)</cite>. All implementation details, including optimizer settings, training schedule, and sampling policy, are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS3\" title=\"L.3 Fine-tuning Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "zeroshot",
                    "finetune",
                    "model",
                    "all",
                    "fewshot",
                    "drameeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics for Role-play Evaluation Model.</span> For most dimensions with 1-5 scale, we report Pearson correlation. For <span class=\"ltx_text ltx_font_italic\">Content Pass</span> estimation, we report accuracy.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "correlation",
                    "report",
                    "model",
                    "pearson",
                    "dimensions",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype Evaluation.</span>\nThe results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T2\" title=\"Table 2 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reveal clear distinctions across evaluation settings. In the zero-shot condition, proprietary ALLMs such as <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> substantially outperform public models, achieving strong <span class=\"ltx_text ltx_font_italic\">content pass</span> accuracy (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>90%) and moderate correlations with other dimensions(<math alttext=\"0.43\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mn>0.43</mn><annotation encoding=\"application/x-tex\">0.43</annotation></semantics></math>&#8211;<math alttext=\"0.48\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mn>0.48</mn><annotation encoding=\"application/x-tex\">0.48</annotation></semantics></math>). By contrast, public models exhibit limited predictive ability, with correlations often below 0.2. Few-shot prompting provides modest gains for proprietary models, improving both accuracy and correlation (e.g., <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> rises from 0.480 to 0.493 average correlation). The most significant improvement arises from finetuning: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves an average correlation of 0.629, marking a large margin over zero-shot/few-shot models and demonstrating the effectiveness of task-specific supervision.</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "correlation",
                    "zeroshot",
                    "average",
                    "gemini25pro",
                    "fewshot",
                    "accuracy",
                    "drameeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real Recording Test Set.</span>\nThe real-recording evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T4\" title=\"Table 4 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> is notably more challenging, with all models suffering substantial performance drops. Proprietary models in the zero-shot setting achieve only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.04&#8211;0.08 average correlation, and public models often fall below zero. Few-shot prompting yields small but noticeable improvements, especially for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> (0.226 avg.), suggesting that exemplars partially mitigate domain mismatch between synthetic and real human speech. Interestingly, while finetuning achieves the highest overall average (0.247), its relative advantage over few-shot prompting is narrower than in the synthetic setting, and some dimensions (e.g., <span class=\"ltx_text ltx_font_italic\">Character Identity</span>) remain weak. We emphasize that this observed gap between synthetic and real recordings is not merely an artifact but an intended feature of Speech-DRAME, designed to expose the mismatch and drive future research into bridging synthetic-to-real generalization.</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "avg",
                    "zeroshot",
                    "correlation",
                    "identity",
                    "average",
                    "character",
                    "all",
                    "dimensions",
                    "fewshot",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall.</span> Across both archetype and realism evaluations, we observe three consistent trends: (i) zero-shot performance is limited, particularly for open-source models, (ii) few-shot prompting offers moderate but consistent gains, especially for proprietary ALLMs, and (iii) finetuning with task-specific supervision (<span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>) delivers the largest improvements, achieving state-of-the-art correlations across nearly all dimensions. Nevertheless, results on the real-recording test set underscore the gap between synthetic and real human role-play scenarios, motivating future work in data diversification and domain adaptation.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "zeroshot",
                    "all",
                    "dimensions",
                    "fewshot",
                    "drameeval",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on fine-tuned <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, we establish the DRAME-RoleBench to evaluate SFMs&#8217; role play ability. For archetype evaluation, 1,250 scenarios are constructed following the same data construction pipeline as the <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>. For realism evaluation, we reuse 2,000 scenarios in the <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> test set. We follow the same prompt strategy as outlined in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "drameeval",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, we consider a comprehensive set of candidate models, including six end-to-end models: <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib47\" title=\"\">2025b</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib75\" title=\"\">2024</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib71\" title=\"\">2025a</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Qwen3Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib72\" title=\"\">2025b</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">StepAudio2Mini</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib68\" title=\"\">2025</a>)</cite>; and ten cascaded models by combining two widely-used textual LLMs (i.e., <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib12\" title=\"\">2025</a>)</cite> and <span class=\"ltx_text ltx_font_typewriter\">Qwen3-30B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib73\" title=\"\">2025</a>)</cite>) and five recent TTS models (i.e., <span class=\"ltx_text ltx_font_typewriter\">F5TTS</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib8\" title=\"\">2024c</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">IndexTTS2</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib82\" title=\"\">2025b</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">MaskGCT</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib66\" title=\"\">2025d</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Vevo1.5</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib79\" title=\"\">2025b</a>)</cite>, and <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib17\" title=\"\">2024b</a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "kimiaudio",
                    "qwen25omni",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype evaluation.</span>\nOn archetype role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T5\" title=\"Table 5 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), cascaded pipelines consistently outperform end-to-end systems. The strongest cascaded systems, particularly <span class=\"ltx_text ltx_font_typewriter\">G-IndexTTS2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-IndexTTS2</span>, achieve both high content pass rates (above 94%) and balanced scores across audio quality, human likeness, and appropriateness. By contrast, end-to-end models such as <span class=\"ltx_text ltx_font_typewriter\">StepAudio2Mini</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> perform competitively in terms of content coverage (above 88%), but their expressive scores remain slightly lower than their cascaded counterparts. This gap suggests that separating reasoning (LLM) from speech rendering (TTS) remains advantageous when archetype fidelity is emphasized. Nevertheless, the fact that top end-to-end systems already surpass 3.0 in average rating indicates rapid closing of the performance gap.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "average",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism evaluation.</span>\nRealism role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T6\" title=\"Table 6 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) presents a different landscape. Here, performance differences between end-to-end and cascaded systems are less pronounced. Several end-to-end models, such as <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>, reach average scores of 3.24, matching or even exceeding the best cascaded pipelines. Importantly, realism scores are more evenly distributed across prosodic, emotional, character, and contextual dimensions, with most models clustering between 2.9&#8211;3.2. This convergence reflects the difficulty of realism-style evaluation: since the data preparation emphasized contrast with authentic human speech, models are judged under similar conditions, leading to smaller observed gaps.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "contextual",
                    "average",
                    "emotional",
                    "qwen25omni",
                    "character",
                    "prosodic",
                    "dimensions",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human alignment.</span>\nWe further conducted small-scale human studies with 10% of the benchmark data to verify the reliability of the benchmark. Archetype evaluation achieves a Spearman correlation of 0.706 between human judgments and <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, validating that the automatic evaluator closely tracks human preferences in this setting. Realism evaluation, however, achieves a more modest Spearman correlation of 0.375. This lower alignment is expected, since realism data construction did not prioritize fine-grained model separation but instead emphasized distinguishing human versus synthetic speech.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>We view this as evidence that realism presents a harder, less-saturated frontier. Rather than a limitation of DRAME-Eval, this highlights the challenge of designing evaluators that capture nuanced human perception, which motivates future work.</span></span></span> As a result, all models occupy a relatively narrow performance band, reducing sensitivity to model-level differences. We view this as evidence that realism evaluation highlights a harder, less saturated frontier where both SFMs and SEMs require further advances.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "realism",
                    "evaluator",
                    "model",
                    "all",
                    "drameeval",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, cascaded pipelines retain an edge in archetype scenarios, especially in content robustness and expressive fidelity, but end-to-end models are catching up quickly. In realism scenarios, the field remains unsolved: existing systems exhibit similar performance bands and evaluators show weaker alignment with human judgments, underscoring the need for improved realism-grounded data and evaluation. Together, these findings establish <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> as both a reliable testbed for archetype fidelity and a challenging benchmark for realism fidelity.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework for assessing spoken role-play through two complementary lenses: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em> (top-down) and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> (bottom-up, human-grounded). Our human-annotated <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> enables training and testing of SEMs, and our fine-tuned <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span> substantially outperforms zero-/few-shot ALLM judges. Building on this, <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> provides system-level comparison across various SFMs, revealing a consistent edge for cascaded pipelines on archetypes and a tighter spread under realism, where real human performance evaluation remain challenging. Together, these resources establish a reproducible, analyzable foundation for spoken role-play evaluation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>Limitations and future works are discussed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A13\" title=\"Appendix M Limitations and Future Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">M</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "drameeval",
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All data collection and annotation in this work followed strict ethical standards. Our bilingual datasets were constructed exclusively from legally accessible sources, including licensed media, open-source corpora, and original recordings conducted with informed consent. Annotators were recruited with appropriate compensation and provided with clear instructions, including the right to withdraw at any point. To minimize risks, no personally identifiable information was included in the released data, and all recordings were anonymized before annotation. We ensured cultural and linguistic diversity in annotator pools to mitigate potential bias across Mandarin and English role-play scenarios. While this study did not require formal IRB approval, we adhered to best practices in human-subject research, including transparency in task design and careful consideration of annotator well-being. Full details of our annotation protocol and ethical safeguards are presented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A9\" title=\"Appendix I Annotation Ethics &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments were conducted on widely available GPUs (NVIDIA H100). Random seeds are fixed across all runs to ensure consistency. By releasing datasets, models, and code, we aim to make all major results in this paper fully reproducible and facilitate future extensions of our framework.</p>\n\n",
                "matched_terms": [
                    "all",
                    "consistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data generation:</span> LLMs were employed to expand prompts (e.g., character profiles, scene descriptions) and to produce negative samples in realism evaluation. All outputs were screened by human annotators to ensure validity and reduce bias.</p>\n\n",
                "matched_terms": [
                    "all",
                    "realism",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline evaluators:</span> State-of-the-art audio LLMs (e.g., <span class=\"ltx_text ltx_font_typewriter\">GPT4o</span>, <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>) were included as zero-shot judges in our evaluation benchmark, enabling direct comparison with human-aligned SEMs.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "qwen25omni",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our distinction between <span class=\"ltx_text ltx_font_italic\">Archetype Evaluation</span> and <span class=\"ltx_text ltx_font_italic\">Realism Evaluation</span> is not merely pragmatic, but is grounded in established theories of social identity, communication, and performance. The two perspectives capture complementary dimensions of how humans perceive and evaluate roles in interaction.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "dimensions",
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetypes as Collective Schemas</span>\nThe notion of archetypes originates from Jungian psychology, where they represent recurrent, culturally embedded character prototypes in the collective unconscious&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jung, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib36\" title=\"\">2014</a>)</cite>. In media and performance studies, archetypes have been shown to guide audience expectations, providing top-down interpretive frames for evaluating whether a character &#8220;fits&#8221; within familiar categories such as hero, mentor, or trickster&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Campbell, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib4\" title=\"\">2008</a>)</cite>. Archetype-based reasoning allows for scalable, general-purpose judgments because it leverages culturally shared schemas rather than idiosyncratic details.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism as Situated Performance</span>\nIn contrast, realism highlights the fine-grained enactment of roles in situated interaction. Goffman&#8217;s dramaturgical sociology emphasizes that everyday social life is itself a performance, where believability depends on subtle paralinguistic and pragmatic cues such as prosody, hesitation, and emotional shading&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goffman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib23\" title=\"\">1949</a>)</cite>. Similarly, interactional sociolinguistics has demonstrated how micro-level features like intonation, pacing, and turn-taking patterns shape the perception of authenticity in communication&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gumperz, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib25\" title=\"\">1982</a>)</cite>. Realism Evaluation thus anchors role-play assessment in bottom-up sensitivity to authentic delivery, beyond schematic expectations.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "emotional",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Complementarity of the Two Perspectives</span>\nPerformance theory provides a unifying lens for these dual dimensions. Schechner&#8217;s view of performance as both &#8220;restored behavior&#8221; (scripted, repeatable structures) and &#8220;living process&#8221; (situated enactment)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Schechner, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib52\" title=\"\">2017</a>)</cite> underscores the need to consider both archetypal scripts and enacted realism. Likewise, sociolinguistic theories of identity recognize both <span class=\"ltx_text ltx_font_italic\">ascribed categories</span> (archetypes imposed by social convention) and <span class=\"ltx_text ltx_font_italic\">performed identities</span> (emergent in interaction)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bucholtz &amp; Hall, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib2\" title=\"\">2005</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "dimensions",
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p4.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> may be a simple mean, a recency-weighted average, or a learned temporal model that captures consistency across turns.\nThe choice of <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p4.m3\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> reflects the intended evaluation focus, e.g., stability of persona traits, adaptation to scene progression, or coherence across multiple speakers.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "average",
                    "consistency",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Discussion.</span>\nWhile our released dataset and experiments concentrate on single-turn evaluation, the modular nature of the formulation ensures that extending to multi-turn is straightforward.\nFuture work can leverage <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p5.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> to study richer aspects of speech role-play, such as persona persistence, interaction dynamics, and role consistency across extended dialogues.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dynamics",
                    "consistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Role dimensions.</span>\nWe operationalize archetypes through five principal categories, each representing a stereotypical dimension of role identity or behavior, including (1) <span class=\"ltx_text ltx_font_bold\">Occupation / Functional Role (Modern Daily Life)</span> (e.g., a firefighter rescuing a trapped civilian). (2) <span class=\"ltx_text ltx_font_bold\">Occupation / Functional Role (Fictional or Fantasy)</span> (e.g., a martial arts cult leader confronting orthodox rivals). (3)&#160;<span class=\"ltx_text ltx_font_bold\">Social Identity</span> (e.g., a doting parent worrying about a child, or a &#8220;Buddhist youth&#8221; eschewing competition). (4) <span class=\"ltx_text ltx_font_bold\">Named / Specific Characters</span> (e.g., Sun Wukong mocking celestial authorities in <em class=\"ltx_emph ltx_font_italic\">Journey to the West</em>). (5) <span class=\"ltx_text ltx_font_bold\">Personality Traits</span> (e.g., &#8220;hot-blooded&#8221; in a competitive setting, or &#8220;complaining&#8221; in response to poor service). (6) <span class=\"ltx_text ltx_font_bold\">Relative Roles</span> (e.g., parent or stranger).</p>\n\n",
                "matched_terms": [
                    "identity",
                    "dimensions",
                    "traits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scene dimensions.</span>\nIn addition to roles, we incorporate contextual triggers that anchor role-play in stereotypical situations.\nA central focus is on <span class=\"ltx_text ltx_font_bold\">everyday external events or state changes</span> (e.g., receiving a promotion, being caught in gossip), which provide concrete scenarios for testing archetypical responses.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "contextual",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Exclusion of fine-grained controls.</span>\nWe deliberately excluded explicit emotional states, prosodic instructions, or linguistic constraints.\nThese dimensions are better suited to realism-oriented or style-controlled evaluation, whereas archetype evaluation benefits from concrete, stereotype-level scenarios that sharpen judgments and improve human agreement.</p>\n\n",
                "matched_terms": [
                    "prosodic",
                    "emotional",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Pipeline overview.</span>\nAt a high level, archetype evaluation data are generated through a three-stage pipeline:\n(1) selecting keywords that specify role and scene;\n(2) expanding them into full prompts via structured templates; and\n(3) rendering prompts into speech with contemporary TTS systems.\nThis modular pipeline ensures both scalability and reproducibility, while keeping prompts vivid enough for reliable judgments. (4) Based on the generated data (paired scene and synthesized speech), expert human annotators are recruited for detailed annotation on semantic check (i.e., <span class=\"ltx_text ltx_font_italic\">Content Pass</span>) and three different spoken role-play dimensions (i.e., <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human-likeness</span>, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>).\nImplementation details, including template design, alternative prompting strategies, and annotation guidelines, are provided in the following subsections of this Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, archetype evaluation emphasizes breadth and recognizability over subtlety.\nBy grounding role-play in socially recognizable categories and stereotypical situations, it provides a practical balance between scalability and interpretability, complementing the more fine-grained realism evaluation described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS2\" title=\"3.2 Role-play Evaluation with Two Strategies &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Excluded Dimensions.</span> We initially considered more fine-grained control dimensions, but excluded them to preserve the archetype-level focus. Examples of excluded dimensions include:\n<em class=\"ltx_emph ltx_font_italic\">Emotional State &amp; Expression, Single Emotion Prompts, Explicit User Input / Assistant Output structures, Emotional Transition / Fluctuation, Voice Style &amp; Prosody (pacing, intonation, vocal timbre, expressive style, accent/dialect), and Content / Interaction Patterns (task/topic controls, specific linguistic structures, or speech production challenges).</em></p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "transition",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All steps are automatically processed by <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib12\" title=\"\">2025</a>)</cite>, but each output is manually verified to ensure high quality for downstream usage. We demonstrate an example for Roles: Occupation / Functional Role in Fictional or Fantasy as follows:</p>\n\n",
                "matched_terms": [
                    "all",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cascaded Model Generation.</span>\nFor cascaded systems, we only use Cosyvoice2 as the TTS model in our collection of data. We select <span class=\"ltx_text ltx_font_typewriter\">Qwen3-30B-Instruct</span> and <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> as the text LLM candidates&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib73\" title=\"\">2025</a>; Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib12\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2\" title=\"Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the internal annotation interface used for <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>.\nThe platform provides an integrated environment for both playback and evaluation of model-generated role-play speech samples.\nAnnotators first interact with the audio player interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf1\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>), which visualizes waveforms and spectrograms for multiple versions of the same prompt, enabling side-by-side comparison under a shared role instruction (e.g., firefighter, teacher, or actor).\nThen, they proceed to the annotation interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf2\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a>), where they assign scores for multiple dimensions such as <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, along with reasoning and confidence inputs.\nThis structured interface supports high-consistency, fine-grained evaluation of role-play behaviors in speech models.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">Appropriateness</span>: comment for <em class=\"ltx_emph ltx_font_italic\">all</em> scores 1&#8211;5. Explain <em class=\"ltx_emph ltx_font_italic\">why</em> the <em class=\"ltx_emph ltx_font_italic\">delivery</em> was/was not a good match to the in-context traits (role &amp; scene).</p>\n\n",
                "matched_terms": [
                    "traits",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Rate how likely an average listener would believe the clip is human-delivered (ignore Audio Quality issues and contextual fit).</p>\n\n",
                "matched_terms": [
                    "contextual",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realism Evaluation adopts a bottom&#8211;up perspective grounded in real human speech data, with the goal of capturing the nuanced delivery, prosody, and contextual fit that synthetic archetype prompts cannot fully represent.\nRather than relying on stereotypes, this strategy leverages real-world recordings and carefully constructed contrasts, enabling fine-grained assessment of model performance under realistic conditions.</p>\n\n",
                "matched_terms": [
                    "contextual",
                    "realism",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generated local scenes.</span>\nWe supplement retrieved character information with automatically generated local scene descriptions summarizing the surrounding plot or setting.\nAppendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS3\" title=\"F.3 Prompts for Local Scene Generation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.3</span></a> includes representative prompts and generation details.</p>\n\n",
                "matched_terms": [
                    "local",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative character profiles.</span>\nWe also create role-profile mismatches by resynthesizing transcripts with diverse TTS systems, thereby breaking the link between authentic delivery and intended role.\nWe explore both text-instruction-based prompting and comparisons across commercial and non-commercial TTS frameworks.\nDetailed model lists and parameter settings are documented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS6\" title=\"F.6 TTS Systems and Settings &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.6</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, real human ground-truth segments and systematically constructed negatives form a balanced dataset that supports realism-oriented evaluation.\nThe positives provide natural anchors of role-play performance, while the negatives introduce controlled mismatches that test whether evaluation models can distinguish real human speech from synthetic approximations.\nThis design ensures that realism evaluation emphasizes subtle paralinguistic and contextual cues, moving beyond intelligibility or content fidelity.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "contextual",
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During annotation, raters are presented with either: (i) ground-truth stories, (ii) stories with negative local scenes, or (iii) stories with negative character profile. Annotators assign realism scores without being exposed to the underlying mismatch reasons. Instead, these reasons are provided only to the quality-assurance team, who use them as reference for internal validation and consistency checks. While the mismatch reasons may not be perfect, they help streamline quality control procedures and maintain reliability across the dataset.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "local",
                    "consistency",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To make the construction of the realism evaluation task concrete, we present a worked example that begins with a curated local scene and character profile, followed by the same transcript adapted into mismatched contexts. These mismatches appear either at the <em class=\"ltx_emph ltx_font_italic\">local-scene</em> level or at both the <em class=\"ltx_emph ltx_font_italic\">character profile and scene</em> level, thereby providing controlled degradations of realism.</p>\n\n",
                "matched_terms": [
                    "local",
                    "realism",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the character Yunjin from <span class=\"ltx_text ltx_font_italic\">Genshin Impact</span> as a running case study, which is also included in our released dataset.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>As acknowledged, the use of real-world recordings is conducted under legal and ethical review. All curated data will be released as part of our open-source package.</span></span></span> This example illustrates how alignment between a speaker&#8217;s persona, local context, and delivery underpins realism, and how systematically breaking that alignment produces contrastive test cases.</p>\n\n",
                "matched_terms": [
                    "all",
                    "local",
                    "realism",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Curated Local Scene.</span>\nYunjin is narrating a fierce battle through operatic singing. Her verses depict a hero charging into enemy lines, slaying the enemy commander, and securing peace. The atmosphere is filled with tension and intensity, highlighting the hero&#8217;s extraordinary bravery and determination.</p>\n\n",
                "matched_terms": [
                    "local",
                    "intensity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative Local Scenes.</span>\nTo stress realism evaluation, we insert the same transcript into alternative backgrounds. These reveal how context-shifted delivery degrades narrative coherence:</p>\n\n",
                "matched_terms": [
                    "realism",
                    "stress",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative Character Profiles.</span>\nWe replace Yunjin with new character profiles and scenes. Although the transcript remains the same, realism degrades because the delivery no longer matches the speaker&#8217;s persona or situation.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Yunjin case study illustrates a fundamental difference between realism evaluation and archetype evaluation. Archetype prompts, as described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>, are constructed by pairing abstract role categories with synthetic contextual scenes. These cover a broad space of possible situations, ranging from modern occupations (e.g., firefighters) to fictional personas (e.g., cult leaders in martial arts fantasy) or social identities (e.g., overprotective parents, laid-back friends). While such prompts provide controlled and diverse coverage, they are ultimately template-based and rely on stereotypes to elicit role-play behaviors.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "contextual",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the Yunjin example is drawn from authentic performance, where the character&#8217;s persona (an opera singer) and local scene (a battlefield narration) are naturally aligned through artistic delivery. The realism task does not merely test whether a model can follow a prompt, but whether it can preserve coherence between a speaker&#8217;s global identity, immediate context, and expressive prosody. When Yunjin&#8217;s transcript is transplanted into mismatched settings or paired with incongruent speaker profiles, the breakdown in realism becomes apparent: the same words shift from inspiring to awkward, jarring, or even absurd.</p>\n\n",
                "matched_terms": [
                    "global",
                    "realism",
                    "local",
                    "identity",
                    "model",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This distinction highlights the complementary strengths of the two evaluation paradigms. Archetype evaluation systematically explores a wide combinatorial space of roles and situations, but may lack the paralinguistic and contextual richness of real speech. Realism evaluation, on the other hand, anchors assessment in authentic delivery and contextual fit, providing a finer-grained lens on coherence and naturalness. Together, they ensure that role-play evaluation spans both broad stereotypical coverage and nuanced real-world plausibility.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "contextual",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed a diverse set of TTS systems to generate negative character profiles and resynthesized speech for Realism Evaluation.\nThese included both commercial and non-commercial models, with variation in style control, conditioning prompts, and reference usage.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.T8\" title=\"Table 8 &#8227; Same-Speaker Voice Clone. &#8227; F.6 TTS Systems and Settings &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> summarizes the systems and settings.\nBelow, we detail the major strategies used across systems:</p>\n\n",
                "matched_terms": [
                    "realism",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We leveraged all publicly available speaker style information released by the corresponding websites.\nFor each input sample, we employed <span class=\"ltx_text ltx_font_typewriter\">deepseek-v3</span> to select the best-matching speaker based on the original speech caption.\nIn the case of <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5TTS</span>, the character profile was also appended as an additional instruction to guide synthesis.</p>\n\n",
                "matched_terms": [
                    "all",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A character profile was first converted into a descriptive voice specification by prompting <span class=\"ltx_text ltx_font_typewriter\">deepseek-v3</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib38\" title=\"\">2024a</a>)</cite>.\nThis description was submitted to the TTS service to generate candidate voices.\nWe then previewed samples via Gemini2.5Pro, selecting the voice that best matched the speaker style of the original recording before proceeding to generate speech.</p>\n\n",
                "matched_terms": [
                    "gemini25pro",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS3</span> (Embedding&#8211;AudioCode Pipeline).\nHere, the input is Qwen2.5 BPE embeddings, which a 0.4B GPT-2 model maps directly to 4,096-dimensional audio codes. These codes are decoded via a cascaded architecture: <span class=\"ltx_text ltx_font_typewriter\">RepCodec</span> decoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib32\" title=\"\">2024</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> content representation (with speaker embedding) <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DDIM</span> (denoising\ndiffusion implicit model) module&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jeong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib34\" title=\"\">2021</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DAC</span> latent representation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib37\" title=\"\">2023</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DAC</span> decoder. Conditioning is provided through prefix text embeddings (e.g., instructions expressing emotional states such as &#8220;angry, hateful: you betrayed me&#8230;&#8221;), along with speaker ID. The input format is <span class=\"ltx_text ltx_font_typewriter\">[spk_id, BPE embedding, audio codes]</span>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The annotation interface for <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> focuses on assessing contextual and narrative coherence in model-generated speech.\nUnlike the archetype-oriented evaluation, which emphasizes stylistic and emotional fidelity, this platform examines how well the voice performance aligns with a character&#8217;s personality, dialogue situation, and scene progression.</p>\n\n",
                "matched_terms": [
                    "contextual",
                    "realism",
                    "emotional",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.F3\" title=\"Figure 3 &#8227; F.7 Annotation Platform for Realism Evaluation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the core playback interface.\nAnnotators can preview each audio sample with synchronized spectrogram and waveform displays, while reviewing contextual metadata including the <span class=\"ltx_text ltx_font_italic\">Local Scene</span>, <span class=\"ltx_text ltx_font_italic\">Character Style</span>, and <span class=\"ltx_text ltx_font_italic\">Character Profile</span>.\nThese descriptions help situate the listening task, providing the background necessary for judging realism in the character&#8217;s vocal and behavioral expression.</p>\n\n",
                "matched_terms": [
                    "local",
                    "realism",
                    "contextual",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Read the <span class=\"ltx_text ltx_font_bold\">Character Profile</span> (long-term identity, style) and <span class=\"ltx_text ltx_font_bold\">Local Scene</span> (immediate context, goal/emotion) before listening.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the audio&#8217;s speaker is <em class=\"ltx_emph ltx_font_italic\">clearly not</em> the intended target identity, assign a score of <span class=\"ltx_text ltx_font_bold\">1</span> to all Profile-dependent dimensions (e.g., Voice Identity Matching, Trait Embodiment) and proceed with other dimensions as applicable.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "all",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Detailed case examples and progressive gating for emotional dimensions are provided in the full instruction document.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt five dimensions, each rated from 1 (worst) to 5 (best). Emotional Expressiveness uses <em class=\"ltx_emph ltx_font_italic\">progressive gating</em>.</p>\n\n",
                "matched_terms": [
                    "expressiveness",
                    "dimensions",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prosodic Dynamics</span> (Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis): naturalness of patterns and rhythms.</p>\n\n",
                "matched_terms": [
                    "prosodic",
                    "dynamics",
                    "stress",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotional Expressiveness</span> (Progressive gating):</p>\n\n",
                "matched_terms": [
                    "expressiveness",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Character Consistency</span> (Voice Identity Matching, Trait Embodiment): alignment with Character Profile.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "consistency",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contextual Relevance</span> (Local Scene Fit, Global Story Coherence): fit to immediate scene and long-term identity.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "identity",
                    "relevance",
                    "contextual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Read first:</span> Carefully read the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em> to understand the long-term identity and immediate goal/emotion.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identity mismatch:</span> If the clip&#8217;s speaker is clearly not the intended target speaker, assign a score of <span class=\"ltx_text ltx_font_bold\">1</span> for every rating dimension that relies on the Character Profile (e.g., Character Style Traits, Trait Embodiment, Character Consistency).</p>\n\n",
                "matched_terms": [
                    "identity",
                    "consistency",
                    "traits",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt contains contradictory information.</span> Conflicts make accurate annotation difficult or impossible. Examples: incompatible accent specifications; Local Scene drastically at odds with intended emotion (see <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>); Local and global scenes inconsistent; traits described as &#8220;evolving over time&#8221; without a timeline.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "global",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt is insufficient to evaluate the voice sample.</span> Audio too short to gauge the dimensions; or the prompt lacks specific characteristics/is too vague or multi-directional to know which version of the character is present. (It is OK if audio does not match transcript exactly; if too much is missing to annotate, mark as insufficient.)</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Question 2: Observed Character Style Traits.</span> Based on the Character Profile, are the defined Style Traits clearly present in the delivery?</p>\n\n",
                "matched_terms": [
                    "traits",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rubric Rating Instructions.</span>\nScore each subcriterion on a 1&#8211;5 scale. Use the detailed rubrics below and the examples to maintain calibration across characters, tones, and emotional styles. Prioritize in-character believability; do not reward theatricality, emotional intensity, or technical polish unless they naturally fit the character <em class=\"ltx_emph ltx_font_italic\">and</em> scene.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "intensity",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Focus: pitch, rhythm, and emphasis in speech.\n<span class=\"ltx_text ltx_font_bold\">Guideline 1.1 Pitch Variation (check Local Scene, Character Profile, and audio).</span></p>\n\n",
                "matched_terms": [
                    "character",
                    "local",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 1.2 Rhythmic Naturalness (check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "character",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 1.3 Stress and Emphasis (check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "character",
                    "stress",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 2.1 Emotion Accuracy (Required; check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "character",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 2.2 Emotion Intensity Control (Only if 2.1 <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"A7.SS4.SSSx5.p3.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math> 3; check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "character",
                    "intensity",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 2.3 Dynamic Range (Only if 2.2 <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"A7.SS4.SSSx5.p4.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math> 3; check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "character",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 3.1 Voice Identity Matching (Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 4.2 Global Story (Character Profile) Coherence (check Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "global",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Progressive gating for emotion.</span>\nOur <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness</em> uses gated sub-scores (accuracy <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A8.SS2.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> intensity control <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A8.SS2.p3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> dynamic range), preventing spurious high scores when base emotion is misidentified.\nThis reduces label noise and improves SEM alignment stability.</p>\n\n",
                "matched_terms": [
                    "expressiveness",
                    "accuracy",
                    "emotional",
                    "intensity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The archetype evaluation provides a detailed view of how speech foundation models (SFMs) perform when tasked with role-play scenarios grounded in stereotypical characters and situations. Human annotations cover four dimensions, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, and <span class=\"ltx_text ltx_font_italic\">Content Pass</span>, and the results reveal important trends, as well as challenges in interpreting scores across languages.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A critical observation is that annotations in Mandarin and English cannot be directly compared. The annotation groups for the two languages differed in their linguistic backgrounds and cultural interpretations, which leads to systematic differences in scoring. Our bilingual pilot further confirmed that annotators often interpret role-play appropriateness and expressiveness differently depending on their cultural frame of reference. For example, Mandarin raters tend to reward delivery styles that align with culturally expected prosody and politeness strategies, while English raters prioritize spontaneity and naturalness. These discrepancies parallel the findings of the singing voice conversion challenge (SVCC2023) from <cite class=\"ltx_cite ltx_citemacro_citet\">Huang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib31\" title=\"\">2023</a>)</cite>, where native and non-native listeners provided significantly different ratings for the same samples, particularly for naturalness and speaker similarity. Consequently, while absolute scores differ substantially across Mandarin and English, only within-language comparisons are meaningful. Cross-lingual ranking is not valid given the different annotation bases.</p>\n\n",
                "matched_terms": [
                    "expressiveness",
                    "roleplay"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Within Mandarin from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, the relative rankings of models are clear. <span class=\"ltx_text ltx_font_typewriter\">Doubao</span> stands out as the strongest performer across all four dimensions, reaching near-ceiling levels in both <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> (4.37) and <span class=\"ltx_text ltx_font_italic\">Content Pass</span> (0.98). It also leads in role-related dimensions such as <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, indicating balanced competence across linguistic accuracy, delivery, and expressiveness. <span class=\"ltx_text ltx_font_typewriter\">G-CosyVoice2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-CosyVoice2</span> follow closely, especially in audio quality, though they lag behind in role appropriateness. <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> achieve mid-range results, reflecting higher variance across samples, while <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> shows competitive content understanding but lower naturalness and appropriateness. <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> consistently ranks the lowest, with large variance, underscoring instability and weaker alignment with archetypal role expectations.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "all",
                    "expressiveness",
                    "accuracy",
                    "kimiaudio",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The picture shifts in English, elaborated in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Here, <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> emerges as the most effective model by a wide margin, achieving top scores in both <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> (3.46) and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> (3.49), as well as high content pass rates (0.97). This reflects its strong grounding in English discourse norms and role adherence. In contrast, <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> variants perform well in <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> but are weaker in role alignment, highlighting a trade-off between acoustic fidelity and role-play embodiment. Other models such as <span class=\"ltx_text ltx_font_typewriter\">Doubao</span>, <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span>, and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> achieve moderate results, while <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> trail far behind, showing limited robustness in English scenarios. Taken together, the English results reveal both a sharper separation across models and a wider variance in appropriateness scores compared to Mandarin.</p>\n\n",
                "matched_terms": [
                    "model",
                    "roleplay",
                    "kimiaudio",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although realism evaluation targets nuanced paralinguistic behaviors, human annotators achieve <em class=\"ltx_emph ltx_font_italic\">consistently high agreement</em> once the rubric disentangles the aspects to be judged.\nAs shown in the agreement summary in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F7\" title=\"Figure 7 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, most dimensions fall in the <math alttext=\"0.86\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>0.86</mn><annotation encoding=\"application/x-tex\">0.86</annotation></semantics></math>&#8211;<math alttext=\"0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mn>0.88</mn><annotation encoding=\"application/x-tex\">0.88</annotation></semantics></math> range, with the lowest agreement on <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> (<math alttext=\"\\approx 0.83\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.83</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.83</annotation></semantics></math>) and the highest on <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em> / <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> (<math alttext=\"\\approx 0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.88</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.88</annotation></semantics></math>).\nEmotion-related, progressive metrics (Intensity and Transition) are slightly lower (<math alttext=\"\\approx 0.85\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.85</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.85</annotation></semantics></math>), reflecting the added difficulty of judging graded expressivity.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "local",
                    "intensity",
                    "dimensions",
                    "transition",
                    "traits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F8\" title=\"Figure 8 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F9\" title=\"Figure 9 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> visualize the score distributions per dimension.\nWe highlight three patterns:\n(i) <span class=\"ltx_text ltx_font_bold\">Progressive/gated emotion metrics.</span> For <em class=\"ltx_emph ltx_font_italic\">Emotion Intensity</em> and <em class=\"ltx_emph ltx_font_italic\">Emotion Transition</em>, we assign <span class=\"ltx_text ltx_font_typewriter\">1</span> when the scene is marked <em class=\"ltx_emph ltx_font_italic\">N/A</em> (i.e., emotion accuracy is insufficient to meaningfully judge intensity/transition). This produces a visible mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, a sparse bin at <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>, and slightly lower overall agreement; downstream analyses conditioning on adequate <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> (e.g., mean&#160;<math alttext=\"\\geq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 3</annotation></semantics></math>) recover more bell-shaped distributions.\n(ii) <span class=\"ltx_text ltx_font_bold\">Traits Embodiment.</span> Scores tend to avoid the middle (<math alttext=\"\\sim\\!3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!3</annotation></semantics></math>), concentrating on low values when traits clearly mismatch and high values when traits are strongly realized. This yields the lowest inter-rater agreement among dimensions, reflecting the inherently compositional and role-specific nature of trait judgments.\n(iii) <span class=\"ltx_text ltx_font_bold\">Other dimensions.</span> <em class=\"ltx_emph ltx_font_italic\">Pitch Variation</em>, <em class=\"ltx_emph ltx_font_italic\">Rhythmic Naturalness</em>, <em class=\"ltx_emph ltx_font_italic\">Stress &amp; Emphasis</em>, <em class=\"ltx_emph ltx_font_italic\">Voice Identity Matching</em>, <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em>, <em class=\"ltx_emph ltx_font_italic\">Global Story Coherence</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> are approximately unimodal with modes at <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>&#8211;<math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math>, consistent with moderate&#8211;strong realization in the dataset and with the high agreement reported.</p>\n\n",
                "matched_terms": [
                    "global",
                    "pitch",
                    "identity",
                    "local",
                    "intensity",
                    "dimensions",
                    "transition",
                    "stress",
                    "traits",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prosody control (Pitch, Rhythm, Stress).</span> Professional speech is shifted toward higher bins with tighter spread, indicating steadier control and fewer low-end tails.</p>\n\n",
                "matched_terms": [
                    "stress",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotion rendering (Accuracy, Intensity, Transition).</span> Professionals show higher centers and reduced mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.I1.i2.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> (fewer gated <span class=\"ltx_text ltx_font_typewriter\">N/A</span>s), suggesting more consistent emotional grounding and smoother dynamics; amateurs exhibit wider variance and more mass at the extremes.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "intensity",
                    "accuracy",
                    "transition",
                    "dynamics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identity and traits (Voice Identity, Traits Embodiment).</span> Professionals present clearer peaks in upper bins, while amateurs are broader or bimodal, reflecting inconsistent character anchoring.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "traits",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scene and semantics (Local Scene Fit, Global Coherence, Semantic Match).</span> Differences are present but smaller; both groups cluster in the mid&#8211;high range, with professionals exhibiting slightly narrower distributions.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, the consistently high agreement confirms that the rubric isolates perceptually coherent factors, so learned evaluators can target them reliably.\nSecond, the gated behavior of <em class=\"ltx_emph ltx_font_italic\">Intensity/Transition</em> motivates a <em class=\"ltx_emph ltx_font_italic\">hierarchical</em> evaluator: verify <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> before scoring graded expressivity, or model the joint structure with conditional heads.\nThird, <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> remains challenging; models may benefit from explicit character-trait priors, long-term context aggregation, or contrastive supervision between matched and mismatched traits.\nFinally, prosody-focused objectives (e.g., pitch/rhythm/stress control) are likely to yield the largest quality gains for amateur-style speech.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "evaluator",
                    "model",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For both zeroshot and fewshot experiments, we adopt a streamlined version of the annotation guidelines to ensure consistency with the human annotation process. The exact rubric prompts for archetype and realism evaluation used are provided below:</p>\n\n",
                "matched_terms": [
                    "realism",
                    "consistency",
                    "zeroshot",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and fewshot settings, we condition the model with the rubric prompt and then append few-shot demonstration triplets (input, audio, gold JSON) to mirror the human annotation protocol. As shown in Listing&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#LST2\" title=\"Listing 2 &#8227; L.1 Zeroshot/Fewshot Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, We use <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> for these few-shot exemplars and require a minified JSON output consistent with the rubric schema.</p>\n\n",
                "matched_terms": [
                    "model",
                    "zeroshot",
                    "gemini25pro",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and few-shot experiments on commercial models such as <span class=\"ltx_text ltx_font_typewriter\">Gemini</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o</span>, we report results as the average of five independent runs to reduce randomness and obtain more stable predictions. In contrast, for open-source models, we observed that their instruction-following ability is relatively weak. To mitigate this, we prompt them to predict one dimension at a time rather than all dimensions jointly. Specifically, for each metric, after showing the rubric prompt, we request completion in a fixed template and extract the probability distribution over the discrete tokens <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>. The final score is then computed from these probabilities, which we found makes open-source models usable to some extent under this setting.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "report",
                    "average",
                    "all",
                    "dimensions",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For few-shot experiments, we randomly sample three exemplars from the training set, with each exemplar chosen such that its human scores are close to the rounded average across all annotators. These exemplars are appended as demonstrations in the prompt. As with the zeroshot case, we run five trials for commercial models and report the averaged results for evaluation.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "report",
                    "average",
                    "all",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.T12\" title=\"Table 12 &#8227; L.2 Zeroshot/Fewshot Ablation Experiments &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> shows how different prompting strategies affect realism evaluation performance with <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>. Several observations can be drawn:</p>\n\n",
                "matched_terms": [
                    "realism",
                    "performance",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Effect of repeated runs.\nWhen comparing single-run decoding (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot-1run</span>) against averaging over five runs (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot</span>), we see consistent gains across nearly all dimensions (average score improves from 0.353 to 0.390). This suggests that sampling variance plays a substantial role in the reliability of large language model (LLM) evaluators, and averaging multiple outputs reduces this variance, producing more stable correlation with human annotations.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "zeroshot",
                    "average",
                    "model",
                    "all",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Impact of few-shot conditioning.\nIntroducing even three examples (<span class=\"ltx_text ltx_font_typewriter\">3-Shot</span>) yields a clear improvement over zeroshot (0.433 vs. 0.390). This indicates that providing explicit annotated demonstrations helps the model better align with the rubric. Interestingly, when using only a single run with few-shot demonstrations (<span class=\"ltx_text ltx_font_typewriter\">3-Shot-1run</span>), the performance is only marginally better than pure zeroshot, highlighting that both averaging and demonstrations are complementary: demonstrations improve grounding, while multiple runs mitigate stochasticity.</p>\n\n",
                "matched_terms": [
                    "model",
                    "zeroshot",
                    "performance",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Scaling the number of demonstrations.\nIncreasing the few-shot set size from three to six and nine further boosts performance (0.459 and 0.469, respectively). The improvements are most pronounced in prosodic and emotional categories (e.g., <em class=\"ltx_emph ltx_font_italic\">Pitch</em>, <em class=\"ltx_emph ltx_font_italic\">Accuracy</em>, <em class=\"ltx_emph ltx_font_italic\">Intensity</em>), suggesting that more examples provide stronger guidance in modeling subtle paralinguistic cues. Gains in <em class=\"ltx_emph ltx_font_italic\">Character Consistency</em> and <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance</em> are also noticeable, although smaller in magnitude, which may indicate that these higher-level judgments are less sensitive to prompt scaling once a few demonstrations are provided.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "relevance",
                    "contextual",
                    "consistency",
                    "emotional",
                    "character",
                    "intensity",
                    "prosodic",
                    "fewshot",
                    "accuracy",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In general, the results demonstrate two complementary strategies for stabilizing LLM-based evaluators: (i) averaging multiple runs to reduce randomness, and (ii) enriching prompts with more few-shot exemplars to improve adherence to rubric-based evaluation. Combining both leads to the strongest overall performance, with the nine-shot condition achieving the highest average correlation&#160;(0.469).</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "average",
                    "performance",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the archetype test set, all fine-tuned variants achieve strong <span class=\"ltx_text ltx_font_italic\">Content Pass</span> accuracy above 92%. Differences appear mainly in correlation-based metrics. Here, <span class=\"ltx_text ltx_font_typewriter\">Base-FT</span> yields the highest score on <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> delivers the strongest performance in <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> and the best overall average (0.629). This suggests that instruction-tuned checkpoints paired with parameter-efficient adaptation can better capture higher-level perceptual traits.</p>\n\n",
                "matched_terms": [
                    "average",
                    "all",
                    "traits",
                    "accuracy",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For realism evaluation on the basic test set, <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> consistently achieves the strongest correlations across most dimensions, especially for <span class=\"ltx_text ltx_font_italic\">Character Identity</span> and <span class=\"ltx_text ltx_font_italic\">Traits</span>, leading to the best average correlation (0.668). <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> also performs competitively (0.630), while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> lags slightly behind (0.625). These results highlight that full fine-tuning may still offer an advantage when the evaluation set is relatively homogeneous and closer to the training distribution.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "realism",
                    "identity",
                    "average",
                    "character",
                    "dimensions",
                    "traits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A different pattern emerges on the real recording test set. Both <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> and <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> struggle, yielding negative or near-zero correlations across most dimensions. In contrast, <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> shows clear improvements, achieving positive correlations in all prosodic and emotional dimensions and producing the highest overall average (0.247). This indicates that LoRA adaptation on top of an instruction-tuned backbone provides better generalization to out-of-distribution, real-world data.</p>\n\n",
                "matched_terms": [
                    "average",
                    "emotional",
                    "prosodic",
                    "dimensions",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the comprehensive design of Speech-DRAME, several limitations remain.\nFirst, while our dual evaluation paradigm combines top-down archetype assessment with bottom-up realism evaluation, both strategies are constrained by the quality and scope of available data. Archetype evaluation inevitably relies on simplified stereotypes, which can overlook subtler paralinguistic and contextual cues. Realism evaluation, though richer, still struggles with domain mismatch between synthetic data and real human recordings, as evidenced by the performance drop observed in the real-recording test set. This highlights the persistent gap between benchmark-driven evaluation and real-world variability in speech role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "contextual",
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Third, while DRAME-Eval substantially outperforms zero-shot and few-shot ALLMs, its reliance on fine-tuning from a strong base model raises questions of accessibility and scalability. Future work should explore lightweight adaptation techniques, semi-supervised training with partially annotated data, and leveraging emergent capabilities of open-source multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "model",
                    "drameeval",
                    "zeroshot",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, although we validated DRAME-Eval against human annotations, alignment under realism settings remains modest. This suggests that even human-annotated benchmarks are insufficiently fine-grained to capture the full complexity of human perception in speech role-play. Richer annotation protocols, more diverse speaker populations, and integration of perceptual psychology insights could further improve evaluation fidelity.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "drameeval",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Looking forward, we see several promising research directions. We intentionally restrict evaluation to single-turn responses for clarity and tractability. However, multi-turn extensions that capture narrative progression, long-term consistency, and dynamic emotion shifts remain a natural next step. Incorporating multilingual and cross-cultural datasets will enhance generality and fairness. On the modeling side, exploring hybrid evaluation strategies that combine discrete rubrics with continuous perceptual signals (e.g., prosody embeddings or affective measures) may better bridge the gap between automatic and human judgments. Finally, integrating Speech-DRAME into reinforcement learning pipelines as a reward model could help drive the next generation of expressive, human-aligned speech role-play systems.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "consistency",
                    "model"
                ]
            }
        ]
    },
    "S5.T4": {
        "caption": "Table 4: Realism role-play evaluator performance (real recording test set). We report Pearson correlation coefficients for all dimensions. The Avg. column is the average of all dimensions.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" rowspan=\"2\">Config.</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" rowspan=\"2\">Model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" colspan=\"3\">Prosodic Dynamics</td>\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" colspan=\"3\">Emotional Expressiveness</td>\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" colspan=\"2\">Character Consistency</td>\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" colspan=\"2\">Contextual Relevance</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\">Avg.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Pitch</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Rynthm</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Stress</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Accuracy</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Intensity</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Transition</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Identity</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Traits</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Local</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Global</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" rowspan=\"5\">Zeroshot</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.159</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.070</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.103</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.070</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.052</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.096</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.092</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.077</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.075</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.028</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.082</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.067</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.013</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.035</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.013</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.042</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.036</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.004</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.130</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.065</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.018</td>\n<td class=\"ltx_td ltx_align_center\">0.039</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.115</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.039</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.116</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.020</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.122</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.080</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.114</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.032</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.137</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.080</td>\n<td class=\"ltx_td ltx_align_center\">-0.063</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.078</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.071</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.043</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.074</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.047</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.045</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.082</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.076</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.102</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.098</td>\n<td class=\"ltx_td ltx_align_center\">0.055</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.055</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.057</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.088</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.072</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.077</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.057</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.006</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.040</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.050</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.111</td>\n<td class=\"ltx_td ltx_align_center\">0.052</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" rowspan=\"2\">FewShot</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.319</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.179</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.263</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.111</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.078</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.184</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">-0.001</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.118</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.216</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.074</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.166</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.299</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.193</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.222</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.121</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.377</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.344</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.013</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.248</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.301</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.168</td>\n<td class=\"ltx_td ltx_align_center\">0.226</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\">Finetune</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">0.288</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.270</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.362</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.277</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">0.331</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.397</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">-0.034</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">0.077</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">0.217</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.279</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.247</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "roleplay",
            "column",
            "pitch",
            "identity",
            "consistency",
            "pearson",
            "prosodic",
            "fewshot",
            "dynamics",
            "config",
            "qwen2audio",
            "realism",
            "coefficients",
            "zeroshot",
            "evaluator",
            "average",
            "emotional",
            "recording",
            "character",
            "gemini25pro",
            "rynthm",
            "stress",
            "test",
            "accuracy",
            "performance",
            "real",
            "gpt4oaudio",
            "avg",
            "finetune",
            "relevance",
            "report",
            "intensity",
            "transition",
            "traits",
            "kimiaudio",
            "qwen25omni",
            "set",
            "global",
            "correlation",
            "local",
            "contextual",
            "model",
            "all",
            "dimensions",
            "expressiveness",
            "drameeval"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real Recording Test Set.</span>\nThe real-recording evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T4\" title=\"Table 4 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> is notably more challenging, with all models suffering substantial performance drops. Proprietary models in the zero-shot setting achieve only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.04&#8211;0.08 average correlation, and public models often fall below zero. Few-shot prompting yields small but noticeable improvements, especially for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> (0.226 avg.), suggesting that exemplars partially mitigate domain mismatch between synthetic and real human speech. Interestingly, while finetuning achieves the highest overall average (0.247), its relative advantage over few-shot prompting is narrower than in the synthetic setting, and some dimensions (e.g., <span class=\"ltx_text ltx_font_italic\">Character Identity</span>) remain weak. We emphasize that this observed gap between synthetic and real recordings is not merely an artifact but an intended feature of Speech-DRAME, designed to expose the mismatch and drive future research into bridging synthetic-to-real generalization.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Role-play has become a key testbed for generative models, expanding from text-only dialogue to multimodal interaction. Extending role-play to speech captures prosody, emotion, and delivery, but also poses new evaluation challenges. Current pipelines often use audio large language models (ALLMs) as zero-shot judges, which miss paralinguistic cues, collapse multiple aspects into coarse scores, and rely on synthetic speech references that fail to reflect real-world roles. We present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework that contributes at three levels: (i) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-EvalBench</em>, an evaluation benchmark with bilingual human-annotated data and protocols for training and testing speech evaluation models&#160;(SEMs), (ii) <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, a fine-tuned evaluation model, which substantially outperforms zero-shot and few-shot ALLMs, and (iii) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-RoleBench</em>, a speech role-play benchmark that leverages <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> as an automatic judge to compare speech foundation models (SFMs). Speech-DRAME distinguishes between two complementary evaluation strategies: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>, a top-down approach measuring adherence to broad role archetypes, and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em>, a bottom-up approach grounded in real human speech that emphasizes nuanced role quality. Compared to zero-shot ALLM judges, <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves stronger agreement with human ratings (Pearson correlation from 0.480 to 0.629 in archetypes, and 0.390 to 0.625 in realism). By integrating transparent benchmark resources, modeling approaches, and system-level evaluation, Speech-DRAME provides the first comprehensive, reproducible foundation for assessing spoken role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "correlation",
                    "realism",
                    "zeroshot",
                    "model",
                    "pearson",
                    "fewshot",
                    "drameeval",
                    "real"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Role-play has been recently recognized a cornerstone of interactive systems, from initial text-based focuses to the growing ecosystem of multimodal dialogue agents&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib6\" title=\"\">2024a</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib59\" title=\"\">2024</a>; Dai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib13\" title=\"\">2025</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib43\" title=\"\">2024</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>)</cite>. With the rapid advancement of large language models (LLMs), role-play has evolved beyond scripted interactions into dynamic character-driven conversations, supporting applications in education, entertainment, and human&#8211;AI collaboration&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib69\" title=\"\">2024</a>; Agatsuma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib1\" title=\"\">2024</a>; Fung &amp; Laing, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib20\" title=\"\">2024</a>)</cite>. Extending role-play to speech-based interaction offers an even richer communication channel, capturing prosody, emotion, and character consistency in ways that text alone cannot&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "consistency",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Evaluating speech role-play, however, remains a major challenge. Current pipelines often rely on ALLMs as judges, typically in a zero-shot setting&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>; Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>)</cite>. While this strategy can provide rough ratings, it falls short for speech role-play:</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">it lacks sensitivity to paralinguistic cues such as intonation, pacing, and emotional dynamics that define role delivery&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>; Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib81\" title=\"\">2025a</a>)</cite>,</p>\n\n",
                "matched_terms": [
                    "dynamics",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">it collapses diverse aspects of performance into limited coarse dimensions, obscuring critical dimensions of role-play quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite>, and</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dimensions",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">it depends on synthetic speech references rather than real human speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>)</cite>, limiting its ability to reflect real-world diversity in speech role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "real"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome these limitations, we present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span> (Speech Detailed Role-play Assessment with Modeling and Evaluation), a unified framework that moves beyond synthetic or judge-only pipelines. Speech-DRAME is distinguished by three aspects: [i] a dual evaluation paradigm that balances archetype-driven generality with human-grounded realism (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS2\" title=\"3.2 Role-play Evaluation with Two Strategies &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), [ii] human annotations that provide perceptual grounding absent in prior resources (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4\" title=\"4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), and [iii] evaluation-model alignment that enables not only benchmarking of speech foundation models (SFMs) but also systematic analysis of the evaluation models (SEMs) themselves (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5\" title=\"5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). These aspects establish Speech-DRAME as a comprehensive and trustworthy framework for measuring and improving speech role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, Speech-DRAME contributes at three levels: (i) <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>, an evaluation benchmark consisting of human-annotated datasets for archetype and realism role-play; (ii) <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span>, a fine-tuned SEM trained on EvalBench that surpasses zero-shot and few-shot ALLMs; and (iii) <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span>, a speech role-play benchmark that leverages DRAME-Eval as an automatic judge to systematically compare SFMs. By combining dual evaluation strategies, human-anchored supervision, and evaluation-model alignment, Speech-DRAME delivers systematic insights into both role-play systems and evaluation models themselves.\nOur contributions are four-fold:</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "zeroshot",
                    "fewshot",
                    "drameeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a <span class=\"ltx_text ltx_font_bold\">formal task definition</span> of speech role-play and its evaluation via speech foundation models (SFMs) for generation and speech evaluation models (SEMs) for assessment, unifying linguistic and paralinguistic dimensions.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span>, a fine-tuned SEM trained on EvalBench that substantially outperforms zero-shot and few-shot ALLMs, demonstrating the importance of human supervision for evaluation models.</p>\n\n",
                "matched_terms": [
                    "drameeval",
                    "zeroshot",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We establish <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span>, a speech role-play benchmark that leverages DRAME-Eval as an automatic judge to systematically compare both end-to-end and cascaded SFMs across multiple role quality dimensions.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Relevant codes, pre-trained models, and datasets will be open-sourced at \n<br class=\"ltx_break\"/><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/Anuttacon/speech_drame\" title=\"\">https://github.com/Anuttacon/speech_drame</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "drameeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-based role-play evaluation.</span>\nEarly research focused on text-only dialogue benchmarks. Datasets such as RoleLLM, CharacterBox, and RoleMRC <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib62\" title=\"\">2024</a>; Tu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib60\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib61\" title=\"\">2025a</a>; Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib53\" title=\"\">2023</a>; Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib41\" title=\"\">2025</a>)</cite> test whether models can sustain persona consistency and character fidelity, using LLM-as-judge pipelines for scalability. However, studies like PersonaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib81\" title=\"\">2025a</a>)</cite> have shown that LLM-as-judge approaches are far from perfect, struggling with tasks as basic as tracking speaker identity. This highlights broader concerns about reliability even in text settings, which become only more severe when extended to speech, where prosody, emotion, and vocal style play a central role.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "identity",
                    "consistency",
                    "character",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-native benchmarks.</span>\nMore recently, a number of resources have been introduced to evaluate spoken role-play agents. OmniCharacter-10K <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>)</cite> emphasizes character voice traits, while others&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib39\" title=\"\">2025</a>; Hou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib27\" title=\"\">2025</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib15\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib7\" title=\"\">2024b</a>)</cite> extend evaluation toward vocal communication, assistant capability, and multi-turn realism. Omni models such as StepAudio and KimiAudio <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite> also include role-play scenarios in their evaluation. SpeechRole and VStyle <cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>; Zhan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib76\" title=\"\">2025</a>)</cite> are closer to our work, offering datasets that test interaction ability, expressiveness, and fidelity. However, these resources are primarily built from LLM-generated dialogues and TTS outputs, with limited human validation and a heavy reliance on proprietary ALLM-as-judge pipelines. As a result, they lack grounding in diverse human speech and typically apply only coarse rubrics.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "character",
                    "expressiveness",
                    "test",
                    "traits",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Automatic judging with ALLMs.</span>\nIn parallel, advances in automatic judging highlight both opportunities and limitations. Audio-capable LLMs such as AudioJudge and Audio-Aware LLMs as Judges <cite class=\"ltx_cite ltx_citemacro_citep\">(Manakul et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib44\" title=\"\">2025</a>; Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>)</cite> can assess style, emotion, and delivery, while systems like Auto-ATT <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib65\" title=\"\">2025c</a>)</cite> scale this to Audio Turing Tests. Yet studies consistently reveal high prompt sensitivity and shallow capture of paralinguistic cues, suggesting that zero-shot judging remains insufficient for nuanced role-play evaluation.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Expressive TTS evaluation.</span>\nRecent work has explored evaluation of instruction-following and style-controlled TTS. InstructTTSEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite> explicitly includes role-play scenarios, while EmergentTTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">(Manku et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib45\" title=\"\">2025</a>)</cite> and discrete speech LM benchmarks <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang &amp; Sz&#233;kely, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib63\" title=\"\">2024</a>)</cite> emphasize prosody, spontaneity, and speaker consistency. These efforts push beyond intelligibility and naturalness, but still center on synthetic outputs rather than real human role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "consistency",
                    "real"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our distinction.</span>\nIn contrast, Speech-DRAME unifies these considerations through three elements: (i) a dual evaluation paradigm that combines archetype-driven generality with realism grounded in authentic human speech, (ii) human annotations that provide a reliable perceptual gold standard, and (iii) alignment of SEMs to human ratings, enabling systematic evaluation of both SFMs and the evaluation models themselves. This combination moves beyond synthetic benchmarks or zero-shot judging to provide a comprehensive and trustworthy framework for speech role-play assessment.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech role-play.</span>\nWe formalize <em class=\"ltx_emph ltx_font_italic\">speech role-play</em> as a conditional speech generation problem grounded in a character profile and scene description.\nGiven a dialogue context <math alttext=\"{\\bm{C}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{speech}}</annotation></semantics></math>, a character profile <math alttext=\"{\\bm{C}}_{\\text{profile}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>profile</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{profile}}</annotation></semantics></math>, and a scene specification <math alttext=\"{\\bm{C}}_{\\text{scene}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>scene</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{scene}}</annotation></semantics></math>, a <em class=\"ltx_emph ltx_font_italic\">speech foundation model</em> (SFM) generates a role-played speech response <math alttext=\"{\\bm{S}}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119930;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{S}}_{r}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "model",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech role-play evaluation can be framed in multiple ways depending on the available data and the desired granularity of analysis.\nWithin Speech-DRAME, we formalize two distinct strategies inspired by sociology theories<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Please refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A2\" title=\"Appendix B Theoretical Motivation for Dual Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> for detailed discussion of the dual evaluation design.</span></span></span>, including a top-down, archetype-oriented option using synthetic speech and a bottom-up, realism-oriented option from real human speech.\nRather than merging these into a single rubric, we treat them as separate perspectives, each with unique advantages and limitations.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarizes the key contrasts, while further implementation details are provided in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>-&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "real"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>). The first strategy follows a <em class=\"ltx_emph ltx_font_italic\">top-down</em> design, inspired by prior text-based role-play benchmarks.\nHere, role-play is judged with respect to <em class=\"ltx_emph ltx_font_italic\">stereotypical archetypes</em> (e.g., &#8220;firefighter&#8221; or &#8220;ER doctor&#8221;), using scene contexts (e.g., &#8220;comfort a child trapped in fire&#8221; or &#8220;ask for help in a serious operation&#8221;).\nThis approach provides accessible, general-purpose scoring that is scalable to a wide range of scenarios, making it suitable for large-scale benchmarking.\nHowever, its reliance on broad stereotypes and global impressions limits its ability to capture fine-grained aspects of delivery (e.g., subtle prosodic shifts, nuanced emotional control)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Holt &amp; Lotto, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib26\" title=\"\">2010</a>)</cite>.\nIn practice, this strategy mirrors simplified annotation settings and is particularly useful when annotation resources are limited&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib21\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "roleplay",
                    "emotional",
                    "prosodic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS5\" title=\"F.5 Examples of Realism Role-play &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.5</span></a>). The second strategy adopts a <em class=\"ltx_emph ltx_font_italic\">bottom-up</em> design, motivated by real-world performance and real human speech.\nInstead of relying solely on stereotypes, this approach grounds evaluation in recordings from professional and non-professional speakers, drawn from realistic dialogue and media sources.\nJudgments in this setting are more fine-grained and multi-dimensional, addressing aspects such as prosodic dynamics, emotional fidelity, character consistency, and contextual fit.\nWhile this strategy requires more human annotation and careful curation, it provides benchmarks that reflect how role-play is perceived in practice.\nIt thus offers a richer framework for analyzing model outputs in settings where realism and subtlety are essential.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "contextual",
                    "consistency",
                    "emotional",
                    "model",
                    "character",
                    "prosodic",
                    "real",
                    "dynamics",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human Annotation.</span>\nAll collected audio undergoes a standardized annotation protocol: (1) an initial semantically-based screening to exclude invalid clips; (2) rubric-based scoring across multiple perceptual dimensions; and (3) meta-annotations such as confidence or prompt-quality judgments. Full rubrics are provided in the appendices&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A5\" title=\"Appendix E Annotation Guidelines for Archetype Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">E</span></a> &amp; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>All data collection follows in high standard ethical consideration as outlined in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A9\" title=\"Appendix I Annotation Ethics &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "all",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Archetype Data Collection &#8227; 4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents a summary of the curated <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> dataset, where we further elaborate the details of archetype and realism evaluation data collection in following subsections, respectively. For benchmark purpose, we provide an official train-test split for the human annotated dataset for <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> so that following researchers can use the dataset to train and evaluate their SEMs for speech role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Role and Scene Dimensions.</span>\nRoles are operationalized through seven categories: (1) functional occupations in daily life, (2) functional occupations in fictional or fantasy settings, (3) social identities, (4) named or specific characters, (5) personality traits, (6) relative relationship, (7) basic events. Scenes are anchored by everyday events or state changes that evoke stereotypical reactions. Details of template design and alternative prompting strategies are given in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>We note that some archetypes (e.g., firefighter) inevitably reflect cultural stereotypes. These are used solely for evaluation scalability and do not reflect normative judgments.</span></span></span></p>\n\n",
                "matched_terms": [
                    "traits",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype Evaluation Data Summary.</span>\nThe resulting archetype evaluation dataset comprises 552 distinct scenarios for both Mandarin and English, evaluated across eight models,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>The model list is discussed in Appdenix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS4\" title=\"D.4 Candidate Models for Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.4</span></a>. Note we only include seven models for English due to the deactivation of ChatGPT-4O advanced voice model.</span></span></span> yielding 8,280 samples. In total, the dataset are split into 6,780 for training and 1,500 for testing. For benchmarking, an additional 1,250 scenarios spanning seven role/scene categories are prepared to facilitate archetype role-play benchmarking. More detailed statistics and analyses are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10\" title=\"Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">J</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realism evaluation adopts a bottom&#8211;up perspective, emphasizing authentic delivery, prosody, and contextual fit.</p>\n\n",
                "matched_terms": [
                    "contextual",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Sources and Negative Samples.</span>\nWe curate speech from (i) ground-truth media segments, (ii) retrieved character profiles, and (iii) generated local scene descriptions. To stress-test evaluation models, we construct contrastive mismatches: (i) negative local scenes with varying degrees of mismatch, and (ii) negative character profiles created by re-synthesizing transcripts with TTS systems. Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6\" title=\"Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F</span></a> details the prompt design and TTS generation details.</p>\n\n",
                "matched_terms": [
                    "local",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotators judge whether performances align with the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em>, using rubrics for <em class=\"ltx_emph ltx_font_italic\">Prosodic Dynamics (i.e., Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis)</em>, <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness (with progressive gating over three sub-dimensions: Emotion Accuracy, Emotion Intensity, Emotion Transition)</em>, <em class=\"ltx_emph ltx_font_italic\">Character Consistency (i.e., Voice Identity Matching and Traits Embodiment)</em>, <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance (i.e., Local Scene Fit and Global Story Coherence)</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>. A prompt quality check and confidence ratings are also collected. Full scales and gating details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "pitch",
                    "identity",
                    "relevance",
                    "local",
                    "consistency",
                    "emotional",
                    "contextual",
                    "character",
                    "accuracy",
                    "prosodic",
                    "intensity",
                    "transition",
                    "stress",
                    "dimensions",
                    "expressiveness",
                    "traits",
                    "dynamics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real-world Recoding.</span>\nAs noted in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S2\" title=\"2 Related Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, most prior efforts have relied heavily on synthetic data, mostly from TTS systems based on curated professional role-play voices. While such data can achieve high fidelity, it often lacks the diversity and spontaneity of real-world delivery, limiting the generalizability of evaluation models. To address this gap, we expand realism evaluation beyond synthetic and curated voices by incorporating newly recorded speech from both professional and amateur voice actors. This design introduces a broader spectrum of delivery styles, from polished professional acting to more natural and less controlled amateur performance.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation Data Summary.</span>\nThe resulting realism evaluation dataset is constructed from a combination of curated media sources, including internal collections, public datasets (e.g., NCSSD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib40\" title=\"\">2024b</a>)</cite>), and original recordings collected for this project.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>All data were obtained under appropriate licenses and are legally available for research release.</span></span></span> We compile 15k annotated role-play samples: 9k in Mandarin and 5k in English. The dataset is divided into 12k samples for training, 2k mixed samples for the base test set, and 1k real-recording samples for the test set. The first test set supports general evaluation using our proposed pipeline with negative samples, while the second focuses exclusively on human speech performed by both professional and amateur voice actors. For benchmarking, 2,000 scenarios from the realism dataset are designated for realism role-play benchmarking. Detailed statistics and analyses are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11\" title=\"Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">K</span></a>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "roleplay",
                    "realism",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the two proposed datasets, we conduct experiments under three settings: zero-shot, few-shot, and finetuning, using pre-trained ALLMs. We formulate the task as a multi-metric estimation problem&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tjandra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib58\" title=\"\">2025</a>; Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib74\" title=\"\">2025</a>; Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib55\" title=\"\">2025b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib54\" title=\"\">a</a>)</cite>, where a single model is trained or prompted to predict multiple evaluation dimensions.</p>\n\n",
                "matched_terms": [
                    "model",
                    "dimensions",
                    "zeroshot",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-shot and Few-shot ALLMs.</span>\nWe first evaluate the zero-shot capabilities of both proprietary and public models, including <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>, <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>, <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio</span>, and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>. All evaluations follow a unified prompting template that provides audio input alongside scene and character context.</p>\n\n",
                "matched_terms": [
                    "qwen2audio",
                    "gpt4oaudio",
                    "zeroshot",
                    "character",
                    "gemini25pro",
                    "all",
                    "fewshot",
                    "kimiaudio",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For proprietary models, we report the average of five independent runs to reduce variance and obtain more stable predictions. For open-source models, we observed weaker instruction-following ability and limited adaptation to unfamiliar tasks. To mitigate this, we did not request simultaneous prediction of multiple metrics. Instead, we queried one metric at a time. Following <cite class=\"ltx_cite ltx_citemacro_citet\">Song &amp; Bahri (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>); Lukasik et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>, we interpret model outputs through token distribution over discrete labels (<span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score.</p>\n\n",
                "matched_terms": [
                    "report",
                    "average",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For few-shot experiments, we focus primarily on proprietary ALLMs, which demonstrate stronger instruction-following capabilities and have been widely used as judges in prior work. For each test query, we provide three additional annotated samples from the training set as in-context exemplars.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>Please find our detailed settings such as prompts, decoding algorithms in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS1\" title=\"L.1 Zeroshot/Fewshot Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.1</span></a> with additional ablation analysis in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS2\" title=\"L.2 Zeroshot/Fewshot Ablation Experiments &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.2</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "set",
                    "test",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Finetuning Model: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>.</span>\nIn addition to zero-shot and few-shot evaluations, we fine-tune a dedicated evaluation model, denoted as <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> for archetype and realism role-play evaluation, respectively.\nWe initialize from <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio-7B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib11\" title=\"\">2024</a>)</cite>, which provides a strong foundation for multi-modal audio-text reasoning.\nTo better leverage the multiple human annotations available in our datasets, we adopt a sampling-based training strategy: at each step, one annotator label is randomly selected, following practices in label distribution learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Geng, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib22\" title=\"\">2016</a>; Wen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib67\" title=\"\">2023</a>)</cite>.\nThis stochastic supervision implicitly exposes the model to the empirical annotation distribution, while during inference we output the full distribution over discrete labels (e.g., <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score, consistent with recent decoding-based regression methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Song &amp; Bahri, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>; Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib70\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib64\" title=\"\">2025b</a>; Lukasik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>. We apply LoRA fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib28\" title=\"\">2022</a>)</cite>. All implementation details, including optimizer settings, training schedule, and sampling policy, are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS3\" title=\"L.3 Fine-tuning Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "zeroshot",
                    "finetune",
                    "model",
                    "all",
                    "fewshot",
                    "drameeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics for Role-play Evaluation Model.</span> For most dimensions with 1-5 scale, we report Pearson correlation. For <span class=\"ltx_text ltx_font_italic\">Content Pass</span> estimation, we report accuracy.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "correlation",
                    "report",
                    "model",
                    "pearson",
                    "dimensions",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype Evaluation.</span>\nThe results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T2\" title=\"Table 2 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reveal clear distinctions across evaluation settings. In the zero-shot condition, proprietary ALLMs such as <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> substantially outperform public models, achieving strong <span class=\"ltx_text ltx_font_italic\">content pass</span> accuracy (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>90%) and moderate correlations with other dimensions(<math alttext=\"0.43\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mn>0.43</mn><annotation encoding=\"application/x-tex\">0.43</annotation></semantics></math>&#8211;<math alttext=\"0.48\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mn>0.48</mn><annotation encoding=\"application/x-tex\">0.48</annotation></semantics></math>). By contrast, public models exhibit limited predictive ability, with correlations often below 0.2. Few-shot prompting provides modest gains for proprietary models, improving both accuracy and correlation (e.g., <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> rises from 0.480 to 0.493 average correlation). The most significant improvement arises from finetuning: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves an average correlation of 0.629, marking a large margin over zero-shot/few-shot models and demonstrating the effectiveness of task-specific supervision.</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "correlation",
                    "zeroshot",
                    "average",
                    "gemini25pro",
                    "fewshot",
                    "accuracy",
                    "drameeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T3\" title=\"Table 3 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> extends the analysis to fine-grained realism evaluation. Similar to archetype results, proprietary ALLMs lead the zero-shot setting, with <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> obtaining an average correlation of 0.390, compared to 0.284 for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>. Public models remain weak baselines. Few-shot prompting consistently improves performance: <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> gains +0.042 and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> gains +0.083 over their zero-shot averages, showing the benefit of in-context supervision. Once again, finetuning is most effective: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves 0.625 average correlation, higher than the best few-shot results. Importantly, the gains are consistent across all 10 realism dimensions, confirming that the sampling&#8211;expectation training strategy generalizes beyond simple classification to nuanced paralinguistic cues.</p>\n\n",
                "matched_terms": [
                    "gpt4oaudio",
                    "realism",
                    "zeroshot",
                    "correlation",
                    "average",
                    "gemini25pro",
                    "all",
                    "dimensions",
                    "fewshot",
                    "drameeval",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall.</span> Across both archetype and realism evaluations, we observe three consistent trends: (i) zero-shot performance is limited, particularly for open-source models, (ii) few-shot prompting offers moderate but consistent gains, especially for proprietary ALLMs, and (iii) finetuning with task-specific supervision (<span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>) delivers the largest improvements, achieving state-of-the-art correlations across nearly all dimensions. Nevertheless, results on the real-recording test set underscore the gap between synthetic and real human role-play scenarios, motivating future work in data diversification and domain adaptation.</p>\n\n",
                "matched_terms": [
                    "set",
                    "roleplay",
                    "realism",
                    "zeroshot",
                    "fewshot",
                    "real",
                    "all",
                    "dimensions",
                    "test",
                    "drameeval",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on fine-tuned <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, we establish the DRAME-RoleBench to evaluate SFMs&#8217; role play ability. For archetype evaluation, 1,250 scenarios are constructed following the same data construction pipeline as the <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>. For realism evaluation, we reuse 2,000 scenarios in the <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> test set. We follow the same prompt strategy as outlined in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "drameeval",
                    "realism",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, we consider a comprehensive set of candidate models, including six end-to-end models: <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib47\" title=\"\">2025b</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib75\" title=\"\">2024</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib71\" title=\"\">2025a</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Qwen3Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib72\" title=\"\">2025b</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">StepAudio2Mini</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib68\" title=\"\">2025</a>)</cite>; and ten cascaded models by combining two widely-used textual LLMs (i.e., <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib12\" title=\"\">2025</a>)</cite> and <span class=\"ltx_text ltx_font_typewriter\">Qwen3-30B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib73\" title=\"\">2025</a>)</cite>) and five recent TTS models (i.e., <span class=\"ltx_text ltx_font_typewriter\">F5TTS</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib8\" title=\"\">2024c</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">IndexTTS2</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib82\" title=\"\">2025b</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">MaskGCT</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib66\" title=\"\">2025d</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Vevo1.5</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib79\" title=\"\">2025b</a>)</cite>, and <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib17\" title=\"\">2024b</a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "set",
                    "kimiaudio",
                    "qwen25omni",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype evaluation.</span>\nOn archetype role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T5\" title=\"Table 5 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), cascaded pipelines consistently outperform end-to-end systems. The strongest cascaded systems, particularly <span class=\"ltx_text ltx_font_typewriter\">G-IndexTTS2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-IndexTTS2</span>, achieve both high content pass rates (above 94%) and balanced scores across audio quality, human likeness, and appropriateness. By contrast, end-to-end models such as <span class=\"ltx_text ltx_font_typewriter\">StepAudio2Mini</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> perform competitively in terms of content coverage (above 88%), but their expressive scores remain slightly lower than their cascaded counterparts. This gap suggests that separating reasoning (LLM) from speech rendering (TTS) remains advantageous when archetype fidelity is emphasized. Nevertheless, the fact that top end-to-end systems already surpass 3.0 in average rating indicates rapid closing of the performance gap.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "average",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism evaluation.</span>\nRealism role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T6\" title=\"Table 6 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) presents a different landscape. Here, performance differences between end-to-end and cascaded systems are less pronounced. Several end-to-end models, such as <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>, reach average scores of 3.24, matching or even exceeding the best cascaded pipelines. Importantly, realism scores are more evenly distributed across prosodic, emotional, character, and contextual dimensions, with most models clustering between 2.9&#8211;3.2. This convergence reflects the difficulty of realism-style evaluation: since the data preparation emphasized contrast with authentic human speech, models are judged under similar conditions, leading to smaller observed gaps.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "contextual",
                    "average",
                    "emotional",
                    "qwen25omni",
                    "character",
                    "prosodic",
                    "dimensions",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human alignment.</span>\nWe further conducted small-scale human studies with 10% of the benchmark data to verify the reliability of the benchmark. Archetype evaluation achieves a Spearman correlation of 0.706 between human judgments and <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, validating that the automatic evaluator closely tracks human preferences in this setting. Realism evaluation, however, achieves a more modest Spearman correlation of 0.375. This lower alignment is expected, since realism data construction did not prioritize fine-grained model separation but instead emphasized distinguishing human versus synthetic speech.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>We view this as evidence that realism presents a harder, less-saturated frontier. Rather than a limitation of DRAME-Eval, this highlights the challenge of designing evaluators that capture nuanced human perception, which motivates future work.</span></span></span> As a result, all models occupy a relatively narrow performance band, reducing sensitivity to model-level differences. We view this as evidence that realism evaluation highlights a harder, less saturated frontier where both SFMs and SEMs require further advances.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "realism",
                    "evaluator",
                    "model",
                    "all",
                    "drameeval",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, cascaded pipelines retain an edge in archetype scenarios, especially in content robustness and expressive fidelity, but end-to-end models are catching up quickly. In realism scenarios, the field remains unsolved: existing systems exhibit similar performance bands and evaluators show weaker alignment with human judgments, underscoring the need for improved realism-grounded data and evaluation. Together, these findings establish <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> as both a reliable testbed for archetype fidelity and a challenging benchmark for realism fidelity.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework for assessing spoken role-play through two complementary lenses: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em> (top-down) and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> (bottom-up, human-grounded). Our human-annotated <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> enables training and testing of SEMs, and our fine-tuned <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span> substantially outperforms zero-/few-shot ALLM judges. Building on this, <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> provides system-level comparison across various SFMs, revealing a consistent edge for cascaded pipelines on archetypes and a tighter spread under realism, where real human performance evaluation remain challenging. Together, these resources establish a reproducible, analyzable foundation for spoken role-play evaluation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>Limitations and future works are discussed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A13\" title=\"Appendix M Limitations and Future Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">M</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "performance",
                    "drameeval",
                    "real"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All data collection and annotation in this work followed strict ethical standards. Our bilingual datasets were constructed exclusively from legally accessible sources, including licensed media, open-source corpora, and original recordings conducted with informed consent. Annotators were recruited with appropriate compensation and provided with clear instructions, including the right to withdraw at any point. To minimize risks, no personally identifiable information was included in the released data, and all recordings were anonymized before annotation. We ensured cultural and linguistic diversity in annotator pools to mitigate potential bias across Mandarin and English role-play scenarios. While this study did not require formal IRB approval, we adhered to best practices in human-subject research, including transparency in task design and careful consideration of annotator well-being. Full details of our annotation protocol and ethical safeguards are presented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A9\" title=\"Appendix I Annotation Ethics &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments were conducted on widely available GPUs (NVIDIA H100). Random seeds are fixed across all runs to ensure consistency. By releasing datasets, models, and code, we aim to make all major results in this paper fully reproducible and facilitate future extensions of our framework.</p>\n\n",
                "matched_terms": [
                    "all",
                    "consistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data generation:</span> LLMs were employed to expand prompts (e.g., character profiles, scene descriptions) and to produce negative samples in realism evaluation. All outputs were screened by human annotators to ensure validity and reduce bias.</p>\n\n",
                "matched_terms": [
                    "all",
                    "realism",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline evaluators:</span> State-of-the-art audio LLMs (e.g., <span class=\"ltx_text ltx_font_typewriter\">GPT4o</span>, <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>) were included as zero-shot judges in our evaluation benchmark, enabling direct comparison with human-aligned SEMs.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "qwen25omni",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our distinction between <span class=\"ltx_text ltx_font_italic\">Archetype Evaluation</span> and <span class=\"ltx_text ltx_font_italic\">Realism Evaluation</span> is not merely pragmatic, but is grounded in established theories of social identity, communication, and performance. The two perspectives capture complementary dimensions of how humans perceive and evaluate roles in interaction.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "dimensions",
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetypes as Collective Schemas</span>\nThe notion of archetypes originates from Jungian psychology, where they represent recurrent, culturally embedded character prototypes in the collective unconscious&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jung, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib36\" title=\"\">2014</a>)</cite>. In media and performance studies, archetypes have been shown to guide audience expectations, providing top-down interpretive frames for evaluating whether a character &#8220;fits&#8221; within familiar categories such as hero, mentor, or trickster&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Campbell, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib4\" title=\"\">2008</a>)</cite>. Archetype-based reasoning allows for scalable, general-purpose judgments because it leverages culturally shared schemas rather than idiosyncratic details.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism as Situated Performance</span>\nIn contrast, realism highlights the fine-grained enactment of roles in situated interaction. Goffman&#8217;s dramaturgical sociology emphasizes that everyday social life is itself a performance, where believability depends on subtle paralinguistic and pragmatic cues such as prosody, hesitation, and emotional shading&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goffman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib23\" title=\"\">1949</a>)</cite>. Similarly, interactional sociolinguistics has demonstrated how micro-level features like intonation, pacing, and turn-taking patterns shape the perception of authenticity in communication&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gumperz, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib25\" title=\"\">1982</a>)</cite>. Realism Evaluation thus anchors role-play assessment in bottom-up sensitivity to authentic delivery, beyond schematic expectations.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "emotional",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Complementarity of the Two Perspectives</span>\nPerformance theory provides a unifying lens for these dual dimensions. Schechner&#8217;s view of performance as both &#8220;restored behavior&#8221; (scripted, repeatable structures) and &#8220;living process&#8221; (situated enactment)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Schechner, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib52\" title=\"\">2017</a>)</cite> underscores the need to consider both archetypal scripts and enacted realism. Likewise, sociolinguistic theories of identity recognize both <span class=\"ltx_text ltx_font_italic\">ascribed categories</span> (archetypes imposed by social convention) and <span class=\"ltx_text ltx_font_italic\">performed identities</span> (emergent in interaction)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bucholtz &amp; Hall, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib2\" title=\"\">2005</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "dimensions",
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p4.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> may be a simple mean, a recency-weighted average, or a learned temporal model that captures consistency across turns.\nThe choice of <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p4.m3\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> reflects the intended evaluation focus, e.g., stability of persona traits, adaptation to scene progression, or coherence across multiple speakers.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "average",
                    "consistency",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Discussion.</span>\nWhile our released dataset and experiments concentrate on single-turn evaluation, the modular nature of the formulation ensures that extending to multi-turn is straightforward.\nFuture work can leverage <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p5.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> to study richer aspects of speech role-play, such as persona persistence, interaction dynamics, and role consistency across extended dialogues.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dynamics",
                    "consistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Role dimensions.</span>\nWe operationalize archetypes through five principal categories, each representing a stereotypical dimension of role identity or behavior, including (1) <span class=\"ltx_text ltx_font_bold\">Occupation / Functional Role (Modern Daily Life)</span> (e.g., a firefighter rescuing a trapped civilian). (2) <span class=\"ltx_text ltx_font_bold\">Occupation / Functional Role (Fictional or Fantasy)</span> (e.g., a martial arts cult leader confronting orthodox rivals). (3)&#160;<span class=\"ltx_text ltx_font_bold\">Social Identity</span> (e.g., a doting parent worrying about a child, or a &#8220;Buddhist youth&#8221; eschewing competition). (4) <span class=\"ltx_text ltx_font_bold\">Named / Specific Characters</span> (e.g., Sun Wukong mocking celestial authorities in <em class=\"ltx_emph ltx_font_italic\">Journey to the West</em>). (5) <span class=\"ltx_text ltx_font_bold\">Personality Traits</span> (e.g., &#8220;hot-blooded&#8221; in a competitive setting, or &#8220;complaining&#8221; in response to poor service). (6) <span class=\"ltx_text ltx_font_bold\">Relative Roles</span> (e.g., parent or stranger).</p>\n\n",
                "matched_terms": [
                    "identity",
                    "dimensions",
                    "traits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scene dimensions.</span>\nIn addition to roles, we incorporate contextual triggers that anchor role-play in stereotypical situations.\nA central focus is on <span class=\"ltx_text ltx_font_bold\">everyday external events or state changes</span> (e.g., receiving a promotion, being caught in gossip), which provide concrete scenarios for testing archetypical responses.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "contextual",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Exclusion of fine-grained controls.</span>\nWe deliberately excluded explicit emotional states, prosodic instructions, or linguistic constraints.\nThese dimensions are better suited to realism-oriented or style-controlled evaluation, whereas archetype evaluation benefits from concrete, stereotype-level scenarios that sharpen judgments and improve human agreement.</p>\n\n",
                "matched_terms": [
                    "prosodic",
                    "emotional",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Pipeline overview.</span>\nAt a high level, archetype evaluation data are generated through a three-stage pipeline:\n(1) selecting keywords that specify role and scene;\n(2) expanding them into full prompts via structured templates; and\n(3) rendering prompts into speech with contemporary TTS systems.\nThis modular pipeline ensures both scalability and reproducibility, while keeping prompts vivid enough for reliable judgments. (4) Based on the generated data (paired scene and synthesized speech), expert human annotators are recruited for detailed annotation on semantic check (i.e., <span class=\"ltx_text ltx_font_italic\">Content Pass</span>) and three different spoken role-play dimensions (i.e., <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human-likeness</span>, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>).\nImplementation details, including template design, alternative prompting strategies, and annotation guidelines, are provided in the following subsections of this Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, archetype evaluation emphasizes breadth and recognizability over subtlety.\nBy grounding role-play in socially recognizable categories and stereotypical situations, it provides a practical balance between scalability and interpretability, complementing the more fine-grained realism evaluation described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS2\" title=\"3.2 Role-play Evaluation with Two Strategies &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Excluded Dimensions.</span> We initially considered more fine-grained control dimensions, but excluded them to preserve the archetype-level focus. Examples of excluded dimensions include:\n<em class=\"ltx_emph ltx_font_italic\">Emotional State &amp; Expression, Single Emotion Prompts, Explicit User Input / Assistant Output structures, Emotional Transition / Fluctuation, Voice Style &amp; Prosody (pacing, intonation, vocal timbre, expressive style, accent/dialect), and Content / Interaction Patterns (task/topic controls, specific linguistic structures, or speech production challenges).</em></p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "transition",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All steps are automatically processed by <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib12\" title=\"\">2025</a>)</cite>, but each output is manually verified to ensure high quality for downstream usage. We demonstrate an example for Roles: Occupation / Functional Role in Fictional or Fantasy as follows:</p>\n\n",
                "matched_terms": [
                    "all",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Collection for Proprietary Models.</span> For proprietary systems, we adopt two collection strategies. For GPT-4o-realtime, responses are obtained directly via the <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-realtime</span> API. For Doubao and GPT-4o (Advanced Voice Mode), we collect outputs by recording from their official web interfaces. All recordings are captured through the interaction device itself, ensuring that no additional processing or distortion is introduced into the final audio.</p>\n\n",
                "matched_terms": [
                    "all",
                    "recording"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">End-to-End Model Generation.</span>\nFor end-to-end systems, the audio prompt is directly used as input. The system prompt is set to default values to ensure consistent speech generation quality.</p>\n\n",
                "matched_terms": [
                    "set",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cascaded Model Generation.</span>\nFor cascaded systems, we only use Cosyvoice2 as the TTS model in our collection of data. We select <span class=\"ltx_text ltx_font_typewriter\">Qwen3-30B-Instruct</span> and <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> as the text LLM candidates&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib73\" title=\"\">2025</a>; Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib12\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2\" title=\"Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the internal annotation interface used for <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>.\nThe platform provides an integrated environment for both playback and evaluation of model-generated role-play speech samples.\nAnnotators first interact with the audio player interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf1\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>), which visualizes waveforms and spectrograms for multiple versions of the same prompt, enabling side-by-side comparison under a shared role instruction (e.g., firefighter, teacher, or actor).\nThen, they proceed to the annotation interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf2\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a>), where they assign scores for multiple dimensions such as <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, along with reasoning and confidence inputs.\nThis structured interface supports high-consistency, fine-grained evaluation of role-play behaviors in speech models.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">Appropriateness</span>: comment for <em class=\"ltx_emph ltx_font_italic\">all</em> scores 1&#8211;5. Explain <em class=\"ltx_emph ltx_font_italic\">why</em> the <em class=\"ltx_emph ltx_font_italic\">delivery</em> was/was not a good match to the in-context traits (role &amp; scene).</p>\n\n",
                "matched_terms": [
                    "traits",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Rate how likely an average listener would believe the clip is human-delivered (ignore Audio Quality issues and contextual fit).</p>\n\n",
                "matched_terms": [
                    "contextual",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realism Evaluation adopts a bottom&#8211;up perspective grounded in real human speech data, with the goal of capturing the nuanced delivery, prosody, and contextual fit that synthetic archetype prompts cannot fully represent.\nRather than relying on stereotypes, this strategy leverages real-world recordings and carefully constructed contrasts, enabling fine-grained assessment of model performance under realistic conditions.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "contextual",
                    "model",
                    "real",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data from real human speech sources.</span>\nWe construct the realism evaluation set primarily from natural speech material:</p>\n\n",
                "matched_terms": [
                    "set",
                    "realism",
                    "real"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generated local scenes.</span>\nWe supplement retrieved character information with automatically generated local scene descriptions summarizing the surrounding plot or setting.\nAppendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS3\" title=\"F.3 Prompts for Local Scene Generation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.3</span></a> includes representative prompts and generation details.</p>\n\n",
                "matched_terms": [
                    "local",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative character profiles.</span>\nWe also create role-profile mismatches by resynthesizing transcripts with diverse TTS systems, thereby breaking the link between authentic delivery and intended role.\nWe explore both text-instruction-based prompting and comparisons across commercial and non-commercial TTS frameworks.\nDetailed model lists and parameter settings are documented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS6\" title=\"F.6 TTS Systems and Settings &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.6</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, real human ground-truth segments and systematically constructed negatives form a balanced dataset that supports realism-oriented evaluation.\nThe positives provide natural anchors of role-play performance, while the negatives introduce controlled mismatches that test whether evaluation models can distinguish real human speech from synthetic approximations.\nThis design ensures that realism evaluation emphasizes subtle paralinguistic and contextual cues, moving beyond intelligibility or content fidelity.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "contextual",
                    "performance",
                    "test",
                    "real"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During annotation, raters are presented with either: (i) ground-truth stories, (ii) stories with negative local scenes, or (iii) stories with negative character profile. Annotators assign realism scores without being exposed to the underlying mismatch reasons. Instead, these reasons are provided only to the quality-assurance team, who use them as reference for internal validation and consistency checks. While the mismatch reasons may not be perfect, they help streamline quality control procedures and maintain reliability across the dataset.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "local",
                    "consistency",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To make the construction of the realism evaluation task concrete, we present a worked example that begins with a curated local scene and character profile, followed by the same transcript adapted into mismatched contexts. These mismatches appear either at the <em class=\"ltx_emph ltx_font_italic\">local-scene</em> level or at both the <em class=\"ltx_emph ltx_font_italic\">character profile and scene</em> level, thereby providing controlled degradations of realism.</p>\n\n",
                "matched_terms": [
                    "local",
                    "realism",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the character Yunjin from <span class=\"ltx_text ltx_font_italic\">Genshin Impact</span> as a running case study, which is also included in our released dataset.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>As acknowledged, the use of real-world recordings is conducted under legal and ethical review. All curated data will be released as part of our open-source package.</span></span></span> This example illustrates how alignment between a speaker&#8217;s persona, local context, and delivery underpins realism, and how systematically breaking that alignment produces contrastive test cases.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "local",
                    "character",
                    "all",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Curated Local Scene.</span>\nYunjin is narrating a fierce battle through operatic singing. Her verses depict a hero charging into enemy lines, slaying the enemy commander, and securing peace. The atmosphere is filled with tension and intensity, highlighting the hero&#8217;s extraordinary bravery and determination.</p>\n\n",
                "matched_terms": [
                    "local",
                    "intensity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative Local Scenes.</span>\nTo stress realism evaluation, we insert the same transcript into alternative backgrounds. These reveal how context-shifted delivery degrades narrative coherence:</p>\n\n",
                "matched_terms": [
                    "realism",
                    "stress",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative Character Profiles.</span>\nWe replace Yunjin with new character profiles and scenes. Although the transcript remains the same, realism degrades because the delivery no longer matches the speaker&#8217;s persona or situation.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Yunjin case study illustrates a fundamental difference between realism evaluation and archetype evaluation. Archetype prompts, as described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>, are constructed by pairing abstract role categories with synthetic contextual scenes. These cover a broad space of possible situations, ranging from modern occupations (e.g., firefighters) to fictional personas (e.g., cult leaders in martial arts fantasy) or social identities (e.g., overprotective parents, laid-back friends). While such prompts provide controlled and diverse coverage, they are ultimately template-based and rely on stereotypes to elicit role-play behaviors.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "contextual",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the Yunjin example is drawn from authentic performance, where the character&#8217;s persona (an opera singer) and local scene (a battlefield narration) are naturally aligned through artistic delivery. The realism task does not merely test whether a model can follow a prompt, but whether it can preserve coherence between a speaker&#8217;s global identity, immediate context, and expressive prosody. When Yunjin&#8217;s transcript is transplanted into mismatched settings or paired with incongruent speaker profiles, the breakdown in realism becomes apparent: the same words shift from inspiring to awkward, jarring, or even absurd.</p>\n\n",
                "matched_terms": [
                    "global",
                    "realism",
                    "local",
                    "identity",
                    "model",
                    "test",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This distinction highlights the complementary strengths of the two evaluation paradigms. Archetype evaluation systematically explores a wide combinatorial space of roles and situations, but may lack the paralinguistic and contextual richness of real speech. Realism evaluation, on the other hand, anchors assessment in authentic delivery and contextual fit, providing a finer-grained lens on coherence and naturalness. Together, they ensure that role-play evaluation spans both broad stereotypical coverage and nuanced real-world plausibility.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "contextual",
                    "realism",
                    "real"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed a diverse set of TTS systems to generate negative character profiles and resynthesized speech for Realism Evaluation.\nThese included both commercial and non-commercial models, with variation in style control, conditioning prompts, and reference usage.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.T8\" title=\"Table 8 &#8227; Same-Speaker Voice Clone. &#8227; F.6 TTS Systems and Settings &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> summarizes the systems and settings.\nBelow, we detail the major strategies used across systems:</p>\n\n",
                "matched_terms": [
                    "set",
                    "realism",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We leveraged all publicly available speaker style information released by the corresponding websites.\nFor each input sample, we employed <span class=\"ltx_text ltx_font_typewriter\">deepseek-v3</span> to select the best-matching speaker based on the original speech caption.\nIn the case of <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5TTS</span>, the character profile was also appended as an additional instruction to guide synthesis.</p>\n\n",
                "matched_terms": [
                    "all",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A character profile was first converted into a descriptive voice specification by prompting <span class=\"ltx_text ltx_font_typewriter\">deepseek-v3</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib38\" title=\"\">2024a</a>)</cite>.\nThis description was submitted to the TTS service to generate candidate voices.\nWe then previewed samples via Gemini2.5Pro, selecting the voice that best matched the speaker style of the original recording before proceeding to generate speech.</p>\n\n",
                "matched_terms": [
                    "recording",
                    "gemini25pro",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS3</span> (Embedding&#8211;AudioCode Pipeline).\nHere, the input is Qwen2.5 BPE embeddings, which a 0.4B GPT-2 model maps directly to 4,096-dimensional audio codes. These codes are decoded via a cascaded architecture: <span class=\"ltx_text ltx_font_typewriter\">RepCodec</span> decoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib32\" title=\"\">2024</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> content representation (with speaker embedding) <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DDIM</span> (denoising\ndiffusion implicit model) module&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jeong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib34\" title=\"\">2021</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DAC</span> latent representation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib37\" title=\"\">2023</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DAC</span> decoder. Conditioning is provided through prefix text embeddings (e.g., instructions expressing emotional states such as &#8220;angry, hateful: you betrayed me&#8230;&#8221;), along with speaker ID. The input format is <span class=\"ltx_text ltx_font_typewriter\">[spk_id, BPE embedding, audio codes]</span>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The annotation interface for <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> focuses on assessing contextual and narrative coherence in model-generated speech.\nUnlike the archetype-oriented evaluation, which emphasizes stylistic and emotional fidelity, this platform examines how well the voice performance aligns with a character&#8217;s personality, dialogue situation, and scene progression.</p>\n\n",
                "matched_terms": [
                    "contextual",
                    "realism",
                    "emotional",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.F3\" title=\"Figure 3 &#8227; F.7 Annotation Platform for Realism Evaluation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the core playback interface.\nAnnotators can preview each audio sample with synchronized spectrogram and waveform displays, while reviewing contextual metadata including the <span class=\"ltx_text ltx_font_italic\">Local Scene</span>, <span class=\"ltx_text ltx_font_italic\">Character Style</span>, and <span class=\"ltx_text ltx_font_italic\">Character Profile</span>.\nThese descriptions help situate the listening task, providing the background necessary for judging realism in the character&#8217;s vocal and behavioral expression.</p>\n\n",
                "matched_terms": [
                    "local",
                    "realism",
                    "contextual",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Read the <span class=\"ltx_text ltx_font_bold\">Character Profile</span> (long-term identity, style) and <span class=\"ltx_text ltx_font_bold\">Local Scene</span> (immediate context, goal/emotion) before listening.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the audio&#8217;s speaker is <em class=\"ltx_emph ltx_font_italic\">clearly not</em> the intended target identity, assign a score of <span class=\"ltx_text ltx_font_bold\">1</span> to all Profile-dependent dimensions (e.g., Voice Identity Matching, Trait Embodiment) and proceed with other dimensions as applicable.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "all",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Detailed case examples and progressive gating for emotional dimensions are provided in the full instruction document.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt five dimensions, each rated from 1 (worst) to 5 (best). Emotional Expressiveness uses <em class=\"ltx_emph ltx_font_italic\">progressive gating</em>.</p>\n\n",
                "matched_terms": [
                    "expressiveness",
                    "dimensions",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prosodic Dynamics</span> (Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis): naturalness of patterns and rhythms.</p>\n\n",
                "matched_terms": [
                    "prosodic",
                    "dynamics",
                    "stress",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotional Expressiveness</span> (Progressive gating):</p>\n\n",
                "matched_terms": [
                    "expressiveness",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Character Consistency</span> (Voice Identity Matching, Trait Embodiment): alignment with Character Profile.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "consistency",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contextual Relevance</span> (Local Scene Fit, Global Story Coherence): fit to immediate scene and long-term identity.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "identity",
                    "relevance",
                    "contextual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Read first:</span> Carefully read the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em> to understand the long-term identity and immediate goal/emotion.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identity mismatch:</span> If the clip&#8217;s speaker is clearly not the intended target speaker, assign a score of <span class=\"ltx_text ltx_font_bold\">1</span> for every rating dimension that relies on the Character Profile (e.g., Character Style Traits, Trait Embodiment, Character Consistency).</p>\n\n",
                "matched_terms": [
                    "identity",
                    "consistency",
                    "traits",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt contains contradictory information.</span> Conflicts make accurate annotation difficult or impossible. Examples: incompatible accent specifications; Local Scene drastically at odds with intended emotion (see <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>); Local and global scenes inconsistent; traits described as &#8220;evolving over time&#8221; without a timeline.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "global",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt is insufficient to evaluate the voice sample.</span> Audio too short to gauge the dimensions; or the prompt lacks specific characteristics/is too vague or multi-directional to know which version of the character is present. (It is OK if audio does not match transcript exactly; if too much is missing to annotate, mark as insufficient.)</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Question 2: Observed Character Style Traits.</span> Based on the Character Profile, are the defined Style Traits clearly present in the delivery?</p>\n\n",
                "matched_terms": [
                    "traits",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rubric Rating Instructions.</span>\nScore each subcriterion on a 1&#8211;5 scale. Use the detailed rubrics below and the examples to maintain calibration across characters, tones, and emotional styles. Prioritize in-character believability; do not reward theatricality, emotional intensity, or technical polish unless they naturally fit the character <em class=\"ltx_emph ltx_font_italic\">and</em> scene.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "intensity",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Focus: pitch, rhythm, and emphasis in speech.\n<span class=\"ltx_text ltx_font_bold\">Guideline 1.1 Pitch Variation (check Local Scene, Character Profile, and audio).</span></p>\n\n",
                "matched_terms": [
                    "character",
                    "local",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 1.2 Rhythmic Naturalness (check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "character",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 1.3 Stress and Emphasis (check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "character",
                    "stress",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 2.1 Emotion Accuracy (Required; check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "character",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 2.2 Emotion Intensity Control (Only if 2.1 <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"A7.SS4.SSSx5.p3.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math> 3; check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "character",
                    "intensity",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 2.3 Dynamic Range (Only if 2.2 <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"A7.SS4.SSSx5.p4.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math> 3; check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "character",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 3.1 Voice Identity Matching (Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 4.2 Global Story (Character Profile) Coherence (check Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "global",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Progressive gating for emotion.</span>\nOur <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness</em> uses gated sub-scores (accuracy <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A8.SS2.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> intensity control <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A8.SS2.p3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> dynamic range), preventing spurious high scores when base emotion is misidentified.\nThis reduces label noise and improves SEM alignment stability.</p>\n\n",
                "matched_terms": [
                    "expressiveness",
                    "accuracy",
                    "emotional",
                    "intensity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The archetype evaluation provides a detailed view of how speech foundation models (SFMs) perform when tasked with role-play scenarios grounded in stereotypical characters and situations. Human annotations cover four dimensions, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, and <span class=\"ltx_text ltx_font_italic\">Content Pass</span>, and the results reveal important trends, as well as challenges in interpreting scores across languages.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A critical observation is that annotations in Mandarin and English cannot be directly compared. The annotation groups for the two languages differed in their linguistic backgrounds and cultural interpretations, which leads to systematic differences in scoring. Our bilingual pilot further confirmed that annotators often interpret role-play appropriateness and expressiveness differently depending on their cultural frame of reference. For example, Mandarin raters tend to reward delivery styles that align with culturally expected prosody and politeness strategies, while English raters prioritize spontaneity and naturalness. These discrepancies parallel the findings of the singing voice conversion challenge (SVCC2023) from <cite class=\"ltx_cite ltx_citemacro_citet\">Huang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib31\" title=\"\">2023</a>)</cite>, where native and non-native listeners provided significantly different ratings for the same samples, particularly for naturalness and speaker similarity. Consequently, while absolute scores differ substantially across Mandarin and English, only within-language comparisons are meaningful. Cross-lingual ranking is not valid given the different annotation bases.</p>\n\n",
                "matched_terms": [
                    "expressiveness",
                    "roleplay"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Within Mandarin from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, the relative rankings of models are clear. <span class=\"ltx_text ltx_font_typewriter\">Doubao</span> stands out as the strongest performer across all four dimensions, reaching near-ceiling levels in both <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> (4.37) and <span class=\"ltx_text ltx_font_italic\">Content Pass</span> (0.98). It also leads in role-related dimensions such as <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, indicating balanced competence across linguistic accuracy, delivery, and expressiveness. <span class=\"ltx_text ltx_font_typewriter\">G-CosyVoice2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-CosyVoice2</span> follow closely, especially in audio quality, though they lag behind in role appropriateness. <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> achieve mid-range results, reflecting higher variance across samples, while <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> shows competitive content understanding but lower naturalness and appropriateness. <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> consistently ranks the lowest, with large variance, underscoring instability and weaker alignment with archetypal role expectations.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "all",
                    "expressiveness",
                    "accuracy",
                    "kimiaudio",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The picture shifts in English, elaborated in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Here, <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> emerges as the most effective model by a wide margin, achieving top scores in both <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> (3.46) and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> (3.49), as well as high content pass rates (0.97). This reflects its strong grounding in English discourse norms and role adherence. In contrast, <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> variants perform well in <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> but are weaker in role alignment, highlighting a trade-off between acoustic fidelity and role-play embodiment. Other models such as <span class=\"ltx_text ltx_font_typewriter\">Doubao</span>, <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span>, and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> achieve moderate results, while <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> trail far behind, showing limited robustness in English scenarios. Taken together, the English results reveal both a sharper separation across models and a wider variance in appropriateness scores compared to Mandarin.</p>\n\n",
                "matched_terms": [
                    "model",
                    "roleplay",
                    "kimiaudio",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although realism evaluation targets nuanced paralinguistic behaviors, human annotators achieve <em class=\"ltx_emph ltx_font_italic\">consistently high agreement</em> once the rubric disentangles the aspects to be judged.\nAs shown in the agreement summary in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F7\" title=\"Figure 7 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, most dimensions fall in the <math alttext=\"0.86\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>0.86</mn><annotation encoding=\"application/x-tex\">0.86</annotation></semantics></math>&#8211;<math alttext=\"0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mn>0.88</mn><annotation encoding=\"application/x-tex\">0.88</annotation></semantics></math> range, with the lowest agreement on <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> (<math alttext=\"\\approx 0.83\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.83</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.83</annotation></semantics></math>) and the highest on <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em> / <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> (<math alttext=\"\\approx 0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.88</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.88</annotation></semantics></math>).\nEmotion-related, progressive metrics (Intensity and Transition) are slightly lower (<math alttext=\"\\approx 0.85\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.85</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.85</annotation></semantics></math>), reflecting the added difficulty of judging graded expressivity.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "local",
                    "intensity",
                    "dimensions",
                    "transition",
                    "traits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F8\" title=\"Figure 8 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F9\" title=\"Figure 9 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> visualize the score distributions per dimension.\nWe highlight three patterns:\n(i) <span class=\"ltx_text ltx_font_bold\">Progressive/gated emotion metrics.</span> For <em class=\"ltx_emph ltx_font_italic\">Emotion Intensity</em> and <em class=\"ltx_emph ltx_font_italic\">Emotion Transition</em>, we assign <span class=\"ltx_text ltx_font_typewriter\">1</span> when the scene is marked <em class=\"ltx_emph ltx_font_italic\">N/A</em> (i.e., emotion accuracy is insufficient to meaningfully judge intensity/transition). This produces a visible mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, a sparse bin at <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>, and slightly lower overall agreement; downstream analyses conditioning on adequate <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> (e.g., mean&#160;<math alttext=\"\\geq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 3</annotation></semantics></math>) recover more bell-shaped distributions.\n(ii) <span class=\"ltx_text ltx_font_bold\">Traits Embodiment.</span> Scores tend to avoid the middle (<math alttext=\"\\sim\\!3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!3</annotation></semantics></math>), concentrating on low values when traits clearly mismatch and high values when traits are strongly realized. This yields the lowest inter-rater agreement among dimensions, reflecting the inherently compositional and role-specific nature of trait judgments.\n(iii) <span class=\"ltx_text ltx_font_bold\">Other dimensions.</span> <em class=\"ltx_emph ltx_font_italic\">Pitch Variation</em>, <em class=\"ltx_emph ltx_font_italic\">Rhythmic Naturalness</em>, <em class=\"ltx_emph ltx_font_italic\">Stress &amp; Emphasis</em>, <em class=\"ltx_emph ltx_font_italic\">Voice Identity Matching</em>, <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em>, <em class=\"ltx_emph ltx_font_italic\">Global Story Coherence</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> are approximately unimodal with modes at <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>&#8211;<math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math>, consistent with moderate&#8211;strong realization in the dataset and with the high agreement reported.</p>\n\n",
                "matched_terms": [
                    "global",
                    "pitch",
                    "identity",
                    "local",
                    "intensity",
                    "dimensions",
                    "transition",
                    "stress",
                    "traits",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prosody control (Pitch, Rhythm, Stress).</span> Professional speech is shifted toward higher bins with tighter spread, indicating steadier control and fewer low-end tails.</p>\n\n",
                "matched_terms": [
                    "stress",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotion rendering (Accuracy, Intensity, Transition).</span> Professionals show higher centers and reduced mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.I1.i2.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> (fewer gated <span class=\"ltx_text ltx_font_typewriter\">N/A</span>s), suggesting more consistent emotional grounding and smoother dynamics; amateurs exhibit wider variance and more mass at the extremes.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "intensity",
                    "accuracy",
                    "transition",
                    "dynamics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identity and traits (Voice Identity, Traits Embodiment).</span> Professionals present clearer peaks in upper bins, while amateurs are broader or bimodal, reflecting inconsistent character anchoring.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "traits",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scene and semantics (Local Scene Fit, Global Coherence, Semantic Match).</span> Differences are present but smaller; both groups cluster in the mid&#8211;high range, with professionals exhibiting slightly narrower distributions.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, the consistently high agreement confirms that the rubric isolates perceptually coherent factors, so learned evaluators can target them reliably.\nSecond, the gated behavior of <em class=\"ltx_emph ltx_font_italic\">Intensity/Transition</em> motivates a <em class=\"ltx_emph ltx_font_italic\">hierarchical</em> evaluator: verify <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> before scoring graded expressivity, or model the joint structure with conditional heads.\nThird, <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> remains challenging; models may benefit from explicit character-trait priors, long-term context aggregation, or contrastive supervision between matched and mismatched traits.\nFinally, prosody-focused objectives (e.g., pitch/rhythm/stress control) are likely to yield the largest quality gains for amateur-style speech.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "evaluator",
                    "model",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For both zeroshot and fewshot experiments, we adopt a streamlined version of the annotation guidelines to ensure consistency with the human annotation process. The exact rubric prompts for archetype and realism evaluation used are provided below:</p>\n\n",
                "matched_terms": [
                    "realism",
                    "consistency",
                    "zeroshot",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and fewshot settings, we condition the model with the rubric prompt and then append few-shot demonstration triplets (input, audio, gold JSON) to mirror the human annotation protocol. As shown in Listing&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#LST2\" title=\"Listing 2 &#8227; L.1 Zeroshot/Fewshot Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, We use <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> for these few-shot exemplars and require a minified JSON output consistent with the rubric schema.</p>\n\n",
                "matched_terms": [
                    "model",
                    "zeroshot",
                    "gemini25pro",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and few-shot experiments on commercial models such as <span class=\"ltx_text ltx_font_typewriter\">Gemini</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o</span>, we report results as the average of five independent runs to reduce randomness and obtain more stable predictions. In contrast, for open-source models, we observed that their instruction-following ability is relatively weak. To mitigate this, we prompt them to predict one dimension at a time rather than all dimensions jointly. Specifically, for each metric, after showing the rubric prompt, we request completion in a fixed template and extract the probability distribution over the discrete tokens <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>. The final score is then computed from these probabilities, which we found makes open-source models usable to some extent under this setting.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "report",
                    "average",
                    "all",
                    "dimensions",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For few-shot experiments, we randomly sample three exemplars from the training set, with each exemplar chosen such that its human scores are close to the rounded average across all annotators. These exemplars are appended as demonstrations in the prompt. As with the zeroshot case, we run five trials for commercial models and report the averaged results for evaluation.</p>\n\n",
                "matched_terms": [
                    "set",
                    "zeroshot",
                    "report",
                    "average",
                    "all",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.T12\" title=\"Table 12 &#8227; L.2 Zeroshot/Fewshot Ablation Experiments &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> shows how different prompting strategies affect realism evaluation performance with <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>. Several observations can be drawn:</p>\n\n",
                "matched_terms": [
                    "realism",
                    "performance",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Effect of repeated runs.\nWhen comparing single-run decoding (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot-1run</span>) against averaging over five runs (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot</span>), we see consistent gains across nearly all dimensions (average score improves from 0.353 to 0.390). This suggests that sampling variance plays a substantial role in the reliability of large language model (LLM) evaluators, and averaging multiple outputs reduces this variance, producing more stable correlation with human annotations.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "zeroshot",
                    "average",
                    "model",
                    "all",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Impact of few-shot conditioning.\nIntroducing even three examples (<span class=\"ltx_text ltx_font_typewriter\">3-Shot</span>) yields a clear improvement over zeroshot (0.433 vs. 0.390). This indicates that providing explicit annotated demonstrations helps the model better align with the rubric. Interestingly, when using only a single run with few-shot demonstrations (<span class=\"ltx_text ltx_font_typewriter\">3-Shot-1run</span>), the performance is only marginally better than pure zeroshot, highlighting that both averaging and demonstrations are complementary: demonstrations improve grounding, while multiple runs mitigate stochasticity.</p>\n\n",
                "matched_terms": [
                    "model",
                    "zeroshot",
                    "performance",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Scaling the number of demonstrations.\nIncreasing the few-shot set size from three to six and nine further boosts performance (0.459 and 0.469, respectively). The improvements are most pronounced in prosodic and emotional categories (e.g., <em class=\"ltx_emph ltx_font_italic\">Pitch</em>, <em class=\"ltx_emph ltx_font_italic\">Accuracy</em>, <em class=\"ltx_emph ltx_font_italic\">Intensity</em>), suggesting that more examples provide stronger guidance in modeling subtle paralinguistic cues. Gains in <em class=\"ltx_emph ltx_font_italic\">Character Consistency</em> and <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance</em> are also noticeable, although smaller in magnitude, which may indicate that these higher-level judgments are less sensitive to prompt scaling once a few demonstrations are provided.</p>\n\n",
                "matched_terms": [
                    "set",
                    "pitch",
                    "relevance",
                    "contextual",
                    "consistency",
                    "emotional",
                    "character",
                    "intensity",
                    "prosodic",
                    "fewshot",
                    "accuracy",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In general, the results demonstrate two complementary strategies for stabilizing LLM-based evaluators: (i) averaging multiple runs to reduce randomness, and (ii) enriching prompts with more few-shot exemplars to improve adherence to rubric-based evaluation. Combining both leads to the strongest overall performance, with the nine-shot condition achieving the highest average correlation&#160;(0.469).</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "average",
                    "performance",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the archetype test set, all fine-tuned variants achieve strong <span class=\"ltx_text ltx_font_italic\">Content Pass</span> accuracy above 92%. Differences appear mainly in correlation-based metrics. Here, <span class=\"ltx_text ltx_font_typewriter\">Base-FT</span> yields the highest score on <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> delivers the strongest performance in <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> and the best overall average (0.629). This suggests that instruction-tuned checkpoints paired with parameter-efficient adaptation can better capture higher-level perceptual traits.</p>\n\n",
                "matched_terms": [
                    "set",
                    "average",
                    "all",
                    "test",
                    "traits",
                    "accuracy",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For realism evaluation on the basic test set, <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> consistently achieves the strongest correlations across most dimensions, especially for <span class=\"ltx_text ltx_font_italic\">Character Identity</span> and <span class=\"ltx_text ltx_font_italic\">Traits</span>, leading to the best average correlation (0.668). <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> also performs competitively (0.630), while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> lags slightly behind (0.625). These results highlight that full fine-tuning may still offer an advantage when the evaluation set is relatively homogeneous and closer to the training distribution.</p>\n\n",
                "matched_terms": [
                    "set",
                    "correlation",
                    "realism",
                    "identity",
                    "average",
                    "character",
                    "dimensions",
                    "test",
                    "traits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A different pattern emerges on the real recording test set. Both <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> and <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> struggle, yielding negative or near-zero correlations across most dimensions. In contrast, <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> shows clear improvements, achieving positive correlations in all prosodic and emotional dimensions and producing the highest overall average (0.247). This indicates that LoRA adaptation on top of an instruction-tuned backbone provides better generalization to out-of-distribution, real-world data.</p>\n\n",
                "matched_terms": [
                    "set",
                    "average",
                    "emotional",
                    "recording",
                    "prosodic",
                    "dimensions",
                    "all",
                    "test",
                    "real"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> is competitive on in-domain evaluation, it suffers a substantial drop when applied to real recordings. By contrast, <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> offers a more robust balance: it maintains reasonable performance on archetype and basic sets, while significantly outperforming other settings on real-world data. Based on this robustness, we adopt <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> as our final configuration for subsequent experiments.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "real"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the comprehensive design of Speech-DRAME, several limitations remain.\nFirst, while our dual evaluation paradigm combines top-down archetype assessment with bottom-up realism evaluation, both strategies are constrained by the quality and scope of available data. Archetype evaluation inevitably relies on simplified stereotypes, which can overlook subtler paralinguistic and contextual cues. Realism evaluation, though richer, still struggles with domain mismatch between synthetic data and real human recordings, as evidenced by the performance drop observed in the real-recording test set. This highlights the persistent gap between benchmark-driven evaluation and real-world variability in speech role-play.</p>\n\n",
                "matched_terms": [
                    "set",
                    "roleplay",
                    "realism",
                    "contextual",
                    "performance",
                    "test",
                    "real"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Third, while DRAME-Eval substantially outperforms zero-shot and few-shot ALLMs, its reliance on fine-tuning from a strong base model raises questions of accessibility and scalability. Future work should explore lightweight adaptation techniques, semi-supervised training with partially annotated data, and leveraging emergent capabilities of open-source multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "model",
                    "drameeval",
                    "zeroshot",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, although we validated DRAME-Eval against human annotations, alignment under realism settings remains modest. This suggests that even human-annotated benchmarks are insufficiently fine-grained to capture the full complexity of human perception in speech role-play. Richer annotation protocols, more diverse speaker populations, and integration of perceptual psychology insights could further improve evaluation fidelity.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "drameeval",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Looking forward, we see several promising research directions. We intentionally restrict evaluation to single-turn responses for clarity and tractability. However, multi-turn extensions that capture narrative progression, long-term consistency, and dynamic emotion shifts remain a natural next step. Incorporating multilingual and cross-cultural datasets will enhance generality and fairness. On the modeling side, exploring hybrid evaluation strategies that combine discrete rubrics with continuous perceptual signals (e.g., prosody embeddings or affective measures) may better bridge the gap between automatic and human judgments. Finally, integrating Speech-DRAME into reinforcement learning pipelines as a reward model could help drive the next generation of expressive, human-aligned speech role-play systems.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "consistency",
                    "model"
                ]
            }
        ]
    },
    "S6.T5": {
        "caption": "Table 5: Speech-DRAME benchmark for archetype role-play. Benchmark results are calculated based on DRAME-Eval discussed in Sec.5.1. G and Q in the cascaded type refer to Gemini2.5Pro and Qwen3-30B-Instruct, respectively. Please refer to Sec.6 for detailed models.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\">Type</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\">Model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">Content Pass Rate</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">Audio Quality</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">Human Likeness</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">Approperiateness</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Avg.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" rowspan=\"6\">End-to-end</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">78.9%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.60</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">66.8%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.93</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.63</td>\n<td class=\"ltx_td ltx_align_center\">2.73</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">88.5%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.95</td>\n<td class=\"ltx_td ltx_align_center\">2.95</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">53.8%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.26</td>\n<td class=\"ltx_td ltx_align_center\">2.28</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Qwen3Omni</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">57.2%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.34</td>\n<td class=\"ltx_td ltx_align_center\">2.34</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">StepAudio2Mini</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">90.5%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.96</td>\n<td class=\"ltx_td ltx_align_center\">3.04</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" rowspan=\"10\">Cascaded</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">Q-F5TTS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">92.3%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.99</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Q-IndexTTS2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">94.7%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">3.05</span></td>\n<td class=\"ltx_td ltx_align_center\">3.03</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Q-MaskGCT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">91.4%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.95</td>\n<td class=\"ltx_td ltx_align_center\">2.93</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Q-Vevo1.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">79.5%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.72</td>\n<td class=\"ltx_td ltx_align_center\">2.75</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Q-CosyVoice2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">94.2%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">3.32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.00</td>\n<td class=\"ltx_td ltx_align_center\">3.02</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">G-F5TTS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">91.7%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.99</td>\n<td class=\"ltx_td ltx_align_center\">3.03</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">G-IndexTTS2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">94.3%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">2.81</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">3.05</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">3.07</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">G-MaskGCT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">92.5%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.00</td>\n<td class=\"ltx_td ltx_align_center\">3.00</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">G-Vevo1.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">72.0%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.81</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.55</td>\n<td class=\"ltx_td ltx_align_center\">2.59</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">G-CosyVoice2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">93.4%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">3.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">2.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">3.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3.04</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "roleplay",
            "cascaded",
            "models",
            "type",
            "qmaskgct",
            "content",
            "gvevo15",
            "gmaskgct",
            "calculated",
            "gpt4orealtime",
            "glm4voice",
            "qindextts2",
            "based",
            "qwen330binstruct",
            "rate",
            "audio",
            "qvevo15",
            "gemini25pro",
            "archetype",
            "gf5tts",
            "avg",
            "pass",
            "stepaudio2mini",
            "refer",
            "gindextts2",
            "qf5tts",
            "qwen3omni",
            "gcosyvoice2",
            "results",
            "kimiaudio",
            "discussed",
            "benchmark",
            "respectively",
            "please",
            "speechdrame",
            "qwen25omni",
            "qcosyvoice2",
            "model",
            "approperiateness",
            "human",
            "endtoend",
            "sec",
            "drameeval",
            "likeness",
            "detailed",
            "quality"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype evaluation.</span>\nOn archetype role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T5\" title=\"Table 5 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), cascaded pipelines consistently outperform end-to-end systems. The strongest cascaded systems, particularly <span class=\"ltx_text ltx_font_typewriter\">G-IndexTTS2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-IndexTTS2</span>, achieve both high content pass rates (above 94%) and balanced scores across audio quality, human likeness, and appropriateness. By contrast, end-to-end models such as <span class=\"ltx_text ltx_font_typewriter\">StepAudio2Mini</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> perform competitively in terms of content coverage (above 88%), but their expressive scores remain slightly lower than their cascaded counterparts. This gap suggests that separating reasoning (LLM) from speech rendering (TTS) remains advantageous when archetype fidelity is emphasized. Nevertheless, the fact that top end-to-end systems already surpass 3.0 in average rating indicates rapid closing of the performance gap.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Role-play has become a key testbed for generative models, expanding from text-only dialogue to multimodal interaction. Extending role-play to speech captures prosody, emotion, and delivery, but also poses new evaluation challenges. Current pipelines often use audio large language models (ALLMs) as zero-shot judges, which miss paralinguistic cues, collapse multiple aspects into coarse scores, and rely on synthetic speech references that fail to reflect real-world roles. We present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework that contributes at three levels: (i) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-EvalBench</em>, an evaluation benchmark with bilingual human-annotated data and protocols for training and testing speech evaluation models&#160;(SEMs), (ii) <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, a fine-tuned evaluation model, which substantially outperforms zero-shot and few-shot ALLMs, and (iii) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-RoleBench</em>, a speech role-play benchmark that leverages <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> as an automatic judge to compare speech foundation models (SFMs). Speech-DRAME distinguishes between two complementary evaluation strategies: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>, a top-down approach measuring adherence to broad role archetypes, and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em>, a bottom-up approach grounded in real human speech that emphasizes nuanced role quality. Compared to zero-shot ALLM judges, <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves stronger agreement with human ratings (Pearson correlation from 0.480 to 0.629 in archetypes, and 0.390 to 0.625 in realism). By integrating transparent benchmark resources, modeling approaches, and system-level evaluation, Speech-DRAME provides the first comprehensive, reproducible foundation for assessing spoken role-play.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "roleplay",
                    "models",
                    "audio",
                    "drameeval",
                    "model",
                    "human",
                    "archetype",
                    "speechdrame",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Role-play has been recently recognized a cornerstone of interactive systems, from initial text-based focuses to the growing ecosystem of multimodal dialogue agents&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib6\" title=\"\">2024a</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib59\" title=\"\">2024</a>; Dai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib13\" title=\"\">2025</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib43\" title=\"\">2024</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>)</cite>. With the rapid advancement of large language models (LLMs), role-play has evolved beyond scripted interactions into dynamic character-driven conversations, supporting applications in education, entertainment, and human&#8211;AI collaboration&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib69\" title=\"\">2024</a>; Agatsuma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib1\" title=\"\">2024</a>; Fung &amp; Laing, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib20\" title=\"\">2024</a>)</cite>. Extending role-play to speech-based interaction offers an even richer communication channel, capturing prosody, emotion, and character consistency in ways that text alone cannot&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">it collapses diverse aspects of performance into limited coarse dimensions, obscuring critical dimensions of role-play quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite>, and</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">it depends on synthetic speech references rather than real human speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>)</cite>, limiting its ability to reflect real-world diversity in speech role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome these limitations, we present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span> (Speech Detailed Role-play Assessment with Modeling and Evaluation), a unified framework that moves beyond synthetic or judge-only pipelines. Speech-DRAME is distinguished by three aspects: [i] a dual evaluation paradigm that balances archetype-driven generality with human-grounded realism (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS2\" title=\"3.2 Role-play Evaluation with Two Strategies &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), [ii] human annotations that provide perceptual grounding absent in prior resources (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4\" title=\"4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), and [iii] evaluation-model alignment that enables not only benchmarking of speech foundation models (SFMs) but also systematic analysis of the evaluation models (SEMs) themselves (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5\" title=\"5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). These aspects establish Speech-DRAME as a comprehensive and trustworthy framework for measuring and improving speech role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "models",
                    "human",
                    "sec",
                    "speechdrame",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, Speech-DRAME contributes at three levels: (i) <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>, an evaluation benchmark consisting of human-annotated datasets for archetype and realism role-play; (ii) <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span>, a fine-tuned SEM trained on EvalBench that surpasses zero-shot and few-shot ALLMs; and (iii) <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span>, a speech role-play benchmark that leverages DRAME-Eval as an automatic judge to systematically compare SFMs. By combining dual evaluation strategies, human-anchored supervision, and evaluation-model alignment, Speech-DRAME delivers systematic insights into both role-play systems and evaluation models themselves.\nOur contributions are four-fold:</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "roleplay",
                    "models",
                    "drameeval",
                    "archetype",
                    "speechdrame"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a <span class=\"ltx_text ltx_font_bold\">formal task definition</span> of speech role-play and its evaluation via speech foundation models (SFMs) for generation and speech evaluation models (SEMs) for assessment, unifying linguistic and paralinguistic dimensions.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>, the first evaluation benchmark with human annotations under both archetype- and realism-based strategies, supporting the training and testing of SEMs.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span>, a fine-tuned SEM trained on EvalBench that substantially outperforms zero-shot and few-shot ALLMs, demonstrating the importance of human supervision for evaluation models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "drameeval",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We establish <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span>, a speech role-play benchmark that leverages DRAME-Eval as an automatic judge to systematically compare both end-to-end and cascaded SFMs across multiple role quality dimensions.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Relevant codes, pre-trained models, and datasets will be open-sourced at \n<br class=\"ltx_break\"/><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/Anuttacon/speech_drame\" title=\"\">https://github.com/Anuttacon/speech_drame</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "roleplay",
                    "cascaded",
                    "models",
                    "endtoend",
                    "drameeval",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through this work, we aim to move from acting in text to acting in speech, equipping the community with the models, evaluation strategies, and benchmarks needed to measure and improve speech role-play.</p>\n\n",
                "matched_terms": [
                    "models",
                    "roleplay"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-based role-play evaluation.</span>\nEarly research focused on text-only dialogue benchmarks. Datasets such as RoleLLM, CharacterBox, and RoleMRC <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib62\" title=\"\">2024</a>; Tu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib60\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib61\" title=\"\">2025a</a>; Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib53\" title=\"\">2023</a>; Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib41\" title=\"\">2025</a>)</cite> test whether models can sustain persona consistency and character fidelity, using LLM-as-judge pipelines for scalability. However, studies like PersonaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib81\" title=\"\">2025a</a>)</cite> have shown that LLM-as-judge approaches are far from perfect, struggling with tasks as basic as tracking speaker identity. This highlights broader concerns about reliability even in text settings, which become only more severe when extended to speech, where prosody, emotion, and vocal style play a central role.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-native benchmarks.</span>\nMore recently, a number of resources have been introduced to evaluate spoken role-play agents. OmniCharacter-10K <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>)</cite> emphasizes character voice traits, while others&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib39\" title=\"\">2025</a>; Hou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib27\" title=\"\">2025</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib15\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib7\" title=\"\">2024b</a>)</cite> extend evaluation toward vocal communication, assistant capability, and multi-turn realism. Omni models such as StepAudio and KimiAudio <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite> also include role-play scenarios in their evaluation. SpeechRole and VStyle <cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>; Zhan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib76\" title=\"\">2025</a>)</cite> are closer to our work, offering datasets that test interaction ability, expressiveness, and fidelity. However, these resources are primarily built from LLM-generated dialogues and TTS outputs, with limited human validation and a heavy reliance on proprietary ALLM-as-judge pipelines. As a result, they lack grounding in diverse human speech and typically apply only coarse rubrics.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "kimiaudio",
                    "human",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Automatic judging with ALLMs.</span>\nIn parallel, advances in automatic judging highlight both opportunities and limitations. Audio-capable LLMs such as AudioJudge and Audio-Aware LLMs as Judges <cite class=\"ltx_cite ltx_citemacro_citep\">(Manakul et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib44\" title=\"\">2025</a>; Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>)</cite> can assess style, emotion, and delivery, while systems like Auto-ATT <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib65\" title=\"\">2025c</a>)</cite> scale this to Audio Turing Tests. Yet studies consistently reveal high prompt sensitivity and shallow capture of paralinguistic cues, suggesting that zero-shot judging remains insufficient for nuanced role-play evaluation.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Expressive TTS evaluation.</span>\nRecent work has explored evaluation of instruction-following and style-controlled TTS. InstructTTSEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite> explicitly includes role-play scenarios, while EmergentTTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">(Manku et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib45\" title=\"\">2025</a>)</cite> and discrete speech LM benchmarks <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang &amp; Sz&#233;kely, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib63\" title=\"\">2024</a>)</cite> emphasize prosody, spontaneity, and speaker consistency. These efforts push beyond intelligibility and naturalness, but still center on synthetic outputs rather than real human role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our distinction.</span>\nIn contrast, Speech-DRAME unifies these considerations through three elements: (i) a dual evaluation paradigm that combines archetype-driven generality with realism grounded in authentic human speech, (ii) human annotations that provide a reliable perceptual gold standard, and (iii) alignment of SEMs to human ratings, enabling systematic evaluation of both SFMs and the evaluation models themselves. This combination moves beyond synthetic benchmarks or zero-shot judging to provide a comprehensive and trustworthy framework for speech role-play assessment.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "speechdrame",
                    "human",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech role-play.</span>\nWe formalize <em class=\"ltx_emph ltx_font_italic\">speech role-play</em> as a conditional speech generation problem grounded in a character profile and scene description.\nGiven a dialogue context <math alttext=\"{\\bm{C}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{speech}}</annotation></semantics></math>, a character profile <math alttext=\"{\\bm{C}}_{\\text{profile}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>profile</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{profile}}</annotation></semantics></math>, and a scene specification <math alttext=\"{\\bm{C}}_{\\text{scene}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>scene</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{scene}}</annotation></semantics></math>, a <em class=\"ltx_emph ltx_font_italic\">speech foundation model</em> (SFM) generates a role-played speech response <math alttext=\"{\\bm{S}}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119930;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{S}}_{r}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech role-play evaluation can be framed in multiple ways depending on the available data and the desired granularity of analysis.\nWithin Speech-DRAME, we formalize two distinct strategies inspired by sociology theories<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Please refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A2\" title=\"Appendix B Theoretical Motivation for Dual Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> for detailed discussion of the dual evaluation design.</span></span></span>, including a top-down, archetype-oriented option using synthetic speech and a bottom-up, realism-oriented option from real human speech.\nRather than merging these into a single rubric, we treat them as separate perspectives, each with unique advantages and limitations.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarizes the key contrasts, while further implementation details are provided in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>-&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "refer",
                    "human",
                    "speechdrame",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS5\" title=\"F.5 Examples of Realism Role-play &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.5</span></a>). The second strategy adopts a <em class=\"ltx_emph ltx_font_italic\">bottom-up</em> design, motivated by real-world performance and real human speech.\nInstead of relying solely on stereotypes, this approach grounds evaluation in recordings from professional and non-professional speakers, drawn from realistic dialogue and media sources.\nJudgments in this setting are more fine-grained and multi-dimensional, addressing aspects such as prosodic dynamics, emotional fidelity, character consistency, and contextual fit.\nWhile this strategy requires more human annotation and careful curation, it provides benchmarks that reflect how role-play is perceived in practice.\nIt thus offers a richer framework for analyzing model outputs in settings where realism and subtlety are essential.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "model",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human Annotation.</span>\nAll collected audio undergoes a standardized annotation protocol: (1) an initial semantically-based screening to exclude invalid clips; (2) rubric-based scoring across multiple perceptual dimensions; and (3) meta-annotations such as confidence or prompt-quality judgments. Full rubrics are provided in the appendices&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A5\" title=\"Appendix E Annotation Guidelines for Archetype Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">E</span></a> &amp; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>All data collection follows in high standard ethical consideration as outlined in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A9\" title=\"Appendix I Annotation Ethics &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "audio",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Archetype Data Collection &#8227; 4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents a summary of the curated <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> dataset, where we further elaborate the details of archetype and realism evaluation data collection in following subsections, respectively. For benchmark purpose, we provide an official train-test split for the human annotated dataset for <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> so that following researchers can use the dataset to train and evaluate their SEMs for speech role-play.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "roleplay",
                    "respectively",
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Archetype evaluation adopts a top&#8211;down perspective from sociology and role theory&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goffman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib23\" title=\"\">1949</a>; Campbell, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib4\" title=\"\">2008</a>)</cite>, abstracting role-play into broad and recognizable categories.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Sources.</span> The data is collected from a range of speech foundation models, including both cascaded and end-to-end models. We limit that only audio prompts are used as the input but adopt different prompt strategies for best quality with regards to different models. Details about the selected models and prompting details are discussed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS4\" title=\"D.4 Candidate Models for Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "cascaded",
                    "audio",
                    "endtoend",
                    "quality",
                    "discussed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotation follows two stages: an initial semantic check (i.e., <span class=\"ltx_text ltx_font_italic\">Content Pass</span>) to filter out instruction non-compliance or audio defects, followed by rubric-based scoring on three dimensions: <em class=\"ltx_emph ltx_font_italic\">Audio Quality</em>, <em class=\"ltx_emph ltx_font_italic\">Human-likeness</em>, and <em class=\"ltx_emph ltx_font_italic\">Appropriateness</em>. Confidence judgments are also recorded. Detailed rubric scales are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A5\" title=\"Appendix E Annotation Guidelines for Archetype Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</p>\n\n",
                "matched_terms": [
                    "pass",
                    "content",
                    "audio",
                    "detailed",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype Evaluation Data Summary.</span>\nThe resulting archetype evaluation dataset comprises 552 distinct scenarios for both Mandarin and English, evaluated across eight models,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>The model list is discussed in Appdenix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS4\" title=\"D.4 Candidate Models for Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.4</span></a>. Note we only include seven models for English due to the deactivation of ChatGPT-4O advanced voice model.</span></span></span> yielding 8,280 samples. In total, the dataset are split into 6,780 for training and 1,500 for testing. For benchmarking, an additional 1,250 scenarios spanning seven role/scene categories are prepared to facilitate archetype role-play benchmarking. More detailed statistics and analyses are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10\" title=\"Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">J</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "models",
                    "model",
                    "archetype",
                    "detailed",
                    "discussed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real-world Recoding.</span>\nAs noted in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S2\" title=\"2 Related Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, most prior efforts have relied heavily on synthetic data, mostly from TTS systems based on curated professional role-play voices. While such data can achieve high fidelity, it often lacks the diversity and spontaneity of real-world delivery, limiting the generalizability of evaluation models. To address this gap, we expand realism evaluation beyond synthetic and curated voices by incorporating newly recorded speech from both professional and amateur voice actors. This design introduces a broader spectrum of delivery styles, from polished professional acting to more natural and less controlled amateur performance.</p>\n\n",
                "matched_terms": [
                    "sec",
                    "roleplay",
                    "based",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation Data Summary.</span>\nThe resulting realism evaluation dataset is constructed from a combination of curated media sources, including internal collections, public datasets (e.g., NCSSD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib40\" title=\"\">2024b</a>)</cite>), and original recordings collected for this project.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>All data were obtained under appropriate licenses and are legally available for research release.</span></span></span> We compile 15k annotated role-play samples: 9k in Mandarin and 5k in English. The dataset is divided into 12k samples for training, 2k mixed samples for the base test set, and 1k real-recording samples for the test set. The first test set supports general evaluation using our proposed pipeline with negative samples, while the second focuses exclusively on human speech performed by both professional and amateur voice actors. For benchmarking, 2,000 scenarios from the realism dataset are designated for realism role-play benchmarking. Detailed statistics and analyses are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11\" title=\"Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">K</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "detailed",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-shot and Few-shot ALLMs.</span>\nWe first evaluate the zero-shot capabilities of both proprietary and public models, including <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>, <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>, <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio</span>, and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>. All evaluations follow a unified prompting template that provides audio input alongside scene and character context.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio",
                    "gemini25pro",
                    "kimiaudio",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For proprietary models, we report the average of five independent runs to reduce variance and obtain more stable predictions. For open-source models, we observed weaker instruction-following ability and limited adaptation to unfamiliar tasks. To mitigate this, we did not request simultaneous prediction of multiple metrics. Instead, we queried one metric at a time. Following <cite class=\"ltx_cite ltx_citemacro_citet\">Song &amp; Bahri (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>); Lukasik et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>, we interpret model outputs through token distribution over discrete labels (<span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Finetuning Model: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>.</span>\nIn addition to zero-shot and few-shot evaluations, we fine-tune a dedicated evaluation model, denoted as <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> for archetype and realism role-play evaluation, respectively.\nWe initialize from <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio-7B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib11\" title=\"\">2024</a>)</cite>, which provides a strong foundation for multi-modal audio-text reasoning.\nTo better leverage the multiple human annotations available in our datasets, we adopt a sampling-based training strategy: at each step, one annotator label is randomly selected, following practices in label distribution learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Geng, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib22\" title=\"\">2016</a>; Wen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib67\" title=\"\">2023</a>)</cite>.\nThis stochastic supervision implicitly exposes the model to the empirical annotation distribution, while during inference we output the full distribution over discrete labels (e.g., <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score, consistent with recent decoding-based regression methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Song &amp; Bahri, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>; Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib70\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib64\" title=\"\">2025b</a>; Lukasik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>. We apply LoRA fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib28\" title=\"\">2022</a>)</cite>. All implementation details, including optimizer settings, training schedule, and sampling policy, are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS3\" title=\"L.3 Fine-tuning Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "respectively",
                    "model",
                    "human",
                    "archetype",
                    "drameeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further conduct ablation studies in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS4\" title=\"L.4 Fine-tuning Ablation Experiments &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.4</span></a>, examining: the choice of different base audio LLMs and full-parameter fine-tuning versus LoRA adaptation.\nThese analyses highlight the robustness of our sampling&#8211;expectation training strategy and clarify the trade-offs in model design.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics for Role-play Evaluation Model.</span> For most dimensions with 1-5 scale, we report Pearson correlation. For <span class=\"ltx_text ltx_font_italic\">Content Pass</span> estimation, we report accuracy.</p>\n\n",
                "matched_terms": [
                    "content",
                    "roleplay",
                    "pass",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype Evaluation.</span>\nThe results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T2\" title=\"Table 2 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reveal clear distinctions across evaluation settings. In the zero-shot condition, proprietary ALLMs such as <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> substantially outperform public models, achieving strong <span class=\"ltx_text ltx_font_italic\">content pass</span> accuracy (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>90%) and moderate correlations with other dimensions(<math alttext=\"0.43\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mn>0.43</mn><annotation encoding=\"application/x-tex\">0.43</annotation></semantics></math>&#8211;<math alttext=\"0.48\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mn>0.48</mn><annotation encoding=\"application/x-tex\">0.48</annotation></semantics></math>). By contrast, public models exhibit limited predictive ability, with correlations often below 0.2. Few-shot prompting provides modest gains for proprietary models, improving both accuracy and correlation (e.g., <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> rises from 0.480 to 0.493 average correlation). The most significant improvement arises from finetuning: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves an average correlation of 0.629, marking a large margin over zero-shot/few-shot models and demonstrating the effectiveness of task-specific supervision.</p>\n\n",
                "matched_terms": [
                    "models",
                    "pass",
                    "content",
                    "gemini25pro",
                    "results",
                    "archetype",
                    "drameeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T3\" title=\"Table 3 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> extends the analysis to fine-grained realism evaluation. Similar to archetype results, proprietary ALLMs lead the zero-shot setting, with <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> obtaining an average correlation of 0.390, compared to 0.284 for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>. Public models remain weak baselines. Few-shot prompting consistently improves performance: <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> gains +0.042 and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> gains +0.083 over their zero-shot averages, showing the benefit of in-context supervision. Once again, finetuning is most effective: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves 0.625 average correlation, higher than the best few-shot results. Importantly, the gains are consistent across all 10 realism dimensions, confirming that the sampling&#8211;expectation training strategy generalizes beyond simple classification to nuanced paralinguistic cues.</p>\n\n",
                "matched_terms": [
                    "models",
                    "gemini25pro",
                    "results",
                    "archetype",
                    "drameeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real Recording Test Set.</span>\nThe real-recording evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T4\" title=\"Table 4 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> is notably more challenging, with all models suffering substantial performance drops. Proprietary models in the zero-shot setting achieve only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.04&#8211;0.08 average correlation, and public models often fall below zero. Few-shot prompting yields small but noticeable improvements, especially for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> (0.226 avg.), suggesting that exemplars partially mitigate domain mismatch between synthetic and real human speech. Interestingly, while finetuning achieves the highest overall average (0.247), its relative advantage over few-shot prompting is narrower than in the synthetic setting, and some dimensions (e.g., <span class=\"ltx_text ltx_font_italic\">Character Identity</span>) remain weak. We emphasize that this observed gap between synthetic and real recordings is not merely an artifact but an intended feature of Speech-DRAME, designed to expose the mismatch and drive future research into bridging synthetic-to-real generalization.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speechdrame",
                    "avg",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall.</span> Across both archetype and realism evaluations, we observe three consistent trends: (i) zero-shot performance is limited, particularly for open-source models, (ii) few-shot prompting offers moderate but consistent gains, especially for proprietary ALLMs, and (iii) finetuning with task-specific supervision (<span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>) delivers the largest improvements, achieving state-of-the-art correlations across nearly all dimensions. Nevertheless, results on the real-recording test set underscore the gap between synthetic and real human role-play scenarios, motivating future work in data diversification and domain adaptation.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "models",
                    "human",
                    "results",
                    "archetype",
                    "drameeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on fine-tuned <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, we establish the DRAME-RoleBench to evaluate SFMs&#8217; role play ability. For archetype evaluation, 1,250 scenarios are constructed following the same data construction pipeline as the <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>. For realism evaluation, we reuse 2,000 scenarios in the <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> test set. We follow the same prompt strategy as outlined in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "drameeval",
                    "based",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, we consider a comprehensive set of candidate models, including six end-to-end models: <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib47\" title=\"\">2025b</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib75\" title=\"\">2024</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib71\" title=\"\">2025a</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Qwen3Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib72\" title=\"\">2025b</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">StepAudio2Mini</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib68\" title=\"\">2025</a>)</cite>; and ten cascaded models by combining two widely-used textual LLMs (i.e., <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib12\" title=\"\">2025</a>)</cite> and <span class=\"ltx_text ltx_font_typewriter\">Qwen3-30B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib73\" title=\"\">2025</a>)</cite>) and five recent TTS models (i.e., <span class=\"ltx_text ltx_font_typewriter\">F5TTS</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib8\" title=\"\">2024c</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">IndexTTS2</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib82\" title=\"\">2025b</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">MaskGCT</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib66\" title=\"\">2025d</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Vevo1.5</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib79\" title=\"\">2025b</a>)</cite>, and <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib17\" title=\"\">2024b</a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "models",
                    "cascaded",
                    "stepaudio2mini",
                    "qwen330binstruct",
                    "gemini25pro",
                    "qwen3omni",
                    "gpt4orealtime",
                    "glm4voice",
                    "endtoend",
                    "kimiaudio",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism evaluation.</span>\nRealism role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T6\" title=\"Table 6 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) presents a different landscape. Here, performance differences between end-to-end and cascaded systems are less pronounced. Several end-to-end models, such as <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>, reach average scores of 3.24, matching or even exceeding the best cascaded pipelines. Importantly, realism scores are more evenly distributed across prosodic, emotional, character, and contextual dimensions, with most models clustering between 2.9&#8211;3.2. This convergence reflects the difficulty of realism-style evaluation: since the data preparation emphasized contrast with authentic human speech, models are judged under similar conditions, leading to smaller observed gaps.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "cascaded",
                    "models",
                    "glm4voice",
                    "human",
                    "endtoend",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human alignment.</span>\nWe further conducted small-scale human studies with 10% of the benchmark data to verify the reliability of the benchmark. Archetype evaluation achieves a Spearman correlation of 0.706 between human judgments and <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, validating that the automatic evaluator closely tracks human preferences in this setting. Realism evaluation, however, achieves a more modest Spearman correlation of 0.375. This lower alignment is expected, since realism data construction did not prioritize fine-grained model separation but instead emphasized distinguishing human versus synthetic speech.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>We view this as evidence that realism presents a harder, less-saturated frontier. Rather than a limitation of DRAME-Eval, this highlights the challenge of designing evaluators that capture nuanced human perception, which motivates future work.</span></span></span> As a result, all models occupy a relatively narrow performance band, reducing sensitivity to model-level differences. We view this as evidence that realism evaluation highlights a harder, less saturated frontier where both SFMs and SEMs require further advances.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "models",
                    "model",
                    "human",
                    "archetype",
                    "drameeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, cascaded pipelines retain an edge in archetype scenarios, especially in content robustness and expressive fidelity, but end-to-end models are catching up quickly. In realism scenarios, the field remains unsolved: existing systems exhibit similar performance bands and evaluators show weaker alignment with human judgments, underscoring the need for improved realism-grounded data and evaluation. Together, these findings establish <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> as both a reliable testbed for archetype fidelity and a challenging benchmark for realism fidelity.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "cascaded",
                    "models",
                    "content",
                    "human",
                    "endtoend",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework for assessing spoken role-play through two complementary lenses: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em> (top-down) and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> (bottom-up, human-grounded). Our human-annotated <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> enables training and testing of SEMs, and our fine-tuned <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span> substantially outperforms zero-/few-shot ALLM judges. Building on this, <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> provides system-level comparison across various SFMs, revealing a consistent edge for cascaded pipelines on archetypes and a tighter spread under realism, where real human performance evaluation remain challenging. Together, these resources establish a reproducible, analyzable foundation for spoken role-play evaluation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>Limitations and future works are discussed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A13\" title=\"Appendix M Limitations and Future Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">M</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "cascaded",
                    "drameeval",
                    "human",
                    "archetype",
                    "speechdrame",
                    "discussed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments were conducted on widely available GPUs (NVIDIA H100). Random seeds are fixed across all runs to ensure consistency. By releasing datasets, models, and code, we aim to make all major results in this paper fully reproducible and facilitate future extensions of our framework.</p>\n\n",
                "matched_terms": [
                    "models",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline evaluators:</span> State-of-the-art audio LLMs (e.g., <span class=\"ltx_text ltx_font_typewriter\">GPT4o</span>, <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>) were included as zero-shot judges in our evaluation benchmark, enabling direct comparison with human-aligned SEMs.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "audio",
                    "qwen25omni",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation design and task construction:</span> <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> was used to assist with building instructions and summarizing story contexts. Drafted materials were subsequently refined by the authors and validated by human annotators.</p>\n\n",
                "matched_terms": [
                    "human",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Proprietary LLM outputs were used in a limited way (e.g., instruction building, story summarization, or baseline evaluations) and only for internal research purposes. Released datasets in Speech-DRAME are constructed from open-source models, licensed corpora, and original human recordings.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speechdrame",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, they mirror the dual nature of human role assessment, balancing scalability and comparability with depth and nuance. This theoretical grounding reinforces the design of Speech-DRAME as a framework that does not privilege one perspective over the other, but instead integrates both to reflect the full spectrum of how roles are evaluated in speech-based interaction.</p>\n\n",
                "matched_terms": [
                    "speechdrame",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our main formulation (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS1\" title=\"3.1 Problem Formulation &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) focuses on the <em class=\"ltx_emph ltx_font_italic\">single-turn</em> setting, where each response <math alttext=\"{\\bm{S}}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119930;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{S}}_{r}</annotation></semantics></math> is generated and evaluated independently.\nThis simplification enables clearer analysis of speech role-play quality and aligns with our bilingual annotation protocol.\nNevertheless, the same formulation extends naturally to multi-turn dialogues.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Archetype Evaluation follows a top&#8211;down perspective rooted in sociology and role theory, where social behavior is interpreted through shared stereotypes and generalized expectations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goffman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib23\" title=\"\">1949</a>; Campbell, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib4\" title=\"\">2008</a>)</cite>.\nThis strategy abstracts role-play into broad and easily recognizable archetypes, enabling scalable evaluation of whether a generated response aligns with salient role features.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Exclusion of fine-grained controls.</span>\nWe deliberately excluded explicit emotional states, prosodic instructions, or linguistic constraints.\nThese dimensions are better suited to realism-oriented or style-controlled evaluation, whereas archetype evaluation benefits from concrete, stereotype-level scenarios that sharpen judgments and improve human agreement.</p>\n\n",
                "matched_terms": [
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Pipeline overview.</span>\nAt a high level, archetype evaluation data are generated through a three-stage pipeline:\n(1) selecting keywords that specify role and scene;\n(2) expanding them into full prompts via structured templates; and\n(3) rendering prompts into speech with contemporary TTS systems.\nThis modular pipeline ensures both scalability and reproducibility, while keeping prompts vivid enough for reliable judgments. (4) Based on the generated data (paired scene and synthesized speech), expert human annotators are recruited for detailed annotation on semantic check (i.e., <span class=\"ltx_text ltx_font_italic\">Content Pass</span>) and three different spoken role-play dimensions (i.e., <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human-likeness</span>, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>).\nImplementation details, including template design, alternative prompting strategies, and annotation guidelines, are provided in the following subsections of this Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "pass",
                    "based",
                    "content",
                    "audio",
                    "human",
                    "archetype",
                    "detailed",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, archetype evaluation emphasizes breadth and recognizability over subtlety.\nBy grounding role-play in socially recognizable categories and stereotypical situations, it provides a practical balance between scalability and interpretability, complementing the more fine-grained realism evaluation described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS2\" title=\"3.2 Role-play Evaluation with Two Strategies &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">The Choice of Prompts.</span> We also experimented with broader, less specific templates that allowed more freedom for generative models but produced less discriminative evaluation results:</p>\n\n",
                "matched_terms": [
                    "models",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All steps are automatically processed by <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib12\" title=\"\">2025</a>)</cite>, but each output is manually verified to ensure high quality for downstream usage. We demonstrate an example for Roles: Occupation / Functional Role in Fictional or Fantasy as follows:</p>\n\n",
                "matched_terms": [
                    "quality",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate a diverse set of speech foundation models spanning both English and Mandarin. In total, we include 8 English and 7 Mandarin models, covering open-source and proprietary systems, and balancing end-to-end and cascaded architectures. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.T7\" title=\"Table 7 &#8227; D.4 Candidate Models for Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> provides the full list of models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "cascaded",
                    "endtoend"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Collection for Proprietary Models.</span> For proprietary systems, we adopt two collection strategies. For GPT-4o-realtime, responses are obtained directly via the <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-realtime</span> API. For Doubao and GPT-4o (Advanced Voice Mode), we collect outputs by recording from their official web interfaces. All recordings are captured through the interaction device itself, ensuring that no additional processing or distortion is introduced into the final audio.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio",
                    "gpt4orealtime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">End-to-End Model Generation.</span>\nFor end-to-end systems, the audio prompt is directly used as input. The system prompt is set to default values to ensure consistent speech generation quality.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "endtoend",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cascaded Model Generation.</span>\nFor cascaded systems, we only use Cosyvoice2 as the TTS model in our collection of data. We select <span class=\"ltx_text ltx_font_typewriter\">Qwen3-30B-Instruct</span> and <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> as the text LLM candidates&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib73\" title=\"\">2025</a>; Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib12\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "qwen330binstruct",
                    "cascaded",
                    "model",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2\" title=\"Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the internal annotation interface used for <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>.\nThe platform provides an integrated environment for both playback and evaluation of model-generated role-play speech samples.\nAnnotators first interact with the audio player interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf1\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>), which visualizes waveforms and spectrograms for multiple versions of the same prompt, enabling side-by-side comparison under a shared role instruction (e.g., firefighter, teacher, or actor).\nThen, they proceed to the annotation interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf2\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a>), where they assign scores for multiple dimensions such as <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, along with reasoning and confidence inputs.\nThis structured interface supports high-consistency, fine-grained evaluation of role-play behaviors in speech models.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "models",
                    "audio",
                    "human",
                    "archetype",
                    "likeness",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This appendix contains the full human annotation instructions used for archetype evaluation.</p>\n\n",
                "matched_terms": [
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Quality (No Context):</span> independence from content, focus on distortions or synthesis artifacts.</p>\n\n",
                "matched_terms": [
                    "content",
                    "audio",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human Likeness (No Context):</span> perceived naturalness and plausibility of human delivery.</p>\n\n",
                "matched_terms": [
                    "likeness",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comments for Audio Quality and Human Likeness are only required for scores <math alttext=\"\\leq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I3.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8804;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\leq 3</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "likeness",
                    "human",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Foreign language content:</span> If <em class=\"ltx_emph ltx_font_italic\">any</em> part before 30s contains a foreign language (even a single word/short phrase) in otherwise English audio, reject the entire clip. \n<br class=\"ltx_break\"/><em class=\"ltx_emph ltx_font_italic\">Exception:</em> Extremely common/recognizable borrowed words (e.g., &#8220;croissant&#8221;) may pass. Niche items (e.g., &#8220;coq au vin&#8221;) should be rejected.</p>\n\n",
                "matched_terms": [
                    "content",
                    "audio",
                    "pass"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><em class=\"ltx_emph ltx_font_italic\">Example (pass):</em> Same prompt; audio says: &#8220;It looks like your stylist put you through a wood chipper.&#8221; <math alttext=\"\\Rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I5.i1.I0.i2.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8658;</mo><annotation encoding=\"application/x-tex\">\\Rightarrow</annotation></semantics></math> content pass = true.</p>\n\n",
                "matched_terms": [
                    "content",
                    "audio",
                    "pass"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><em class=\"ltx_emph ltx_font_italic\">Example (pass):</em> &#8220;Oh no. Are you hurt? Don&#8217;t worry. We&#8217;ll get you taken care of, little one.&#8221; <math alttext=\"\\Rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I5.i2.I0.i2.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8658;</mo><annotation encoding=\"application/x-tex\">\\Rightarrow</annotation></semantics></math> content pass = true.</p>\n\n",
                "matched_terms": [
                    "content",
                    "pass"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Important:</span> Once an audio receives content pass = false, <em class=\"ltx_emph ltx_font_italic\">do not</em> rate rubrics; proceed to the next clip.</p>\n\n",
                "matched_terms": [
                    "content",
                    "audio",
                    "pass",
                    "rate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rubric Rating Instructions (for content pass = true).</span>\nWe shift focus from <em class=\"ltx_emph ltx_font_italic\">what</em> was said to <em class=\"ltx_emph ltx_font_italic\">how</em> it was said (vocal performance). Use gut intuition guided by rubric definitions. Score each audio <em class=\"ltx_emph ltx_font_italic\">in isolation</em> (no ranking across clips).</p>\n\n",
                "matched_terms": [
                    "content",
                    "audio",
                    "pass"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">Audio Quality</span> &amp; <span class=\"ltx_text ltx_font_bold\">Human Likeness</span>: comment only if score <math alttext=\"\\leq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I6.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8804;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\leq 3</annotation></semantics></math>; if 4 or 5, write &#8220;N/A&#8221;.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "likeness",
                    "human",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Pronunciation errors: penalize only on <span class=\"ltx_text ltx_font_bold\">Human Likeness</span> if they significantly affect comprehension; do not affect <span class=\"ltx_text ltx_font_bold\">Appropriateness</span>.</p>\n\n",
                "matched_terms": [
                    "likeness",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Independently</span> rate technical clarity (ignore content and fit). After scoring Audio Quality, <em class=\"ltx_emph ltx_font_italic\">disregard</em> its issues for the other two dimensions.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "content",
                    "audio",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Rate how likely an average listener would believe the clip is human-delivered (ignore Audio Quality issues and contextual fit).</p>\n\n",
                "matched_terms": [
                    "rate",
                    "audio",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Rate <em class=\"ltx_emph ltx_font_italic\">only</em> how well the <em class=\"ltx_emph ltx_font_italic\">delivery</em> fits the role/scene described in the prompt (tonal fit). Ignore Audio Quality and Human Likeness issues while scoring this dimension.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "audio",
                    "human",
                    "likeness",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realism Evaluation adopts a bottom&#8211;up perspective grounded in real human speech data, with the goal of capturing the nuanced delivery, prosody, and contextual fit that synthetic archetype prompts cannot fully represent.\nRather than relying on stereotypes, this strategy leverages real-world recordings and carefully constructed contrasts, enabling fine-grained assessment of model performance under realistic conditions.</p>\n\n",
                "matched_terms": [
                    "model",
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Retrieved character profiles.</span>\nEach audio segment is paired with a concise character description derived from the media context.\nThe detailed prompt design for generating these character profiles is provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS2\" title=\"F.2 Prompts for Character Profile Creation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative character profiles.</span>\nWe also create role-profile mismatches by resynthesizing transcripts with diverse TTS systems, thereby breaking the link between authentic delivery and intended role.\nWe explore both text-instruction-based prompting and comparisons across commercial and non-commercial TTS frameworks.\nDetailed model lists and parameter settings are documented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS6\" title=\"F.6 TTS Systems and Settings &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.6</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, real human ground-truth segments and systematically constructed negatives form a balanced dataset that supports realism-oriented evaluation.\nThe positives provide natural anchors of role-play performance, while the negatives introduce controlled mismatches that test whether evaluation models can distinguish real human speech from synthetic approximations.\nThis design ensures that realism evaluation emphasizes subtle paralinguistic and contextual cues, moving beyond intelligibility or content fidelity.</p>\n\n",
                "matched_terms": [
                    "content",
                    "roleplay",
                    "human",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Yunjin case study illustrates a fundamental difference between realism evaluation and archetype evaluation. Archetype prompts, as described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>, are constructed by pairing abstract role categories with synthetic contextual scenes. These cover a broad space of possible situations, ranging from modern occupations (e.g., firefighters) to fictional personas (e.g., cult leaders in martial arts fantasy) or social identities (e.g., overprotective parents, laid-back friends). While such prompts provide controlled and diverse coverage, they are ultimately template-based and rely on stereotypes to elicit role-play behaviors.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This distinction highlights the complementary strengths of the two evaluation paradigms. Archetype evaluation systematically explores a wide combinatorial space of roles and situations, but may lack the paralinguistic and contextual richness of real speech. Realism evaluation, on the other hand, anchors assessment in authentic delivery and contextual fit, providing a finer-grained lens on coherence and naturalness. Together, they ensure that role-play evaluation spans both broad stereotypical coverage and nuanced real-world plausibility.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For models using voice prompts, we use an inhouse enhancement model to first process the prompt audio so as to minimize the effect of audio events and background noise.</p>\n\n",
                "matched_terms": [
                    "models",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS1</span> (Phone&#8211;Cosy2 Pipeline).\nThe input is a phone sequence produced by a graphme-to-phoneme&#160;(G2P) front-end. A 0.4B GPT-2 model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib50\" title=\"\">2019</a>)</cite> predicts <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> S3-tokens, which are then passed through a flow-matching model to generate mel-spectrograms. A neural vocoder from <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> converts the mel features into audio. Both the GPT-2 and flow-matching modules are augmented with speaker embeddings for speaker control.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS3</span> (Embedding&#8211;AudioCode Pipeline).\nHere, the input is Qwen2.5 BPE embeddings, which a 0.4B GPT-2 model maps directly to 4,096-dimensional audio codes. These codes are decoded via a cascaded architecture: <span class=\"ltx_text ltx_font_typewriter\">RepCodec</span> decoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib32\" title=\"\">2024</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> content representation (with speaker embedding) <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DDIM</span> (denoising\ndiffusion implicit model) module&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jeong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib34\" title=\"\">2021</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DAC</span> latent representation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib37\" title=\"\">2023</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DAC</span> decoder. Conditioning is provided through prefix text embeddings (e.g., instructions expressing emotional states such as &#8220;angry, hateful: you betrayed me&#8230;&#8221;), along with speaker ID. The input format is <span class=\"ltx_text ltx_font_typewriter\">[spk_id, BPE embedding, audio codes]</span>.</p>\n\n",
                "matched_terms": [
                    "content",
                    "cascaded",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS4</span> (Instruction-Conditioned AudioCode Pipeline).\nThis model also employs a 0.4B GPT-2 to predict 4,096-dimensional audio codes, but conditions generation on instruction embeddings. A <span class=\"ltx_text ltx_font_typewriter\">CLIPTextEncoder</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib51\" title=\"\">2021</a>)</cite> converts instruction text into embeddings, which serve as a prefix, combined with speaker ID and phone sequence. The input format is <span class=\"ltx_text ltx_font_typewriter\">[spk_id, instruction embedding sequence, phone sequence, audio codes]</span>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Once familiar with the scene, annotators proceed to the detailed evaluation form shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.F4\" title=\"Figure 4 &#8227; F.7 Annotation Platform for Realism Evaluation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nThis interface is divided into two major sections: prosodic/emotional quality, higher-level coherence and semantic alignment.</p>\n\n",
                "matched_terms": [
                    "detailed",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the prompt information is <span class=\"ltx_text ltx_font_bold\">contradictory</span> or <span class=\"ltx_text ltx_font_bold\">insufficient</span> to evaluate (e.g., audio too short, traits mutually incompatible), mark the prompt quality accordingly and <em class=\"ltx_emph ltx_font_italic\">do not</em> force granular ratings that cannot be judged.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Use both sources:</span> Play the audio and skim the transcript together to assess delivery quality, expression, and alignment with character intent (do not penalize transcript mismatches).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Focus only on human speech:</span> Do not factor in background music/soundtracks, environmental or ambient noise, or non-speech audio events; these may reflect the scene but should not influence scoring of the spoken performance.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Guidance:</span> Rate low confidence for noisy/ambiguous audio, vague/contradictory traits, multiple dominant speakers, or hard-to-distinguish emotion/tone.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Better for SEM training.</span> Rubric-aligned labels let us <em class=\"ltx_emph ltx_font_italic\">train</em> and <em class=\"ltx_emph ltx_font_italic\">audit</em> SEMs, measure human alignment, and analyze error modes; judge outputs are harder to trust long-term as they change with model versions and prompts.</p>\n\n",
                "matched_terms": [
                    "model",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All human annotation in this work was conducted in accordance with standard ethical practices for research involving human annotators. Annotators were recruited via public hiring for Mandarin and internal lab hiring for English. Prior to participation, all annotators were provided with a clear description of the task, the type of data they would encounter, and their rights as participants, including the ability to opt out at any time without penalty.</p>\n\n",
                "matched_terms": [
                    "human",
                    "type"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Annotators gave informed consent before starting the tasks. They were compensated at a rate from $20 to $30 per hour, which corresponds to at least the local minimum wage and was applied uniformly regardless of whether their contributions were later filtered for quality.</p>\n\n",
                "matched_terms": [
                    "rate",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The archetype evaluation provides a detailed view of how speech foundation models (SFMs) perform when tasked with role-play scenarios grounded in stereotypical characters and situations. Human annotations cover four dimensions, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, and <span class=\"ltx_text ltx_font_italic\">Content Pass</span>, and the results reveal important trends, as well as challenges in interpreting scores across languages.</p>\n\n",
                "matched_terms": [
                    "models",
                    "roleplay",
                    "pass",
                    "content",
                    "audio",
                    "human",
                    "results",
                    "archetype",
                    "likeness",
                    "detailed",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Within Mandarin from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, the relative rankings of models are clear. <span class=\"ltx_text ltx_font_typewriter\">Doubao</span> stands out as the strongest performer across all four dimensions, reaching near-ceiling levels in both <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> (4.37) and <span class=\"ltx_text ltx_font_italic\">Content Pass</span> (0.98). It also leads in role-related dimensions such as <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, indicating balanced competence across linguistic accuracy, delivery, and expressiveness. <span class=\"ltx_text ltx_font_typewriter\">G-CosyVoice2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-CosyVoice2</span> follow closely, especially in audio quality, though they lag behind in role appropriateness. <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> achieve mid-range results, reflecting higher variance across samples, while <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> shows competitive content understanding but lower naturalness and appropriateness. <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> consistently ranks the lowest, with large variance, underscoring instability and weaker alignment with archetypal role expectations.</p>\n\n",
                "matched_terms": [
                    "models",
                    "quality",
                    "pass",
                    "qcosyvoice2",
                    "content",
                    "audio",
                    "gpt4orealtime",
                    "glm4voice",
                    "gcosyvoice2",
                    "results",
                    "human",
                    "likeness",
                    "kimiaudio",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The picture shifts in English, elaborated in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Here, <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> emerges as the most effective model by a wide margin, achieving top scores in both <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> (3.46) and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> (3.49), as well as high content pass rates (0.97). This reflects its strong grounding in English discourse norms and role adherence. In contrast, <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> variants perform well in <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> but are weaker in role alignment, highlighting a trade-off between acoustic fidelity and role-play embodiment. Other models such as <span class=\"ltx_text ltx_font_typewriter\">Doubao</span>, <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span>, and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> achieve moderate results, while <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> trail far behind, showing limited robustness in English scenarios. Taken together, the English results reveal both a sharper separation across models and a wider variance in appropriateness scores compared to Mandarin.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "models",
                    "quality",
                    "pass",
                    "content",
                    "audio",
                    "model",
                    "gpt4orealtime",
                    "glm4voice",
                    "results",
                    "human",
                    "likeness",
                    "kimiaudio",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although cross-lingual comparisons are invalid, this does not diminish the utility of archetype evaluation for role-play benchmarking. In fact, the observed inconsistencies reinforce a central point: successful role-play requires models to adapt to the cultural and linguistic norms of the target language community. A system that performs well in Mandarin but poorly in English cannot be considered universally capable, since end users will evaluate appropriateness and naturalness according to their native expectations. Thus, archetype evaluation highlights not only model-specific strengths and weaknesses but also the need for culturally grounded evaluation pipelines.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "models",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another notable pattern is the trade-off between acoustic quality and role embodiment. CosyVoice2 variants achieve strong audio quality but weaker human likeness and appropriateness, whereas <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> prioritizes role coherence at the expense of audio fidelity. This suggests that future systems must better integrate high-quality synthesis with nuanced role adherence. Similarly, the consistently lower scores for imaginative and identity-driven roles underscore the limitations of current SFMs in capturing subtle sociocultural expectations and narrative context.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "likeness",
                    "human",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In sum, archetype evaluation reveals three major findings:\n(1) Mandarin and English annotations cannot be directly compared, due to linguistic and cultural annotation inconsistencies, consistent with prior evidence from SVCC 2023.\n(2) Within each language, clear model rankings emerge, <span class=\"ltx_text ltx_font_typewriter\">Doubao</span> dominates in Mandarin, while <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> excels in English.\n(3) Domain sensitivity remains a challenge, particularly for imaginative or culturally rich roles, where scores are substantially lower and inter-model variance is higher.</p>\n\n",
                "matched_terms": [
                    "model",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, the consistently high agreement confirms that the rubric isolates perceptually coherent factors, so learned evaluators can target them reliably.\nSecond, the gated behavior of <em class=\"ltx_emph ltx_font_italic\">Intensity/Transition</em> motivates a <em class=\"ltx_emph ltx_font_italic\">hierarchical</em> evaluator: verify <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> before scoring graded expressivity, or model the joint structure with conditional heads.\nThird, <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> remains challenging; models may benefit from explicit character-trait priors, long-term context aggregation, or contrastive supervision between matched and mismatched traits.\nFinally, prosody-focused objectives (e.g., pitch/rhythm/stress control) are likely to yield the largest quality gains for amateur-style speech.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For both zeroshot and fewshot experiments, we adopt a streamlined version of the annotation guidelines to ensure consistency with the human annotation process. The exact rubric prompts for archetype and realism evaluation used are provided below:</p>\n\n",
                "matched_terms": [
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and fewshot settings, we condition the model with the rubric prompt and then append few-shot demonstration triplets (input, audio, gold JSON) to mirror the human annotation protocol. As shown in Listing&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#LST2\" title=\"Listing 2 &#8227; L.1 Zeroshot/Fewshot Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, We use <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> for these few-shot exemplars and require a minified JSON output consistent with the rubric schema.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "human",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and few-shot experiments on commercial models such as <span class=\"ltx_text ltx_font_typewriter\">Gemini</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o</span>, we report results as the average of five independent runs to reduce randomness and obtain more stable predictions. In contrast, for open-source models, we observed that their instruction-following ability is relatively weak. To mitigate this, we prompt them to predict one dimension at a time rather than all dimensions jointly. Specifically, for each metric, after showing the rubric prompt, we request completion in a fixed template and extract the probability distribution over the discrete tokens <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>. The final score is then computed from these probabilities, which we found makes open-source models usable to some extent under this setting.</p>\n\n",
                "matched_terms": [
                    "models",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For few-shot experiments, we randomly sample three exemplars from the training set, with each exemplar chosen such that its human scores are close to the rounded average across all annotators. These exemplars are appended as demonstrations in the prompt. As with the zeroshot case, we run five trials for commercial models and report the averaged results for evaluation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "human",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Effect of repeated runs.\nWhen comparing single-run decoding (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot-1run</span>) against averaging over five runs (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot</span>), we see consistent gains across nearly all dimensions (average score improves from 0.353 to 0.390). This suggests that sampling variance plays a substantial role in the reliability of large language model (LLM) evaluators, and averaging multiple outputs reduces this variance, producing more stable correlation with human annotations.</p>\n\n",
                "matched_terms": [
                    "model",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For supervised fine-tuning, we primarily adopted a parameter-efficient strategy using LoRA, applied to the <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio-7B-Instruct</span> model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib11\" title=\"\">2024</a>)</cite>. LoRA was configured with a rank of 16, <math alttext=\"\\alpha=32\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>32</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=32</annotation></semantics></math>, and a dropout of 0.1, targeting the major projection layers, and trained for 10,000 steps to ensure stable convergence. The training pipeline employed a per-device batch size of 4 with gradient accumulation of 4 steps (effective batch size 32), AdamW optimization with cosine scheduling, a peak learning rate of <math alttext=\"5\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\times 10^{-5}</annotation></semantics></math>, weight decay of 0.01, and a warmup strategy defined by 500 steps. Gradient clipping with a norm of 1.0, <span class=\"ltx_text ltx_font_typewriter\">bf16</span> precision, and gradient checkpointing were enabled to stabilize training and reduce memory overhead. Distributed training was performed on 2xH100 GPUs using DeepSpeed (ZeRO stage-1).</p>\n\n",
                "matched_terms": [
                    "rate",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the archetype test set, all fine-tuned variants achieve strong <span class=\"ltx_text ltx_font_italic\">Content Pass</span> accuracy above 92%. Differences appear mainly in correlation-based metrics. Here, <span class=\"ltx_text ltx_font_typewriter\">Base-FT</span> yields the highest score on <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> delivers the strongest performance in <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> and the best overall average (0.629). This suggests that instruction-tuned checkpoints paired with parameter-efficient adaptation can better capture higher-level perceptual traits.</p>\n\n",
                "matched_terms": [
                    "pass",
                    "content",
                    "audio",
                    "human",
                    "archetype",
                    "likeness",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> is competitive on in-domain evaluation, it suffers a substantial drop when applied to real recordings. By contrast, <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> offers a more robust balance: it maintains reasonable performance on archetype and basic sets, while significantly outperforming other settings on real-world data. Based on this robustness, we adopt <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> as our final configuration for subsequent experiments.</p>\n\n",
                "matched_terms": [
                    "based",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the comprehensive design of Speech-DRAME, several limitations remain.\nFirst, while our dual evaluation paradigm combines top-down archetype assessment with bottom-up realism evaluation, both strategies are constrained by the quality and scope of available data. Archetype evaluation inevitably relies on simplified stereotypes, which can overlook subtler paralinguistic and contextual cues. Realism evaluation, though richer, still struggles with domain mismatch between synthetic data and real human recordings, as evidenced by the performance drop observed in the real-recording test set. This highlights the persistent gap between benchmark-driven evaluation and real-world variability in speech role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "human",
                    "archetype",
                    "speechdrame",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Third, while DRAME-Eval substantially outperforms zero-shot and few-shot ALLMs, its reliance on fine-tuning from a strong base model raises questions of accessibility and scalability. Future work should explore lightweight adaptation techniques, semi-supervised training with partially annotated data, and leveraging emergent capabilities of open-source multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "drameeval",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, although we validated DRAME-Eval against human annotations, alignment under realism settings remains modest. This suggests that even human-annotated benchmarks are insufficiently fine-grained to capture the full complexity of human perception in speech role-play. Richer annotation protocols, more diverse speaker populations, and integration of perceptual psychology insights could further improve evaluation fidelity.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "drameeval",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Looking forward, we see several promising research directions. We intentionally restrict evaluation to single-turn responses for clarity and tractability. However, multi-turn extensions that capture narrative progression, long-term consistency, and dynamic emotion shifts remain a natural next step. Incorporating multilingual and cross-cultural datasets will enhance generality and fairness. On the modeling side, exploring hybrid evaluation strategies that combine discrete rubrics with continuous perceptual signals (e.g., prosody embeddings or affective measures) may better bridge the gap between automatic and human judgments. Finally, integrating Speech-DRAME into reinforcement learning pipelines as a reward model could help drive the next generation of expressive, human-aligned speech role-play systems.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "speechdrame",
                    "model",
                    "human"
                ]
            }
        ]
    },
    "S6.T6": {
        "caption": "Table 6: Speech-DRAME benchmark for realism role-play. Benchmark results are calculated based on DRAME-Eval discussed in Sec.5.1. G and Q in the cascaded type refer to Gemini2.5Pro and Qwen3-30B-Instruct, respectively. Please refer to Sec.6 for detailed models.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" rowspan=\"2\">Type</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" rowspan=\"2\">Model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" colspan=\"3\">Prosodic Dynamics</td>\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" colspan=\"3\">Emotional Expressiveness</td>\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" colspan=\"2\">Character Consistency</td>\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" colspan=\"2\">Contextual Relevance</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\">Avg.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Pitch</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Rynthm</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Stress</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Accuracy</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Intensity</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Transition</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Identity</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Traits</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Local</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Global</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" rowspan=\"6\">End-to-end</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.22</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.94</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.03</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.09</td>\n<td class=\"ltx_td ltx_align_center\">2.98</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">3.34</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">3.12</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">3.03</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.35</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">3.24</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">3.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">3.12</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">3.26</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">3.12</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">3.34</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">3.24</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Qwen3Omni</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.15</td>\n<td class=\"ltx_td ltx_align_center\">3.03</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">StepAudio2Mini</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.81</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.97</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.11</td>\n<td class=\"ltx_td ltx_align_center\">2.98</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\" rowspan=\"10\">Cascaded</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">Q-F5TTS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2.96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.20</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Q-IndexTTS2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">3.36</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">3.33</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.33</td>\n<td class=\"ltx_td ltx_align_center\">3.23</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Q-MaskGCT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.60</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.53</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.70</td>\n<td class=\"ltx_td ltx_align_center\">2.63</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Q-Vevo1.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.93</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.93</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.81</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.01</td>\n<td class=\"ltx_td ltx_align_center\">2.91</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Q-CosyVoice2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.25</td>\n<td class=\"ltx_td ltx_align_center\">3.15</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">G-F5TTS</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.30</td>\n<td class=\"ltx_td ltx_align_center\">3.21</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">G-IndexTTS2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">3.24</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.30</td>\n<td class=\"ltx_td ltx_align_center\">3.22</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">G-MaskGCT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.08</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.98</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.09</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.29</td>\n<td class=\"ltx_td ltx_align_center\">3.20</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">G-Vevo1.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.97</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.04</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.83</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">2.86</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">3.04</td>\n<td class=\"ltx_td ltx_align_center\">2.95</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">G-CosyVoice2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">3.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">3.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">3.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">3.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">3.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">2.97</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">3.22</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">3.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">3.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">3.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3.18</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "roleplay",
            "cascaded",
            "models",
            "type",
            "pitch",
            "identity",
            "qmaskgct",
            "consistency",
            "gvevo15",
            "gmaskgct",
            "calculated",
            "prosodic",
            "gpt4orealtime",
            "glm4voice",
            "dynamics",
            "qindextts2",
            "realism",
            "based",
            "qwen330binstruct",
            "emotional",
            "qvevo15",
            "gemini25pro",
            "expressiveness",
            "character",
            "rynthm",
            "stress",
            "accuracy",
            "gf5tts",
            "avg",
            "stepaudio2mini",
            "refer",
            "relevance",
            "gindextts2",
            "qf5tts",
            "intensity",
            "qwen3omni",
            "transition",
            "gcosyvoice2",
            "results",
            "traits",
            "kimiaudio",
            "discussed",
            "benchmark",
            "respectively",
            "please",
            "speechdrame",
            "global",
            "qwen25omni",
            "qcosyvoice2",
            "local",
            "contextual",
            "model",
            "endtoend",
            "sec",
            "drameeval",
            "detailed"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism evaluation.</span>\nRealism role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T6\" title=\"Table 6 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) presents a different landscape. Here, performance differences between end-to-end and cascaded systems are less pronounced. Several end-to-end models, such as <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>, reach average scores of 3.24, matching or even exceeding the best cascaded pipelines. Importantly, realism scores are more evenly distributed across prosodic, emotional, character, and contextual dimensions, with most models clustering between 2.9&#8211;3.2. This convergence reflects the difficulty of realism-style evaluation: since the data preparation emphasized contrast with authentic human speech, models are judged under similar conditions, leading to smaller observed gaps.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Role-play has become a key testbed for generative models, expanding from text-only dialogue to multimodal interaction. Extending role-play to speech captures prosody, emotion, and delivery, but also poses new evaluation challenges. Current pipelines often use audio large language models (ALLMs) as zero-shot judges, which miss paralinguistic cues, collapse multiple aspects into coarse scores, and rely on synthetic speech references that fail to reflect real-world roles. We present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework that contributes at three levels: (i) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-EvalBench</em>, an evaluation benchmark with bilingual human-annotated data and protocols for training and testing speech evaluation models&#160;(SEMs), (ii) <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, a fine-tuned evaluation model, which substantially outperforms zero-shot and few-shot ALLMs, and (iii) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-RoleBench</em>, a speech role-play benchmark that leverages <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> as an automatic judge to compare speech foundation models (SFMs). Speech-DRAME distinguishes between two complementary evaluation strategies: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>, a top-down approach measuring adherence to broad role archetypes, and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em>, a bottom-up approach grounded in real human speech that emphasizes nuanced role quality. Compared to zero-shot ALLM judges, <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves stronger agreement with human ratings (Pearson correlation from 0.480 to 0.629 in archetypes, and 0.390 to 0.625 in realism). By integrating transparent benchmark resources, modeling approaches, and system-level evaluation, Speech-DRAME provides the first comprehensive, reproducible foundation for assessing spoken role-play.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "roleplay",
                    "models",
                    "realism",
                    "drameeval",
                    "model",
                    "speechdrame"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Role-play has been recently recognized a cornerstone of interactive systems, from initial text-based focuses to the growing ecosystem of multimodal dialogue agents&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib6\" title=\"\">2024a</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib59\" title=\"\">2024</a>; Dai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib13\" title=\"\">2025</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib43\" title=\"\">2024</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>)</cite>. With the rapid advancement of large language models (LLMs), role-play has evolved beyond scripted interactions into dynamic character-driven conversations, supporting applications in education, entertainment, and human&#8211;AI collaboration&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib69\" title=\"\">2024</a>; Agatsuma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib1\" title=\"\">2024</a>; Fung &amp; Laing, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib20\" title=\"\">2024</a>)</cite>. Extending role-play to speech-based interaction offers an even richer communication channel, capturing prosody, emotion, and character consistency in ways that text alone cannot&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "consistency",
                    "models",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">it lacks sensitivity to paralinguistic cues such as intonation, pacing, and emotional dynamics that define role delivery&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>; Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib81\" title=\"\">2025a</a>)</cite>,</p>\n\n",
                "matched_terms": [
                    "dynamics",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome these limitations, we present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span> (Speech Detailed Role-play Assessment with Modeling and Evaluation), a unified framework that moves beyond synthetic or judge-only pipelines. Speech-DRAME is distinguished by three aspects: [i] a dual evaluation paradigm that balances archetype-driven generality with human-grounded realism (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS2\" title=\"3.2 Role-play Evaluation with Two Strategies &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), [ii] human annotations that provide perceptual grounding absent in prior resources (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4\" title=\"4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), and [iii] evaluation-model alignment that enables not only benchmarking of speech foundation models (SFMs) but also systematic analysis of the evaluation models (SEMs) themselves (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5\" title=\"5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). These aspects establish Speech-DRAME as a comprehensive and trustworthy framework for measuring and improving speech role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "models",
                    "realism",
                    "sec",
                    "speechdrame",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, Speech-DRAME contributes at three levels: (i) <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>, an evaluation benchmark consisting of human-annotated datasets for archetype and realism role-play; (ii) <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span>, a fine-tuned SEM trained on EvalBench that surpasses zero-shot and few-shot ALLMs; and (iii) <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span>, a speech role-play benchmark that leverages DRAME-Eval as an automatic judge to systematically compare SFMs. By combining dual evaluation strategies, human-anchored supervision, and evaluation-model alignment, Speech-DRAME delivers systematic insights into both role-play systems and evaluation models themselves.\nOur contributions are four-fold:</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "roleplay",
                    "models",
                    "realism",
                    "drameeval",
                    "speechdrame"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a <span class=\"ltx_text ltx_font_bold\">formal task definition</span> of speech role-play and its evaluation via speech foundation models (SFMs) for generation and speech evaluation models (SEMs) for assessment, unifying linguistic and paralinguistic dimensions.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span>, a fine-tuned SEM trained on EvalBench that substantially outperforms zero-shot and few-shot ALLMs, demonstrating the importance of human supervision for evaluation models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "drameeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We establish <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span>, a speech role-play benchmark that leverages DRAME-Eval as an automatic judge to systematically compare both end-to-end and cascaded SFMs across multiple role quality dimensions.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Relevant codes, pre-trained models, and datasets will be open-sourced at \n<br class=\"ltx_break\"/><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/Anuttacon/speech_drame\" title=\"\">https://github.com/Anuttacon/speech_drame</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "roleplay",
                    "cascaded",
                    "models",
                    "endtoend",
                    "drameeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through this work, we aim to move from acting in text to acting in speech, equipping the community with the models, evaluation strategies, and benchmarks needed to measure and improve speech role-play.</p>\n\n",
                "matched_terms": [
                    "models",
                    "roleplay"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-based role-play evaluation.</span>\nEarly research focused on text-only dialogue benchmarks. Datasets such as RoleLLM, CharacterBox, and RoleMRC <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib62\" title=\"\">2024</a>; Tu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib60\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib61\" title=\"\">2025a</a>; Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib53\" title=\"\">2023</a>; Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib41\" title=\"\">2025</a>)</cite> test whether models can sustain persona consistency and character fidelity, using LLM-as-judge pipelines for scalability. However, studies like PersonaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib81\" title=\"\">2025a</a>)</cite> have shown that LLM-as-judge approaches are far from perfect, struggling with tasks as basic as tracking speaker identity. This highlights broader concerns about reliability even in text settings, which become only more severe when extended to speech, where prosody, emotion, and vocal style play a central role.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "models",
                    "identity",
                    "consistency",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-native benchmarks.</span>\nMore recently, a number of resources have been introduced to evaluate spoken role-play agents. OmniCharacter-10K <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>)</cite> emphasizes character voice traits, while others&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib39\" title=\"\">2025</a>; Hou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib27\" title=\"\">2025</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib15\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib7\" title=\"\">2024b</a>)</cite> extend evaluation toward vocal communication, assistant capability, and multi-turn realism. Omni models such as StepAudio and KimiAudio <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite> also include role-play scenarios in their evaluation. SpeechRole and VStyle <cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>; Zhan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib76\" title=\"\">2025</a>)</cite> are closer to our work, offering datasets that test interaction ability, expressiveness, and fidelity. However, these resources are primarily built from LLM-generated dialogues and TTS outputs, with limited human validation and a heavy reliance on proprietary ALLM-as-judge pipelines. As a result, they lack grounding in diverse human speech and typically apply only coarse rubrics.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "models",
                    "realism",
                    "character",
                    "kimiaudio",
                    "expressiveness",
                    "traits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Expressive TTS evaluation.</span>\nRecent work has explored evaluation of instruction-following and style-controlled TTS. InstructTTSEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite> explicitly includes role-play scenarios, while EmergentTTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">(Manku et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib45\" title=\"\">2025</a>)</cite> and discrete speech LM benchmarks <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang &amp; Sz&#233;kely, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib63\" title=\"\">2024</a>)</cite> emphasize prosody, spontaneity, and speaker consistency. These efforts push beyond intelligibility and naturalness, but still center on synthetic outputs rather than real human role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "consistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our distinction.</span>\nIn contrast, Speech-DRAME unifies these considerations through three elements: (i) a dual evaluation paradigm that combines archetype-driven generality with realism grounded in authentic human speech, (ii) human annotations that provide a reliable perceptual gold standard, and (iii) alignment of SEMs to human ratings, enabling systematic evaluation of both SFMs and the evaluation models themselves. This combination moves beyond synthetic benchmarks or zero-shot judging to provide a comprehensive and trustworthy framework for speech role-play assessment.</p>\n\n",
                "matched_terms": [
                    "models",
                    "roleplay",
                    "realism",
                    "speechdrame"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech role-play.</span>\nWe formalize <em class=\"ltx_emph ltx_font_italic\">speech role-play</em> as a conditional speech generation problem grounded in a character profile and scene description.\nGiven a dialogue context <math alttext=\"{\\bm{C}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{speech}}</annotation></semantics></math>, a character profile <math alttext=\"{\\bm{C}}_{\\text{profile}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>profile</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{profile}}</annotation></semantics></math>, and a scene specification <math alttext=\"{\\bm{C}}_{\\text{scene}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>scene</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{scene}}</annotation></semantics></math>, a <em class=\"ltx_emph ltx_font_italic\">speech foundation model</em> (SFM) generates a role-played speech response <math alttext=\"{\\bm{S}}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119930;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{S}}_{r}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "model",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech role-play evaluation can be framed in multiple ways depending on the available data and the desired granularity of analysis.\nWithin Speech-DRAME, we formalize two distinct strategies inspired by sociology theories<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Please refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A2\" title=\"Appendix B Theoretical Motivation for Dual Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> for detailed discussion of the dual evaluation design.</span></span></span>, including a top-down, archetype-oriented option using synthetic speech and a bottom-up, realism-oriented option from real human speech.\nRather than merging these into a single rubric, we treat them as separate perspectives, each with unique advantages and limitations.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarizes the key contrasts, while further implementation details are provided in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>-&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "refer",
                    "roleplay",
                    "speechdrame",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>). The first strategy follows a <em class=\"ltx_emph ltx_font_italic\">top-down</em> design, inspired by prior text-based role-play benchmarks.\nHere, role-play is judged with respect to <em class=\"ltx_emph ltx_font_italic\">stereotypical archetypes</em> (e.g., &#8220;firefighter&#8221; or &#8220;ER doctor&#8221;), using scene contexts (e.g., &#8220;comfort a child trapped in fire&#8221; or &#8220;ask for help in a serious operation&#8221;).\nThis approach provides accessible, general-purpose scoring that is scalable to a wide range of scenarios, making it suitable for large-scale benchmarking.\nHowever, its reliance on broad stereotypes and global impressions limits its ability to capture fine-grained aspects of delivery (e.g., subtle prosodic shifts, nuanced emotional control)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Holt &amp; Lotto, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib26\" title=\"\">2010</a>)</cite>.\nIn practice, this strategy mirrors simplified annotation settings and is particularly useful when annotation resources are limited&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib21\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "roleplay",
                    "emotional",
                    "prosodic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS5\" title=\"F.5 Examples of Realism Role-play &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.5</span></a>). The second strategy adopts a <em class=\"ltx_emph ltx_font_italic\">bottom-up</em> design, motivated by real-world performance and real human speech.\nInstead of relying solely on stereotypes, this approach grounds evaluation in recordings from professional and non-professional speakers, drawn from realistic dialogue and media sources.\nJudgments in this setting are more fine-grained and multi-dimensional, addressing aspects such as prosodic dynamics, emotional fidelity, character consistency, and contextual fit.\nWhile this strategy requires more human annotation and careful curation, it provides benchmarks that reflect how role-play is perceived in practice.\nIt thus offers a richer framework for analyzing model outputs in settings where realism and subtlety are essential.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "contextual",
                    "consistency",
                    "emotional",
                    "model",
                    "character",
                    "prosodic",
                    "dynamics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Archetype Data Collection &#8227; 4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents a summary of the curated <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> dataset, where we further elaborate the details of archetype and realism evaluation data collection in following subsections, respectively. For benchmark purpose, we provide an official train-test split for the human annotated dataset for <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> so that following researchers can use the dataset to train and evaluate their SEMs for speech role-play.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "roleplay",
                    "realism",
                    "respectively"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Sources.</span> The data is collected from a range of speech foundation models, including both cascaded and end-to-end models. We limit that only audio prompts are used as the input but adopt different prompt strategies for best quality with regards to different models. Details about the selected models and prompting details are discussed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS4\" title=\"D.4 Candidate Models for Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "cascaded",
                    "endtoend",
                    "discussed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype Evaluation Data Summary.</span>\nThe resulting archetype evaluation dataset comprises 552 distinct scenarios for both Mandarin and English, evaluated across eight models,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>The model list is discussed in Appdenix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS4\" title=\"D.4 Candidate Models for Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.4</span></a>. Note we only include seven models for English due to the deactivation of ChatGPT-4O advanced voice model.</span></span></span> yielding 8,280 samples. In total, the dataset are split into 6,780 for training and 1,500 for testing. For benchmarking, an additional 1,250 scenarios spanning seven role/scene categories are prepared to facilitate archetype role-play benchmarking. More detailed statistics and analyses are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10\" title=\"Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">J</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "roleplay",
                    "model",
                    "detailed",
                    "discussed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realism evaluation adopts a bottom&#8211;up perspective, emphasizing authentic delivery, prosody, and contextual fit.</p>\n\n",
                "matched_terms": [
                    "contextual",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Sources and Negative Samples.</span>\nWe curate speech from (i) ground-truth media segments, (ii) retrieved character profiles, and (iii) generated local scene descriptions. To stress-test evaluation models, we construct contrastive mismatches: (i) negative local scenes with varying degrees of mismatch, and (ii) negative character profiles created by re-synthesizing transcripts with TTS systems. Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6\" title=\"Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F</span></a> details the prompt design and TTS generation details.</p>\n\n",
                "matched_terms": [
                    "models",
                    "local",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotators judge whether performances align with the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em>, using rubrics for <em class=\"ltx_emph ltx_font_italic\">Prosodic Dynamics (i.e., Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis)</em>, <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness (with progressive gating over three sub-dimensions: Emotion Accuracy, Emotion Intensity, Emotion Transition)</em>, <em class=\"ltx_emph ltx_font_italic\">Character Consistency (i.e., Voice Identity Matching and Traits Embodiment)</em>, <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance (i.e., Local Scene Fit and Global Story Coherence)</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>. A prompt quality check and confidence ratings are also collected. Full scales and gating details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "pitch",
                    "identity",
                    "relevance",
                    "local",
                    "consistency",
                    "emotional",
                    "contextual",
                    "character",
                    "accuracy",
                    "prosodic",
                    "intensity",
                    "transition",
                    "stress",
                    "expressiveness",
                    "dynamics",
                    "traits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real-world Recoding.</span>\nAs noted in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S2\" title=\"2 Related Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, most prior efforts have relied heavily on synthetic data, mostly from TTS systems based on curated professional role-play voices. While such data can achieve high fidelity, it often lacks the diversity and spontaneity of real-world delivery, limiting the generalizability of evaluation models. To address this gap, we expand realism evaluation beyond synthetic and curated voices by incorporating newly recorded speech from both professional and amateur voice actors. This design introduces a broader spectrum of delivery styles, from polished professional acting to more natural and less controlled amateur performance.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "models",
                    "realism",
                    "based",
                    "sec"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation Data Summary.</span>\nThe resulting realism evaluation dataset is constructed from a combination of curated media sources, including internal collections, public datasets (e.g., NCSSD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib40\" title=\"\">2024b</a>)</cite>), and original recordings collected for this project.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>All data were obtained under appropriate licenses and are legally available for research release.</span></span></span> We compile 15k annotated role-play samples: 9k in Mandarin and 5k in English. The dataset is divided into 12k samples for training, 2k mixed samples for the base test set, and 1k real-recording samples for the test set. The first test set supports general evaluation using our proposed pipeline with negative samples, while the second focuses exclusively on human speech performed by both professional and amateur voice actors. For benchmarking, 2,000 scenarios from the realism dataset are designated for realism role-play benchmarking. Detailed statistics and analyses are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11\" title=\"Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">K</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-shot and Few-shot ALLMs.</span>\nWe first evaluate the zero-shot capabilities of both proprietary and public models, including <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>, <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>, <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio</span>, and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>. All evaluations follow a unified prompting template that provides audio input alongside scene and character context.</p>\n\n",
                "matched_terms": [
                    "models",
                    "character",
                    "gemini25pro",
                    "kimiaudio",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For proprietary models, we report the average of five independent runs to reduce variance and obtain more stable predictions. For open-source models, we observed weaker instruction-following ability and limited adaptation to unfamiliar tasks. To mitigate this, we did not request simultaneous prediction of multiple metrics. Instead, we queried one metric at a time. Following <cite class=\"ltx_cite ltx_citemacro_citet\">Song &amp; Bahri (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>); Lukasik et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>, we interpret model outputs through token distribution over discrete labels (<span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Finetuning Model: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>.</span>\nIn addition to zero-shot and few-shot evaluations, we fine-tune a dedicated evaluation model, denoted as <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> for archetype and realism role-play evaluation, respectively.\nWe initialize from <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio-7B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib11\" title=\"\">2024</a>)</cite>, which provides a strong foundation for multi-modal audio-text reasoning.\nTo better leverage the multiple human annotations available in our datasets, we adopt a sampling-based training strategy: at each step, one annotator label is randomly selected, following practices in label distribution learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Geng, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib22\" title=\"\">2016</a>; Wen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib67\" title=\"\">2023</a>)</cite>.\nThis stochastic supervision implicitly exposes the model to the empirical annotation distribution, while during inference we output the full distribution over discrete labels (e.g., <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score, consistent with recent decoding-based regression methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Song &amp; Bahri, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>; Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib70\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib64\" title=\"\">2025b</a>; Lukasik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>. We apply LoRA fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib28\" title=\"\">2022</a>)</cite>. All implementation details, including optimizer settings, training schedule, and sampling policy, are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS3\" title=\"L.3 Fine-tuning Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "respectively",
                    "realism",
                    "model",
                    "drameeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics for Role-play Evaluation Model.</span> For most dimensions with 1-5 scale, we report Pearson correlation. For <span class=\"ltx_text ltx_font_italic\">Content Pass</span> estimation, we report accuracy.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "model",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype Evaluation.</span>\nThe results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T2\" title=\"Table 2 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reveal clear distinctions across evaluation settings. In the zero-shot condition, proprietary ALLMs such as <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> substantially outperform public models, achieving strong <span class=\"ltx_text ltx_font_italic\">content pass</span> accuracy (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>90%) and moderate correlations with other dimensions(<math alttext=\"0.43\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mn>0.43</mn><annotation encoding=\"application/x-tex\">0.43</annotation></semantics></math>&#8211;<math alttext=\"0.48\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mn>0.48</mn><annotation encoding=\"application/x-tex\">0.48</annotation></semantics></math>). By contrast, public models exhibit limited predictive ability, with correlations often below 0.2. Few-shot prompting provides modest gains for proprietary models, improving both accuracy and correlation (e.g., <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> rises from 0.480 to 0.493 average correlation). The most significant improvement arises from finetuning: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves an average correlation of 0.629, marking a large margin over zero-shot/few-shot models and demonstrating the effectiveness of task-specific supervision.</p>\n\n",
                "matched_terms": [
                    "models",
                    "gemini25pro",
                    "results",
                    "accuracy",
                    "drameeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T3\" title=\"Table 3 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> extends the analysis to fine-grained realism evaluation. Similar to archetype results, proprietary ALLMs lead the zero-shot setting, with <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> obtaining an average correlation of 0.390, compared to 0.284 for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>. Public models remain weak baselines. Few-shot prompting consistently improves performance: <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> gains +0.042 and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> gains +0.083 over their zero-shot averages, showing the benefit of in-context supervision. Once again, finetuning is most effective: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves 0.625 average correlation, higher than the best few-shot results. Importantly, the gains are consistent across all 10 realism dimensions, confirming that the sampling&#8211;expectation training strategy generalizes beyond simple classification to nuanced paralinguistic cues.</p>\n\n",
                "matched_terms": [
                    "models",
                    "realism",
                    "gemini25pro",
                    "results",
                    "drameeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real Recording Test Set.</span>\nThe real-recording evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T4\" title=\"Table 4 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> is notably more challenging, with all models suffering substantial performance drops. Proprietary models in the zero-shot setting achieve only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.04&#8211;0.08 average correlation, and public models often fall below zero. Few-shot prompting yields small but noticeable improvements, especially for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> (0.226 avg.), suggesting that exemplars partially mitigate domain mismatch between synthetic and real human speech. Interestingly, while finetuning achieves the highest overall average (0.247), its relative advantage over few-shot prompting is narrower than in the synthetic setting, and some dimensions (e.g., <span class=\"ltx_text ltx_font_italic\">Character Identity</span>) remain weak. We emphasize that this observed gap between synthetic and real recordings is not merely an artifact but an intended feature of Speech-DRAME, designed to expose the mismatch and drive future research into bridging synthetic-to-real generalization.</p>\n\n",
                "matched_terms": [
                    "models",
                    "avg",
                    "identity",
                    "character",
                    "speechdrame"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall.</span> Across both archetype and realism evaluations, we observe three consistent trends: (i) zero-shot performance is limited, particularly for open-source models, (ii) few-shot prompting offers moderate but consistent gains, especially for proprietary ALLMs, and (iii) finetuning with task-specific supervision (<span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>) delivers the largest improvements, achieving state-of-the-art correlations across nearly all dimensions. Nevertheless, results on the real-recording test set underscore the gap between synthetic and real human role-play scenarios, motivating future work in data diversification and domain adaptation.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "models",
                    "realism",
                    "results",
                    "drameeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on fine-tuned <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, we establish the DRAME-RoleBench to evaluate SFMs&#8217; role play ability. For archetype evaluation, 1,250 scenarios are constructed following the same data construction pipeline as the <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>. For realism evaluation, we reuse 2,000 scenarios in the <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> test set. We follow the same prompt strategy as outlined in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "drameeval",
                    "realism",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, we consider a comprehensive set of candidate models, including six end-to-end models: <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib47\" title=\"\">2025b</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib75\" title=\"\">2024</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib71\" title=\"\">2025a</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Qwen3Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib72\" title=\"\">2025b</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">StepAudio2Mini</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib68\" title=\"\">2025</a>)</cite>; and ten cascaded models by combining two widely-used textual LLMs (i.e., <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib12\" title=\"\">2025</a>)</cite> and <span class=\"ltx_text ltx_font_typewriter\">Qwen3-30B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib73\" title=\"\">2025</a>)</cite>) and five recent TTS models (i.e., <span class=\"ltx_text ltx_font_typewriter\">F5TTS</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib8\" title=\"\">2024c</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">IndexTTS2</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib82\" title=\"\">2025b</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">MaskGCT</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib66\" title=\"\">2025d</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Vevo1.5</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib79\" title=\"\">2025b</a>)</cite>, and <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib17\" title=\"\">2024b</a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "models",
                    "cascaded",
                    "stepaudio2mini",
                    "qwen330binstruct",
                    "gemini25pro",
                    "qwen3omni",
                    "gpt4orealtime",
                    "glm4voice",
                    "endtoend",
                    "kimiaudio",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype evaluation.</span>\nOn archetype role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T5\" title=\"Table 5 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), cascaded pipelines consistently outperform end-to-end systems. The strongest cascaded systems, particularly <span class=\"ltx_text ltx_font_typewriter\">G-IndexTTS2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-IndexTTS2</span>, achieve both high content pass rates (above 94%) and balanced scores across audio quality, human likeness, and appropriateness. By contrast, end-to-end models such as <span class=\"ltx_text ltx_font_typewriter\">StepAudio2Mini</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> perform competitively in terms of content coverage (above 88%), but their expressive scores remain slightly lower than their cascaded counterparts. This gap suggests that separating reasoning (LLM) from speech rendering (TTS) remains advantageous when archetype fidelity is emphasized. Nevertheless, the fact that top end-to-end systems already surpass 3.0 in average rating indicates rapid closing of the performance gap.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "cascaded",
                    "models",
                    "stepaudio2mini",
                    "gindextts2",
                    "glm4voice",
                    "endtoend",
                    "qindextts2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human alignment.</span>\nWe further conducted small-scale human studies with 10% of the benchmark data to verify the reliability of the benchmark. Archetype evaluation achieves a Spearman correlation of 0.706 between human judgments and <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, validating that the automatic evaluator closely tracks human preferences in this setting. Realism evaluation, however, achieves a more modest Spearman correlation of 0.375. This lower alignment is expected, since realism data construction did not prioritize fine-grained model separation but instead emphasized distinguishing human versus synthetic speech.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>We view this as evidence that realism presents a harder, less-saturated frontier. Rather than a limitation of DRAME-Eval, this highlights the challenge of designing evaluators that capture nuanced human perception, which motivates future work.</span></span></span> As a result, all models occupy a relatively narrow performance band, reducing sensitivity to model-level differences. We view this as evidence that realism evaluation highlights a harder, less saturated frontier where both SFMs and SEMs require further advances.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "models",
                    "realism",
                    "model",
                    "drameeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, cascaded pipelines retain an edge in archetype scenarios, especially in content robustness and expressive fidelity, but end-to-end models are catching up quickly. In realism scenarios, the field remains unsolved: existing systems exhibit similar performance bands and evaluators show weaker alignment with human judgments, underscoring the need for improved realism-grounded data and evaluation. Together, these findings establish <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> as both a reliable testbed for archetype fidelity and a challenging benchmark for realism fidelity.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "cascaded",
                    "models",
                    "realism",
                    "endtoend"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework for assessing spoken role-play through two complementary lenses: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em> (top-down) and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> (bottom-up, human-grounded). Our human-annotated <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> enables training and testing of SEMs, and our fine-tuned <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span> substantially outperforms zero-/few-shot ALLM judges. Building on this, <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> provides system-level comparison across various SFMs, revealing a consistent edge for cascaded pipelines on archetypes and a tighter spread under realism, where real human performance evaluation remain challenging. Together, these resources establish a reproducible, analyzable foundation for spoken role-play evaluation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>Limitations and future works are discussed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A13\" title=\"Appendix M Limitations and Future Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">M</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "cascaded",
                    "realism",
                    "drameeval",
                    "speechdrame",
                    "discussed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments were conducted on widely available GPUs (NVIDIA H100). Random seeds are fixed across all runs to ensure consistency. By releasing datasets, models, and code, we aim to make all major results in this paper fully reproducible and facilitate future extensions of our framework.</p>\n\n",
                "matched_terms": [
                    "models",
                    "consistency",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data generation:</span> LLMs were employed to expand prompts (e.g., character profiles, scene descriptions) and to produce negative samples in realism evaluation. All outputs were screened by human annotators to ensure validity and reduce bias.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline evaluators:</span> State-of-the-art audio LLMs (e.g., <span class=\"ltx_text ltx_font_typewriter\">GPT4o</span>, <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>) were included as zero-shot judges in our evaluation benchmark, enabling direct comparison with human-aligned SEMs.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "qwen25omni",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Proprietary LLM outputs were used in a limited way (e.g., instruction building, story summarization, or baseline evaluations) and only for internal research purposes. Released datasets in Speech-DRAME are constructed from open-source models, licensed corpora, and original human recordings.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speechdrame"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our distinction between <span class=\"ltx_text ltx_font_italic\">Archetype Evaluation</span> and <span class=\"ltx_text ltx_font_italic\">Realism Evaluation</span> is not merely pragmatic, but is grounded in established theories of social identity, communication, and performance. The two perspectives capture complementary dimensions of how humans perceive and evaluate roles in interaction.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism as Situated Performance</span>\nIn contrast, realism highlights the fine-grained enactment of roles in situated interaction. Goffman&#8217;s dramaturgical sociology emphasizes that everyday social life is itself a performance, where believability depends on subtle paralinguistic and pragmatic cues such as prosody, hesitation, and emotional shading&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goffman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib23\" title=\"\">1949</a>)</cite>. Similarly, interactional sociolinguistics has demonstrated how micro-level features like intonation, pacing, and turn-taking patterns shape the perception of authenticity in communication&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gumperz, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib25\" title=\"\">1982</a>)</cite>. Realism Evaluation thus anchors role-play assessment in bottom-up sensitivity to authentic delivery, beyond schematic expectations.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Complementarity of the Two Perspectives</span>\nPerformance theory provides a unifying lens for these dual dimensions. Schechner&#8217;s view of performance as both &#8220;restored behavior&#8221; (scripted, repeatable structures) and &#8220;living process&#8221; (situated enactment)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Schechner, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib52\" title=\"\">2017</a>)</cite> underscores the need to consider both archetypal scripts and enacted realism. Likewise, sociolinguistic theories of identity recognize both <span class=\"ltx_text ltx_font_italic\">ascribed categories</span> (archetypes imposed by social convention) and <span class=\"ltx_text ltx_font_italic\">performed identities</span> (emergent in interaction)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bucholtz &amp; Hall, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib2\" title=\"\">2005</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p4.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> may be a simple mean, a recency-weighted average, or a learned temporal model that captures consistency across turns.\nThe choice of <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p4.m3\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> reflects the intended evaluation focus, e.g., stability of persona traits, adaptation to scene progression, or coherence across multiple speakers.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "consistency",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Discussion.</span>\nWhile our released dataset and experiments concentrate on single-turn evaluation, the modular nature of the formulation ensures that extending to multi-turn is straightforward.\nFuture work can leverage <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p5.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> to study richer aspects of speech role-play, such as persona persistence, interaction dynamics, and role consistency across extended dialogues.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dynamics",
                    "consistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Role dimensions.</span>\nWe operationalize archetypes through five principal categories, each representing a stereotypical dimension of role identity or behavior, including (1) <span class=\"ltx_text ltx_font_bold\">Occupation / Functional Role (Modern Daily Life)</span> (e.g., a firefighter rescuing a trapped civilian). (2) <span class=\"ltx_text ltx_font_bold\">Occupation / Functional Role (Fictional or Fantasy)</span> (e.g., a martial arts cult leader confronting orthodox rivals). (3)&#160;<span class=\"ltx_text ltx_font_bold\">Social Identity</span> (e.g., a doting parent worrying about a child, or a &#8220;Buddhist youth&#8221; eschewing competition). (4) <span class=\"ltx_text ltx_font_bold\">Named / Specific Characters</span> (e.g., Sun Wukong mocking celestial authorities in <em class=\"ltx_emph ltx_font_italic\">Journey to the West</em>). (5) <span class=\"ltx_text ltx_font_bold\">Personality Traits</span> (e.g., &#8220;hot-blooded&#8221; in a competitive setting, or &#8220;complaining&#8221; in response to poor service). (6) <span class=\"ltx_text ltx_font_bold\">Relative Roles</span> (e.g., parent or stranger).</p>\n\n",
                "matched_terms": [
                    "identity",
                    "traits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scene dimensions.</span>\nIn addition to roles, we incorporate contextual triggers that anchor role-play in stereotypical situations.\nA central focus is on <span class=\"ltx_text ltx_font_bold\">everyday external events or state changes</span> (e.g., receiving a promotion, being caught in gossip), which provide concrete scenarios for testing archetypical responses.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "contextual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Exclusion of fine-grained controls.</span>\nWe deliberately excluded explicit emotional states, prosodic instructions, or linguistic constraints.\nThese dimensions are better suited to realism-oriented or style-controlled evaluation, whereas archetype evaluation benefits from concrete, stereotype-level scenarios that sharpen judgments and improve human agreement.</p>\n\n",
                "matched_terms": [
                    "prosodic",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Pipeline overview.</span>\nAt a high level, archetype evaluation data are generated through a three-stage pipeline:\n(1) selecting keywords that specify role and scene;\n(2) expanding them into full prompts via structured templates; and\n(3) rendering prompts into speech with contemporary TTS systems.\nThis modular pipeline ensures both scalability and reproducibility, while keeping prompts vivid enough for reliable judgments. (4) Based on the generated data (paired scene and synthesized speech), expert human annotators are recruited for detailed annotation on semantic check (i.e., <span class=\"ltx_text ltx_font_italic\">Content Pass</span>) and three different spoken role-play dimensions (i.e., <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human-likeness</span>, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>).\nImplementation details, including template design, alternative prompting strategies, and annotation guidelines, are provided in the following subsections of this Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "detailed",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, archetype evaluation emphasizes breadth and recognizability over subtlety.\nBy grounding role-play in socially recognizable categories and stereotypical situations, it provides a practical balance between scalability and interpretability, complementing the more fine-grained realism evaluation described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS2\" title=\"3.2 Role-play Evaluation with Two Strategies &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Excluded Dimensions.</span> We initially considered more fine-grained control dimensions, but excluded them to preserve the archetype-level focus. Examples of excluded dimensions include:\n<em class=\"ltx_emph ltx_font_italic\">Emotional State &amp; Expression, Single Emotion Prompts, Explicit User Input / Assistant Output structures, Emotional Transition / Fluctuation, Voice Style &amp; Prosody (pacing, intonation, vocal timbre, expressive style, accent/dialect), and Content / Interaction Patterns (task/topic controls, specific linguistic structures, or speech production challenges).</em></p>\n\n",
                "matched_terms": [
                    "transition",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">The Choice of Prompts.</span> We also experimented with broader, less specific templates that allowed more freedom for generative models but produced less discriminative evaluation results:</p>\n\n",
                "matched_terms": [
                    "models",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate a diverse set of speech foundation models spanning both English and Mandarin. In total, we include 8 English and 7 Mandarin models, covering open-source and proprietary systems, and balancing end-to-end and cascaded architectures. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.T7\" title=\"Table 7 &#8227; D.4 Candidate Models for Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> provides the full list of models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "cascaded",
                    "endtoend"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Collection for Proprietary Models.</span> For proprietary systems, we adopt two collection strategies. For GPT-4o-realtime, responses are obtained directly via the <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-realtime</span> API. For Doubao and GPT-4o (Advanced Voice Mode), we collect outputs by recording from their official web interfaces. All recordings are captured through the interaction device itself, ensuring that no additional processing or distortion is introduced into the final audio.</p>\n\n",
                "matched_terms": [
                    "models",
                    "gpt4orealtime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">End-to-End Model Generation.</span>\nFor end-to-end systems, the audio prompt is directly used as input. The system prompt is set to default values to ensure consistent speech generation quality.</p>\n\n",
                "matched_terms": [
                    "model",
                    "endtoend"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cascaded Model Generation.</span>\nFor cascaded systems, we only use Cosyvoice2 as the TTS model in our collection of data. We select <span class=\"ltx_text ltx_font_typewriter\">Qwen3-30B-Instruct</span> and <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> as the text LLM candidates&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib73\" title=\"\">2025</a>; Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib12\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "qwen330binstruct",
                    "cascaded",
                    "model",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2\" title=\"Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the internal annotation interface used for <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>.\nThe platform provides an integrated environment for both playback and evaluation of model-generated role-play speech samples.\nAnnotators first interact with the audio player interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf1\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>), which visualizes waveforms and spectrograms for multiple versions of the same prompt, enabling side-by-side comparison under a shared role instruction (e.g., firefighter, teacher, or actor).\nThen, they proceed to the annotation interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf2\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a>), where they assign scores for multiple dimensions such as <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, along with reasoning and confidence inputs.\nThis structured interface supports high-consistency, fine-grained evaluation of role-play behaviors in speech models.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This appendix starts with a summary of the task preparation pipeline for our proposed realism evaluation. Then, we provides the detailed prompts and system setups for constructing the Realism Evaluation dataset described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4.SS2\" title=\"4.2 Realism Data Collection &#8227; 4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realism Evaluation adopts a bottom&#8211;up perspective grounded in real human speech data, with the goal of capturing the nuanced delivery, prosody, and contextual fit that synthetic archetype prompts cannot fully represent.\nRather than relying on stereotypes, this strategy leverages real-world recordings and carefully constructed contrasts, enabling fine-grained assessment of model performance under realistic conditions.</p>\n\n",
                "matched_terms": [
                    "contextual",
                    "realism",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Retrieved character profiles.</span>\nEach audio segment is paired with a concise character description derived from the media context.\nThe detailed prompt design for generating these character profiles is provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS2\" title=\"F.2 Prompts for Character Profile Creation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "detailed",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generated local scenes.</span>\nWe supplement retrieved character information with automatically generated local scene descriptions summarizing the surrounding plot or setting.\nAppendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS3\" title=\"F.3 Prompts for Local Scene Generation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.3</span></a> includes representative prompts and generation details.</p>\n\n",
                "matched_terms": [
                    "local",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative character profiles.</span>\nWe also create role-profile mismatches by resynthesizing transcripts with diverse TTS systems, thereby breaking the link between authentic delivery and intended role.\nWe explore both text-instruction-based prompting and comparisons across commercial and non-commercial TTS frameworks.\nDetailed model lists and parameter settings are documented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS6\" title=\"F.6 TTS Systems and Settings &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.6</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "detailed",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, real human ground-truth segments and systematically constructed negatives form a balanced dataset that supports realism-oriented evaluation.\nThe positives provide natural anchors of role-play performance, while the negatives introduce controlled mismatches that test whether evaluation models can distinguish real human speech from synthetic approximations.\nThis design ensures that realism evaluation emphasizes subtle paralinguistic and contextual cues, moving beyond intelligibility or content fidelity.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "contextual",
                    "realism",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During annotation, raters are presented with either: (i) ground-truth stories, (ii) stories with negative local scenes, or (iii) stories with negative character profile. Annotators assign realism scores without being exposed to the underlying mismatch reasons. Instead, these reasons are provided only to the quality-assurance team, who use them as reference for internal validation and consistency checks. While the mismatch reasons may not be perfect, they help streamline quality control procedures and maintain reliability across the dataset.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "local",
                    "consistency",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To make the construction of the realism evaluation task concrete, we present a worked example that begins with a curated local scene and character profile, followed by the same transcript adapted into mismatched contexts. These mismatches appear either at the <em class=\"ltx_emph ltx_font_italic\">local-scene</em> level or at both the <em class=\"ltx_emph ltx_font_italic\">character profile and scene</em> level, thereby providing controlled degradations of realism.</p>\n\n",
                "matched_terms": [
                    "local",
                    "realism",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the character Yunjin from <span class=\"ltx_text ltx_font_italic\">Genshin Impact</span> as a running case study, which is also included in our released dataset.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>As acknowledged, the use of real-world recordings is conducted under legal and ethical review. All curated data will be released as part of our open-source package.</span></span></span> This example illustrates how alignment between a speaker&#8217;s persona, local context, and delivery underpins realism, and how systematically breaking that alignment produces contrastive test cases.</p>\n\n",
                "matched_terms": [
                    "local",
                    "realism",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Curated Local Scene.</span>\nYunjin is narrating a fierce battle through operatic singing. Her verses depict a hero charging into enemy lines, slaying the enemy commander, and securing peace. The atmosphere is filled with tension and intensity, highlighting the hero&#8217;s extraordinary bravery and determination.</p>\n\n",
                "matched_terms": [
                    "local",
                    "intensity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative Local Scenes.</span>\nTo stress realism evaluation, we insert the same transcript into alternative backgrounds. These reveal how context-shifted delivery degrades narrative coherence:</p>\n\n",
                "matched_terms": [
                    "realism",
                    "stress",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative Character Profiles.</span>\nWe replace Yunjin with new character profiles and scenes. Although the transcript remains the same, realism degrades because the delivery no longer matches the speaker&#8217;s persona or situation.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Yunjin case study illustrates a fundamental difference between realism evaluation and archetype evaluation. Archetype prompts, as described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>, are constructed by pairing abstract role categories with synthetic contextual scenes. These cover a broad space of possible situations, ranging from modern occupations (e.g., firefighters) to fictional personas (e.g., cult leaders in martial arts fantasy) or social identities (e.g., overprotective parents, laid-back friends). While such prompts provide controlled and diverse coverage, they are ultimately template-based and rely on stereotypes to elicit role-play behaviors.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "contextual",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the Yunjin example is drawn from authentic performance, where the character&#8217;s persona (an opera singer) and local scene (a battlefield narration) are naturally aligned through artistic delivery. The realism task does not merely test whether a model can follow a prompt, but whether it can preserve coherence between a speaker&#8217;s global identity, immediate context, and expressive prosody. When Yunjin&#8217;s transcript is transplanted into mismatched settings or paired with incongruent speaker profiles, the breakdown in realism becomes apparent: the same words shift from inspiring to awkward, jarring, or even absurd.</p>\n\n",
                "matched_terms": [
                    "global",
                    "realism",
                    "local",
                    "identity",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This distinction highlights the complementary strengths of the two evaluation paradigms. Archetype evaluation systematically explores a wide combinatorial space of roles and situations, but may lack the paralinguistic and contextual richness of real speech. Realism evaluation, on the other hand, anchors assessment in authentic delivery and contextual fit, providing a finer-grained lens on coherence and naturalness. Together, they ensure that role-play evaluation spans both broad stereotypical coverage and nuanced real-world plausibility.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "contextual",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed a diverse set of TTS systems to generate negative character profiles and resynthesized speech for Realism Evaluation.\nThese included both commercial and non-commercial models, with variation in style control, conditioning prompts, and reference usage.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.T8\" title=\"Table 8 &#8227; Same-Speaker Voice Clone. &#8227; F.6 TTS Systems and Settings &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> summarizes the systems and settings.\nBelow, we detail the major strategies used across systems:</p>\n\n",
                "matched_terms": [
                    "models",
                    "realism",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We leveraged all publicly available speaker style information released by the corresponding websites.\nFor each input sample, we employed <span class=\"ltx_text ltx_font_typewriter\">deepseek-v3</span> to select the best-matching speaker based on the original speech caption.\nIn the case of <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5TTS</span>, the character profile was also appended as an additional instruction to guide synthesis.</p>\n\n",
                "matched_terms": [
                    "based",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A character profile was first converted into a descriptive voice specification by prompting <span class=\"ltx_text ltx_font_typewriter\">deepseek-v3</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib38\" title=\"\">2024a</a>)</cite>.\nThis description was submitted to the TTS service to generate candidate voices.\nWe then previewed samples via Gemini2.5Pro, selecting the voice that best matched the speaker style of the original recording before proceeding to generate speech.</p>\n\n",
                "matched_terms": [
                    "gemini25pro",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For models using voice prompts, we use an inhouse enhancement model to first process the prompt audio so as to minimize the effect of audio events and background noise.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS3</span> (Embedding&#8211;AudioCode Pipeline).\nHere, the input is Qwen2.5 BPE embeddings, which a 0.4B GPT-2 model maps directly to 4,096-dimensional audio codes. These codes are decoded via a cascaded architecture: <span class=\"ltx_text ltx_font_typewriter\">RepCodec</span> decoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib32\" title=\"\">2024</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> content representation (with speaker embedding) <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DDIM</span> (denoising\ndiffusion implicit model) module&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jeong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib34\" title=\"\">2021</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DAC</span> latent representation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib37\" title=\"\">2023</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DAC</span> decoder. Conditioning is provided through prefix text embeddings (e.g., instructions expressing emotional states such as &#8220;angry, hateful: you betrayed me&#8230;&#8221;), along with speaker ID. The input format is <span class=\"ltx_text ltx_font_typewriter\">[spk_id, BPE embedding, audio codes]</span>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "cascaded",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The annotation interface for <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> focuses on assessing contextual and narrative coherence in model-generated speech.\nUnlike the archetype-oriented evaluation, which emphasizes stylistic and emotional fidelity, this platform examines how well the voice performance aligns with a character&#8217;s personality, dialogue situation, and scene progression.</p>\n\n",
                "matched_terms": [
                    "contextual",
                    "realism",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.F3\" title=\"Figure 3 &#8227; F.7 Annotation Platform for Realism Evaluation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the core playback interface.\nAnnotators can preview each audio sample with synchronized spectrogram and waveform displays, while reviewing contextual metadata including the <span class=\"ltx_text ltx_font_italic\">Local Scene</span>, <span class=\"ltx_text ltx_font_italic\">Character Style</span>, and <span class=\"ltx_text ltx_font_italic\">Character Profile</span>.\nThese descriptions help situate the listening task, providing the background necessary for judging realism in the character&#8217;s vocal and behavioral expression.</p>\n\n",
                "matched_terms": [
                    "local",
                    "realism",
                    "contextual",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Read the <span class=\"ltx_text ltx_font_bold\">Character Profile</span> (long-term identity, style) and <span class=\"ltx_text ltx_font_bold\">Local Scene</span> (immediate context, goal/emotion) before listening.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Detailed case examples and progressive gating for emotional dimensions are provided in the full instruction document.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt five dimensions, each rated from 1 (worst) to 5 (best). Emotional Expressiveness uses <em class=\"ltx_emph ltx_font_italic\">progressive gating</em>.</p>\n\n",
                "matched_terms": [
                    "expressiveness",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prosodic Dynamics</span> (Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis): naturalness of patterns and rhythms.</p>\n\n",
                "matched_terms": [
                    "prosodic",
                    "dynamics",
                    "stress",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotional Expressiveness</span> (Progressive gating):</p>\n\n",
                "matched_terms": [
                    "expressiveness",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Character Consistency</span> (Voice Identity Matching, Trait Embodiment): alignment with Character Profile.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "consistency",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contextual Relevance</span> (Local Scene Fit, Global Story Coherence): fit to immediate scene and long-term identity.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "identity",
                    "relevance",
                    "contextual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Read first:</span> Carefully read the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em> to understand the long-term identity and immediate goal/emotion.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identity mismatch:</span> If the clip&#8217;s speaker is clearly not the intended target speaker, assign a score of <span class=\"ltx_text ltx_font_bold\">1</span> for every rating dimension that relies on the Character Profile (e.g., Character Style Traits, Trait Embodiment, Character Consistency).</p>\n\n",
                "matched_terms": [
                    "identity",
                    "consistency",
                    "traits",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt contains contradictory information.</span> Conflicts make accurate annotation difficult or impossible. Examples: incompatible accent specifications; Local Scene drastically at odds with intended emotion (see <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>); Local and global scenes inconsistent; traits described as &#8220;evolving over time&#8221; without a timeline.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "global",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Question 2: Observed Character Style Traits.</span> Based on the Character Profile, are the defined Style Traits clearly present in the delivery?</p>\n\n",
                "matched_terms": [
                    "traits",
                    "based",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rubric Rating Instructions.</span>\nScore each subcriterion on a 1&#8211;5 scale. Use the detailed rubrics below and the examples to maintain calibration across characters, tones, and emotional styles. Prioritize in-character believability; do not reward theatricality, emotional intensity, or technical polish unless they naturally fit the character <em class=\"ltx_emph ltx_font_italic\">and</em> scene.</p>\n\n",
                "matched_terms": [
                    "detailed",
                    "emotional",
                    "intensity",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Focus: pitch, rhythm, and emphasis in speech.\n<span class=\"ltx_text ltx_font_bold\">Guideline 1.1 Pitch Variation (check Local Scene, Character Profile, and audio).</span></p>\n\n",
                "matched_terms": [
                    "character",
                    "local",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 1.2 Rhythmic Naturalness (check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "character",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 1.3 Stress and Emphasis (check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "character",
                    "stress",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 2.1 Emotion Accuracy (Required; check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "character",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 2.2 Emotion Intensity Control (Only if 2.1 <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"A7.SS4.SSSx5.p3.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math> 3; check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "character",
                    "intensity",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 2.3 Dynamic Range (Only if 2.2 <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"A7.SS4.SSSx5.p4.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math> 3; check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "character",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 3.1 Voice Identity Matching (Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 4.2 Global Story (Character Profile) Coherence (check Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "global",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Progressive gating for emotion.</span>\nOur <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness</em> uses gated sub-scores (accuracy <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A8.SS2.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> intensity control <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A8.SS2.p3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> dynamic range), preventing spurious high scores when base emotion is misidentified.\nThis reduces label noise and improves SEM alignment stability.</p>\n\n",
                "matched_terms": [
                    "expressiveness",
                    "accuracy",
                    "emotional",
                    "intensity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The archetype evaluation provides a detailed view of how speech foundation models (SFMs) perform when tasked with role-play scenarios grounded in stereotypical characters and situations. Human annotations cover four dimensions, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, and <span class=\"ltx_text ltx_font_italic\">Content Pass</span>, and the results reveal important trends, as well as challenges in interpreting scores across languages.</p>\n\n",
                "matched_terms": [
                    "models",
                    "roleplay",
                    "detailed",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A critical observation is that annotations in Mandarin and English cannot be directly compared. The annotation groups for the two languages differed in their linguistic backgrounds and cultural interpretations, which leads to systematic differences in scoring. Our bilingual pilot further confirmed that annotators often interpret role-play appropriateness and expressiveness differently depending on their cultural frame of reference. For example, Mandarin raters tend to reward delivery styles that align with culturally expected prosody and politeness strategies, while English raters prioritize spontaneity and naturalness. These discrepancies parallel the findings of the singing voice conversion challenge (SVCC2023) from <cite class=\"ltx_cite ltx_citemacro_citet\">Huang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib31\" title=\"\">2023</a>)</cite>, where native and non-native listeners provided significantly different ratings for the same samples, particularly for naturalness and speaker similarity. Consequently, while absolute scores differ substantially across Mandarin and English, only within-language comparisons are meaningful. Cross-lingual ranking is not valid given the different annotation bases.</p>\n\n",
                "matched_terms": [
                    "expressiveness",
                    "roleplay"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Within Mandarin from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, the relative rankings of models are clear. <span class=\"ltx_text ltx_font_typewriter\">Doubao</span> stands out as the strongest performer across all four dimensions, reaching near-ceiling levels in both <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> (4.37) and <span class=\"ltx_text ltx_font_italic\">Content Pass</span> (0.98). It also leads in role-related dimensions such as <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, indicating balanced competence across linguistic accuracy, delivery, and expressiveness. <span class=\"ltx_text ltx_font_typewriter\">G-CosyVoice2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-CosyVoice2</span> follow closely, especially in audio quality, though they lag behind in role appropriateness. <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> achieve mid-range results, reflecting higher variance across samples, while <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> shows competitive content understanding but lower naturalness and appropriateness. <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> consistently ranks the lowest, with large variance, underscoring instability and weaker alignment with archetypal role expectations.</p>\n\n",
                "matched_terms": [
                    "models",
                    "qcosyvoice2",
                    "gpt4orealtime",
                    "glm4voice",
                    "gcosyvoice2",
                    "results",
                    "expressiveness",
                    "accuracy",
                    "kimiaudio",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The picture shifts in English, elaborated in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Here, <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> emerges as the most effective model by a wide margin, achieving top scores in both <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> (3.46) and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> (3.49), as well as high content pass rates (0.97). This reflects its strong grounding in English discourse norms and role adherence. In contrast, <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> variants perform well in <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> but are weaker in role alignment, highlighting a trade-off between acoustic fidelity and role-play embodiment. Other models such as <span class=\"ltx_text ltx_font_typewriter\">Doubao</span>, <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span>, and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> achieve moderate results, while <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> trail far behind, showing limited robustness in English scenarios. Taken together, the English results reveal both a sharper separation across models and a wider variance in appropriateness scores compared to Mandarin.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "models",
                    "model",
                    "gpt4orealtime",
                    "glm4voice",
                    "results",
                    "kimiaudio",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although cross-lingual comparisons are invalid, this does not diminish the utility of archetype evaluation for role-play benchmarking. In fact, the observed inconsistencies reinforce a central point: successful role-play requires models to adapt to the cultural and linguistic norms of the target language community. A system that performs well in Mandarin but poorly in English cannot be considered universally capable, since end users will evaluate appropriateness and naturalness according to their native expectations. Thus, archetype evaluation highlights not only model-specific strengths and weaknesses but also the need for culturally grounded evaluation pipelines.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although realism evaluation targets nuanced paralinguistic behaviors, human annotators achieve <em class=\"ltx_emph ltx_font_italic\">consistently high agreement</em> once the rubric disentangles the aspects to be judged.\nAs shown in the agreement summary in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F7\" title=\"Figure 7 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, most dimensions fall in the <math alttext=\"0.86\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>0.86</mn><annotation encoding=\"application/x-tex\">0.86</annotation></semantics></math>&#8211;<math alttext=\"0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mn>0.88</mn><annotation encoding=\"application/x-tex\">0.88</annotation></semantics></math> range, with the lowest agreement on <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> (<math alttext=\"\\approx 0.83\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.83</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.83</annotation></semantics></math>) and the highest on <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em> / <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> (<math alttext=\"\\approx 0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.88</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.88</annotation></semantics></math>).\nEmotion-related, progressive metrics (Intensity and Transition) are slightly lower (<math alttext=\"\\approx 0.85\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.85</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.85</annotation></semantics></math>), reflecting the added difficulty of judging graded expressivity.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "local",
                    "intensity",
                    "transition",
                    "traits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F8\" title=\"Figure 8 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F9\" title=\"Figure 9 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> visualize the score distributions per dimension.\nWe highlight three patterns:\n(i) <span class=\"ltx_text ltx_font_bold\">Progressive/gated emotion metrics.</span> For <em class=\"ltx_emph ltx_font_italic\">Emotion Intensity</em> and <em class=\"ltx_emph ltx_font_italic\">Emotion Transition</em>, we assign <span class=\"ltx_text ltx_font_typewriter\">1</span> when the scene is marked <em class=\"ltx_emph ltx_font_italic\">N/A</em> (i.e., emotion accuracy is insufficient to meaningfully judge intensity/transition). This produces a visible mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, a sparse bin at <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>, and slightly lower overall agreement; downstream analyses conditioning on adequate <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> (e.g., mean&#160;<math alttext=\"\\geq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 3</annotation></semantics></math>) recover more bell-shaped distributions.\n(ii) <span class=\"ltx_text ltx_font_bold\">Traits Embodiment.</span> Scores tend to avoid the middle (<math alttext=\"\\sim\\!3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!3</annotation></semantics></math>), concentrating on low values when traits clearly mismatch and high values when traits are strongly realized. This yields the lowest inter-rater agreement among dimensions, reflecting the inherently compositional and role-specific nature of trait judgments.\n(iii) <span class=\"ltx_text ltx_font_bold\">Other dimensions.</span> <em class=\"ltx_emph ltx_font_italic\">Pitch Variation</em>, <em class=\"ltx_emph ltx_font_italic\">Rhythmic Naturalness</em>, <em class=\"ltx_emph ltx_font_italic\">Stress &amp; Emphasis</em>, <em class=\"ltx_emph ltx_font_italic\">Voice Identity Matching</em>, <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em>, <em class=\"ltx_emph ltx_font_italic\">Global Story Coherence</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> are approximately unimodal with modes at <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>&#8211;<math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math>, consistent with moderate&#8211;strong realization in the dataset and with the high agreement reported.</p>\n\n",
                "matched_terms": [
                    "global",
                    "pitch",
                    "identity",
                    "local",
                    "intensity",
                    "transition",
                    "stress",
                    "traits",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prosody control (Pitch, Rhythm, Stress).</span> Professional speech is shifted toward higher bins with tighter spread, indicating steadier control and fewer low-end tails.</p>\n\n",
                "matched_terms": [
                    "stress",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotion rendering (Accuracy, Intensity, Transition).</span> Professionals show higher centers and reduced mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.I1.i2.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> (fewer gated <span class=\"ltx_text ltx_font_typewriter\">N/A</span>s), suggesting more consistent emotional grounding and smoother dynamics; amateurs exhibit wider variance and more mass at the extremes.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "intensity",
                    "accuracy",
                    "transition",
                    "dynamics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identity and traits (Voice Identity, Traits Embodiment).</span> Professionals present clearer peaks in upper bins, while amateurs are broader or bimodal, reflecting inconsistent character anchoring.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "traits",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scene and semantics (Local Scene Fit, Global Coherence, Semantic Match).</span> Differences are present but smaller; both groups cluster in the mid&#8211;high range, with professionals exhibiting slightly narrower distributions.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, the consistently high agreement confirms that the rubric isolates perceptually coherent factors, so learned evaluators can target them reliably.\nSecond, the gated behavior of <em class=\"ltx_emph ltx_font_italic\">Intensity/Transition</em> motivates a <em class=\"ltx_emph ltx_font_italic\">hierarchical</em> evaluator: verify <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> before scoring graded expressivity, or model the joint structure with conditional heads.\nThird, <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> remains challenging; models may benefit from explicit character-trait priors, long-term context aggregation, or contrastive supervision between matched and mismatched traits.\nFinally, prosody-focused objectives (e.g., pitch/rhythm/stress control) are likely to yield the largest quality gains for amateur-style speech.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "models",
                    "model",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For both zeroshot and fewshot experiments, we adopt a streamlined version of the annotation guidelines to ensure consistency with the human annotation process. The exact rubric prompts for archetype and realism evaluation used are provided below:</p>\n\n",
                "matched_terms": [
                    "realism",
                    "consistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and fewshot settings, we condition the model with the rubric prompt and then append few-shot demonstration triplets (input, audio, gold JSON) to mirror the human annotation protocol. As shown in Listing&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#LST2\" title=\"Listing 2 &#8227; L.1 Zeroshot/Fewshot Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, We use <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> for these few-shot exemplars and require a minified JSON output consistent with the rubric schema.</p>\n\n",
                "matched_terms": [
                    "model",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and few-shot experiments on commercial models such as <span class=\"ltx_text ltx_font_typewriter\">Gemini</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o</span>, we report results as the average of five independent runs to reduce randomness and obtain more stable predictions. In contrast, for open-source models, we observed that their instruction-following ability is relatively weak. To mitigate this, we prompt them to predict one dimension at a time rather than all dimensions jointly. Specifically, for each metric, after showing the rubric prompt, we request completion in a fixed template and extract the probability distribution over the discrete tokens <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>. The final score is then computed from these probabilities, which we found makes open-source models usable to some extent under this setting.</p>\n\n",
                "matched_terms": [
                    "models",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For few-shot experiments, we randomly sample three exemplars from the training set, with each exemplar chosen such that its human scores are close to the rounded average across all annotators. These exemplars are appended as demonstrations in the prompt. As with the zeroshot case, we run five trials for commercial models and report the averaged results for evaluation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.T12\" title=\"Table 12 &#8227; L.2 Zeroshot/Fewshot Ablation Experiments &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> shows how different prompting strategies affect realism evaluation performance with <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>. Several observations can be drawn:</p>\n\n",
                "matched_terms": [
                    "realism",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Scaling the number of demonstrations.\nIncreasing the few-shot set size from three to six and nine further boosts performance (0.459 and 0.469, respectively). The improvements are most pronounced in prosodic and emotional categories (e.g., <em class=\"ltx_emph ltx_font_italic\">Pitch</em>, <em class=\"ltx_emph ltx_font_italic\">Accuracy</em>, <em class=\"ltx_emph ltx_font_italic\">Intensity</em>), suggesting that more examples provide stronger guidance in modeling subtle paralinguistic cues. Gains in <em class=\"ltx_emph ltx_font_italic\">Character Consistency</em> and <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance</em> are also noticeable, although smaller in magnitude, which may indicate that these higher-level judgments are less sensitive to prompt scaling once a few demonstrations are provided.</p>\n\n",
                "matched_terms": [
                    "respectively",
                    "pitch",
                    "relevance",
                    "contextual",
                    "consistency",
                    "emotional",
                    "character",
                    "intensity",
                    "prosodic",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the archetype test set, all fine-tuned variants achieve strong <span class=\"ltx_text ltx_font_italic\">Content Pass</span> accuracy above 92%. Differences appear mainly in correlation-based metrics. Here, <span class=\"ltx_text ltx_font_typewriter\">Base-FT</span> yields the highest score on <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> delivers the strongest performance in <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> and the best overall average (0.629). This suggests that instruction-tuned checkpoints paired with parameter-efficient adaptation can better capture higher-level perceptual traits.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For realism evaluation on the basic test set, <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> consistently achieves the strongest correlations across most dimensions, especially for <span class=\"ltx_text ltx_font_italic\">Character Identity</span> and <span class=\"ltx_text ltx_font_italic\">Traits</span>, leading to the best average correlation (0.668). <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> also performs competitively (0.630), while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> lags slightly behind (0.625). These results highlight that full fine-tuning may still offer an advantage when the evaluation set is relatively homogeneous and closer to the training distribution.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "identity",
                    "character",
                    "results",
                    "traits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A different pattern emerges on the real recording test set. Both <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> and <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> struggle, yielding negative or near-zero correlations across most dimensions. In contrast, <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> shows clear improvements, achieving positive correlations in all prosodic and emotional dimensions and producing the highest overall average (0.247). This indicates that LoRA adaptation on top of an instruction-tuned backbone provides better generalization to out-of-distribution, real-world data.</p>\n\n",
                "matched_terms": [
                    "prosodic",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the comprehensive design of Speech-DRAME, several limitations remain.\nFirst, while our dual evaluation paradigm combines top-down archetype assessment with bottom-up realism evaluation, both strategies are constrained by the quality and scope of available data. Archetype evaluation inevitably relies on simplified stereotypes, which can overlook subtler paralinguistic and contextual cues. Realism evaluation, though richer, still struggles with domain mismatch between synthetic data and real human recordings, as evidenced by the performance drop observed in the real-recording test set. This highlights the persistent gap between benchmark-driven evaluation and real-world variability in speech role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "speechdrame",
                    "realism",
                    "contextual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Third, while DRAME-Eval substantially outperforms zero-shot and few-shot ALLMs, its reliance on fine-tuning from a strong base model raises questions of accessibility and scalability. Future work should explore lightweight adaptation techniques, semi-supervised training with partially annotated data, and leveraging emergent capabilities of open-source multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "drameeval",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, although we validated DRAME-Eval against human annotations, alignment under realism settings remains modest. This suggests that even human-annotated benchmarks are insufficiently fine-grained to capture the full complexity of human perception in speech role-play. Richer annotation protocols, more diverse speaker populations, and integration of perceptual psychology insights could further improve evaluation fidelity.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "drameeval",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Looking forward, we see several promising research directions. We intentionally restrict evaluation to single-turn responses for clarity and tractability. However, multi-turn extensions that capture narrative progression, long-term consistency, and dynamic emotion shifts remain a natural next step. Incorporating multilingual and cross-cultural datasets will enhance generality and fairness. On the modeling side, exploring hybrid evaluation strategies that combine discrete rubrics with continuous perceptual signals (e.g., prosody embeddings or affective measures) may better bridge the gap between automatic and human judgments. Finally, integrating Speech-DRAME into reinforcement learning pipelines as a reward model could help drive the next generation of expressive, human-aligned speech role-play systems.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "speechdrame",
                    "consistency",
                    "model"
                ]
            }
        ]
    },
    "A4.T7": {
        "caption": "Table 7: Candidate speech foundation models for evaluation across English and Mandarin. G and Q in for cascaed models refer to Gemini2.5Pro and Qwen3-30B-Instruct.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">License / Availability</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Architecture</span></td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Reference</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\">English Models (8)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Proprietary</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Unknown</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib47\" title=\"\">2025b</a>)</cite></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_typewriter\">Doubao</span></td>\n<td class=\"ltx_td ltx_align_left\">Proprietary</td>\n<td class=\"ltx_td ltx_align_left\">Unknown</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\"><cite class=\"ltx_cite ltx_citemacro_citep\">(ByteDance, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib3\" title=\"\">2025</a>)</cite></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text ltx_font_typewriter\">GPT-4o</span>(Advanced Voice Mode)</td>\n<td class=\"ltx_td ltx_align_left\">Proprietary</td>\n<td class=\"ltx_td ltx_align_left\">Unknown</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\"><cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib46\" title=\"\">2025a</a>)</cite></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span></td>\n<td class=\"ltx_td ltx_align_left\">Open-source</td>\n<td class=\"ltx_td ltx_align_left\">End-to-end</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\"><cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib75\" title=\"\">2024</a>)</cite></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span></td>\n<td class=\"ltx_td ltx_align_left\">Open-source</td>\n<td class=\"ltx_td ltx_align_left\">End-to-end</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\"><cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_typewriter\">Qwen-2.5-Omni</span></td>\n<td class=\"ltx_td ltx_align_left\">Open-source</td>\n<td class=\"ltx_td ltx_align_left\">End-to-end</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\"><cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib71\" title=\"\">2025a</a>)</cite></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_typewriter\">Q-CosyVoice2</span></td>\n<td class=\"ltx_td ltx_align_left\">Open-source</td>\n<td class=\"ltx_td ltx_align_left\">Cascaded</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\"><cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib73\" title=\"\">2025</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib17\" title=\"\">2024b</a>)</cite></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_typewriter\">G-Cosyvoice2</span></td>\n<td class=\"ltx_td ltx_align_left\">Proprietary + Open-source</td>\n<td class=\"ltx_td ltx_align_left\">Cascaded</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\"><cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib12\" title=\"\">2025</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib17\" title=\"\">2024b</a>)</cite></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\">Mandarin Models (7)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Proprietary</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Unknown</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib47\" title=\"\">2025b</a>)</cite></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_typewriter\">Doubao</span></td>\n<td class=\"ltx_td ltx_align_left\">Proprietary</td>\n<td class=\"ltx_td ltx_align_left\">Unknown</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\"><cite class=\"ltx_cite ltx_citemacro_citep\">(ByteDance, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib3\" title=\"\">2025</a>)</cite></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span></td>\n<td class=\"ltx_td ltx_align_left\">Open-source</td>\n<td class=\"ltx_td ltx_align_left\">End-to-end</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\"><cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib75\" title=\"\">2024</a>)</cite></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span></td>\n<td class=\"ltx_td ltx_align_left\">Open-source</td>\n<td class=\"ltx_td ltx_align_left\">End-to-end</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\"><cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_typewriter\">Qwen-2.5-Omni</span></td>\n<td class=\"ltx_td ltx_align_left\">Open-source</td>\n<td class=\"ltx_td ltx_align_left\">End-to-end</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\"><cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib71\" title=\"\">2025a</a>)</cite></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text ltx_font_typewriter\">Q-CosyVoice2</span></td>\n<td class=\"ltx_td ltx_align_left\">Open-source</td>\n<td class=\"ltx_td ltx_align_left\">Cascaded</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\"><cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib73\" title=\"\">2025</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib17\" title=\"\">2024b</a>)</cite></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_typewriter\">G-Cosyvoice2</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Proprietary + Open-source</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Cascaded</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_bb\"><cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib12\" title=\"\">2025</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib17\" title=\"\">2024b</a>)</cite></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "models",
            "cascaded",
            "2024b",
            "foundation",
            "yang",
            "mode",
            "evaluation",
            "gpt4orealtime",
            "glm4voice",
            "candidate",
            "bytedance",
            "doubao",
            "qwen330binstruct",
            "gemini25pro",
            "architecture",
            "voice",
            "comanici",
            "gpt4oadvanced",
            "openai",
            "across",
            "english",
            "refer",
            "ding",
            "2025a",
            "gcosyvoice2",
            "cascaed",
            "proprietary",
            "speech",
            "kimiaudio",
            "zeng",
            "qwen25omni",
            "unknown",
            "qcosyvoice2",
            "model",
            "mandarin",
            "license",
            "endtoend",
            "2025b",
            "opensource",
            "reference",
            "availability"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We evaluate a diverse set of speech foundation models spanning both English and Mandarin. In total, we include 8 English and 7 Mandarin models, covering open-source and proprietary systems, and balancing end-to-end and cascaded architectures. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.T7\" title=\"Table 7 &#8227; D.4 Candidate Models for Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> provides the full list of models.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Role-play has become a key testbed for generative models, expanding from text-only dialogue to multimodal interaction. Extending role-play to speech captures prosody, emotion, and delivery, but also poses new evaluation challenges. Current pipelines often use audio large language models (ALLMs) as zero-shot judges, which miss paralinguistic cues, collapse multiple aspects into coarse scores, and rely on synthetic speech references that fail to reflect real-world roles. We present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework that contributes at three levels: (i) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-EvalBench</em>, an evaluation benchmark with bilingual human-annotated data and protocols for training and testing speech evaluation models&#160;(SEMs), (ii) <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, a fine-tuned evaluation model, which substantially outperforms zero-shot and few-shot ALLMs, and (iii) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-RoleBench</em>, a speech role-play benchmark that leverages <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> as an automatic judge to compare speech foundation models (SFMs). Speech-DRAME distinguishes between two complementary evaluation strategies: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>, a top-down approach measuring adherence to broad role archetypes, and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em>, a bottom-up approach grounded in real human speech that emphasizes nuanced role quality. Compared to zero-shot ALLM judges, <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves stronger agreement with human ratings (Pearson correlation from 0.480 to 0.629 in archetypes, and 0.390 to 0.625 in realism). By integrating transparent benchmark resources, modeling approaches, and system-level evaluation, Speech-DRAME provides the first comprehensive, reproducible foundation for assessing spoken role-play.</p>\n\n",
                "matched_terms": [
                    "models",
                    "foundation",
                    "model",
                    "evaluation",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Role-play has been recently recognized a cornerstone of interactive systems, from initial text-based focuses to the growing ecosystem of multimodal dialogue agents&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib6\" title=\"\">2024a</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib59\" title=\"\">2024</a>; Dai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib13\" title=\"\">2025</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib43\" title=\"\">2024</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>)</cite>. With the rapid advancement of large language models (LLMs), role-play has evolved beyond scripted interactions into dynamic character-driven conversations, supporting applications in education, entertainment, and human&#8211;AI collaboration&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib69\" title=\"\">2024</a>; Agatsuma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib1\" title=\"\">2024</a>; Fung &amp; Laing, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib20\" title=\"\">2024</a>)</cite>. Extending role-play to speech-based interaction offers an even richer communication channel, capturing prosody, emotion, and character consistency in ways that text alone cannot&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "2025a",
                    "models",
                    "ding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Evaluating speech role-play, however, remains a major challenge. Current pipelines often rely on ALLMs as judges, typically in a zero-shot setting&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>; Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>)</cite>. While this strategy can provide rough ratings, it falls short for speech role-play:</p>\n\n",
                "matched_terms": [
                    "2025a",
                    "speech",
                    "2025b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">it lacks sensitivity to paralinguistic cues such as intonation, pacing, and emotional dynamics that define role delivery&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>; Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib81\" title=\"\">2025a</a>)</cite>,</p>\n\n",
                "matched_terms": [
                    "2025a",
                    "2025b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">it depends on synthetic speech references rather than real human speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>)</cite>, limiting its ability to reflect real-world diversity in speech role-play.</p>\n\n",
                "matched_terms": [
                    "2025a",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome these limitations, we present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span> (Speech Detailed Role-play Assessment with Modeling and Evaluation), a unified framework that moves beyond synthetic or judge-only pipelines. Speech-DRAME is distinguished by three aspects: [i] a dual evaluation paradigm that balances archetype-driven generality with human-grounded realism (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS2\" title=\"3.2 Role-play Evaluation with Two Strategies &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), [ii] human annotations that provide perceptual grounding absent in prior resources (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4\" title=\"4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), and [iii] evaluation-model alignment that enables not only benchmarking of speech foundation models (SFMs) but also systematic analysis of the evaluation models (SEMs) themselves (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5\" title=\"5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). These aspects establish Speech-DRAME as a comprehensive and trustworthy framework for measuring and improving speech role-play.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models",
                    "evaluation",
                    "foundation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, Speech-DRAME contributes at three levels: (i) <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>, an evaluation benchmark consisting of human-annotated datasets for archetype and realism role-play; (ii) <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span>, a fine-tuned SEM trained on EvalBench that surpasses zero-shot and few-shot ALLMs; and (iii) <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span>, a speech role-play benchmark that leverages DRAME-Eval as an automatic judge to systematically compare SFMs. By combining dual evaluation strategies, human-anchored supervision, and evaluation-model alignment, Speech-DRAME delivers systematic insights into both role-play systems and evaluation models themselves.\nOur contributions are four-fold:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a <span class=\"ltx_text ltx_font_bold\">formal task definition</span> of speech role-play and its evaluation via speech foundation models (SFMs) for generation and speech evaluation models (SEMs) for assessment, unifying linguistic and paralinguistic dimensions.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models",
                    "evaluation",
                    "foundation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span>, a fine-tuned SEM trained on EvalBench that substantially outperforms zero-shot and few-shot ALLMs, demonstrating the importance of human supervision for evaluation models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We establish <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span>, a speech role-play benchmark that leverages DRAME-Eval as an automatic judge to systematically compare both end-to-end and cascaded SFMs across multiple role quality dimensions.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Relevant codes, pre-trained models, and datasets will be open-sourced at \n<br class=\"ltx_break\"/><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/Anuttacon/speech_drame\" title=\"\">https://github.com/Anuttacon/speech_drame</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "cascaded",
                    "across",
                    "models",
                    "endtoend",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through this work, we aim to move from acting in text to acting in speech, equipping the community with the models, evaluation strategies, and benchmarks needed to measure and improve speech role-play.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-based role-play evaluation.</span>\nEarly research focused on text-only dialogue benchmarks. Datasets such as RoleLLM, CharacterBox, and RoleMRC <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib62\" title=\"\">2024</a>; Tu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib60\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib61\" title=\"\">2025a</a>; Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib53\" title=\"\">2023</a>; Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib41\" title=\"\">2025</a>)</cite> test whether models can sustain persona consistency and character fidelity, using LLM-as-judge pipelines for scalability. However, studies like PersonaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib81\" title=\"\">2025a</a>)</cite> have shown that LLM-as-judge approaches are far from perfect, struggling with tasks as basic as tracking speaker identity. This highlights broader concerns about reliability even in text settings, which become only more severe when extended to speech, where prosody, emotion, and vocal style play a central role.</p>\n\n",
                "matched_terms": [
                    "2025a",
                    "models",
                    "evaluation",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-native benchmarks.</span>\nMore recently, a number of resources have been introduced to evaluate spoken role-play agents. OmniCharacter-10K <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>)</cite> emphasizes character voice traits, while others&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib39\" title=\"\">2025</a>; Hou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib27\" title=\"\">2025</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib15\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib7\" title=\"\">2024b</a>)</cite> extend evaluation toward vocal communication, assistant capability, and multi-turn realism. Omni models such as StepAudio and KimiAudio <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite> also include role-play scenarios in their evaluation. SpeechRole and VStyle <cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>; Zhan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib76\" title=\"\">2025</a>)</cite> are closer to our work, offering datasets that test interaction ability, expressiveness, and fidelity. However, these resources are primarily built from LLM-generated dialogues and TTS outputs, with limited human validation and a heavy reliance on proprietary ALLM-as-judge pipelines. As a result, they lack grounding in diverse human speech and typically apply only coarse rubrics.</p>\n\n",
                "matched_terms": [
                    "models",
                    "ding",
                    "2024b",
                    "voice",
                    "2025a",
                    "evaluation",
                    "speech",
                    "proprietary",
                    "kimiaudio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Expressive TTS evaluation.</span>\nRecent work has explored evaluation of instruction-following and style-controlled TTS. InstructTTSEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite> explicitly includes role-play scenarios, while EmergentTTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">(Manku et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib45\" title=\"\">2025</a>)</cite> and discrete speech LM benchmarks <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang &amp; Sz&#233;kely, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib63\" title=\"\">2024</a>)</cite> emphasize prosody, spontaneity, and speaker consistency. These efforts push beyond intelligibility and naturalness, but still center on synthetic outputs rather than real human role-play.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "evaluation",
                    "2025b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our distinction.</span>\nIn contrast, Speech-DRAME unifies these considerations through three elements: (i) a dual evaluation paradigm that combines archetype-driven generality with realism grounded in authentic human speech, (ii) human annotations that provide a reliable perceptual gold standard, and (iii) alignment of SEMs to human ratings, enabling systematic evaluation of both SFMs and the evaluation models themselves. This combination moves beyond synthetic benchmarks or zero-shot judging to provide a comprehensive and trustworthy framework for speech role-play assessment.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech role-play.</span>\nWe formalize <em class=\"ltx_emph ltx_font_italic\">speech role-play</em> as a conditional speech generation problem grounded in a character profile and scene description.\nGiven a dialogue context <math alttext=\"{\\bm{C}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{speech}}</annotation></semantics></math>, a character profile <math alttext=\"{\\bm{C}}_{\\text{profile}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>profile</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{profile}}</annotation></semantics></math>, and a scene specification <math alttext=\"{\\bm{C}}_{\\text{scene}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>scene</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{scene}}</annotation></semantics></math>, a <em class=\"ltx_emph ltx_font_italic\">speech foundation model</em> (SFM) generates a role-played speech response <math alttext=\"{\\bm{S}}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119930;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{S}}_{r}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "foundation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation.</span>\nIn principle, a <em class=\"ltx_emph ltx_font_italic\">speech evaluation model</em> (SEM) can assess a generated response using the same conditioning signals:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "evaluation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This formulation is intentionally modular: a dialogue-level evaluation can be obtained by aggregating single-turn scores across turns (e.g., averaging, recency-weighted pooling). We leave the detailed discussion of this multi-turn extension to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A3\" title=\"Appendix C Extension to Multi-turn Role-play Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech role-play evaluation can be framed in multiple ways depending on the available data and the desired granularity of analysis.\nWithin Speech-DRAME, we formalize two distinct strategies inspired by sociology theories<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Please refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A2\" title=\"Appendix B Theoretical Motivation for Dual Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> for detailed discussion of the dual evaluation design.</span></span></span>, including a top-down, archetype-oriented option using synthetic speech and a bottom-up, realism-oriented option from real human speech.\nRather than merging these into a single rubric, we treat them as separate perspectives, each with unique advantages and limitations.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarizes the key contrasts, while further implementation details are provided in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>-&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "refer",
                    "evaluation",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS5\" title=\"F.5 Examples of Realism Role-play &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.5</span></a>). The second strategy adopts a <em class=\"ltx_emph ltx_font_italic\">bottom-up</em> design, motivated by real-world performance and real human speech.\nInstead of relying solely on stereotypes, this approach grounds evaluation in recordings from professional and non-professional speakers, drawn from realistic dialogue and media sources.\nJudgments in this setting are more fine-grained and multi-dimensional, addressing aspects such as prosodic dynamics, emotional fidelity, character consistency, and contextual fit.\nWhile this strategy requires more human annotation and careful curation, it provides benchmarks that reflect how role-play is perceived in practice.\nIt thus offers a richer framework for analyzing model outputs in settings where realism and subtlety are essential.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "evaluation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section details the <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> data curation process. Our data curation methodology follows a two&#8211;stage design: (i) <em class=\"ltx_emph ltx_font_italic\">task preparation</em>, which specifies the roles, scenes, and evaluation settings used to elicit speech; and (ii) <em class=\"ltx_emph ltx_font_italic\">human annotation</em>, which provides reliable ground-truth judgments on generated or real human speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Archetype Data Collection &#8227; 4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents a summary of the curated <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> dataset, where we further elaborate the details of archetype and realism evaluation data collection in following subsections, respectively. For benchmark purpose, we provide an official train-test split for the human annotated dataset for <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> so that following researchers can use the dataset to train and evaluate their SEMs for speech role-play.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Sources.</span> The data is collected from a range of speech foundation models, including both cascaded and end-to-end models. We limit that only audio prompts are used as the input but adopt different prompt strategies for best quality with regards to different models. Details about the selected models and prompting details are discussed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS4\" title=\"D.4 Candidate Models for Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "cascaded",
                    "foundation",
                    "endtoend",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype Evaluation Data Summary.</span>\nThe resulting archetype evaluation dataset comprises 552 distinct scenarios for both Mandarin and English, evaluated across eight models,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>The model list is discussed in Appdenix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS4\" title=\"D.4 Candidate Models for Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.4</span></a>. Note we only include seven models for English due to the deactivation of ChatGPT-4O advanced voice model.</span></span></span> yielding 8,280 samples. In total, the dataset are split into 6,780 for training and 1,500 for testing. For benchmarking, an additional 1,250 scenarios spanning seven role/scene categories are prepared to facilitate archetype role-play benchmarking. More detailed statistics and analyses are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10\" title=\"Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">J</span></a>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "english",
                    "model",
                    "voice",
                    "mandarin",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Sources and Negative Samples.</span>\nWe curate speech from (i) ground-truth media segments, (ii) retrieved character profiles, and (iii) generated local scene descriptions. To stress-test evaluation models, we construct contrastive mismatches: (i) negative local scenes with varying degrees of mismatch, and (ii) negative character profiles created by re-synthesizing transcripts with TTS systems. Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6\" title=\"Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F</span></a> details the prompt design and TTS generation details.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotators judge whether performances align with the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em>, using rubrics for <em class=\"ltx_emph ltx_font_italic\">Prosodic Dynamics (i.e., Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis)</em>, <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness (with progressive gating over three sub-dimensions: Emotion Accuracy, Emotion Intensity, Emotion Transition)</em>, <em class=\"ltx_emph ltx_font_italic\">Character Consistency (i.e., Voice Identity Matching and Traits Embodiment)</em>, <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance (i.e., Local Scene Fit and Global Story Coherence)</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>. A prompt quality check and confidence ratings are also collected. Full scales and gating details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real-world Recoding.</span>\nAs noted in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S2\" title=\"2 Related Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, most prior efforts have relied heavily on synthetic data, mostly from TTS systems based on curated professional role-play voices. While such data can achieve high fidelity, it often lacks the diversity and spontaneity of real-world delivery, limiting the generalizability of evaluation models. To address this gap, we expand realism evaluation beyond synthetic and curated voices by incorporating newly recorded speech from both professional and amateur voice actors. This design introduces a broader spectrum of delivery styles, from polished professional acting to more natural and less controlled amateur performance.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models",
                    "evaluation",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation Data Summary.</span>\nThe resulting realism evaluation dataset is constructed from a combination of curated media sources, including internal collections, public datasets (e.g., NCSSD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib40\" title=\"\">2024b</a>)</cite>), and original recordings collected for this project.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>All data were obtained under appropriate licenses and are legally available for research release.</span></span></span> We compile 15k annotated role-play samples: 9k in Mandarin and 5k in English. The dataset is divided into 12k samples for training, 2k mixed samples for the base test set, and 1k real-recording samples for the test set. The first test set supports general evaluation using our proposed pipeline with negative samples, while the second focuses exclusively on human speech performed by both professional and amateur voice actors. For benchmarking, 2,000 scenarios from the realism dataset are designated for realism role-play benchmarking. Detailed statistics and analyses are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11\" title=\"Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">K</span></a>.</p>\n\n",
                "matched_terms": [
                    "english",
                    "2024b",
                    "voice",
                    "mandarin",
                    "evaluation",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the two proposed datasets, we conduct experiments under three settings: zero-shot, few-shot, and finetuning, using pre-trained ALLMs. We formulate the task as a multi-metric estimation problem&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tjandra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib58\" title=\"\">2025</a>; Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib74\" title=\"\">2025</a>; Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib55\" title=\"\">2025b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib54\" title=\"\">a</a>)</cite>, where a single model is trained or prompted to predict multiple evaluation dimensions.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "model",
                    "2025b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-shot and Few-shot ALLMs.</span>\nWe first evaluate the zero-shot capabilities of both proprietary and public models, including <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>, <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>, <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio</span>, and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>. All evaluations follow a unified prompting template that provides audio input alongside scene and character context.</p>\n\n",
                "matched_terms": [
                    "models",
                    "gemini25pro",
                    "proprietary",
                    "kimiaudio",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For proprietary models, we report the average of five independent runs to reduce variance and obtain more stable predictions. For open-source models, we observed weaker instruction-following ability and limited adaptation to unfamiliar tasks. To mitigate this, we did not request simultaneous prediction of multiple metrics. Instead, we queried one metric at a time. Following <cite class=\"ltx_cite ltx_citemacro_citet\">Song &amp; Bahri (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>); Lukasik et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>, we interpret model outputs through token distribution over discrete labels (<span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score.</p>\n\n",
                "matched_terms": [
                    "proprietary",
                    "models",
                    "model",
                    "opensource"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Finetuning Model: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>.</span>\nIn addition to zero-shot and few-shot evaluations, we fine-tune a dedicated evaluation model, denoted as <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> for archetype and realism role-play evaluation, respectively.\nWe initialize from <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio-7B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib11\" title=\"\">2024</a>)</cite>, which provides a strong foundation for multi-modal audio-text reasoning.\nTo better leverage the multiple human annotations available in our datasets, we adopt a sampling-based training strategy: at each step, one annotator label is randomly selected, following practices in label distribution learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Geng, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib22\" title=\"\">2016</a>; Wen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib67\" title=\"\">2023</a>)</cite>.\nThis stochastic supervision implicitly exposes the model to the empirical annotation distribution, while during inference we output the full distribution over discrete labels (e.g., <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score, consistent with recent decoding-based regression methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Song &amp; Bahri, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>; Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib70\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib64\" title=\"\">2025b</a>; Lukasik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>. We apply LoRA fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib28\" title=\"\">2022</a>)</cite>. All implementation details, including optimizer settings, training schedule, and sampling policy, are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS3\" title=\"L.3 Fine-tuning Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "evaluation",
                    "foundation",
                    "2025b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics for Role-play Evaluation Model.</span> For most dimensions with 1-5 scale, we report Pearson correlation. For <span class=\"ltx_text ltx_font_italic\">Content Pass</span> estimation, we report accuracy.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype Evaluation.</span>\nThe results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T2\" title=\"Table 2 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reveal clear distinctions across evaluation settings. In the zero-shot condition, proprietary ALLMs such as <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> substantially outperform public models, achieving strong <span class=\"ltx_text ltx_font_italic\">content pass</span> accuracy (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>90%) and moderate correlations with other dimensions(<math alttext=\"0.43\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mn>0.43</mn><annotation encoding=\"application/x-tex\">0.43</annotation></semantics></math>&#8211;<math alttext=\"0.48\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mn>0.48</mn><annotation encoding=\"application/x-tex\">0.48</annotation></semantics></math>). By contrast, public models exhibit limited predictive ability, with correlations often below 0.2. Few-shot prompting provides modest gains for proprietary models, improving both accuracy and correlation (e.g., <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> rises from 0.480 to 0.493 average correlation). The most significant improvement arises from finetuning: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves an average correlation of 0.629, marking a large margin over zero-shot/few-shot models and demonstrating the effectiveness of task-specific supervision.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "gemini25pro",
                    "evaluation",
                    "proprietary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T3\" title=\"Table 3 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> extends the analysis to fine-grained realism evaluation. Similar to archetype results, proprietary ALLMs lead the zero-shot setting, with <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> obtaining an average correlation of 0.390, compared to 0.284 for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>. Public models remain weak baselines. Few-shot prompting consistently improves performance: <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> gains +0.042 and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> gains +0.083 over their zero-shot averages, showing the benefit of in-context supervision. Once again, finetuning is most effective: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves 0.625 average correlation, higher than the best few-shot results. Importantly, the gains are consistent across all 10 realism dimensions, confirming that the sampling&#8211;expectation training strategy generalizes beyond simple classification to nuanced paralinguistic cues.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "gemini25pro",
                    "evaluation",
                    "proprietary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real Recording Test Set.</span>\nThe real-recording evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T4\" title=\"Table 4 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> is notably more challenging, with all models suffering substantial performance drops. Proprietary models in the zero-shot setting achieve only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.04&#8211;0.08 average correlation, and public models often fall below zero. Few-shot prompting yields small but noticeable improvements, especially for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> (0.226 avg.), suggesting that exemplars partially mitigate domain mismatch between synthetic and real human speech. Interestingly, while finetuning achieves the highest overall average (0.247), its relative advantage over few-shot prompting is narrower than in the synthetic setting, and some dimensions (e.g., <span class=\"ltx_text ltx_font_italic\">Character Identity</span>) remain weak. We emphasize that this observed gap between synthetic and real recordings is not merely an artifact but an intended feature of Speech-DRAME, designed to expose the mismatch and drive future research into bridging synthetic-to-real generalization.</p>\n\n",
                "matched_terms": [
                    "proprietary",
                    "models",
                    "evaluation",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall.</span> Across both archetype and realism evaluations, we observe three consistent trends: (i) zero-shot performance is limited, particularly for open-source models, (ii) few-shot prompting offers moderate but consistent gains, especially for proprietary ALLMs, and (iii) finetuning with task-specific supervision (<span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>) delivers the largest improvements, achieving state-of-the-art correlations across nearly all dimensions. Nevertheless, results on the real-recording test set underscore the gap between synthetic and real human role-play scenarios, motivating future work in data diversification and domain adaptation.</p>\n\n",
                "matched_terms": [
                    "proprietary",
                    "models",
                    "across",
                    "opensource"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, we consider a comprehensive set of candidate models, including six end-to-end models: <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib47\" title=\"\">2025b</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib75\" title=\"\">2024</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib71\" title=\"\">2025a</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Qwen3Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib72\" title=\"\">2025b</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">StepAudio2Mini</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib68\" title=\"\">2025</a>)</cite>; and ten cascaded models by combining two widely-used textual LLMs (i.e., <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib12\" title=\"\">2025</a>)</cite> and <span class=\"ltx_text ltx_font_typewriter\">Qwen3-30B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib73\" title=\"\">2025</a>)</cite>) and five recent TTS models (i.e., <span class=\"ltx_text ltx_font_typewriter\">F5TTS</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib8\" title=\"\">2024c</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">IndexTTS2</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib82\" title=\"\">2025b</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">MaskGCT</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib66\" title=\"\">2025d</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Vevo1.5</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib79\" title=\"\">2025b</a>)</cite>, and <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib17\" title=\"\">2024b</a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "models",
                    "cascaded",
                    "2024b",
                    "yang",
                    "evaluation",
                    "gpt4orealtime",
                    "glm4voice",
                    "candidate",
                    "qwen330binstruct",
                    "gemini25pro",
                    "comanici",
                    "openai",
                    "ding",
                    "2025a",
                    "kimiaudio",
                    "zeng",
                    "qwen25omni",
                    "endtoend",
                    "2025b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype evaluation.</span>\nOn archetype role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T5\" title=\"Table 5 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), cascaded pipelines consistently outperform end-to-end systems. The strongest cascaded systems, particularly <span class=\"ltx_text ltx_font_typewriter\">G-IndexTTS2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-IndexTTS2</span>, achieve both high content pass rates (above 94%) and balanced scores across audio quality, human likeness, and appropriateness. By contrast, end-to-end models such as <span class=\"ltx_text ltx_font_typewriter\">StepAudio2Mini</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> perform competitively in terms of content coverage (above 88%), but their expressive scores remain slightly lower than their cascaded counterparts. This gap suggests that separating reasoning (LLM) from speech rendering (TTS) remains advantageous when archetype fidelity is emphasized. Nevertheless, the fact that top end-to-end systems already surpass 3.0 in average rating indicates rapid closing of the performance gap.</p>\n\n",
                "matched_terms": [
                    "models",
                    "cascaded",
                    "across",
                    "evaluation",
                    "glm4voice",
                    "endtoend",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism evaluation.</span>\nRealism role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T6\" title=\"Table 6 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) presents a different landscape. Here, performance differences between end-to-end and cascaded systems are less pronounced. Several end-to-end models, such as <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>, reach average scores of 3.24, matching or even exceeding the best cascaded pipelines. Importantly, realism scores are more evenly distributed across prosodic, emotional, character, and contextual dimensions, with most models clustering between 2.9&#8211;3.2. This convergence reflects the difficulty of realism-style evaluation: since the data preparation emphasized contrast with authentic human speech, models are judged under similar conditions, leading to smaller observed gaps.</p>\n\n",
                "matched_terms": [
                    "models",
                    "cascaded",
                    "across",
                    "evaluation",
                    "glm4voice",
                    "endtoend",
                    "speech",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human alignment.</span>\nWe further conducted small-scale human studies with 10% of the benchmark data to verify the reliability of the benchmark. Archetype evaluation achieves a Spearman correlation of 0.706 between human judgments and <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, validating that the automatic evaluator closely tracks human preferences in this setting. Realism evaluation, however, achieves a more modest Spearman correlation of 0.375. This lower alignment is expected, since realism data construction did not prioritize fine-grained model separation but instead emphasized distinguishing human versus synthetic speech.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>We view this as evidence that realism presents a harder, less-saturated frontier. Rather than a limitation of DRAME-Eval, this highlights the challenge of designing evaluators that capture nuanced human perception, which motivates future work.</span></span></span> As a result, all models occupy a relatively narrow performance band, reducing sensitivity to model-level differences. We view this as evidence that realism evaluation highlights a harder, less saturated frontier where both SFMs and SEMs require further advances.</p>\n\n",
                "matched_terms": [
                    "models",
                    "evaluation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, cascaded pipelines retain an edge in archetype scenarios, especially in content robustness and expressive fidelity, but end-to-end models are catching up quickly. In realism scenarios, the field remains unsolved: existing systems exhibit similar performance bands and evaluators show weaker alignment with human judgments, underscoring the need for improved realism-grounded data and evaluation. Together, these findings establish <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> as both a reliable testbed for archetype fidelity and a challenging benchmark for realism fidelity.</p>\n\n",
                "matched_terms": [
                    "models",
                    "cascaded",
                    "evaluation",
                    "endtoend"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework for assessing spoken role-play through two complementary lenses: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em> (top-down) and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> (bottom-up, human-grounded). Our human-annotated <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> enables training and testing of SEMs, and our fine-tuned <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span> substantially outperforms zero-/few-shot ALLM judges. Building on this, <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> provides system-level comparison across various SFMs, revealing a consistent edge for cascaded pipelines on archetypes and a tighter spread under realism, where real human performance evaluation remain challenging. Together, these resources establish a reproducible, analyzable foundation for spoken role-play evaluation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>Limitations and future works are discussed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A13\" title=\"Appendix M Limitations and Future Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">M</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "cascaded",
                    "evaluation",
                    "foundation",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All data collection and annotation in this work followed strict ethical standards. Our bilingual datasets were constructed exclusively from legally accessible sources, including licensed media, open-source corpora, and original recordings conducted with informed consent. Annotators were recruited with appropriate compensation and provided with clear instructions, including the right to withdraw at any point. To minimize risks, no personally identifiable information was included in the released data, and all recordings were anonymized before annotation. We ensured cultural and linguistic diversity in annotator pools to mitigate potential bias across Mandarin and English role-play scenarios. While this study did not require formal IRB approval, we adhered to best practices in human-subject research, including transparency in task design and careful consideration of annotator well-being. Full details of our annotation protocol and ethical safeguards are presented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A9\" title=\"Appendix I Annotation Ethics &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "opensource",
                    "across",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments were conducted on widely available GPUs (NVIDIA H100). Random seeds are fixed across all runs to ensure consistency. By releasing datasets, models, and code, we aim to make all major results in this paper fully reproducible and facilitate future extensions of our framework.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline evaluators:</span> State-of-the-art audio LLMs (e.g., <span class=\"ltx_text ltx_font_typewriter\">GPT4o</span>, <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>) were included as zero-shot judges in our evaluation benchmark, enabling direct comparison with human-aligned SEMs.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "qwen25omni",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Proprietary LLM outputs were used in a limited way (e.g., instruction building, story summarization, or baseline evaluations) and only for internal research purposes. Released datasets in Speech-DRAME are constructed from open-source models, licensed corpora, and original human recordings.</p>\n\n",
                "matched_terms": [
                    "proprietary",
                    "models",
                    "opensource"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multi-turn evaluation.</span>\nEach generated speech segment is scored by an SEM:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p4.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> may be a simple mean, a recency-weighted average, or a learned temporal model that captures consistency across turns.\nThe choice of <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p4.m3\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> reflects the intended evaluation focus, e.g., stability of persona traits, adaptation to scene progression, or coherence across multiple speakers.</p>\n\n",
                "matched_terms": [
                    "across",
                    "evaluation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Discussion.</span>\nWhile our released dataset and experiments concentrate on single-turn evaluation, the modular nature of the formulation ensures that extending to multi-turn is straightforward.\nFuture work can leverage <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p5.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> to study richer aspects of speech role-play, such as persona persistence, interaction dynamics, and role consistency across extended dialogues.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "across",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Pipeline overview.</span>\nAt a high level, archetype evaluation data are generated through a three-stage pipeline:\n(1) selecting keywords that specify role and scene;\n(2) expanding them into full prompts via structured templates; and\n(3) rendering prompts into speech with contemporary TTS systems.\nThis modular pipeline ensures both scalability and reproducibility, while keeping prompts vivid enough for reliable judgments. (4) Based on the generated data (paired scene and synthesized speech), expert human annotators are recruited for detailed annotation on semantic check (i.e., <span class=\"ltx_text ltx_font_italic\">Content Pass</span>) and three different spoken role-play dimensions (i.e., <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human-likeness</span>, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>).\nImplementation details, including template design, alternative prompting strategies, and annotation guidelines, are provided in the following subsections of this Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Excluded Dimensions.</span> We initially considered more fine-grained control dimensions, but excluded them to preserve the archetype-level focus. Examples of excluded dimensions include:\n<em class=\"ltx_emph ltx_font_italic\">Emotional State &amp; Expression, Single Emotion Prompts, Explicit User Input / Assistant Output structures, Emotional Transition / Fluctuation, Voice Style &amp; Prosody (pacing, intonation, vocal timbre, expressive style, accent/dialect), and Content / Interaction Patterns (task/topic controls, specific linguistic structures, or speech production challenges).</em></p>\n\n",
                "matched_terms": [
                    "speech",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">The Choice of Prompts.</span> We also experimented with broader, less specific templates that allowed more freedom for generative models but produced less discriminative evaluation results:</p>\n\n",
                "matched_terms": [
                    "models",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All steps are automatically processed by <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib12\" title=\"\">2025</a>)</cite>, but each output is manually verified to ensure high quality for downstream usage. We demonstrate an example for Roles: Occupation / Functional Role in Fictional or Fantasy as follows:</p>\n\n",
                "matched_terms": [
                    "comanici",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech generation.</span>\nPrompts were rendered into audio with CosyVoice2-0.5B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib17\" title=\"\">2024b</a>)</cite>. The prompts are selected from human-verified high-quality samples, where the selection is conducted from open-source datasets, such as LibriSpeech/Gigaspeech for English, and WenetSpeech for Chinese&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib49\" title=\"\">2015</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib5\" title=\"\">2021</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib77\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "opensource",
                    "2024b",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Collection for Proprietary Models.</span> For proprietary systems, we adopt two collection strategies. For GPT-4o-realtime, responses are obtained directly via the <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-realtime</span> API. For Doubao and GPT-4o (Advanced Voice Mode), we collect outputs by recording from their official web interfaces. All recordings are captured through the interaction device itself, ensuring that no additional processing or distortion is introduced into the final audio.</p>\n\n",
                "matched_terms": [
                    "models",
                    "doubao",
                    "mode",
                    "voice",
                    "gpt4orealtime",
                    "proprietary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">End-to-End Model Generation.</span>\nFor end-to-end systems, the audio prompt is directly used as input. The system prompt is set to default values to ensure consistent speech generation quality.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model",
                    "endtoend"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cascaded Model Generation.</span>\nFor cascaded systems, we only use Cosyvoice2 as the TTS model in our collection of data. We select <span class=\"ltx_text ltx_font_typewriter\">Qwen3-30B-Instruct</span> and <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> as the text LLM candidates&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib73\" title=\"\">2025</a>; Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib12\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "cascaded",
                    "qwen330binstruct",
                    "yang",
                    "model",
                    "comanici",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Step 1: We first build a speech database (LibriSpeech and GigaSpeech for English, WenetSpeech for Mandarin&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib49\" title=\"\">2015</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib5\" title=\"\">2021</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib77\" title=\"\">2022</a>)</cite>). Each sample is annotated with Gemini-based speech captions describing salient acoustic attributes.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "english",
                    "mandarin"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2\" title=\"Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the internal annotation interface used for <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>.\nThe platform provides an integrated environment for both playback and evaluation of model-generated role-play speech samples.\nAnnotators first interact with the audio player interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf1\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>), which visualizes waveforms and spectrograms for multiple versions of the same prompt, enabling side-by-side comparison under a shared role instruction (e.g., firefighter, teacher, or actor).\nThen, they proceed to the annotation interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf2\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a>), where they assign scores for multiple dimensions such as <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, along with reasoning and confidence inputs.\nThis structured interface supports high-consistency, fine-grained evaluation of role-play behaviors in speech models.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realism Evaluation adopts a bottom&#8211;up perspective grounded in real human speech data, with the goal of capturing the nuanced delivery, prosody, and contextual fit that synthetic archetype prompts cannot fully represent.\nRather than relying on stereotypes, this strategy leverages real-world recordings and carefully constructed contrasts, enabling fine-grained assessment of model performance under realistic conditions.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "evaluation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data from real human speech sources.</span>\nWe construct the realism evaluation set primarily from natural speech material:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative sample generation.</span>\nTo stress-test evaluation models, we construct contrastive negative examples that deliberately mismatch role and scene information:</p>\n\n",
                "matched_terms": [
                    "models",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative character profiles.</span>\nWe also create role-profile mismatches by resynthesizing transcripts with diverse TTS systems, thereby breaking the link between authentic delivery and intended role.\nWe explore both text-instruction-based prompting and comparisons across commercial and non-commercial TTS frameworks.\nDetailed model lists and parameter settings are documented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS6\" title=\"F.6 TTS Systems and Settings &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.6</span></a>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, real human ground-truth segments and systematically constructed negatives form a balanced dataset that supports realism-oriented evaluation.\nThe positives provide natural anchors of role-play performance, while the negatives introduce controlled mismatches that test whether evaluation models can distinguish real human speech from synthetic approximations.\nThis design ensures that realism evaluation emphasizes subtle paralinguistic and contextual cues, moving beyond intelligibility or content fidelity.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During annotation, raters are presented with either: (i) ground-truth stories, (ii) stories with negative local scenes, or (iii) stories with negative character profile. Annotators assign realism scores without being exposed to the underlying mismatch reasons. Instead, these reasons are provided only to the quality-assurance team, who use them as reference for internal validation and consistency checks. While the mismatch reasons may not be perfect, they help streamline quality control procedures and maintain reliability across the dataset.</p>\n\n",
                "matched_terms": [
                    "across",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This distinction highlights the complementary strengths of the two evaluation paradigms. Archetype evaluation systematically explores a wide combinatorial space of roles and situations, but may lack the paralinguistic and contextual richness of real speech. Realism evaluation, on the other hand, anchors assessment in authentic delivery and contextual fit, providing a finer-grained lens on coherence and naturalness. Together, they ensure that role-play evaluation spans both broad stereotypical coverage and nuanced real-world plausibility.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed a diverse set of TTS systems to generate negative character profiles and resynthesized speech for Realism Evaluation.\nThese included both commercial and non-commercial models, with variation in style control, conditioning prompts, and reference usage.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.T8\" title=\"Table 8 &#8227; Same-Speaker Voice Clone. &#8227; F.6 TTS Systems and Settings &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> summarizes the systems and settings.\nBelow, we detail the major strategies used across systems:</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "evaluation",
                    "speech",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A character profile was first converted into a descriptive voice specification by prompting <span class=\"ltx_text ltx_font_typewriter\">deepseek-v3</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib38\" title=\"\">2024a</a>)</cite>.\nThis description was submitted to the TTS service to generate candidate voices.\nWe then previewed samples via Gemini2.5Pro, selecting the voice that best matched the speaker style of the original recording before proceeding to generate speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "candidate",
                    "gemini25pro",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For models using voice prompts, we use an inhouse enhancement model to first process the prompt audio so as to minimize the effect of audio events and background noise.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We additionally employed five proprietary inhouse TTS systems, each exploring different conditioning strategies for realism-oriented speech synthesis.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "proprietary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS2</span> (BPE&#8211;Cosy2 Pipeline).\nInstead of phones, this model takes Qwen2.5 byte-pair-encoding (BPE) tokens as input&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib71\" title=\"\">2025a</a>)</cite>. The same 0.4B GPT-2 predicts <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> tokens, followed by the identical flow-matching and vocoder stages as in <span class=\"ltx_text ltx_font_typewriter\">InhouseTTS1</span>. Speaker embeddings are also injected to maintain consistent speaker style.</p>\n\n",
                "matched_terms": [
                    "2025a",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS3</span> (Embedding&#8211;AudioCode Pipeline).\nHere, the input is Qwen2.5 BPE embeddings, which a 0.4B GPT-2 model maps directly to 4,096-dimensional audio codes. These codes are decoded via a cascaded architecture: <span class=\"ltx_text ltx_font_typewriter\">RepCodec</span> decoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib32\" title=\"\">2024</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> content representation (with speaker embedding) <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DDIM</span> (denoising\ndiffusion implicit model) module&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jeong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib34\" title=\"\">2021</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DAC</span> latent representation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib37\" title=\"\">2023</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DAC</span> decoder. Conditioning is provided through prefix text embeddings (e.g., instructions expressing emotional states such as &#8220;angry, hateful: you betrayed me&#8230;&#8221;), along with speaker ID. The input format is <span class=\"ltx_text ltx_font_typewriter\">[spk_id, BPE embedding, audio codes]</span>.</p>\n\n",
                "matched_terms": [
                    "architecture",
                    "cascaded",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The annotation interface for <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> focuses on assessing contextual and narrative coherence in model-generated speech.\nUnlike the archetype-oriented evaluation, which emphasizes stylistic and emotional fidelity, this platform examines how well the voice performance aligns with a character&#8217;s personality, dialogue situation, and scene progression.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "evaluation",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human-grounded, bilingual labels.</span>\nWe annotate in English <em class=\"ltx_emph ltx_font_italic\">and</em> Mandarin with trained raters, capturing cross-lingual prosody and style, while recording rater confidence to qualify downstream usage.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transparent evaluation substrate.</span> Ours produces per-dimension human rationales and confidence; judge-only pipelines (ALLM/Gemini) provide scores with limited per-sample interpretability&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "2025b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All human annotation in this work was conducted in accordance with standard ethical practices for research involving human annotators. Annotators were recruited via public hiring for Mandarin and internal lab hiring for English. Prior to participation, all annotators were provided with a clear description of the task, the type of data they would encounter, and their rights as participants, including the ability to opt out at any time without penalty.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The archetype evaluation provides a detailed view of how speech foundation models (SFMs) perform when tasked with role-play scenarios grounded in stereotypical characters and situations. Human annotations cover four dimensions, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, and <span class=\"ltx_text ltx_font_italic\">Content Pass</span>, and the results reveal important trends, as well as challenges in interpreting scores across languages.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "foundation",
                    "evaluation",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A critical observation is that annotations in Mandarin and English cannot be directly compared. The annotation groups for the two languages differed in their linguistic backgrounds and cultural interpretations, which leads to systematic differences in scoring. Our bilingual pilot further confirmed that annotators often interpret role-play appropriateness and expressiveness differently depending on their cultural frame of reference. For example, Mandarin raters tend to reward delivery styles that align with culturally expected prosody and politeness strategies, while English raters prioritize spontaneity and naturalness. These discrepancies parallel the findings of the singing voice conversion challenge (SVCC2023) from <cite class=\"ltx_cite ltx_citemacro_citet\">Huang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib31\" title=\"\">2023</a>)</cite>, where native and non-native listeners provided significantly different ratings for the same samples, particularly for naturalness and speaker similarity. Consequently, while absolute scores differ substantially across Mandarin and English, only within-language comparisons are meaningful. Cross-lingual ranking is not valid given the different annotation bases.</p>\n\n",
                "matched_terms": [
                    "across",
                    "english",
                    "voice",
                    "mandarin",
                    "reference"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Within Mandarin from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, the relative rankings of models are clear. <span class=\"ltx_text ltx_font_typewriter\">Doubao</span> stands out as the strongest performer across all four dimensions, reaching near-ceiling levels in both <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> (4.37) and <span class=\"ltx_text ltx_font_italic\">Content Pass</span> (0.98). It also leads in role-related dimensions such as <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, indicating balanced competence across linguistic accuracy, delivery, and expressiveness. <span class=\"ltx_text ltx_font_typewriter\">G-CosyVoice2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-CosyVoice2</span> follow closely, especially in audio quality, though they lag behind in role appropriateness. <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> achieve mid-range results, reflecting higher variance across samples, while <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> shows competitive content understanding but lower naturalness and appropriateness. <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> consistently ranks the lowest, with large variance, underscoring instability and weaker alignment with archetypal role expectations.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "doubao",
                    "qcosyvoice2",
                    "mandarin",
                    "gpt4orealtime",
                    "glm4voice",
                    "gcosyvoice2",
                    "kimiaudio",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The picture shifts in English, elaborated in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Here, <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> emerges as the most effective model by a wide margin, achieving top scores in both <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> (3.46) and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> (3.49), as well as high content pass rates (0.97). This reflects its strong grounding in English discourse norms and role adherence. In contrast, <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> variants perform well in <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> but are weaker in role alignment, highlighting a trade-off between acoustic fidelity and role-play embodiment. Other models such as <span class=\"ltx_text ltx_font_typewriter\">Doubao</span>, <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span>, and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> achieve moderate results, while <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> trail far behind, showing limited robustness in English scenarios. Taken together, the English results reveal both a sharper separation across models and a wider variance in appropriateness scores compared to Mandarin.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "doubao",
                    "english",
                    "model",
                    "mandarin",
                    "gpt4orealtime",
                    "glm4voice",
                    "kimiaudio",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T11\" title=\"Table 11 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> and Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.F5\" title=\"Figure 5 &#8227; English Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.F6\" title=\"Figure 6 &#8227; English Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> further highlight differences across domains. In Mandarin, performance is relatively consistent across categories, with means clustering around 3.1&#8211;3.3. The highest scores appear in <span class=\"ltx_text ltx_font_typewriter\">SocialIdentity</span> and <span class=\"ltx_text ltx_font_typewriter\">DailyOccupation</span>, suggesting that models handle culturally grounded archetypes more reliably. In English, however, domain variation is much greater. Categories such as <span class=\"ltx_text ltx_font_typewriter\">FantasyOccupation</span> and <span class=\"ltx_text ltx_font_typewriter\">NamedCharacters</span> receive notably lower scores (means below 2.0), reflecting the difficulty of producing expressive, role-appropriate speech in imaginative or culturally specific contexts. Occupational roles remain relatively easier for models in both languages, likely because these scenarios are closer to training data distributions.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across",
                    "english",
                    "mandarin",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although cross-lingual comparisons are invalid, this does not diminish the utility of archetype evaluation for role-play benchmarking. In fact, the observed inconsistencies reinforce a central point: successful role-play requires models to adapt to the cultural and linguistic norms of the target language community. A system that performs well in Mandarin but poorly in English cannot be considered universally capable, since end users will evaluate appropriateness and naturalness according to their native expectations. Thus, archetype evaluation highlights not only model-specific strengths and weaknesses but also the need for culturally grounded evaluation pipelines.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "models",
                    "evaluation",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In sum, archetype evaluation reveals three major findings:\n(1) Mandarin and English annotations cannot be directly compared, due to linguistic and cultural annotation inconsistencies, consistent with prior evidence from SVCC 2023.\n(2) Within each language, clear model rankings emerge, <span class=\"ltx_text ltx_font_typewriter\">Doubao</span> dominates in Mandarin, while <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> excels in English.\n(3) Domain sensitivity remains a challenge, particularly for imaginative or culturally rich roles, where scores are substantially lower and inter-model variance is higher.</p>\n\n",
                "matched_terms": [
                    "doubao",
                    "english",
                    "model",
                    "mandarin",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, the consistently high agreement confirms that the rubric isolates perceptually coherent factors, so learned evaluators can target them reliably.\nSecond, the gated behavior of <em class=\"ltx_emph ltx_font_italic\">Intensity/Transition</em> motivates a <em class=\"ltx_emph ltx_font_italic\">hierarchical</em> evaluator: verify <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> before scoring graded expressivity, or model the joint structure with conditional heads.\nThird, <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> remains challenging; models may benefit from explicit character-trait priors, long-term context aggregation, or contrastive supervision between matched and mismatched traits.\nFinally, prosody-focused objectives (e.g., pitch/rhythm/stress control) are likely to yield the largest quality gains for amateur-style speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and fewshot settings, we condition the model with the rubric prompt and then append few-shot demonstration triplets (input, audio, gold JSON) to mirror the human annotation protocol. As shown in Listing&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#LST2\" title=\"Listing 2 &#8227; L.1 Zeroshot/Fewshot Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, We use <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> for these few-shot exemplars and require a minified JSON output consistent with the rubric schema.</p>\n\n",
                "matched_terms": [
                    "model",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and few-shot experiments on commercial models such as <span class=\"ltx_text ltx_font_typewriter\">Gemini</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o</span>, we report results as the average of five independent runs to reduce randomness and obtain more stable predictions. In contrast, for open-source models, we observed that their instruction-following ability is relatively weak. To mitigate this, we prompt them to predict one dimension at a time rather than all dimensions jointly. Specifically, for each metric, after showing the rubric prompt, we request completion in a fixed template and extract the probability distribution over the discrete tokens <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>. The final score is then computed from these probabilities, which we found makes open-source models usable to some extent under this setting.</p>\n\n",
                "matched_terms": [
                    "models",
                    "opensource"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For few-shot experiments, we randomly sample three exemplars from the training set, with each exemplar chosen such that its human scores are close to the rounded average across all annotators. These exemplars are appended as demonstrations in the prompt. As with the zeroshot case, we run five trials for commercial models and report the averaged results for evaluation.</p>\n\n",
                "matched_terms": [
                    "models",
                    "evaluation",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.T12\" title=\"Table 12 &#8227; L.2 Zeroshot/Fewshot Ablation Experiments &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> shows how different prompting strategies affect realism evaluation performance with <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>. Several observations can be drawn:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Effect of repeated runs.\nWhen comparing single-run decoding (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot-1run</span>) against averaging over five runs (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot</span>), we see consistent gains across nearly all dimensions (average score improves from 0.353 to 0.390). This suggests that sampling variance plays a substantial role in the reliability of large language model (LLM) evaluators, and averaging multiple outputs reduces this variance, producing more stable correlation with human annotations.</p>\n\n",
                "matched_terms": [
                    "across",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For realism evaluation on the basic test set, <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> consistently achieves the strongest correlations across most dimensions, especially for <span class=\"ltx_text ltx_font_italic\">Character Identity</span> and <span class=\"ltx_text ltx_font_italic\">Traits</span>, leading to the best average correlation (0.668). <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> also performs competitively (0.630), while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> lags slightly behind (0.625). These results highlight that full fine-tuning may still offer an advantage when the evaluation set is relatively homogeneous and closer to the training distribution.</p>\n\n",
                "matched_terms": [
                    "across",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the comprehensive design of Speech-DRAME, several limitations remain.\nFirst, while our dual evaluation paradigm combines top-down archetype assessment with bottom-up realism evaluation, both strategies are constrained by the quality and scope of available data. Archetype evaluation inevitably relies on simplified stereotypes, which can overlook subtler paralinguistic and contextual cues. Realism evaluation, though richer, still struggles with domain mismatch between synthetic data and real human recordings, as evidenced by the performance drop observed in the real-recording test set. This highlights the persistent gap between benchmark-driven evaluation and real-world variability in speech role-play.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, the present framework emphasizes bilingual evaluation (Mandarin and English), but linguistic and cultural diversity is far broader. Cross-lingual generalization and culturally sensitive evaluation are still open challenges, particularly since we observed annotation discrepancies across language groups. This limits the extent to which Speech-DRAME can claim universal coverage.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "across",
                    "evaluation",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Third, while DRAME-Eval substantially outperforms zero-shot and few-shot ALLMs, its reliance on fine-tuning from a strong base model raises questions of accessibility and scalability. Future work should explore lightweight adaptation techniques, semi-supervised training with partially annotated data, and leveraging emergent capabilities of open-source multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "opensource",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, although we validated DRAME-Eval against human annotations, alignment under realism settings remains modest. This suggests that even human-annotated benchmarks are insufficiently fine-grained to capture the full complexity of human perception in speech role-play. Richer annotation protocols, more diverse speaker populations, and integration of perceptual psychology insights could further improve evaluation fidelity.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Looking forward, we see several promising research directions. We intentionally restrict evaluation to single-turn responses for clarity and tractability. However, multi-turn extensions that capture narrative progression, long-term consistency, and dynamic emotion shifts remain a natural next step. Incorporating multilingual and cross-cultural datasets will enhance generality and fairness. On the modeling side, exploring hybrid evaluation strategies that combine discrete rubrics with continuous perceptual signals (e.g., prosody embeddings or affective measures) may better bridge the gap between automatic and human judgments. Finally, integrating Speech-DRAME into reinforcement learning pipelines as a reward model could help drive the next generation of expressive, human-aligned speech role-play systems.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "evaluation",
                    "model"
                ]
            }
        ]
    },
    "A5.SS4.SSSx7.p1": {
        "caption": "",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\"><span class=\"ltx_text ltx_font_bold\">Score</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:330.1pt;\"><span class=\"ltx_text ltx_font_bold\">Definition</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\">5 Excellent</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:330.1pt;\">Clean, clear, high-quality throughout; no digital noise/glitches/issues.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\">4 Good</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:330.1pt;\">Good overall; minor artifacts detectable on close listen (slight sharpness/hiss, etc.).</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\">3 Fair</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:330.1pt;\">Acceptable but obvious artifacts (buzz, echo, synthetic timbre) impacting clarity.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\">2 Poor</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:330.1pt;\">Frequent/consistent glitches or severe issues; distracting metallic/clipping/harsh tones.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:51.2pt;\">1 Very Poor</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:330.1pt;\">Extremely poor; layered artifacts dominate; severe clipping/static/failures, hard to understand.</span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "poor",
            "issues",
            "layered",
            "slight",
            "overall",
            "good",
            "distracting",
            "very",
            "etc",
            "artifacts",
            "listen",
            "understand",
            "digital",
            "definition",
            "metallicclippingharsh",
            "impacting",
            "sharpnesshiss",
            "severe",
            "tones",
            "dominate",
            "highquality",
            "fair",
            "noiseglitchesissues",
            "excellent",
            "clarity",
            "minor",
            "clear",
            "timbre",
            "obvious",
            "acceptable",
            "detectable",
            "frequentconsistent",
            "score",
            "echo",
            "clean",
            "glitches",
            "clippingstaticfailures",
            "throughout",
            "close",
            "buzz",
            "extremely",
            "hard",
            "synthetic"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real Recording Test Set.</span>\nThe real-recording evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T4\" title=\"Table 4 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> is notably more challenging, with all models suffering substantial performance drops. Proprietary models in the zero-shot setting achieve only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.04&#8211;0.08 average correlation, and public models often fall below zero. Few-shot prompting yields small but noticeable improvements, especially for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> (0.226 avg.), suggesting that exemplars partially mitigate domain mismatch between synthetic and real human speech. Interestingly, while finetuning achieves the highest overall average (0.247), its relative advantage over few-shot prompting is narrower than in the synthetic setting, and some dimensions (e.g., <span class=\"ltx_text ltx_font_italic\">Character Identity</span>) remain weak. We emphasize that this observed gap between synthetic and real recordings is not merely an artifact but an intended feature of Speech-DRAME, designed to expose the mismatch and drive future research into bridging synthetic-to-real generalization.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall.</span> Across both archetype and realism evaluations, we observe three consistent trends: (i) zero-shot performance is limited, particularly for open-source models, (ii) few-shot prompting offers moderate but consistent gains, especially for proprietary ALLMs, and (iii) finetuning with task-specific supervision (<span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>) delivers the largest improvements, achieving state-of-the-art correlations across nearly all dimensions. Nevertheless, results on the real-recording test set underscore the gap between synthetic and real human role-play scenarios, motivating future work in data diversification and domain adaptation.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><em class=\"ltx_emph ltx_font_italic\">Placeholders:</em> Very short/abstract placeholders like &#8220;nurse&#8221; or &#8220;concept&#8221; are acceptable; longer placeholders like &#8220;and you can fill in the details here&#8221; are rejectable.</p>\n\n",
                "matched_terms": [
                    "very",
                    "acceptable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If any portion of the clip is ill-fitting, prefer the lower overall score; do not average across good and bad segments.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "score",
                    "good"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Independently</span> rate technical clarity (ignore content and fit). After scoring Audio Quality, <em class=\"ltx_emph ltx_font_italic\">disregard</em> its issues for the other two dimensions.</p>\n\n",
                "matched_terms": [
                    "clarity",
                    "issues"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rubric Rating Instructions.</span>\nScore each subcriterion on a 1&#8211;5 scale. Use the detailed rubrics below and the examples to maintain calibration across characters, tones, and emotional styles. Prioritize in-character believability; do not reward theatricality, emotional intensity, or technical polish unless they naturally fit the character <em class=\"ltx_emph ltx_font_italic\">and</em> scene.</p>\n\n",
                "matched_terms": [
                    "tones",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Use the comment box to note contradictions, missing information, vague descriptions, or multi-speaker issues that affected your evaluation. Keep comments brief, clear, and non-technical; no transcript quotes are required.</p>\n\n",
                "matched_terms": [
                    "clear",
                    "issues"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F8\" title=\"Figure 8 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F9\" title=\"Figure 9 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> visualize the score distributions per dimension.\nWe highlight three patterns:\n(i) <span class=\"ltx_text ltx_font_bold\">Progressive/gated emotion metrics.</span> For <em class=\"ltx_emph ltx_font_italic\">Emotion Intensity</em> and <em class=\"ltx_emph ltx_font_italic\">Emotion Transition</em>, we assign <span class=\"ltx_text ltx_font_typewriter\">1</span> when the scene is marked <em class=\"ltx_emph ltx_font_italic\">N/A</em> (i.e., emotion accuracy is insufficient to meaningfully judge intensity/transition). This produces a visible mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, a sparse bin at <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>, and slightly lower overall agreement; downstream analyses conditioning on adequate <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> (e.g., mean&#160;<math alttext=\"\\geq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 3</annotation></semantics></math>) recover more bell-shaped distributions.\n(ii) <span class=\"ltx_text ltx_font_bold\">Traits Embodiment.</span> Scores tend to avoid the middle (<math alttext=\"\\sim\\!3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!3</annotation></semantics></math>), concentrating on low values when traits clearly mismatch and high values when traits are strongly realized. This yields the lowest inter-rater agreement among dimensions, reflecting the inherently compositional and role-specific nature of trait judgments.\n(iii) <span class=\"ltx_text ltx_font_bold\">Other dimensions.</span> <em class=\"ltx_emph ltx_font_italic\">Pitch Variation</em>, <em class=\"ltx_emph ltx_font_italic\">Rhythmic Naturalness</em>, <em class=\"ltx_emph ltx_font_italic\">Stress &amp; Emphasis</em>, <em class=\"ltx_emph ltx_font_italic\">Voice Identity Matching</em>, <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em>, <em class=\"ltx_emph ltx_font_italic\">Global Story Coherence</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> are approximately unimodal with modes at <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>&#8211;<math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math>, consistent with moderate&#8211;strong realization in the dataset and with the high agreement reported.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the archetype test set, all fine-tuned variants achieve strong <span class=\"ltx_text ltx_font_italic\">Content Pass</span> accuracy above 92%. Differences appear mainly in correlation-based metrics. Here, <span class=\"ltx_text ltx_font_typewriter\">Base-FT</span> yields the highest score on <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> delivers the strongest performance in <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> and the best overall average (0.629). This suggests that instruction-tuned checkpoints paired with parameter-efficient adaptation can better capture higher-level perceptual traits.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A different pattern emerges on the real recording test set. Both <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> and <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> struggle, yielding negative or near-zero correlations across most dimensions. In contrast, <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> shows clear improvements, achieving positive correlations in all prosodic and emotional dimensions and producing the highest overall average (0.247). This indicates that LoRA adaptation on top of an instruction-tuned backbone provides better generalization to out-of-distribution, real-world data.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "clear"
                ]
            }
        ]
    },
    "A5.SS4.SSSx8.p1": {
        "caption": "",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\"><span class=\"ltx_text ltx_font_bold\">Score</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:256.1pt;\"><span class=\"ltx_text ltx_font_bold\">Definition</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">5 Definitely Human</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:256.1pt;\">Indistinguishable from human: rich intonation, natural pacing, consistent identity; organic pauses/breaths, logical inflection.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">4 Most Likely Human</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:256.1pt;\">Generally convincing; slight stiffness/over-even pacing or minor awkward pauses.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">3 Could be Human or AI</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:256.1pt;\">Mix of natural and unnatural traits; borderline cases with both good and bad sections.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">2 Mostly Likely AI</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:256.1pt;\">Clearly artificial patterns: choppy timing, missing pauses, flat/illogical inflections, forced breaths.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">1 Definitely AI</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:256.1pt;\">Rigid, machine-like delivery throughout; flat/monotone, &#8220;read-aloud&#8221; style, no natural rhythm.</span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "logical",
            "natural",
            "identity",
            "choppy",
            "slight",
            "rigid",
            "inflections",
            "style",
            "intonation",
            "good",
            "rich",
            "likely",
            "flatmonotone",
            "definition",
            "most",
            "unnatural",
            "bad",
            "delivery",
            "stiffnessovereven",
            "from",
            "mix",
            "forced",
            "breaths",
            "minor",
            "could",
            "mostly",
            "patterns",
            "rhythm",
            "convincing",
            "flatillogical",
            "missing",
            "awkward",
            "both",
            "artificial",
            "organic",
            "pacing",
            "borderline",
            "pausesbreaths",
            "score",
            "indistinguishable",
            "pauses",
            "machinelike",
            "inflection",
            "consistent",
            "cases",
            "generally",
            "throughout",
            "human",
            "definitely",
            "clearly",
            "traits",
            "timing",
            "readaloud",
            "sections"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Role-play has become a key testbed for generative models, expanding from text-only dialogue to multimodal interaction. Extending role-play to speech captures prosody, emotion, and delivery, but also poses new evaluation challenges. Current pipelines often use audio large language models (ALLMs) as zero-shot judges, which miss paralinguistic cues, collapse multiple aspects into coarse scores, and rely on synthetic speech references that fail to reflect real-world roles. We present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework that contributes at three levels: (i) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-EvalBench</em>, an evaluation benchmark with bilingual human-annotated data and protocols for training and testing speech evaluation models&#160;(SEMs), (ii) <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, a fine-tuned evaluation model, which substantially outperforms zero-shot and few-shot ALLMs, and (iii) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-RoleBench</em>, a speech role-play benchmark that leverages <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> as an automatic judge to compare speech foundation models (SFMs). Speech-DRAME distinguishes between two complementary evaluation strategies: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>, a top-down approach measuring adherence to broad role archetypes, and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em>, a bottom-up approach grounded in real human speech that emphasizes nuanced role quality. Compared to zero-shot ALLM judges, <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves stronger agreement with human ratings (Pearson correlation from 0.480 to 0.629 in archetypes, and 0.390 to 0.625 in realism). By integrating transparent benchmark resources, modeling approaches, and system-level evaluation, Speech-DRAME provides the first comprehensive, reproducible foundation for assessing spoken role-play.</p>\n\n",
                "matched_terms": [
                    "from",
                    "human",
                    "delivery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">it lacks sensitivity to paralinguistic cues such as intonation, pacing, and emotional dynamics that define role delivery&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>; Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib81\" title=\"\">2025a</a>)</cite>,</p>\n\n",
                "matched_terms": [
                    "intonation",
                    "pacing",
                    "delivery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>, the first evaluation benchmark with human annotations under both archetype- and realism-based strategies, supporting the training and testing of SEMs.</p>\n\n",
                "matched_terms": [
                    "both",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-based role-play evaluation.</span>\nEarly research focused on text-only dialogue benchmarks. Datasets such as RoleLLM, CharacterBox, and RoleMRC <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib62\" title=\"\">2024</a>; Tu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib60\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib61\" title=\"\">2025a</a>; Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib53\" title=\"\">2023</a>; Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib41\" title=\"\">2025</a>)</cite> test whether models can sustain persona consistency and character fidelity, using LLM-as-judge pipelines for scalability. However, studies like PersonaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib81\" title=\"\">2025a</a>)</cite> have shown that LLM-as-judge approaches are far from perfect, struggling with tasks as basic as tracking speaker identity. This highlights broader concerns about reliability even in text settings, which become only more severe when extended to speech, where prosody, emotion, and vocal style play a central role.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "from",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-native benchmarks.</span>\nMore recently, a number of resources have been introduced to evaluate spoken role-play agents. OmniCharacter-10K <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>)</cite> emphasizes character voice traits, while others&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib39\" title=\"\">2025</a>; Hou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib27\" title=\"\">2025</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib15\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib7\" title=\"\">2024b</a>)</cite> extend evaluation toward vocal communication, assistant capability, and multi-turn realism. Omni models such as StepAudio and KimiAudio <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite> also include role-play scenarios in their evaluation. SpeechRole and VStyle <cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>; Zhan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib76\" title=\"\">2025</a>)</cite> are closer to our work, offering datasets that test interaction ability, expressiveness, and fidelity. However, these resources are primarily built from LLM-generated dialogues and TTS outputs, with limited human validation and a heavy reliance on proprietary ALLM-as-judge pipelines. As a result, they lack grounding in diverse human speech and typically apply only coarse rubrics.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "from",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Automatic judging with ALLMs.</span>\nIn parallel, advances in automatic judging highlight both opportunities and limitations. Audio-capable LLMs such as AudioJudge and Audio-Aware LLMs as Judges <cite class=\"ltx_cite ltx_citemacro_citep\">(Manakul et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib44\" title=\"\">2025</a>; Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>)</cite> can assess style, emotion, and delivery, while systems like Auto-ATT <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib65\" title=\"\">2025c</a>)</cite> scale this to Audio Turing Tests. Yet studies consistently reveal high prompt sensitivity and shallow capture of paralinguistic cues, suggesting that zero-shot judging remains insufficient for nuanced role-play evaluation.</p>\n\n",
                "matched_terms": [
                    "delivery",
                    "both",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our distinction.</span>\nIn contrast, Speech-DRAME unifies these considerations through three elements: (i) a dual evaluation paradigm that combines archetype-driven generality with realism grounded in authentic human speech, (ii) human annotations that provide a reliable perceptual gold standard, and (iii) alignment of SEMs to human ratings, enabling systematic evaluation of both SFMs and the evaluation models themselves. This combination moves beyond synthetic benchmarks or zero-shot judging to provide a comprehensive and trustworthy framework for speech role-play assessment.</p>\n\n",
                "matched_terms": [
                    "human",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here <math alttext=\"{\\bm{S}}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#119930;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{S}}_{r}</annotation></semantics></math> represents the spoken output (waveform or latent representation). This definition captures both the linguistic dimension (coherence with context and scene) and the paralinguistic dimension (prosody, style, emotion) expected of the role.</p>\n\n",
                "matched_terms": [
                    "definition",
                    "both",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech role-play evaluation can be framed in multiple ways depending on the available data and the desired granularity of analysis.\nWithin Speech-DRAME, we formalize two distinct strategies inspired by sociology theories<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Please refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A2\" title=\"Appendix B Theoretical Motivation for Dual Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> for detailed discussion of the dual evaluation design.</span></span></span>, including a top-down, archetype-oriented option using synthetic speech and a bottom-up, realism-oriented option from real human speech.\nRather than merging these into a single rubric, we treat them as separate perspectives, each with unique advantages and limitations.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarizes the key contrasts, while further implementation details are provided in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>-&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "from",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS5\" title=\"F.5 Examples of Realism Role-play &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.5</span></a>). The second strategy adopts a <em class=\"ltx_emph ltx_font_italic\">bottom-up</em> design, motivated by real-world performance and real human speech.\nInstead of relying solely on stereotypes, this approach grounds evaluation in recordings from professional and non-professional speakers, drawn from realistic dialogue and media sources.\nJudgments in this setting are more fine-grained and multi-dimensional, addressing aspects such as prosodic dynamics, emotional fidelity, character consistency, and contextual fit.\nWhile this strategy requires more human annotation and careful curation, it provides benchmarks that reflect how role-play is perceived in practice.\nIt thus offers a richer framework for analyzing model outputs in settings where realism and subtlety are essential.</p>\n\n",
                "matched_terms": [
                    "from",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task Preparation.</span>\nWe employ complementary strategies: a <em class=\"ltx_emph ltx_font_italic\">top&#8211;down</em> archetype-based approach, rooted in stereotypes and generalized role expectations, and a <em class=\"ltx_emph ltx_font_italic\">bottom&#8211;up</em> realism-based approach, derived from real human speech. Both follow a modular pipeline of role/scene definition, prompt expansion, and speech rendering or retrieval, ensuring scalability and validity.</p>\n\n",
                "matched_terms": [
                    "definition",
                    "human",
                    "from",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Sources.</span> The data is collected from a range of speech foundation models, including both cascaded and end-to-end models. We limit that only audio prompts are used as the input but adopt different prompt strategies for best quality with regards to different models. Details about the selected models and prompting details are discussed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS4\" title=\"D.4 Candidate Models for Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "from",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotators judge whether performances align with the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em>, using rubrics for <em class=\"ltx_emph ltx_font_italic\">Prosodic Dynamics (i.e., Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis)</em>, <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness (with progressive gating over three sub-dimensions: Emotion Accuracy, Emotion Intensity, Emotion Transition)</em>, <em class=\"ltx_emph ltx_font_italic\">Character Consistency (i.e., Voice Identity Matching and Traits Embodiment)</em>, <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance (i.e., Local Scene Fit and Global Story Coherence)</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>. A prompt quality check and confidence ratings are also collected. Full scales and gating details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "traits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real-world Recoding.</span>\nAs noted in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S2\" title=\"2 Related Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, most prior efforts have relied heavily on synthetic data, mostly from TTS systems based on curated professional role-play voices. While such data can achieve high fidelity, it often lacks the diversity and spontaneity of real-world delivery, limiting the generalizability of evaluation models. To address this gap, we expand realism evaluation beyond synthetic and curated voices by incorporating newly recorded speech from both professional and amateur voice actors. This design introduces a broader spectrum of delivery styles, from polished professional acting to more natural and less controlled amateur performance.</p>\n\n",
                "matched_terms": [
                    "natural",
                    "mostly",
                    "most",
                    "delivery",
                    "from",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation Data Summary.</span>\nThe resulting realism evaluation dataset is constructed from a combination of curated media sources, including internal collections, public datasets (e.g., NCSSD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib40\" title=\"\">2024b</a>)</cite>), and original recordings collected for this project.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>All data were obtained under appropriate licenses and are legally available for research release.</span></span></span> We compile 15k annotated role-play samples: 9k in Mandarin and 5k in English. The dataset is divided into 12k samples for training, 2k mixed samples for the base test set, and 1k real-recording samples for the test set. The first test set supports general evaluation using our proposed pipeline with negative samples, while the second focuses exclusively on human speech performed by both professional and amateur voice actors. For benchmarking, 2,000 scenarios from the realism dataset are designated for realism role-play benchmarking. Detailed statistics and analyses are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11\" title=\"Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">K</span></a>.</p>\n\n",
                "matched_terms": [
                    "human",
                    "from",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Finetuning Model: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>.</span>\nIn addition to zero-shot and few-shot evaluations, we fine-tune a dedicated evaluation model, denoted as <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> for archetype and realism role-play evaluation, respectively.\nWe initialize from <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio-7B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib11\" title=\"\">2024</a>)</cite>, which provides a strong foundation for multi-modal audio-text reasoning.\nTo better leverage the multiple human annotations available in our datasets, we adopt a sampling-based training strategy: at each step, one annotator label is randomly selected, following practices in label distribution learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Geng, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib22\" title=\"\">2016</a>; Wen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib67\" title=\"\">2023</a>)</cite>.\nThis stochastic supervision implicitly exposes the model to the empirical annotation distribution, while during inference we output the full distribution over discrete labels (e.g., <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score, consistent with recent decoding-based regression methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Song &amp; Bahri, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>; Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib70\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib64\" title=\"\">2025b</a>; Lukasik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>. We apply LoRA fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib28\" title=\"\">2022</a>)</cite>. All implementation details, including optimizer settings, training schedule, and sampling policy, are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS3\" title=\"L.3 Fine-tuning Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "consistent",
                    "score",
                    "from",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype Evaluation.</span>\nThe results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T2\" title=\"Table 2 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reveal clear distinctions across evaluation settings. In the zero-shot condition, proprietary ALLMs such as <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> substantially outperform public models, achieving strong <span class=\"ltx_text ltx_font_italic\">content pass</span> accuracy (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>90%) and moderate correlations with other dimensions(<math alttext=\"0.43\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mn>0.43</mn><annotation encoding=\"application/x-tex\">0.43</annotation></semantics></math>&#8211;<math alttext=\"0.48\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mn>0.48</mn><annotation encoding=\"application/x-tex\">0.48</annotation></semantics></math>). By contrast, public models exhibit limited predictive ability, with correlations often below 0.2. Few-shot prompting provides modest gains for proprietary models, improving both accuracy and correlation (e.g., <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> rises from 0.480 to 0.493 average correlation). The most significant improvement arises from finetuning: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves an average correlation of 0.629, marking a large margin over zero-shot/few-shot models and demonstrating the effectiveness of task-specific supervision.</p>\n\n",
                "matched_terms": [
                    "most",
                    "from",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T3\" title=\"Table 3 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> extends the analysis to fine-grained realism evaluation. Similar to archetype results, proprietary ALLMs lead the zero-shot setting, with <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> obtaining an average correlation of 0.390, compared to 0.284 for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>. Public models remain weak baselines. Few-shot prompting consistently improves performance: <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> gains +0.042 and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> gains +0.083 over their zero-shot averages, showing the benefit of in-context supervision. Once again, finetuning is most effective: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves 0.625 average correlation, higher than the best few-shot results. Importantly, the gains are consistent across all 10 realism dimensions, confirming that the sampling&#8211;expectation training strategy generalizes beyond simple classification to nuanced paralinguistic cues.</p>\n\n",
                "matched_terms": [
                    "consistent",
                    "most"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real Recording Test Set.</span>\nThe real-recording evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T4\" title=\"Table 4 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> is notably more challenging, with all models suffering substantial performance drops. Proprietary models in the zero-shot setting achieve only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.04&#8211;0.08 average correlation, and public models often fall below zero. Few-shot prompting yields small but noticeable improvements, especially for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> (0.226 avg.), suggesting that exemplars partially mitigate domain mismatch between synthetic and real human speech. Interestingly, while finetuning achieves the highest overall average (0.247), its relative advantage over few-shot prompting is narrower than in the synthetic setting, and some dimensions (e.g., <span class=\"ltx_text ltx_font_italic\">Character Identity</span>) remain weak. We emphasize that this observed gap between synthetic and real recordings is not merely an artifact but an intended feature of Speech-DRAME, designed to expose the mismatch and drive future research into bridging synthetic-to-real generalization.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall.</span> Across both archetype and realism evaluations, we observe three consistent trends: (i) zero-shot performance is limited, particularly for open-source models, (ii) few-shot prompting offers moderate but consistent gains, especially for proprietary ALLMs, and (iii) finetuning with task-specific supervision (<span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>) delivers the largest improvements, achieving state-of-the-art correlations across nearly all dimensions. Nevertheless, results on the real-recording test set underscore the gap between synthetic and real human role-play scenarios, motivating future work in data diversification and domain adaptation.</p>\n\n",
                "matched_terms": [
                    "consistent",
                    "both",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype evaluation.</span>\nOn archetype role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T5\" title=\"Table 5 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), cascaded pipelines consistently outperform end-to-end systems. The strongest cascaded systems, particularly <span class=\"ltx_text ltx_font_typewriter\">G-IndexTTS2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-IndexTTS2</span>, achieve both high content pass rates (above 94%) and balanced scores across audio quality, human likeness, and appropriateness. By contrast, end-to-end models such as <span class=\"ltx_text ltx_font_typewriter\">StepAudio2Mini</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> perform competitively in terms of content coverage (above 88%), but their expressive scores remain slightly lower than their cascaded counterparts. This gap suggests that separating reasoning (LLM) from speech rendering (TTS) remains advantageous when archetype fidelity is emphasized. Nevertheless, the fact that top end-to-end systems already surpass 3.0 in average rating indicates rapid closing of the performance gap.</p>\n\n",
                "matched_terms": [
                    "human",
                    "from",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism evaluation.</span>\nRealism role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T6\" title=\"Table 6 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) presents a different landscape. Here, performance differences between end-to-end and cascaded systems are less pronounced. Several end-to-end models, such as <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>, reach average scores of 3.24, matching or even exceeding the best cascaded pipelines. Importantly, realism scores are more evenly distributed across prosodic, emotional, character, and contextual dimensions, with most models clustering between 2.9&#8211;3.2. This convergence reflects the difficulty of realism-style evaluation: since the data preparation emphasized contrast with authentic human speech, models are judged under similar conditions, leading to smaller observed gaps.</p>\n\n",
                "matched_terms": [
                    "most",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human alignment.</span>\nWe further conducted small-scale human studies with 10% of the benchmark data to verify the reliability of the benchmark. Archetype evaluation achieves a Spearman correlation of 0.706 between human judgments and <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, validating that the automatic evaluator closely tracks human preferences in this setting. Realism evaluation, however, achieves a more modest Spearman correlation of 0.375. This lower alignment is expected, since realism data construction did not prioritize fine-grained model separation but instead emphasized distinguishing human versus synthetic speech.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>We view this as evidence that realism presents a harder, less-saturated frontier. Rather than a limitation of DRAME-Eval, this highlights the challenge of designing evaluators that capture nuanced human perception, which motivates future work.</span></span></span> As a result, all models occupy a relatively narrow performance band, reducing sensitivity to model-level differences. We view this as evidence that realism evaluation highlights a harder, less saturated frontier where both SFMs and SEMs require further advances.</p>\n\n",
                "matched_terms": [
                    "human",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, cascaded pipelines retain an edge in archetype scenarios, especially in content robustness and expressive fidelity, but end-to-end models are catching up quickly. In realism scenarios, the field remains unsolved: existing systems exhibit similar performance bands and evaluators show weaker alignment with human judgments, underscoring the need for improved realism-grounded data and evaluation. Together, these findings establish <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> as both a reliable testbed for archetype fidelity and a challenging benchmark for realism fidelity.</p>\n\n",
                "matched_terms": [
                    "human",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework for assessing spoken role-play through two complementary lenses: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em> (top-down) and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> (bottom-up, human-grounded). Our human-annotated <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> enables training and testing of SEMs, and our fine-tuned <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span> substantially outperforms zero-/few-shot ALLM judges. Building on this, <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> provides system-level comparison across various SFMs, revealing a consistent edge for cascaded pipelines on archetypes and a tighter spread under realism, where real human performance evaluation remain challenging. Together, these resources establish a reproducible, analyzable foundation for spoken role-play evaluation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>Limitations and future works are discussed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A13\" title=\"Appendix M Limitations and Future Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">M</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "consistent",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Proprietary LLM outputs were used in a limited way (e.g., instruction building, story summarization, or baseline evaluations) and only for internal research purposes. Released datasets in Speech-DRAME are constructed from open-source models, licensed corpora, and original human recordings.</p>\n\n",
                "matched_terms": [
                    "from",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism as Situated Performance</span>\nIn contrast, realism highlights the fine-grained enactment of roles in situated interaction. Goffman&#8217;s dramaturgical sociology emphasizes that everyday social life is itself a performance, where believability depends on subtle paralinguistic and pragmatic cues such as prosody, hesitation, and emotional shading&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goffman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib23\" title=\"\">1949</a>)</cite>. Similarly, interactional sociolinguistics has demonstrated how micro-level features like intonation, pacing, and turn-taking patterns shape the perception of authenticity in communication&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gumperz, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib25\" title=\"\">1982</a>)</cite>. Realism Evaluation thus anchors role-play assessment in bottom-up sensitivity to authentic delivery, beyond schematic expectations.</p>\n\n",
                "matched_terms": [
                    "delivery",
                    "pacing",
                    "intonation",
                    "patterns"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Complementarity of the Two Perspectives</span>\nPerformance theory provides a unifying lens for these dual dimensions. Schechner&#8217;s view of performance as both &#8220;restored behavior&#8221; (scripted, repeatable structures) and &#8220;living process&#8221; (situated enactment)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Schechner, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib52\" title=\"\">2017</a>)</cite> underscores the need to consider both archetypal scripts and enacted realism. Likewise, sociolinguistic theories of identity recognize both <span class=\"ltx_text ltx_font_italic\">ascribed categories</span> (archetypes imposed by social convention) and <span class=\"ltx_text ltx_font_italic\">performed identities</span> (emergent in interaction)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bucholtz &amp; Hall, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib2\" title=\"\">2005</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, they mirror the dual nature of human role assessment, balancing scalability and comparability with depth and nuance. This theoretical grounding reinforces the design of Speech-DRAME as a framework that does not privilege one perspective over the other, but instead integrates both to reflect the full spectrum of how roles are evaluated in speech-based interaction.</p>\n\n",
                "matched_terms": [
                    "human",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Role dimensions.</span>\nWe operationalize archetypes through five principal categories, each representing a stereotypical dimension of role identity or behavior, including (1) <span class=\"ltx_text ltx_font_bold\">Occupation / Functional Role (Modern Daily Life)</span> (e.g., a firefighter rescuing a trapped civilian). (2) <span class=\"ltx_text ltx_font_bold\">Occupation / Functional Role (Fictional or Fantasy)</span> (e.g., a martial arts cult leader confronting orthodox rivals). (3)&#160;<span class=\"ltx_text ltx_font_bold\">Social Identity</span> (e.g., a doting parent worrying about a child, or a &#8220;Buddhist youth&#8221; eschewing competition). (4) <span class=\"ltx_text ltx_font_bold\">Named / Specific Characters</span> (e.g., Sun Wukong mocking celestial authorities in <em class=\"ltx_emph ltx_font_italic\">Journey to the West</em>). (5) <span class=\"ltx_text ltx_font_bold\">Personality Traits</span> (e.g., &#8220;hot-blooded&#8221; in a competitive setting, or &#8220;complaining&#8221; in response to poor service). (6) <span class=\"ltx_text ltx_font_bold\">Relative Roles</span> (e.g., parent or stranger).</p>\n\n",
                "matched_terms": [
                    "identity",
                    "traits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Exclusion of fine-grained controls.</span>\nWe deliberately excluded explicit emotional states, prosodic instructions, or linguistic constraints.\nThese dimensions are better suited to realism-oriented or style-controlled evaluation, whereas archetype evaluation benefits from concrete, stereotype-level scenarios that sharpen judgments and improve human agreement.</p>\n\n",
                "matched_terms": [
                    "from",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Pipeline overview.</span>\nAt a high level, archetype evaluation data are generated through a three-stage pipeline:\n(1) selecting keywords that specify role and scene;\n(2) expanding them into full prompts via structured templates; and\n(3) rendering prompts into speech with contemporary TTS systems.\nThis modular pipeline ensures both scalability and reproducibility, while keeping prompts vivid enough for reliable judgments. (4) Based on the generated data (paired scene and synthesized speech), expert human annotators are recruited for detailed annotation on semantic check (i.e., <span class=\"ltx_text ltx_font_italic\">Content Pass</span>) and three different spoken role-play dimensions (i.e., <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human-likeness</span>, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>).\nImplementation details, including template design, alternative prompting strategies, and annotation guidelines, are provided in the following subsections of this Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "human",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Excluded Dimensions.</span> We initially considered more fine-grained control dimensions, but excluded them to preserve the archetype-level focus. Examples of excluded dimensions include:\n<em class=\"ltx_emph ltx_font_italic\">Emotional State &amp; Expression, Single Emotion Prompts, Explicit User Input / Assistant Output structures, Emotional Transition / Fluctuation, Voice Style &amp; Prosody (pacing, intonation, vocal timbre, expressive style, accent/dialect), and Content / Interaction Patterns (task/topic controls, specific linguistic structures, or speech production challenges).</em></p>\n\n",
                "matched_terms": [
                    "patterns",
                    "intonation",
                    "pacing",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These dimensions are more closely aligned with explicit instruction-following or style transfer rather than broad stereotype judgment, and they reduced clarity in human evaluations.</p>\n\n",
                "matched_terms": [
                    "human",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2\" title=\"Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the internal annotation interface used for <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>.\nThe platform provides an integrated environment for both playback and evaluation of model-generated role-play speech samples.\nAnnotators first interact with the audio player interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf1\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>), which visualizes waveforms and spectrograms for multiple versions of the same prompt, enabling side-by-side comparison under a shared role instruction (e.g., firefighter, teacher, or actor).\nThen, they proceed to the annotation interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf2\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a>), where they assign scores for multiple dimensions such as <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, along with reasoning and confidence inputs.\nThis structured interface supports high-consistency, fine-grained evaluation of role-play behaviors in speech models.</p>\n\n",
                "matched_terms": [
                    "human",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human Likeness (No Context):</span> perceived naturalness and plausibility of human delivery.</p>\n\n",
                "matched_terms": [
                    "human",
                    "delivery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rubric Rating Instructions (for content pass = true).</span>\nWe shift focus from <em class=\"ltx_emph ltx_font_italic\">what</em> was said to <em class=\"ltx_emph ltx_font_italic\">how</em> it was said (vocal performance). Use gut intuition guided by rubric definitions. Score each audio <em class=\"ltx_emph ltx_font_italic\">in isolation</em> (no ranking across clips).</p>\n\n",
                "matched_terms": [
                    "score",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">Audio Quality</span> &amp; <span class=\"ltx_text ltx_font_bold\">Human Likeness</span>: comment only if score <math alttext=\"\\leq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I6.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8804;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\leq 3</annotation></semantics></math>; if 4 or 5, write &#8220;N/A&#8221;.</p>\n\n",
                "matched_terms": [
                    "score",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">Appropriateness</span>: comment for <em class=\"ltx_emph ltx_font_italic\">all</em> scores 1&#8211;5. Explain <em class=\"ltx_emph ltx_font_italic\">why</em> the <em class=\"ltx_emph ltx_font_italic\">delivery</em> was/was not a good match to the in-context traits (role &amp; scene).</p>\n\n",
                "matched_terms": [
                    "traits",
                    "delivery",
                    "good"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If any portion of the clip is ill-fitting, prefer the lower overall score; do not average across good and bad segments.</p>\n\n",
                "matched_terms": [
                    "score",
                    "bad",
                    "good"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Rate <em class=\"ltx_emph ltx_font_italic\">only</em> how well the <em class=\"ltx_emph ltx_font_italic\">delivery</em> fits the role/scene described in the prompt (tonal fit). Ignore Audio Quality and Human Likeness issues while scoring this dimension.</p>\n\n",
                "matched_terms": [
                    "human",
                    "delivery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realism Evaluation adopts a bottom&#8211;up perspective grounded in real human speech data, with the goal of capturing the nuanced delivery, prosody, and contextual fit that synthetic archetype prompts cannot fully represent.\nRather than relying on stereotypes, this strategy leverages real-world recordings and carefully constructed contrasts, enabling fine-grained assessment of model performance under realistic conditions.</p>\n\n",
                "matched_terms": [
                    "human",
                    "delivery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data from real human speech sources.</span>\nWe construct the realism evaluation set primarily from natural speech material:</p>\n\n",
                "matched_terms": [
                    "human",
                    "from",
                    "natural"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative character profiles.</span>\nWe also create role-profile mismatches by resynthesizing transcripts with diverse TTS systems, thereby breaking the link between authentic delivery and intended role.\nWe explore both text-instruction-based prompting and comparisons across commercial and non-commercial TTS frameworks.\nDetailed model lists and parameter settings are documented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS6\" title=\"F.6 TTS Systems and Settings &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.6</span></a>.</p>\n\n",
                "matched_terms": [
                    "both",
                    "delivery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, real human ground-truth segments and systematically constructed negatives form a balanced dataset that supports realism-oriented evaluation.\nThe positives provide natural anchors of role-play performance, while the negatives introduce controlled mismatches that test whether evaluation models can distinguish real human speech from synthetic approximations.\nThis design ensures that realism evaluation emphasizes subtle paralinguistic and contextual cues, moving beyond intelligibility or content fidelity.</p>\n\n",
                "matched_terms": [
                    "human",
                    "from",
                    "natural"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We designed prompts to extract or generate concise role descriptions corresponding to characters in TV, film, or natural dialogue sources.\nThese descriptions emphasize identity, relationship, and communicative goals.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "natural"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the character Yunjin from <span class=\"ltx_text ltx_font_italic\">Genshin Impact</span> as a running case study, which is also included in our released dataset.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>As acknowledged, the use of real-world recordings is conducted under legal and ethical review. All curated data will be released as part of our open-source package.</span></span></span> This example illustrates how alignment between a speaker&#8217;s persona, local context, and delivery underpins realism, and how systematically breaking that alignment produces contrastive test cases.</p>\n\n",
                "matched_terms": [
                    "cases",
                    "from",
                    "delivery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Level 0 (Mild Mismatch).</span>\n<em class=\"ltx_emph ltx_font_italic\">Fake Scene:</em> A quiet teahouse where Yunjin casually chats with friends about recent weather. She suddenly delivers the heroic verses in full operatic style.\n<em class=\"ltx_emph ltx_font_italic\">Reason:</em> The shift from light conversation to intense battle imagery feels abrupt and unsupported. The mismatch is mild but noticeable.</p>\n\n",
                "matched_terms": [
                    "from",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Level 0 (Mild Mismatch).</span>\n<em class=\"ltx_emph ltx_font_italic\">Fake Scene:</em> At a modern tech company&#8217;s annual party, a shy programmer is asked to perform. Under pressure, he recites opera lines he once saw online.\n<em class=\"ltx_emph ltx_font_italic\">Fake Profile:</em> Zhang Wei, an introverted software engineer with little artistic background, usually immersed in code and algorithms.\n<em class=\"ltx_emph ltx_font_italic\">Reason:</em> The heroic verses sound alien coming from someone unfamiliar with opera or traditional culture. The clash produces awkward humor but still feels like a coping attempt.</p>\n\n",
                "matched_terms": [
                    "awkward",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the Yunjin example is drawn from authentic performance, where the character&#8217;s persona (an opera singer) and local scene (a battlefield narration) are naturally aligned through artistic delivery. The realism task does not merely test whether a model can follow a prompt, but whether it can preserve coherence between a speaker&#8217;s global identity, immediate context, and expressive prosody. When Yunjin&#8217;s transcript is transplanted into mismatched settings or paired with incongruent speaker profiles, the breakdown in realism becomes apparent: the same words shift from inspiring to awkward, jarring, or even absurd.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "awkward",
                    "from",
                    "delivery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This distinction highlights the complementary strengths of the two evaluation paradigms. Archetype evaluation systematically explores a wide combinatorial space of roles and situations, but may lack the paralinguistic and contextual richness of real speech. Realism evaluation, on the other hand, anchors assessment in authentic delivery and contextual fit, providing a finer-grained lens on coherence and naturalness. Together, they ensure that role-play evaluation spans both broad stereotypical coverage and nuanced real-world plausibility.</p>\n\n",
                "matched_terms": [
                    "both",
                    "delivery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed a diverse set of TTS systems to generate negative character profiles and resynthesized speech for Realism Evaluation.\nThese included both commercial and non-commercial models, with variation in style control, conditioning prompts, and reference usage.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.T8\" title=\"Table 8 &#8227; Same-Speaker Voice Clone. &#8227; F.6 TTS Systems and Settings &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> summarizes the systems and settings.\nBelow, we detail the major strategies used across systems:</p>\n\n",
                "matched_terms": [
                    "both",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS1</span> (Phone&#8211;Cosy2 Pipeline).\nThe input is a phone sequence produced by a graphme-to-phoneme&#160;(G2P) front-end. A 0.4B GPT-2 model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib50\" title=\"\">2019</a>)</cite> predicts <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> S3-tokens, which are then passed through a flow-matching model to generate mel-spectrograms. A neural vocoder from <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> converts the mel features into audio. Both the GPT-2 and flow-matching modules are augmented with speaker embeddings for speaker control.</p>\n\n",
                "matched_terms": [
                    "from",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS2</span> (BPE&#8211;Cosy2 Pipeline).\nInstead of phones, this model takes Qwen2.5 byte-pair-encoding (BPE) tokens as input&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib71\" title=\"\">2025a</a>)</cite>. The same 0.4B GPT-2 predicts <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> tokens, followed by the identical flow-matching and vocoder stages as in <span class=\"ltx_text ltx_font_typewriter\">InhouseTTS1</span>. Speaker embeddings are also injected to maintain consistent speaker style.</p>\n\n",
                "matched_terms": [
                    "consistent",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Read the <span class=\"ltx_text ltx_font_bold\">Character Profile</span> (long-term identity, style) and <span class=\"ltx_text ltx_font_bold\">Local Scene</span> (immediate context, goal/emotion) before listening.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the audio&#8217;s speaker is <em class=\"ltx_emph ltx_font_italic\">clearly not</em> the intended target identity, assign a score of <span class=\"ltx_text ltx_font_bold\">1</span> to all Profile-dependent dimensions (e.g., Voice Identity Matching, Trait Embodiment) and proceed with other dimensions as applicable.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "score",
                    "clearly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Use both sources:</span> Play the audio and skim the transcript together to assess delivery quality, expression, and alignment with character intent (do not penalize transcript mismatches).</p>\n\n",
                "matched_terms": [
                    "both",
                    "delivery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identity mismatch:</span> If the clip&#8217;s speaker is clearly not the intended target speaker, assign a score of <span class=\"ltx_text ltx_font_bold\">1</span> for every rating dimension that relies on the Character Profile (e.g., Character Style Traits, Trait Embodiment, Character Consistency).</p>\n\n",
                "matched_terms": [
                    "score",
                    "identity",
                    "style",
                    "clearly",
                    "traits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Question 2: Observed Character Style Traits.</span> Based on the Character Profile, are the defined Style Traits clearly present in the delivery?</p>\n\n",
                "matched_terms": [
                    "delivery",
                    "traits",
                    "style",
                    "clearly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Delete incorrect pre-populated traits; you do not need to replace them. If you delete many traits, <em class=\"ltx_emph ltx_font_italic\">Trait Embodiment</em> should likely be low.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "likely"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dual settings: Archetype vs. Realism.</span>\nWe deliberately separate stereotype-driven <em class=\"ltx_emph ltx_font_italic\">Archetype</em> from bottom-up <em class=\"ltx_emph ltx_font_italic\">Realism</em> tasks.\nThis avoids averaging away nuance: archetype labels reflect schema-level fit; realism labels prioritize fine-grained, in-situ delivery from real-world or carefully constructed contrastive material.</p>\n\n",
                "matched_terms": [
                    "from",
                    "delivery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Sensitivity to delivery.</span> Human raters, bilingual scope, and gated rubrics capture intonation, pacing, and subtle emotional control that judge-only pipelines often miss or compress into single numbers&#160;(see Appdendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>).</p>\n\n",
                "matched_terms": [
                    "delivery",
                    "pacing",
                    "human",
                    "intonation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Reduced synthetic bias.</span> Unlike pipelines built from LLM-generated dialogues or TTS-only references, we anchor annotations to real-world/contrastive human recordings and explicitly stress-test mismatches&#160;(Appdendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS4\" title=\"F.4 Prompts for Negative Scenes &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.4</span></a>, Appdendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS6\" title=\"F.6 TTS Systems and Settings &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.6</span></a>).</p>\n\n",
                "matched_terms": [
                    "from",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The data presented to annotators consisted of synthetic speech samples, anonymized human speech, or recordings from public corpora, and contained no personally identifiable or sensitive information. To mitigate potential risks, annotators were allowed to skip any item that they found uncomfortable or inappropriate.</p>\n\n",
                "matched_terms": [
                    "from",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A critical observation is that annotations in Mandarin and English cannot be directly compared. The annotation groups for the two languages differed in their linguistic backgrounds and cultural interpretations, which leads to systematic differences in scoring. Our bilingual pilot further confirmed that annotators often interpret role-play appropriateness and expressiveness differently depending on their cultural frame of reference. For example, Mandarin raters tend to reward delivery styles that align with culturally expected prosody and politeness strategies, while English raters prioritize spontaneity and naturalness. These discrepancies parallel the findings of the singing voice conversion challenge (SVCC2023) from <cite class=\"ltx_cite ltx_citemacro_citet\">Huang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib31\" title=\"\">2023</a>)</cite>, where native and non-native listeners provided significantly different ratings for the same samples, particularly for naturalness and speaker similarity. Consequently, while absolute scores differ substantially across Mandarin and English, only within-language comparisons are meaningful. Cross-lingual ranking is not valid given the different annotation bases.</p>\n\n",
                "matched_terms": [
                    "from",
                    "delivery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Within Mandarin from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, the relative rankings of models are clear. <span class=\"ltx_text ltx_font_typewriter\">Doubao</span> stands out as the strongest performer across all four dimensions, reaching near-ceiling levels in both <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> (4.37) and <span class=\"ltx_text ltx_font_italic\">Content Pass</span> (0.98). It also leads in role-related dimensions such as <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, indicating balanced competence across linguistic accuracy, delivery, and expressiveness. <span class=\"ltx_text ltx_font_typewriter\">G-CosyVoice2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-CosyVoice2</span> follow closely, especially in audio quality, though they lag behind in role appropriateness. <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> achieve mid-range results, reflecting higher variance across samples, while <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> shows competitive content understanding but lower naturalness and appropriateness. <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> consistently ranks the lowest, with large variance, underscoring instability and weaker alignment with archetypal role expectations.</p>\n\n",
                "matched_terms": [
                    "human",
                    "from",
                    "both",
                    "delivery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The picture shifts in English, elaborated in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Here, <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> emerges as the most effective model by a wide margin, achieving top scores in both <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> (3.46) and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> (3.49), as well as high content pass rates (0.97). This reflects its strong grounding in English discourse norms and role adherence. In contrast, <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> variants perform well in <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> but are weaker in role alignment, highlighting a trade-off between acoustic fidelity and role-play embodiment. Other models such as <span class=\"ltx_text ltx_font_typewriter\">Doubao</span>, <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span>, and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> achieve moderate results, while <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> trail far behind, showing limited robustness in English scenarios. Taken together, the English results reveal both a sharper separation across models and a wider variance in appropriateness scores compared to Mandarin.</p>\n\n",
                "matched_terms": [
                    "human",
                    "most",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T11\" title=\"Table 11 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> and Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.F5\" title=\"Figure 5 &#8227; English Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.F6\" title=\"Figure 6 &#8227; English Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> further highlight differences across domains. In Mandarin, performance is relatively consistent across categories, with means clustering around 3.1&#8211;3.3. The highest scores appear in <span class=\"ltx_text ltx_font_typewriter\">SocialIdentity</span> and <span class=\"ltx_text ltx_font_typewriter\">DailyOccupation</span>, suggesting that models handle culturally grounded archetypes more reliably. In English, however, domain variation is much greater. Categories such as <span class=\"ltx_text ltx_font_typewriter\">FantasyOccupation</span> and <span class=\"ltx_text ltx_font_typewriter\">NamedCharacters</span> receive notably lower scores (means below 2.0), reflecting the difficulty of producing expressive, role-appropriate speech in imaginative or culturally specific contexts. Occupational roles remain relatively easier for models in both languages, likely because these scenarios are closer to training data distributions.</p>\n\n",
                "matched_terms": [
                    "consistent",
                    "both",
                    "likely"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In sum, archetype evaluation reveals three major findings:\n(1) Mandarin and English annotations cannot be directly compared, due to linguistic and cultural annotation inconsistencies, consistent with prior evidence from SVCC 2023.\n(2) Within each language, clear model rankings emerge, <span class=\"ltx_text ltx_font_typewriter\">Doubao</span> dominates in Mandarin, while <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> excels in English.\n(3) Domain sensitivity remains a challenge, particularly for imaginative or culturally rich roles, where scores are substantially lower and inter-model variance is higher.</p>\n\n",
                "matched_terms": [
                    "consistent",
                    "rich",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although realism evaluation targets nuanced paralinguistic behaviors, human annotators achieve <em class=\"ltx_emph ltx_font_italic\">consistently high agreement</em> once the rubric disentangles the aspects to be judged.\nAs shown in the agreement summary in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F7\" title=\"Figure 7 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, most dimensions fall in the <math alttext=\"0.86\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>0.86</mn><annotation encoding=\"application/x-tex\">0.86</annotation></semantics></math>&#8211;<math alttext=\"0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mn>0.88</mn><annotation encoding=\"application/x-tex\">0.88</annotation></semantics></math> range, with the lowest agreement on <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> (<math alttext=\"\\approx 0.83\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.83</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.83</annotation></semantics></math>) and the highest on <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em> / <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> (<math alttext=\"\\approx 0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.88</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.88</annotation></semantics></math>).\nEmotion-related, progressive metrics (Intensity and Transition) are slightly lower (<math alttext=\"\\approx 0.85\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.85</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.85</annotation></semantics></math>), reflecting the added difficulty of judging graded expressivity.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "most",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F8\" title=\"Figure 8 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F9\" title=\"Figure 9 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> visualize the score distributions per dimension.\nWe highlight three patterns:\n(i) <span class=\"ltx_text ltx_font_bold\">Progressive/gated emotion metrics.</span> For <em class=\"ltx_emph ltx_font_italic\">Emotion Intensity</em> and <em class=\"ltx_emph ltx_font_italic\">Emotion Transition</em>, we assign <span class=\"ltx_text ltx_font_typewriter\">1</span> when the scene is marked <em class=\"ltx_emph ltx_font_italic\">N/A</em> (i.e., emotion accuracy is insufficient to meaningfully judge intensity/transition). This produces a visible mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, a sparse bin at <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>, and slightly lower overall agreement; downstream analyses conditioning on adequate <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> (e.g., mean&#160;<math alttext=\"\\geq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 3</annotation></semantics></math>) recover more bell-shaped distributions.\n(ii) <span class=\"ltx_text ltx_font_bold\">Traits Embodiment.</span> Scores tend to avoid the middle (<math alttext=\"\\sim\\!3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!3</annotation></semantics></math>), concentrating on low values when traits clearly mismatch and high values when traits are strongly realized. This yields the lowest inter-rater agreement among dimensions, reflecting the inherently compositional and role-specific nature of trait judgments.\n(iii) <span class=\"ltx_text ltx_font_bold\">Other dimensions.</span> <em class=\"ltx_emph ltx_font_italic\">Pitch Variation</em>, <em class=\"ltx_emph ltx_font_italic\">Rhythmic Naturalness</em>, <em class=\"ltx_emph ltx_font_italic\">Stress &amp; Emphasis</em>, <em class=\"ltx_emph ltx_font_italic\">Voice Identity Matching</em>, <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em>, <em class=\"ltx_emph ltx_font_italic\">Global Story Coherence</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> are approximately unimodal with modes at <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>&#8211;<math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math>, consistent with moderate&#8211;strong realization in the dataset and with the high agreement reported.</p>\n\n",
                "matched_terms": [
                    "score",
                    "identity",
                    "patterns",
                    "consistent",
                    "clearly",
                    "traits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identity and traits (Voice Identity, Traits Embodiment).</span> Professionals present clearer peaks in upper bins, while amateurs are broader or bimodal, reflecting inconsistent character anchoring.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "traits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, the consistently high agreement confirms that the rubric isolates perceptually coherent factors, so learned evaluators can target them reliably.\nSecond, the gated behavior of <em class=\"ltx_emph ltx_font_italic\">Intensity/Transition</em> motivates a <em class=\"ltx_emph ltx_font_italic\">hierarchical</em> evaluator: verify <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> before scoring graded expressivity, or model the joint structure with conditional heads.\nThird, <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> remains challenging; models may benefit from explicit character-trait priors, long-term context aggregation, or contrastive supervision between matched and mismatched traits.\nFinally, prosody-focused objectives (e.g., pitch/rhythm/stress control) are likely to yield the largest quality gains for amateur-style speech.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "from",
                    "likely"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For both zeroshot and fewshot experiments, we adopt a streamlined version of the annotation guidelines to ensure consistency with the human annotation process. The exact rubric prompts for archetype and realism evaluation used are provided below:</p>\n\n",
                "matched_terms": [
                    "human",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and fewshot settings, we condition the model with the rubric prompt and then append few-shot demonstration triplets (input, audio, gold JSON) to mirror the human annotation protocol. As shown in Listing&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#LST2\" title=\"Listing 2 &#8227; L.1 Zeroshot/Fewshot Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, We use <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> for these few-shot exemplars and require a minified JSON output consistent with the rubric schema.</p>\n\n",
                "matched_terms": [
                    "consistent",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and few-shot experiments on commercial models such as <span class=\"ltx_text ltx_font_typewriter\">Gemini</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o</span>, we report results as the average of five independent runs to reduce randomness and obtain more stable predictions. In contrast, for open-source models, we observed that their instruction-following ability is relatively weak. To mitigate this, we prompt them to predict one dimension at a time rather than all dimensions jointly. Specifically, for each metric, after showing the rubric prompt, we request completion in a fixed template and extract the probability distribution over the discrete tokens <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>. The final score is then computed from these probabilities, which we found makes open-source models usable to some extent under this setting.</p>\n\n",
                "matched_terms": [
                    "score",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For few-shot experiments, we randomly sample three exemplars from the training set, with each exemplar chosen such that its human scores are close to the rounded average across all annotators. These exemplars are appended as demonstrations in the prompt. As with the zeroshot case, we run five trials for commercial models and report the averaged results for evaluation.</p>\n\n",
                "matched_terms": [
                    "from",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Effect of repeated runs.\nWhen comparing single-run decoding (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot-1run</span>) against averaging over five runs (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot</span>), we see consistent gains across nearly all dimensions (average score improves from 0.353 to 0.390). This suggests that sampling variance plays a substantial role in the reliability of large language model (LLM) evaluators, and averaging multiple outputs reduces this variance, producing more stable correlation with human annotations.</p>\n\n",
                "matched_terms": [
                    "consistent",
                    "score",
                    "from",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Scaling the number of demonstrations.\nIncreasing the few-shot set size from three to six and nine further boosts performance (0.459 and 0.469, respectively). The improvements are most pronounced in prosodic and emotional categories (e.g., <em class=\"ltx_emph ltx_font_italic\">Pitch</em>, <em class=\"ltx_emph ltx_font_italic\">Accuracy</em>, <em class=\"ltx_emph ltx_font_italic\">Intensity</em>), suggesting that more examples provide stronger guidance in modeling subtle paralinguistic cues. Gains in <em class=\"ltx_emph ltx_font_italic\">Character Consistency</em> and <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance</em> are also noticeable, although smaller in magnitude, which may indicate that these higher-level judgments are less sensitive to prompt scaling once a few demonstrations are provided.</p>\n\n",
                "matched_terms": [
                    "most",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the archetype test set, all fine-tuned variants achieve strong <span class=\"ltx_text ltx_font_italic\">Content Pass</span> accuracy above 92%. Differences appear mainly in correlation-based metrics. Here, <span class=\"ltx_text ltx_font_typewriter\">Base-FT</span> yields the highest score on <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> delivers the strongest performance in <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> and the best overall average (0.629). This suggests that instruction-tuned checkpoints paired with parameter-efficient adaptation can better capture higher-level perceptual traits.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "score",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For realism evaluation on the basic test set, <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> consistently achieves the strongest correlations across most dimensions, especially for <span class=\"ltx_text ltx_font_italic\">Character Identity</span> and <span class=\"ltx_text ltx_font_italic\">Traits</span>, leading to the best average correlation (0.668). <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> also performs competitively (0.630), while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> lags slightly behind (0.625). These results highlight that full fine-tuning may still offer an advantage when the evaluation set is relatively homogeneous and closer to the training distribution.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "most",
                    "traits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A different pattern emerges on the real recording test set. Both <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> and <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> struggle, yielding negative or near-zero correlations across most dimensions. In contrast, <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> shows clear improvements, achieving positive correlations in all prosodic and emotional dimensions and producing the highest overall average (0.247). This indicates that LoRA adaptation on top of an instruction-tuned backbone provides better generalization to out-of-distribution, real-world data.</p>\n\n",
                "matched_terms": [
                    "most",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the comprehensive design of Speech-DRAME, several limitations remain.\nFirst, while our dual evaluation paradigm combines top-down archetype assessment with bottom-up realism evaluation, both strategies are constrained by the quality and scope of available data. Archetype evaluation inevitably relies on simplified stereotypes, which can overlook subtler paralinguistic and contextual cues. Realism evaluation, though richer, still struggles with domain mismatch between synthetic data and real human recordings, as evidenced by the performance drop observed in the real-recording test set. This highlights the persistent gap between benchmark-driven evaluation and real-world variability in speech role-play.</p>\n\n",
                "matched_terms": [
                    "human",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, although we validated DRAME-Eval against human annotations, alignment under realism settings remains modest. This suggests that even human-annotated benchmarks are insufficiently fine-grained to capture the full complexity of human perception in speech role-play. Richer annotation protocols, more diverse speaker populations, and integration of perceptual psychology insights could further improve evaluation fidelity.</p>\n\n",
                "matched_terms": [
                    "human",
                    "could"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Looking forward, we see several promising research directions. We intentionally restrict evaluation to single-turn responses for clarity and tractability. However, multi-turn extensions that capture narrative progression, long-term consistency, and dynamic emotion shifts remain a natural next step. Incorporating multilingual and cross-cultural datasets will enhance generality and fairness. On the modeling side, exploring hybrid evaluation strategies that combine discrete rubrics with continuous perceptual signals (e.g., prosody embeddings or affective measures) may better bridge the gap between automatic and human judgments. Finally, integrating Speech-DRAME into reinforcement learning pipelines as a reward model could help drive the next generation of expressive, human-aligned speech role-play systems.</p>\n\n",
                "matched_terms": [
                    "could",
                    "human",
                    "natural"
                ]
            }
        ]
    },
    "A5.SS4.SSSx9.p1": {
        "caption": "",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\"><span class=\"ltx_text ltx_font_bold\">Score / Criteria</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:256.1pt;\"><span class=\"ltx_text ltx_font_bold\">Definition</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">5 Completely Appropriate</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:256.1pt;\">Fully fits BOTH role and specific scene; aligns with emotional stance, social dynamics, and situational events; strong immersion.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">4 Mostly Appropriate</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:256.1pt;\">Minor issues: role slightly off but scene reaction good; or scene slightly off but role good; or both slightly off with no major issues. Strong overall match with small flaws (e.g., intensity/subtlety).</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">3 Adequately Appropriate</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:256.1pt;\">Generally acceptable for BOTH role and scene but lacking depth; noticeable inconsistencies.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">2 Slightly Appropriate</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:256.1pt;\">EITHER role <em class=\"ltx_emph ltx_font_italic\">or</em> scene reaction is clearly not acceptable; weak alignment or mismatched/missing elements.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">1 Completely Inappropriate</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:256.1pt;\">BOTH role and scene are contextually mismatched; tone/urgency/status illogical; heavy misfit with expected alignment.</span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "fully",
            "events",
            "issues",
            "flaws",
            "lacking",
            "inappropriate",
            "overall",
            "contextually",
            "good",
            "completely",
            "dynamics",
            "noticeable",
            "not",
            "social",
            "heavy",
            "adequately",
            "stance",
            "immersion",
            "slightly",
            "illogical",
            "definition",
            "strong",
            "match",
            "reaction",
            "emotional",
            "criteria",
            "small",
            "scene",
            "inconsistencies",
            "situational",
            "appropriate",
            "elements",
            "aligns",
            "off",
            "minor",
            "major",
            "alignment",
            "mostly",
            "misfit",
            "depth",
            "mismatchedmissing",
            "both",
            "acceptable",
            "mismatched",
            "score",
            "intensitysubtlety",
            "weak",
            "role",
            "toneurgencystatus",
            "specific",
            "generally",
            "either",
            "clearly",
            "expected",
            "fits"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">it lacks sensitivity to paralinguistic cues such as intonation, pacing, and emotional dynamics that define role delivery&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>; Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib81\" title=\"\">2025a</a>)</cite>,</p>\n\n",
                "matched_terms": [
                    "dynamics",
                    "emotional",
                    "role"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome these limitations, we present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span> (Speech Detailed Role-play Assessment with Modeling and Evaluation), a unified framework that moves beyond synthetic or judge-only pipelines. Speech-DRAME is distinguished by three aspects: [i] a dual evaluation paradigm that balances archetype-driven generality with human-grounded realism (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS2\" title=\"3.2 Role-play Evaluation with Two Strategies &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), [ii] human annotations that provide perceptual grounding absent in prior resources (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4\" title=\"4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), and [iii] evaluation-model alignment that enables not only benchmarking of speech foundation models (SFMs) but also systematic analysis of the evaluation models (SEMs) themselves (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5\" title=\"5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). These aspects establish Speech-DRAME as a comprehensive and trustworthy framework for measuring and improving speech role-play.</p>\n\n",
                "matched_terms": [
                    "not",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, Speech-DRAME contributes at three levels: (i) <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>, an evaluation benchmark consisting of human-annotated datasets for archetype and realism role-play; (ii) <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span>, a fine-tuned SEM trained on EvalBench that surpasses zero-shot and few-shot ALLMs; and (iii) <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span>, a speech role-play benchmark that leverages DRAME-Eval as an automatic judge to systematically compare SFMs. By combining dual evaluation strategies, human-anchored supervision, and evaluation-model alignment, Speech-DRAME delivers systematic insights into both role-play systems and evaluation models themselves.\nOur contributions are four-fold:</p>\n\n",
                "matched_terms": [
                    "both",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We establish <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span>, a speech role-play benchmark that leverages DRAME-Eval as an automatic judge to systematically compare both end-to-end and cascaded SFMs across multiple role quality dimensions.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Relevant codes, pre-trained models, and datasets will be open-sourced at \n<br class=\"ltx_break\"/><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/Anuttacon/speech_drame\" title=\"\">https://github.com/Anuttacon/speech_drame</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "both",
                    "role"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our distinction.</span>\nIn contrast, Speech-DRAME unifies these considerations through three elements: (i) a dual evaluation paradigm that combines archetype-driven generality with realism grounded in authentic human speech, (ii) human annotations that provide a reliable perceptual gold standard, and (iii) alignment of SEMs to human ratings, enabling systematic evaluation of both SFMs and the evaluation models themselves. This combination moves beyond synthetic benchmarks or zero-shot judging to provide a comprehensive and trustworthy framework for speech role-play assessment.</p>\n\n",
                "matched_terms": [
                    "both",
                    "alignment",
                    "elements"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here <math alttext=\"{\\bm{S}}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#119930;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{S}}_{r}</annotation></semantics></math> represents the spoken output (waveform or latent representation). This definition captures both the linguistic dimension (coherence with context and scene) and the paralinguistic dimension (prosody, style, emotion) expected of the role.</p>\n\n",
                "matched_terms": [
                    "definition",
                    "role",
                    "both",
                    "scene",
                    "expected"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>). The first strategy follows a <em class=\"ltx_emph ltx_font_italic\">top-down</em> design, inspired by prior text-based role-play benchmarks.\nHere, role-play is judged with respect to <em class=\"ltx_emph ltx_font_italic\">stereotypical archetypes</em> (e.g., &#8220;firefighter&#8221; or &#8220;ER doctor&#8221;), using scene contexts (e.g., &#8220;comfort a child trapped in fire&#8221; or &#8220;ask for help in a serious operation&#8221;).\nThis approach provides accessible, general-purpose scoring that is scalable to a wide range of scenarios, making it suitable for large-scale benchmarking.\nHowever, its reliance on broad stereotypes and global impressions limits its ability to capture fine-grained aspects of delivery (e.g., subtle prosodic shifts, nuanced emotional control)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Holt &amp; Lotto, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib26\" title=\"\">2010</a>)</cite>.\nIn practice, this strategy mirrors simplified annotation settings and is particularly useful when annotation resources are limited&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib21\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS5\" title=\"F.5 Examples of Realism Role-play &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.5</span></a>). The second strategy adopts a <em class=\"ltx_emph ltx_font_italic\">bottom-up</em> design, motivated by real-world performance and real human speech.\nInstead of relying solely on stereotypes, this approach grounds evaluation in recordings from professional and non-professional speakers, drawn from realistic dialogue and media sources.\nJudgments in this setting are more fine-grained and multi-dimensional, addressing aspects such as prosodic dynamics, emotional fidelity, character consistency, and contextual fit.\nWhile this strategy requires more human annotation and careful curation, it provides benchmarks that reflect how role-play is perceived in practice.\nIt thus offers a richer framework for analyzing model outputs in settings where realism and subtlety are essential.</p>\n\n",
                "matched_terms": [
                    "dynamics",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task Preparation.</span>\nWe employ complementary strategies: a <em class=\"ltx_emph ltx_font_italic\">top&#8211;down</em> archetype-based approach, rooted in stereotypes and generalized role expectations, and a <em class=\"ltx_emph ltx_font_italic\">bottom&#8211;up</em> realism-based approach, derived from real human speech. Both follow a modular pipeline of role/scene definition, prompt expansion, and speech rendering or retrieval, ensuring scalability and validity.</p>\n\n",
                "matched_terms": [
                    "definition",
                    "both",
                    "role"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Role and Scene Dimensions.</span>\nRoles are operationalized through seven categories: (1) functional occupations in daily life, (2) functional occupations in fictional or fantasy settings, (3) social identities, (4) named or specific characters, (5) personality traits, (6) relative relationship, (7) basic events. Scenes are anchored by everyday events or state changes that evoke stereotypical reactions. Details of template design and alternative prompting strategies are given in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>We note that some archetypes (e.g., firefighter) inevitably reflect cultural stereotypes. These are used solely for evaluation scalability and do not reflect normative judgments.</span></span></span></p>\n\n",
                "matched_terms": [
                    "events",
                    "role",
                    "specific",
                    "scene",
                    "not",
                    "social"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotators judge whether performances align with the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em>, using rubrics for <em class=\"ltx_emph ltx_font_italic\">Prosodic Dynamics (i.e., Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis)</em>, <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness (with progressive gating over three sub-dimensions: Emotion Accuracy, Emotion Intensity, Emotion Transition)</em>, <em class=\"ltx_emph ltx_font_italic\">Character Consistency (i.e., Voice Identity Matching and Traits Embodiment)</em>, <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance (i.e., Local Scene Fit and Global Story Coherence)</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>. A prompt quality check and confidence ratings are also collected. Full scales and gating details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "dynamics",
                    "match",
                    "emotional",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real-world Recoding.</span>\nAs noted in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S2\" title=\"2 Related Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, most prior efforts have relied heavily on synthetic data, mostly from TTS systems based on curated professional role-play voices. While such data can achieve high fidelity, it often lacks the diversity and spontaneity of real-world delivery, limiting the generalizability of evaluation models. To address this gap, we expand realism evaluation beyond synthetic and curated voices by incorporating newly recorded speech from both professional and amateur voice actors. This design introduces a broader spectrum of delivery styles, from polished professional acting to more natural and less controlled amateur performance.</p>\n\n",
                "matched_terms": [
                    "mostly",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation Data Summary.</span>\nThe resulting realism evaluation dataset is constructed from a combination of curated media sources, including internal collections, public datasets (e.g., NCSSD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib40\" title=\"\">2024b</a>)</cite>), and original recordings collected for this project.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>All data were obtained under appropriate licenses and are legally available for research release.</span></span></span> We compile 15k annotated role-play samples: 9k in Mandarin and 5k in English. The dataset is divided into 12k samples for training, 2k mixed samples for the base test set, and 1k real-recording samples for the test set. The first test set supports general evaluation using our proposed pipeline with negative samples, while the second focuses exclusively on human speech performed by both professional and amateur voice actors. For benchmarking, 2,000 scenarios from the realism dataset are designated for realism role-play benchmarking. Detailed statistics and analyses are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11\" title=\"Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">K</span></a>.</p>\n\n",
                "matched_terms": [
                    "appropriate",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-shot and Few-shot ALLMs.</span>\nWe first evaluate the zero-shot capabilities of both proprietary and public models, including <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>, <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>, <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio</span>, and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>. All evaluations follow a unified prompting template that provides audio input alongside scene and character context.</p>\n\n",
                "matched_terms": [
                    "both",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For proprietary models, we report the average of five independent runs to reduce variance and obtain more stable predictions. For open-source models, we observed weaker instruction-following ability and limited adaptation to unfamiliar tasks. To mitigate this, we did not request simultaneous prediction of multiple metrics. Instead, we queried one metric at a time. Following <cite class=\"ltx_cite ltx_citemacro_citet\">Song &amp; Bahri (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>); Lukasik et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>, we interpret model outputs through token distribution over discrete labels (<span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score.</p>\n\n",
                "matched_terms": [
                    "score",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Finetuning Model: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>.</span>\nIn addition to zero-shot and few-shot evaluations, we fine-tune a dedicated evaluation model, denoted as <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> for archetype and realism role-play evaluation, respectively.\nWe initialize from <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio-7B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib11\" title=\"\">2024</a>)</cite>, which provides a strong foundation for multi-modal audio-text reasoning.\nTo better leverage the multiple human annotations available in our datasets, we adopt a sampling-based training strategy: at each step, one annotator label is randomly selected, following practices in label distribution learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Geng, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib22\" title=\"\">2016</a>; Wen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib67\" title=\"\">2023</a>)</cite>.\nThis stochastic supervision implicitly exposes the model to the empirical annotation distribution, while during inference we output the full distribution over discrete labels (e.g., <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score, consistent with recent decoding-based regression methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Song &amp; Bahri, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>; Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib70\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib64\" title=\"\">2025b</a>; Lukasik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>. We apply LoRA fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib28\" title=\"\">2022</a>)</cite>. All implementation details, including optimizer settings, training schedule, and sampling policy, are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS3\" title=\"L.3 Fine-tuning Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "strong",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype Evaluation.</span>\nThe results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T2\" title=\"Table 2 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reveal clear distinctions across evaluation settings. In the zero-shot condition, proprietary ALLMs such as <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> substantially outperform public models, achieving strong <span class=\"ltx_text ltx_font_italic\">content pass</span> accuracy (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>90%) and moderate correlations with other dimensions(<math alttext=\"0.43\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mn>0.43</mn><annotation encoding=\"application/x-tex\">0.43</annotation></semantics></math>&#8211;<math alttext=\"0.48\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mn>0.48</mn><annotation encoding=\"application/x-tex\">0.48</annotation></semantics></math>). By contrast, public models exhibit limited predictive ability, with correlations often below 0.2. Few-shot prompting provides modest gains for proprietary models, improving both accuracy and correlation (e.g., <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> rises from 0.480 to 0.493 average correlation). The most significant improvement arises from finetuning: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves an average correlation of 0.629, marking a large margin over zero-shot/few-shot models and demonstrating the effectiveness of task-specific supervision.</p>\n\n",
                "matched_terms": [
                    "strong",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real Recording Test Set.</span>\nThe real-recording evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T4\" title=\"Table 4 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> is notably more challenging, with all models suffering substantial performance drops. Proprietary models in the zero-shot setting achieve only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.04&#8211;0.08 average correlation, and public models often fall below zero. Few-shot prompting yields small but noticeable improvements, especially for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> (0.226 avg.), suggesting that exemplars partially mitigate domain mismatch between synthetic and real human speech. Interestingly, while finetuning achieves the highest overall average (0.247), its relative advantage over few-shot prompting is narrower than in the synthetic setting, and some dimensions (e.g., <span class=\"ltx_text ltx_font_italic\">Character Identity</span>) remain weak. We emphasize that this observed gap between synthetic and real recordings is not merely an artifact but an intended feature of Speech-DRAME, designed to expose the mismatch and drive future research into bridging synthetic-to-real generalization.</p>\n\n",
                "matched_terms": [
                    "weak",
                    "overall",
                    "small",
                    "noticeable",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall.</span> Across both archetype and realism evaluations, we observe three consistent trends: (i) zero-shot performance is limited, particularly for open-source models, (ii) few-shot prompting offers moderate but consistent gains, especially for proprietary ALLMs, and (iii) finetuning with task-specific supervision (<span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>) delivers the largest improvements, achieving state-of-the-art correlations across nearly all dimensions. Nevertheless, results on the real-recording test set underscore the gap between synthetic and real human role-play scenarios, motivating future work in data diversification and domain adaptation.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype evaluation.</span>\nOn archetype role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T5\" title=\"Table 5 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), cascaded pipelines consistently outperform end-to-end systems. The strongest cascaded systems, particularly <span class=\"ltx_text ltx_font_typewriter\">G-IndexTTS2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-IndexTTS2</span>, achieve both high content pass rates (above 94%) and balanced scores across audio quality, human likeness, and appropriateness. By contrast, end-to-end models such as <span class=\"ltx_text ltx_font_typewriter\">StepAudio2Mini</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> perform competitively in terms of content coverage (above 88%), but their expressive scores remain slightly lower than their cascaded counterparts. This gap suggests that separating reasoning (LLM) from speech rendering (TTS) remains advantageous when archetype fidelity is emphasized. Nevertheless, the fact that top end-to-end systems already surpass 3.0 in average rating indicates rapid closing of the performance gap.</p>\n\n",
                "matched_terms": [
                    "slightly",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human alignment.</span>\nWe further conducted small-scale human studies with 10% of the benchmark data to verify the reliability of the benchmark. Archetype evaluation achieves a Spearman correlation of 0.706 between human judgments and <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, validating that the automatic evaluator closely tracks human preferences in this setting. Realism evaluation, however, achieves a more modest Spearman correlation of 0.375. This lower alignment is expected, since realism data construction did not prioritize fine-grained model separation but instead emphasized distinguishing human versus synthetic speech.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>We view this as evidence that realism presents a harder, less-saturated frontier. Rather than a limitation of DRAME-Eval, this highlights the challenge of designing evaluators that capture nuanced human perception, which motivates future work.</span></span></span> As a result, all models occupy a relatively narrow performance band, reducing sensitivity to model-level differences. We view this as evidence that realism evaluation highlights a harder, less saturated frontier where both SFMs and SEMs require further advances.</p>\n\n",
                "matched_terms": [
                    "both",
                    "expected",
                    "not",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, cascaded pipelines retain an edge in archetype scenarios, especially in content robustness and expressive fidelity, but end-to-end models are catching up quickly. In realism scenarios, the field remains unsolved: existing systems exhibit similar performance bands and evaluators show weaker alignment with human judgments, underscoring the need for improved realism-grounded data and evaluation. Together, these findings establish <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> as both a reliable testbed for archetype fidelity and a challenging benchmark for realism fidelity.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "both",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All data collection and annotation in this work followed strict ethical standards. Our bilingual datasets were constructed exclusively from legally accessible sources, including licensed media, open-source corpora, and original recordings conducted with informed consent. Annotators were recruited with appropriate compensation and provided with clear instructions, including the right to withdraw at any point. To minimize risks, no personally identifiable information was included in the released data, and all recordings were anonymized before annotation. We ensured cultural and linguistic diversity in annotator pools to mitigate potential bias across Mandarin and English role-play scenarios. While this study did not require formal IRB approval, we adhered to best practices in human-subject research, including transparency in task design and careful consideration of annotator well-being. Full details of our annotation protocol and ethical safeguards are presented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A9\" title=\"Appendix I Annotation Ethics &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</p>\n\n",
                "matched_terms": [
                    "not",
                    "appropriate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments were conducted on widely available GPUs (NVIDIA H100). Random seeds are fixed across all runs to ensure consistency. By releasing datasets, models, and code, we aim to make all major results in this paper fully reproducible and facilitate future extensions of our framework.</p>\n\n",
                "matched_terms": [
                    "fully",
                    "major"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our distinction between <span class=\"ltx_text ltx_font_italic\">Archetype Evaluation</span> and <span class=\"ltx_text ltx_font_italic\">Realism Evaluation</span> is not merely pragmatic, but is grounded in established theories of social identity, communication, and performance. The two perspectives capture complementary dimensions of how humans perceive and evaluate roles in interaction.</p>\n\n",
                "matched_terms": [
                    "not",
                    "social"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism as Situated Performance</span>\nIn contrast, realism highlights the fine-grained enactment of roles in situated interaction. Goffman&#8217;s dramaturgical sociology emphasizes that everyday social life is itself a performance, where believability depends on subtle paralinguistic and pragmatic cues such as prosody, hesitation, and emotional shading&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goffman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib23\" title=\"\">1949</a>)</cite>. Similarly, interactional sociolinguistics has demonstrated how micro-level features like intonation, pacing, and turn-taking patterns shape the perception of authenticity in communication&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gumperz, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib25\" title=\"\">1982</a>)</cite>. Realism Evaluation thus anchors role-play assessment in bottom-up sensitivity to authentic delivery, beyond schematic expectations.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "social"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Complementarity of the Two Perspectives</span>\nPerformance theory provides a unifying lens for these dual dimensions. Schechner&#8217;s view of performance as both &#8220;restored behavior&#8221; (scripted, repeatable structures) and &#8220;living process&#8221; (situated enactment)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Schechner, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib52\" title=\"\">2017</a>)</cite> underscores the need to consider both archetypal scripts and enacted realism. Likewise, sociolinguistic theories of identity recognize both <span class=\"ltx_text ltx_font_italic\">ascribed categories</span> (archetypes imposed by social convention) and <span class=\"ltx_text ltx_font_italic\">performed identities</span> (emergent in interaction)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bucholtz &amp; Hall, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib2\" title=\"\">2005</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "both",
                    "social"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, they mirror the dual nature of human role assessment, balancing scalability and comparability with depth and nuance. This theoretical grounding reinforces the design of Speech-DRAME as a framework that does not privilege one perspective over the other, but instead integrates both to reflect the full spectrum of how roles are evaluated in speech-based interaction.</p>\n\n",
                "matched_terms": [
                    "both",
                    "not",
                    "depth",
                    "role"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Discussion.</span>\nWhile our released dataset and experiments concentrate on single-turn evaluation, the modular nature of the formulation ensures that extending to multi-turn is straightforward.\nFuture work can leverage <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p5.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> to study richer aspects of speech role-play, such as persona persistence, interaction dynamics, and role consistency across extended dialogues.</p>\n\n",
                "matched_terms": [
                    "dynamics",
                    "role"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Archetype Evaluation follows a top&#8211;down perspective rooted in sociology and role theory, where social behavior is interpreted through shared stereotypes and generalized expectations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goffman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib23\" title=\"\">1949</a>; Campbell, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib4\" title=\"\">2008</a>)</cite>.\nThis strategy abstracts role-play into broad and easily recognizable archetypes, enabling scalable evaluation of whether a generated response aligns with salient role features.</p>\n\n",
                "matched_terms": [
                    "role",
                    "social",
                    "aligns"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Role dimensions.</span>\nWe operationalize archetypes through five principal categories, each representing a stereotypical dimension of role identity or behavior, including (1) <span class=\"ltx_text ltx_font_bold\">Occupation / Functional Role (Modern Daily Life)</span> (e.g., a firefighter rescuing a trapped civilian). (2) <span class=\"ltx_text ltx_font_bold\">Occupation / Functional Role (Fictional or Fantasy)</span> (e.g., a martial arts cult leader confronting orthodox rivals). (3)&#160;<span class=\"ltx_text ltx_font_bold\">Social Identity</span> (e.g., a doting parent worrying about a child, or a &#8220;Buddhist youth&#8221; eschewing competition). (4) <span class=\"ltx_text ltx_font_bold\">Named / Specific Characters</span> (e.g., Sun Wukong mocking celestial authorities in <em class=\"ltx_emph ltx_font_italic\">Journey to the West</em>). (5) <span class=\"ltx_text ltx_font_bold\">Personality Traits</span> (e.g., &#8220;hot-blooded&#8221; in a competitive setting, or &#8220;complaining&#8221; in response to poor service). (6) <span class=\"ltx_text ltx_font_bold\">Relative Roles</span> (e.g., parent or stranger).</p>\n\n",
                "matched_terms": [
                    "specific",
                    "social",
                    "role"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scene dimensions.</span>\nIn addition to roles, we incorporate contextual triggers that anchor role-play in stereotypical situations.\nA central focus is on <span class=\"ltx_text ltx_font_bold\">everyday external events or state changes</span> (e.g., receiving a promotion, being caught in gossip), which provide concrete scenarios for testing archetypical responses.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "events"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Pipeline overview.</span>\nAt a high level, archetype evaluation data are generated through a three-stage pipeline:\n(1) selecting keywords that specify role and scene;\n(2) expanding them into full prompts via structured templates; and\n(3) rendering prompts into speech with contemporary TTS systems.\nThis modular pipeline ensures both scalability and reproducibility, while keeping prompts vivid enough for reliable judgments. (4) Based on the generated data (paired scene and synthesized speech), expert human annotators are recruited for detailed annotation on semantic check (i.e., <span class=\"ltx_text ltx_font_italic\">Content Pass</span>) and three different spoken role-play dimensions (i.e., <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human-likeness</span>, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>).\nImplementation details, including template design, alternative prompting strategies, and annotation guidelines, are provided in the following subsections of this Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "both",
                    "scene",
                    "role"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Excluded Dimensions.</span> We initially considered more fine-grained control dimensions, but excluded them to preserve the archetype-level focus. Examples of excluded dimensions include:\n<em class=\"ltx_emph ltx_font_italic\">Emotional State &amp; Expression, Single Emotion Prompts, Explicit User Input / Assistant Output structures, Emotional Transition / Fluctuation, Voice Style &amp; Prosody (pacing, intonation, vocal timbre, expressive style, accent/dialect), and Content / Interaction Patterns (task/topic controls, specific linguistic structures, or speech production challenges).</em></p>\n\n",
                "matched_terms": [
                    "specific",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2\" title=\"Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the internal annotation interface used for <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>.\nThe platform provides an integrated environment for both playback and evaluation of model-generated role-play speech samples.\nAnnotators first interact with the audio player interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf1\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>), which visualizes waveforms and spectrograms for multiple versions of the same prompt, enabling side-by-side comparison under a shared role instruction (e.g., firefighter, teacher, or actor).\nThen, they proceed to the annotation interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf2\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a>), where they assign scores for multiple dimensions such as <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, along with reasoning and confidence inputs.\nThis structured interface supports high-consistency, fine-grained evaluation of role-play behaviors in speech models.</p>\n\n",
                "matched_terms": [
                    "both",
                    "role"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Reject if the clip is too short (1&#8211;2s) or clearly breaks immersion.</p>\n\n",
                "matched_terms": [
                    "immersion",
                    "clearly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Appropriateness (Context Considered):</span> tonal fit of the vocal performance to the described role and scene.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "role"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Annotators also provide a confidence level (high, moderate, low) for their Appropriateness rating, reflecting familiarity with the role and scene. This provides additional granularity in analysis.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "role"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Instruction Failure.</span> Generated content does not adhere to the trait(s) or role described in the prompt (&#8220;question&#8221;).</p>\n\n",
                "matched_terms": [
                    "not",
                    "role"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">Appropriateness</span>: comment for <em class=\"ltx_emph ltx_font_italic\">all</em> scores 1&#8211;5. Explain <em class=\"ltx_emph ltx_font_italic\">why</em> the <em class=\"ltx_emph ltx_font_italic\">delivery</em> was/was not a good match to the in-context traits (role &amp; scene).</p>\n\n",
                "matched_terms": [
                    "match",
                    "role",
                    "scene",
                    "good",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the prompt indicates a specific gender, flag mismatches in comments (e.g., &#8220;GENDER MISMATCH&#8221;). Do <em class=\"ltx_emph ltx_font_italic\">not</em> penalize rubric scores for gender mismatch; rate vocal performance as-is.</p>\n\n",
                "matched_terms": [
                    "specific",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If any portion of the clip is ill-fitting, prefer the lower overall score; do not average across good and bad segments.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "score",
                    "not",
                    "good"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Rate <em class=\"ltx_emph ltx_font_italic\">only</em> how well the <em class=\"ltx_emph ltx_font_italic\">delivery</em> fits the role/scene described in the prompt (tonal fit). Ignore Audio Quality and Human Likeness issues while scoring this dimension.</p>\n\n",
                "matched_terms": [
                    "issues",
                    "fits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative sample generation.</span>\nTo stress-test evaluation models, we construct contrastive negative examples that deliberately mismatch role and scene information:</p>\n\n",
                "matched_terms": [
                    "scene",
                    "role"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative local scenes.</span>\nFor each original segment, we generate additional scene descriptions at different levels of mismatch, ranging from subtle alterations to fully unrelated scenarios.\nAppendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS4\" title=\"F.4 Prompts for Negative Scenes &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.4</span></a> outlines the prompting strategies and example outputs.</p>\n\n",
                "matched_terms": [
                    "fully",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative character profiles.</span>\nWe also create role-profile mismatches by resynthesizing transcripts with diverse TTS systems, thereby breaking the link between authentic delivery and intended role.\nWe explore both text-instruction-based prompting and comparisons across commercial and non-commercial TTS frameworks.\nDetailed model lists and parameter settings are documented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS6\" title=\"F.6 TTS Systems and Settings &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.6</span></a>.</p>\n\n",
                "matched_terms": [
                    "both",
                    "role"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During annotation, raters are presented with either: (i) ground-truth stories, (ii) stories with negative local scenes, or (iii) stories with negative character profile. Annotators assign realism scores without being exposed to the underlying mismatch reasons. Instead, these reasons are provided only to the quality-assurance team, who use them as reference for internal validation and consistency checks. While the mismatch reasons may not be perfect, they help streamline quality control procedures and maintain reliability across the dataset.</p>\n\n",
                "matched_terms": [
                    "either",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide consistent evaluation context, we generate local scene descriptions summarizing the situational background.\nThese prompts aim to capture the immediate narrative context without introducing extraneous detail.</p>\n\n",
                "matched_terms": [
                    "situational",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To make the construction of the realism evaluation task concrete, we present a worked example that begins with a curated local scene and character profile, followed by the same transcript adapted into mismatched contexts. These mismatches appear either at the <em class=\"ltx_emph ltx_font_italic\">local-scene</em> level or at both the <em class=\"ltx_emph ltx_font_italic\">character profile and scene</em> level, thereby providing controlled degradations of realism.</p>\n\n",
                "matched_terms": [
                    "either",
                    "mismatched",
                    "both",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Level 0 (Mild Mismatch).</span>\n<em class=\"ltx_emph ltx_font_italic\">Fake Scene:</em> A quiet teahouse where Yunjin casually chats with friends about recent weather. She suddenly delivers the heroic verses in full operatic style.\n<em class=\"ltx_emph ltx_font_italic\">Reason:</em> The shift from light conversation to intense battle imagery feels abrupt and unsupported. The mismatch is mild but noticeable.</p>\n\n",
                "matched_terms": [
                    "noticeable",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Level 1 (Moderate Mismatch).</span>\n<em class=\"ltx_emph ltx_font_italic\">Fake Scene:</em> In a futuristic laboratory, a robotic engineer celebrates a successful combat simulation. Overwhelmed with excitement, he blurts out the opera verses.\n<em class=\"ltx_emph ltx_font_italic\">Fake Profile:</em> Xiao&#160;A, a calm and rational roboticist, normally concise and analytical, not prone to poetic expression.\n<em class=\"ltx_emph ltx_font_italic\">Reason:</em> The dramatic, metaphorical verses conflict with his logical persona and sterile environment. The mismatch is sharper: the heroic rhetoric feels uncharacteristic and jarring.</p>\n\n",
                "matched_terms": [
                    "not",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Yunjin case study illustrates a fundamental difference between realism evaluation and archetype evaluation. Archetype prompts, as described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>, are constructed by pairing abstract role categories with synthetic contextual scenes. These cover a broad space of possible situations, ranging from modern occupations (e.g., firefighters) to fictional personas (e.g., cult leaders in martial arts fantasy) or social identities (e.g., overprotective parents, laid-back friends). While such prompts provide controlled and diverse coverage, they are ultimately template-based and rely on stereotypes to elicit role-play behaviors.</p>\n\n",
                "matched_terms": [
                    "social",
                    "role"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the Yunjin example is drawn from authentic performance, where the character&#8217;s persona (an opera singer) and local scene (a battlefield narration) are naturally aligned through artistic delivery. The realism task does not merely test whether a model can follow a prompt, but whether it can preserve coherence between a speaker&#8217;s global identity, immediate context, and expressive prosody. When Yunjin&#8217;s transcript is transplanted into mismatched settings or paired with incongruent speaker profiles, the breakdown in realism becomes apparent: the same words shift from inspiring to awkward, jarring, or even absurd.</p>\n\n",
                "matched_terms": [
                    "not",
                    "scene",
                    "mismatched"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed a diverse set of TTS systems to generate negative character profiles and resynthesized speech for Realism Evaluation.\nThese included both commercial and non-commercial models, with variation in style control, conditioning prompts, and reference usage.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.T8\" title=\"Table 8 &#8227; Same-Speaker Voice Clone. &#8227; F.6 TTS Systems and Settings &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> summarizes the systems and settings.\nBelow, we detail the major strategies used across systems:</p>\n\n",
                "matched_terms": [
                    "both",
                    "major"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The annotation interface for <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> focuses on assessing contextual and narrative coherence in model-generated speech.\nUnlike the archetype-oriented evaluation, which emphasizes stylistic and emotional fidelity, this platform examines how well the voice performance aligns with a character&#8217;s personality, dialogue situation, and scene progression.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "scene",
                    "aligns"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Once familiar with the scene, annotators proceed to the detailed evaluation form shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.F4\" title=\"Figure 4 &#8227; F.7 Annotation Platform for Realism Evaluation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nThis interface is divided into two major sections: prosodic/emotional quality, higher-level coherence and semantic alignment.</p>\n\n",
                "matched_terms": [
                    "major",
                    "alignment",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the audio&#8217;s speaker is <em class=\"ltx_emph ltx_font_italic\">clearly not</em> the intended target identity, assign a score of <span class=\"ltx_text ltx_font_bold\">1</span> to all Profile-dependent dimensions (e.g., Voice Identity Matching, Trait Embodiment) and proceed with other dimensions as applicable.</p>\n\n",
                "matched_terms": [
                    "score",
                    "not",
                    "clearly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Semantic Match</span> (spoken content vs. Local Scene goal/emotion/action).</p>\n\n",
                "matched_terms": [
                    "match",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Do not reward theatricality/technical polish unless it naturally fits the character <em class=\"ltx_emph ltx_font_italic\">and</em> scene.</p>\n\n",
                "matched_terms": [
                    "not",
                    "scene",
                    "fits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Use both sources:</span> Play the audio and skim the transcript together to assess delivery quality, expression, and alignment with character intent (do not penalize transcript mismatches).</p>\n\n",
                "matched_terms": [
                    "both",
                    "not",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Focus only on human speech:</span> Do not factor in background music/soundtracks, environmental or ambient noise, or non-speech audio events; these may reflect the scene but should not influence scoring of the spoken performance.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "not",
                    "events"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identity mismatch:</span> If the clip&#8217;s speaker is clearly not the intended target speaker, assign a score of <span class=\"ltx_text ltx_font_bold\">1</span> for every rating dimension that relies on the Character Profile (e.g., Character Style Traits, Trait Embodiment, Character Consistency).</p>\n\n",
                "matched_terms": [
                    "score",
                    "not",
                    "clearly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt contains contradictory information.</span> Conflicts make accurate annotation difficult or impossible. Examples: incompatible accent specifications; Local Scene drastically at odds with intended emotion (see <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>); Local and global scenes inconsistent; traits described as &#8220;evolving over time&#8221; without a timeline.</p>\n\n",
                "matched_terms": [
                    "match",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt is insufficient to evaluate the voice sample.</span> Audio too short to gauge the dimensions; or the prompt lacks specific characteristics/is too vague or multi-directional to know which version of the character is present. (It is OK if audio does not match transcript exactly; if too much is missing to annotate, mark as insufficient.)</p>\n\n",
                "matched_terms": [
                    "match",
                    "specific",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rubric Rating Instructions.</span>\nScore each subcriterion on a 1&#8211;5 scale. Use the detailed rubrics below and the examples to maintain calibration across characters, tones, and emotional styles. Prioritize in-character believability; do not reward theatricality, emotional intensity, or technical polish unless they naturally fit the character <em class=\"ltx_emph ltx_font_italic\">and</em> scene.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "score",
                    "not",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Focus: alignment of expressed emotion with character style and situational demand. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Gating:</span> Only rate 2.2 if 2.1 <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"A7.SS4.SSSx5.p1.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math> 3; only rate 2.3 if 2.2 <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"A7.SS4.SSSx5.p1.m2\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math> 3.</p>\n\n",
                "matched_terms": [
                    "situational",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Understood the character and scene clearly,</p>\n\n",
                "matched_terms": [
                    "scene",
                    "clearly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Progressive gating for emotion.</span>\nOur <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness</em> uses gated sub-scores (accuracy <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A8.SS2.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> intensity control <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A8.SS2.p3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> dynamic range), preventing spurious high scores when base emotion is misidentified.\nThis reduces label noise and improves SEM alignment stability.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Auditability and SEM alignment.</span>\nEvery score is traceable to a rubric with short rater rationales and confidence.\nWe then tune SEMs to these labels (rather than opaque judge outputs), enabling calibration studies and error analyses that would not be possible with black-box LLM judges.</p>\n\n",
                "matched_terms": [
                    "score",
                    "not",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A critical observation is that annotations in Mandarin and English cannot be directly compared. The annotation groups for the two languages differed in their linguistic backgrounds and cultural interpretations, which leads to systematic differences in scoring. Our bilingual pilot further confirmed that annotators often interpret role-play appropriateness and expressiveness differently depending on their cultural frame of reference. For example, Mandarin raters tend to reward delivery styles that align with culturally expected prosody and politeness strategies, while English raters prioritize spontaneity and naturalness. These discrepancies parallel the findings of the singing voice conversion challenge (SVCC2023) from <cite class=\"ltx_cite ltx_citemacro_citet\">Huang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib31\" title=\"\">2023</a>)</cite>, where native and non-native listeners provided significantly different ratings for the same samples, particularly for naturalness and speaker similarity. Consequently, while absolute scores differ substantially across Mandarin and English, only within-language comparisons are meaningful. Cross-lingual ranking is not valid given the different annotation bases.</p>\n\n",
                "matched_terms": [
                    "expected",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Within Mandarin from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, the relative rankings of models are clear. <span class=\"ltx_text ltx_font_typewriter\">Doubao</span> stands out as the strongest performer across all four dimensions, reaching near-ceiling levels in both <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> (4.37) and <span class=\"ltx_text ltx_font_italic\">Content Pass</span> (0.98). It also leads in role-related dimensions such as <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, indicating balanced competence across linguistic accuracy, delivery, and expressiveness. <span class=\"ltx_text ltx_font_typewriter\">G-CosyVoice2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-CosyVoice2</span> follow closely, especially in audio quality, though they lag behind in role appropriateness. <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> achieve mid-range results, reflecting higher variance across samples, while <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> shows competitive content understanding but lower naturalness and appropriateness. <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> consistently ranks the lowest, with large variance, underscoring instability and weaker alignment with archetypal role expectations.</p>\n\n",
                "matched_terms": [
                    "both",
                    "alignment",
                    "role"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The picture shifts in English, elaborated in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Here, <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> emerges as the most effective model by a wide margin, achieving top scores in both <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> (3.46) and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> (3.49), as well as high content pass rates (0.97). This reflects its strong grounding in English discourse norms and role adherence. In contrast, <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> variants perform well in <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> but are weaker in role alignment, highlighting a trade-off between acoustic fidelity and role-play embodiment. Other models such as <span class=\"ltx_text ltx_font_typewriter\">Doubao</span>, <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span>, and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> achieve moderate results, while <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> trail far behind, showing limited robustness in English scenarios. Taken together, the English results reveal both a sharper separation across models and a wider variance in appropriateness scores compared to Mandarin.</p>\n\n",
                "matched_terms": [
                    "strong",
                    "both",
                    "alignment",
                    "role"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T11\" title=\"Table 11 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> and Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.F5\" title=\"Figure 5 &#8227; English Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.F6\" title=\"Figure 6 &#8227; English Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> further highlight differences across domains. In Mandarin, performance is relatively consistent across categories, with means clustering around 3.1&#8211;3.3. The highest scores appear in <span class=\"ltx_text ltx_font_typewriter\">SocialIdentity</span> and <span class=\"ltx_text ltx_font_typewriter\">DailyOccupation</span>, suggesting that models handle culturally grounded archetypes more reliably. In English, however, domain variation is much greater. Categories such as <span class=\"ltx_text ltx_font_typewriter\">FantasyOccupation</span> and <span class=\"ltx_text ltx_font_typewriter\">NamedCharacters</span> receive notably lower scores (means below 2.0), reflecting the difficulty of producing expressive, role-appropriate speech in imaginative or culturally specific contexts. Occupational roles remain relatively easier for models in both languages, likely because these scenarios are closer to training data distributions.</p>\n\n",
                "matched_terms": [
                    "specific",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although cross-lingual comparisons are invalid, this does not diminish the utility of archetype evaluation for role-play benchmarking. In fact, the observed inconsistencies reinforce a central point: successful role-play requires models to adapt to the cultural and linguistic norms of the target language community. A system that performs well in Mandarin but poorly in English cannot be considered universally capable, since end users will evaluate appropriateness and naturalness according to their native expectations. Thus, archetype evaluation highlights not only model-specific strengths and weaknesses but also the need for culturally grounded evaluation pipelines.</p>\n\n",
                "matched_terms": [
                    "inconsistencies",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another notable pattern is the trade-off between acoustic quality and role embodiment. CosyVoice2 variants achieve strong audio quality but weaker human likeness and appropriateness, whereas <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> prioritizes role coherence at the expense of audio fidelity. This suggests that future systems must better integrate high-quality synthesis with nuanced role adherence. Similarly, the consistently lower scores for imaginative and identity-driven roles underscore the limitations of current SFMs in capturing subtle sociocultural expectations and narrative context.</p>\n\n",
                "matched_terms": [
                    "strong",
                    "role"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In sum, archetype evaluation reveals three major findings:\n(1) Mandarin and English annotations cannot be directly compared, due to linguistic and cultural annotation inconsistencies, consistent with prior evidence from SVCC 2023.\n(2) Within each language, clear model rankings emerge, <span class=\"ltx_text ltx_font_typewriter\">Doubao</span> dominates in Mandarin, while <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> excels in English.\n(3) Domain sensitivity remains a challenge, particularly for imaginative or culturally rich roles, where scores are substantially lower and inter-model variance is higher.</p>\n\n",
                "matched_terms": [
                    "inconsistencies",
                    "major"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although realism evaluation targets nuanced paralinguistic behaviors, human annotators achieve <em class=\"ltx_emph ltx_font_italic\">consistently high agreement</em> once the rubric disentangles the aspects to be judged.\nAs shown in the agreement summary in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F7\" title=\"Figure 7 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, most dimensions fall in the <math alttext=\"0.86\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>0.86</mn><annotation encoding=\"application/x-tex\">0.86</annotation></semantics></math>&#8211;<math alttext=\"0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mn>0.88</mn><annotation encoding=\"application/x-tex\">0.88</annotation></semantics></math> range, with the lowest agreement on <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> (<math alttext=\"\\approx 0.83\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.83</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.83</annotation></semantics></math>) and the highest on <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em> / <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> (<math alttext=\"\\approx 0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.88</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.88</annotation></semantics></math>).\nEmotion-related, progressive metrics (Intensity and Transition) are slightly lower (<math alttext=\"\\approx 0.85\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.85</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.85</annotation></semantics></math>), reflecting the added difficulty of judging graded expressivity.</p>\n\n",
                "matched_terms": [
                    "match",
                    "slightly",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F8\" title=\"Figure 8 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F9\" title=\"Figure 9 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> visualize the score distributions per dimension.\nWe highlight three patterns:\n(i) <span class=\"ltx_text ltx_font_bold\">Progressive/gated emotion metrics.</span> For <em class=\"ltx_emph ltx_font_italic\">Emotion Intensity</em> and <em class=\"ltx_emph ltx_font_italic\">Emotion Transition</em>, we assign <span class=\"ltx_text ltx_font_typewriter\">1</span> when the scene is marked <em class=\"ltx_emph ltx_font_italic\">N/A</em> (i.e., emotion accuracy is insufficient to meaningfully judge intensity/transition). This produces a visible mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, a sparse bin at <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>, and slightly lower overall agreement; downstream analyses conditioning on adequate <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> (e.g., mean&#160;<math alttext=\"\\geq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 3</annotation></semantics></math>) recover more bell-shaped distributions.\n(ii) <span class=\"ltx_text ltx_font_bold\">Traits Embodiment.</span> Scores tend to avoid the middle (<math alttext=\"\\sim\\!3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!3</annotation></semantics></math>), concentrating on low values when traits clearly mismatch and high values when traits are strongly realized. This yields the lowest inter-rater agreement among dimensions, reflecting the inherently compositional and role-specific nature of trait judgments.\n(iii) <span class=\"ltx_text ltx_font_bold\">Other dimensions.</span> <em class=\"ltx_emph ltx_font_italic\">Pitch Variation</em>, <em class=\"ltx_emph ltx_font_italic\">Rhythmic Naturalness</em>, <em class=\"ltx_emph ltx_font_italic\">Stress &amp; Emphasis</em>, <em class=\"ltx_emph ltx_font_italic\">Voice Identity Matching</em>, <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em>, <em class=\"ltx_emph ltx_font_italic\">Global Story Coherence</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> are approximately unimodal with modes at <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>&#8211;<math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math>, consistent with moderate&#8211;strong realization in the dataset and with the high agreement reported.</p>\n\n",
                "matched_terms": [
                    "score",
                    "slightly",
                    "match",
                    "overall",
                    "scene",
                    "clearly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotion rendering (Accuracy, Intensity, Transition).</span> Professionals show higher centers and reduced mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.I1.i2.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> (fewer gated <span class=\"ltx_text ltx_font_typewriter\">N/A</span>s), suggesting more consistent emotional grounding and smoother dynamics; amateurs exhibit wider variance and more mass at the extremes.</p>\n\n",
                "matched_terms": [
                    "dynamics",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scene and semantics (Local Scene Fit, Global Coherence, Semantic Match).</span> Differences are present but smaller; both groups cluster in the mid&#8211;high range, with professionals exhibiting slightly narrower distributions.</p>\n\n",
                "matched_terms": [
                    "match",
                    "slightly",
                    "both",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, professionals outperform amateurs on expressivity and control, while narrative/semantic alignment shows a milder gap.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "alignment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and few-shot experiments on commercial models such as <span class=\"ltx_text ltx_font_typewriter\">Gemini</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o</span>, we report results as the average of five independent runs to reduce randomness and obtain more stable predictions. In contrast, for open-source models, we observed that their instruction-following ability is relatively weak. To mitigate this, we prompt them to predict one dimension at a time rather than all dimensions jointly. Specifically, for each metric, after showing the rubric prompt, we request completion in a fixed template and extract the probability distribution over the discrete tokens <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>. The final score is then computed from these probabilities, which we found makes open-source models usable to some extent under this setting.</p>\n\n",
                "matched_terms": [
                    "weak",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Effect of repeated runs.\nWhen comparing single-run decoding (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot-1run</span>) against averaging over five runs (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot</span>), we see consistent gains across nearly all dimensions (average score improves from 0.353 to 0.390). This suggests that sampling variance plays a substantial role in the reliability of large language model (LLM) evaluators, and averaging multiple outputs reduces this variance, producing more stable correlation with human annotations.</p>\n\n",
                "matched_terms": [
                    "score",
                    "role"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Scaling the number of demonstrations.\nIncreasing the few-shot set size from three to six and nine further boosts performance (0.459 and 0.469, respectively). The improvements are most pronounced in prosodic and emotional categories (e.g., <em class=\"ltx_emph ltx_font_italic\">Pitch</em>, <em class=\"ltx_emph ltx_font_italic\">Accuracy</em>, <em class=\"ltx_emph ltx_font_italic\">Intensity</em>), suggesting that more examples provide stronger guidance in modeling subtle paralinguistic cues. Gains in <em class=\"ltx_emph ltx_font_italic\">Character Consistency</em> and <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance</em> are also noticeable, although smaller in magnitude, which may indicate that these higher-level judgments are less sensitive to prompt scaling once a few demonstrations are provided.</p>\n\n",
                "matched_terms": [
                    "noticeable",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In general, the results demonstrate two complementary strategies for stabilizing LLM-based evaluators: (i) averaging multiple runs to reduce randomness, and (ii) enriching prompts with more few-shot exemplars to improve adherence to rubric-based evaluation. Combining both leads to the strongest overall performance, with the nine-shot condition achieving the highest average correlation&#160;(0.469).</p>\n\n",
                "matched_terms": [
                    "overall",
                    "both"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the archetype test set, all fine-tuned variants achieve strong <span class=\"ltx_text ltx_font_italic\">Content Pass</span> accuracy above 92%. Differences appear mainly in correlation-based metrics. Here, <span class=\"ltx_text ltx_font_typewriter\">Base-FT</span> yields the highest score on <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> delivers the strongest performance in <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> and the best overall average (0.629). This suggests that instruction-tuned checkpoints paired with parameter-efficient adaptation can better capture higher-level perceptual traits.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "strong",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A different pattern emerges on the real recording test set. Both <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> and <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> struggle, yielding negative or near-zero correlations across most dimensions. In contrast, <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> shows clear improvements, achieving positive correlations in all prosodic and emotional dimensions and producing the highest overall average (0.247). This indicates that LoRA adaptation on top of an instruction-tuned backbone provides better generalization to out-of-distribution, real-world data.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "emotional",
                    "both"
                ]
            }
        ]
    },
    "A6.T8": {
        "caption": "Table 8: TTS systems and generation settings used in Realism Evaluation.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">System</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Type</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Prompt Type</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">References</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">OpenAI <span class=\"ltx_text ltx_font_typewriter\">TTS-1 HD</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Proprietary</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Speaker Assignment</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">API Call <cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib48\" title=\"\">2025c</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">Gemini2.5pro-preview-TTS</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Proprietary</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Speaker Assignment with Instruction</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">API Call <cite class=\"ltx_cite ltx_citemacro_citep\">(Google Cloud, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib24\" title=\"\">2025</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"3\">ElevenLabs <span class=\"ltx_text ltx_font_typewriter\">Multilingual V2</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Proprietary</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Similar Voice Matching</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">API Call <cite class=\"ltx_cite ltx_citemacro_citep\">(ElevenLabs, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib18\" title=\"\">2025</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Proprietary</td>\n<td class=\"ltx_td ltx_align_left\">Voice Design</td>\n<td class=\"ltx_td ltx_align_left\">API Call <cite class=\"ltx_cite ltx_citemacro_citep\">(ElevenLabs, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib18\" title=\"\">2025</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Proprietary</td>\n<td class=\"ltx_td ltx_align_left\">Voice Clone</td>\n<td class=\"ltx_td ltx_align_left\">API Call <cite class=\"ltx_cite ltx_citemacro_citep\">(ElevenLabs, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib18\" title=\"\">2025</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">HumeTTS</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Proprietary</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Voice Design</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">API Call <cite class=\"ltx_cite ltx_citemacro_citep\">(Hume AI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib33\" title=\"\">2025</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_typewriter\">OpenAudio-S1</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Proprietary</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Identical Voice Clone</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">API Call <cite class=\"ltx_cite ltx_citemacro_citep\">(Fish Audio, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib19\" title=\"\">2025</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Proprietary</td>\n<td class=\"ltx_td ltx_align_left\">Same Speaker Voice Clone</td>\n<td class=\"ltx_td ltx_align_left\">API Call <cite class=\"ltx_cite ltx_citemacro_citep\">(Fish Audio, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib19\" title=\"\">2025</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\">MiniMax <span class=\"ltx_text ltx_font_typewriter\">speech-02-hd</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Proprietary</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Voice Design</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">API Call <cite class=\"ltx_cite ltx_citemacro_citep\">(Fish Audio, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib19\" title=\"\">2025</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Proprietary</td>\n<td class=\"ltx_td ltx_align_left\">Voice Clone</td>\n<td class=\"ltx_td ltx_align_left\">API Call <cite class=\"ltx_cite ltx_citemacro_citep\">(Fish Audio, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib19\" title=\"\">2025</a>)</cite>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS1</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Proprietary</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Identical Voice Clone</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Proprietary</td>\n<td class=\"ltx_td ltx_align_left\">Same Speaker Voice Clone</td>\n<td class=\"ltx_td ltx_align_left\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS2</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Proprietary</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Identical Voice Clone</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Proprietary</td>\n<td class=\"ltx_td ltx_align_left\">Same Speaker Voice Clone</td>\n<td class=\"ltx_td ltx_align_left\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS3</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Proprietary</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Identical Voice Clone</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Proprietary</td>\n<td class=\"ltx_td ltx_align_left\">Same Speaker Voice Clone</td>\n<td class=\"ltx_td ltx_align_left\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS4</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Proprietary</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Identical Voice Clone</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Proprietary</td>\n<td class=\"ltx_td ltx_align_left\">Same Speaker Voice Clone</td>\n<td class=\"ltx_td ltx_align_left\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS5</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Proprietary</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Identical Voice Clone</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Proprietary</td>\n<td class=\"ltx_td ltx_align_left\">Same Speaker Voice Clone</td>\n<td class=\"ltx_td ltx_align_left\">-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Open-source</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Identical Voice Clone</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib16\" title=\"\">2024a</a>)</cite></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Open-source</td>\n<td class=\"ltx_td ltx_align_left\">Same Speaker Voice Clone</td>\n<td class=\"ltx_td ltx_align_left\"><cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib16\" title=\"\">2024a</a>)</cite></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_typewriter\">Vevo1.5</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Open-source</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Identical Voice Clone</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib79\" title=\"\">2025b</a>)</cite></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Open-source</td>\n<td class=\"ltx_td ltx_align_left\">Same Speaker Voice Clone</td>\n<td class=\"ltx_td ltx_align_left\"><cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib79\" title=\"\">2025b</a>)</cite></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_typewriter\">MaskGCT</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Open-source</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Identical Voice Clone</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib66\" title=\"\">2025d</a>)</cite></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Open-source</td>\n<td class=\"ltx_td ltx_align_left\">Same Speaker Voice Clone</td>\n<td class=\"ltx_td ltx_align_left\"><cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib66\" title=\"\">2025d</a>)</cite></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_typewriter\">OpusLM-7B</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Open-source</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Identical Voice Clone</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib57\" title=\"\">2025</a>)</cite></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Open-source</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Same Speaker Voice Clone</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><cite class=\"ltx_cite ltx_citemacro_citep\">(Tian et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib57\" title=\"\">2025</a>)</cite></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "speech02hd",
            "inhousetts5",
            "2025d",
            "type",
            "matching",
            "multilingual",
            "evaluation",
            "settings",
            "vevo15",
            "gemini25propreviewtts",
            "clone",
            "2025b",
            "instruction",
            "realism",
            "cloud",
            "tts",
            "openaudios1",
            "systems",
            "audio",
            "2024a",
            "inhousetts4",
            "voice",
            "speaker",
            "generation",
            "fish",
            "system",
            "used",
            "elevenlabs",
            "openai",
            "assignment",
            "inhousetts1",
            "references",
            "call",
            "zhang",
            "inhousetts3",
            "api",
            "google",
            "maskgct",
            "cosyvoice2",
            "opuslm7b",
            "tian",
            "proprietary",
            "humetts",
            "hume",
            "same",
            "minimax",
            "design",
            "inhousetts2",
            "prompt",
            "identical",
            "2025c",
            "tts1",
            "similar",
            "opensource",
            "wang"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We employed a diverse set of TTS systems to generate negative character profiles and resynthesized speech for Realism Evaluation.\nThese included both commercial and non-commercial models, with variation in style control, conditioning prompts, and reference usage.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.T8\" title=\"Table 8 &#8227; Same-Speaker Voice Clone. &#8227; F.6 TTS Systems and Settings &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> summarizes the systems and settings.\nBelow, we detail the major strategies used across systems:</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Role-play has become a key testbed for generative models, expanding from text-only dialogue to multimodal interaction. Extending role-play to speech captures prosody, emotion, and delivery, but also poses new evaluation challenges. Current pipelines often use audio large language models (ALLMs) as zero-shot judges, which miss paralinguistic cues, collapse multiple aspects into coarse scores, and rely on synthetic speech references that fail to reflect real-world roles. We present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework that contributes at three levels: (i) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-EvalBench</em>, an evaluation benchmark with bilingual human-annotated data and protocols for training and testing speech evaluation models&#160;(SEMs), (ii) <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, a fine-tuned evaluation model, which substantially outperforms zero-shot and few-shot ALLMs, and (iii) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-RoleBench</em>, a speech role-play benchmark that leverages <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> as an automatic judge to compare speech foundation models (SFMs). Speech-DRAME distinguishes between two complementary evaluation strategies: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>, a top-down approach measuring adherence to broad role archetypes, and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em>, a bottom-up approach grounded in real human speech that emphasizes nuanced role quality. Compared to zero-shot ALLM judges, <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves stronger agreement with human ratings (Pearson correlation from 0.480 to 0.629 in archetypes, and 0.390 to 0.625 in realism). By integrating transparent benchmark resources, modeling approaches, and system-level evaluation, Speech-DRAME provides the first comprehensive, reproducible foundation for assessing spoken role-play.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "evaluation",
                    "realism",
                    "references"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Role-play has been recently recognized a cornerstone of interactive systems, from initial text-based focuses to the growing ecosystem of multimodal dialogue agents&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib6\" title=\"\">2024a</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib59\" title=\"\">2024</a>; Dai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib13\" title=\"\">2025</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib43\" title=\"\">2024</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>)</cite>. With the rapid advancement of large language models (LLMs), role-play has evolved beyond scripted interactions into dynamic character-driven conversations, supporting applications in education, entertainment, and human&#8211;AI collaboration&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib69\" title=\"\">2024</a>; Agatsuma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib1\" title=\"\">2024</a>; Fung &amp; Laing, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib20\" title=\"\">2024</a>)</cite>. Extending role-play to speech-based interaction offers an even richer communication channel, capturing prosody, emotion, and character consistency in ways that text alone cannot&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "2024a",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Evaluating speech role-play, however, remains a major challenge. Current pipelines often rely on ALLMs as judges, typically in a zero-shot setting&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>; Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>)</cite>. While this strategy can provide rough ratings, it falls short for speech role-play:</p>\n\n",
                "matched_terms": [
                    "zhang",
                    "2025b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">it lacks sensitivity to paralinguistic cues such as intonation, pacing, and emotional dynamics that define role delivery&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>; Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib81\" title=\"\">2025a</a>)</cite>,</p>\n\n",
                "matched_terms": [
                    "zhang",
                    "2025b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">it depends on synthetic speech references rather than real human speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>)</cite>, limiting its ability to reflect real-world diversity in speech role-play.</p>\n\n",
                "matched_terms": [
                    "references",
                    "zhang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome these limitations, we present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span> (Speech Detailed Role-play Assessment with Modeling and Evaluation), a unified framework that moves beyond synthetic or judge-only pipelines. Speech-DRAME is distinguished by three aspects: [i] a dual evaluation paradigm that balances archetype-driven generality with human-grounded realism (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS2\" title=\"3.2 Role-play Evaluation with Two Strategies &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), [ii] human annotations that provide perceptual grounding absent in prior resources (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4\" title=\"4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), and [iii] evaluation-model alignment that enables not only benchmarking of speech foundation models (SFMs) but also systematic analysis of the evaluation models (SEMs) themselves (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5\" title=\"5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). These aspects establish Speech-DRAME as a comprehensive and trustworthy framework for measuring and improving speech role-play.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, Speech-DRAME contributes at three levels: (i) <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>, an evaluation benchmark consisting of human-annotated datasets for archetype and realism role-play; (ii) <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span>, a fine-tuned SEM trained on EvalBench that surpasses zero-shot and few-shot ALLMs; and (iii) <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span>, a speech role-play benchmark that leverages DRAME-Eval as an automatic judge to systematically compare SFMs. By combining dual evaluation strategies, human-anchored supervision, and evaluation-model alignment, Speech-DRAME delivers systematic insights into both role-play systems and evaluation models themselves.\nOur contributions are four-fold:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a <span class=\"ltx_text ltx_font_bold\">formal task definition</span> of speech role-play and its evaluation via speech foundation models (SFMs) for generation and speech evaluation models (SEMs) for assessment, unifying linguistic and paralinguistic dimensions.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-based role-play evaluation.</span>\nEarly research focused on text-only dialogue benchmarks. Datasets such as RoleLLM, CharacterBox, and RoleMRC <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib62\" title=\"\">2024</a>; Tu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib60\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib61\" title=\"\">2025a</a>; Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib53\" title=\"\">2023</a>; Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib41\" title=\"\">2025</a>)</cite> test whether models can sustain persona consistency and character fidelity, using LLM-as-judge pipelines for scalability. However, studies like PersonaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib81\" title=\"\">2025a</a>)</cite> have shown that LLM-as-judge approaches are far from perfect, struggling with tasks as basic as tracking speaker identity. This highlights broader concerns about reliability even in text settings, which become only more severe when extended to speech, where prosody, emotion, and vocal style play a central role.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "evaluation",
                    "settings",
                    "wang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-native benchmarks.</span>\nMore recently, a number of resources have been introduced to evaluate spoken role-play agents. OmniCharacter-10K <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>)</cite> emphasizes character voice traits, while others&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib39\" title=\"\">2025</a>; Hou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib27\" title=\"\">2025</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib15\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib7\" title=\"\">2024b</a>)</cite> extend evaluation toward vocal communication, assistant capability, and multi-turn realism. Omni models such as StepAudio and KimiAudio <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite> also include role-play scenarios in their evaluation. SpeechRole and VStyle <cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>; Zhan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib76\" title=\"\">2025</a>)</cite> are closer to our work, offering datasets that test interaction ability, expressiveness, and fidelity. However, these resources are primarily built from LLM-generated dialogues and TTS outputs, with limited human validation and a heavy reliance on proprietary ALLM-as-judge pipelines. As a result, they lack grounding in diverse human speech and typically apply only coarse rubrics.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "tts",
                    "zhang",
                    "voice",
                    "evaluation",
                    "proprietary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Automatic judging with ALLMs.</span>\nIn parallel, advances in automatic judging highlight both opportunities and limitations. Audio-capable LLMs such as AudioJudge and Audio-Aware LLMs as Judges <cite class=\"ltx_cite ltx_citemacro_citep\">(Manakul et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib44\" title=\"\">2025</a>; Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>)</cite> can assess style, emotion, and delivery, while systems like Auto-ATT <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib65\" title=\"\">2025c</a>)</cite> scale this to Audio Turing Tests. Yet studies consistently reveal high prompt sensitivity and shallow capture of paralinguistic cues, suggesting that zero-shot judging remains insufficient for nuanced role-play evaluation.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "audio",
                    "prompt",
                    "2025c",
                    "evaluation",
                    "wang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Expressive TTS evaluation.</span>\nRecent work has explored evaluation of instruction-following and style-controlled TTS. InstructTTSEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite> explicitly includes role-play scenarios, while EmergentTTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">(Manku et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib45\" title=\"\">2025</a>)</cite> and discrete speech LM benchmarks <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang &amp; Sz&#233;kely, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib63\" title=\"\">2024</a>)</cite> emphasize prosody, spontaneity, and speaker consistency. These efforts push beyond intelligibility and naturalness, but still center on synthetic outputs rather than real human role-play.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "speaker",
                    "evaluation",
                    "2025b",
                    "wang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our distinction.</span>\nIn contrast, Speech-DRAME unifies these considerations through three elements: (i) a dual evaluation paradigm that combines archetype-driven generality with realism grounded in authentic human speech, (ii) human annotations that provide a reliable perceptual gold standard, and (iii) alignment of SEMs to human ratings, enabling systematic evaluation of both SFMs and the evaluation models themselves. This combination moves beyond synthetic benchmarks or zero-shot judging to provide a comprehensive and trustworthy framework for speech role-play assessment.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation.</span>\nIn principle, a <em class=\"ltx_emph ltx_font_italic\">speech evaluation model</em> (SEM) can assess a generated response using the same conditioning signals:</p>\n\n",
                "matched_terms": [
                    "same",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech role-play evaluation can be framed in multiple ways depending on the available data and the desired granularity of analysis.\nWithin Speech-DRAME, we formalize two distinct strategies inspired by sociology theories<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Please refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A2\" title=\"Appendix B Theoretical Motivation for Dual Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> for detailed discussion of the dual evaluation design.</span></span></span>, including a top-down, archetype-oriented option using synthetic speech and a bottom-up, realism-oriented option from real human speech.\nRather than merging these into a single rubric, we treat them as separate perspectives, each with unique advantages and limitations.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarizes the key contrasts, while further implementation details are provided in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>-&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "design"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>). The first strategy follows a <em class=\"ltx_emph ltx_font_italic\">top-down</em> design, inspired by prior text-based role-play benchmarks.\nHere, role-play is judged with respect to <em class=\"ltx_emph ltx_font_italic\">stereotypical archetypes</em> (e.g., &#8220;firefighter&#8221; or &#8220;ER doctor&#8221;), using scene contexts (e.g., &#8220;comfort a child trapped in fire&#8221; or &#8220;ask for help in a serious operation&#8221;).\nThis approach provides accessible, general-purpose scoring that is scalable to a wide range of scenarios, making it suitable for large-scale benchmarking.\nHowever, its reliance on broad stereotypes and global impressions limits its ability to capture fine-grained aspects of delivery (e.g., subtle prosodic shifts, nuanced emotional control)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Holt &amp; Lotto, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib26\" title=\"\">2010</a>)</cite>.\nIn practice, this strategy mirrors simplified annotation settings and is particularly useful when annotation resources are limited&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib21\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "settings",
                    "design"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS5\" title=\"F.5 Examples of Realism Role-play &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.5</span></a>). The second strategy adopts a <em class=\"ltx_emph ltx_font_italic\">bottom-up</em> design, motivated by real-world performance and real human speech.\nInstead of relying solely on stereotypes, this approach grounds evaluation in recordings from professional and non-professional speakers, drawn from realistic dialogue and media sources.\nJudgments in this setting are more fine-grained and multi-dimensional, addressing aspects such as prosodic dynamics, emotional fidelity, character consistency, and contextual fit.\nWhile this strategy requires more human annotation and careful curation, it provides benchmarks that reflect how role-play is perceived in practice.\nIt thus offers a richer framework for analyzing model outputs in settings where realism and subtlety are essential.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "evaluation",
                    "settings",
                    "design"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section details the <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> data curation process. Our data curation methodology follows a two&#8211;stage design: (i) <em class=\"ltx_emph ltx_font_italic\">task preparation</em>, which specifies the roles, scenes, and evaluation settings used to elicit speech; and (ii) <em class=\"ltx_emph ltx_font_italic\">human annotation</em>, which provides reliable ground-truth judgments on generated or real human speech.</p>\n\n",
                "matched_terms": [
                    "used",
                    "evaluation",
                    "settings",
                    "design"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Archetype Data Collection &#8227; 4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents a summary of the curated <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> dataset, where we further elaborate the details of archetype and realism evaluation data collection in following subsections, respectively. For benchmark purpose, we provide an official train-test split for the human annotated dataset for <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> so that following researchers can use the dataset to train and evaluate their SEMs for speech role-play.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Role and Scene Dimensions.</span>\nRoles are operationalized through seven categories: (1) functional occupations in daily life, (2) functional occupations in fictional or fantasy settings, (3) social identities, (4) named or specific characters, (5) personality traits, (6) relative relationship, (7) basic events. Scenes are anchored by everyday events or state changes that evoke stereotypical reactions. Details of template design and alternative prompting strategies are given in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>We note that some archetypes (e.g., firefighter) inevitably reflect cultural stereotypes. These are used solely for evaluation scalability and do not reflect normative judgments.</span></span></span></p>\n\n",
                "matched_terms": [
                    "used",
                    "evaluation",
                    "settings",
                    "design"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Sources.</span> The data is collected from a range of speech foundation models, including both cascaded and end-to-end models. We limit that only audio prompts are used as the input but adopt different prompt strategies for best quality with regards to different models. Details about the selected models and prompting details are discussed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS4\" title=\"D.4 Candidate Models for Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "used",
                    "prompt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotation follows two stages: an initial semantic check (i.e., <span class=\"ltx_text ltx_font_italic\">Content Pass</span>) to filter out instruction non-compliance or audio defects, followed by rubric-based scoring on three dimensions: <em class=\"ltx_emph ltx_font_italic\">Audio Quality</em>, <em class=\"ltx_emph ltx_font_italic\">Human-likeness</em>, and <em class=\"ltx_emph ltx_font_italic\">Appropriateness</em>. Confidence judgments are also recorded. Detailed rubric scales are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A5\" title=\"Appendix E Annotation Guidelines for Archetype Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "evaluation",
                    "instruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype Evaluation Data Summary.</span>\nThe resulting archetype evaluation dataset comprises 552 distinct scenarios for both Mandarin and English, evaluated across eight models,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>The model list is discussed in Appdenix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS4\" title=\"D.4 Candidate Models for Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.4</span></a>. Note we only include seven models for English due to the deactivation of ChatGPT-4O advanced voice model.</span></span></span> yielding 8,280 samples. In total, the dataset are split into 6,780 for training and 1,500 for testing. For benchmarking, an additional 1,250 scenarios spanning seven role/scene categories are prepared to facilitate archetype role-play benchmarking. More detailed statistics and analyses are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10\" title=\"Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">J</span></a>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realism evaluation adopts a bottom&#8211;up perspective, emphasizing authentic delivery, prosody, and contextual fit.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Sources and Negative Samples.</span>\nWe curate speech from (i) ground-truth media segments, (ii) retrieved character profiles, and (iii) generated local scene descriptions. To stress-test evaluation models, we construct contrastive mismatches: (i) negative local scenes with varying degrees of mismatch, and (ii) negative character profiles created by re-synthesizing transcripts with TTS systems. Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6\" title=\"Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F</span></a> details the prompt design and TTS generation details.</p>\n\n",
                "matched_terms": [
                    "design",
                    "tts",
                    "systems",
                    "prompt",
                    "generation",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotators judge whether performances align with the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em>, using rubrics for <em class=\"ltx_emph ltx_font_italic\">Prosodic Dynamics (i.e., Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis)</em>, <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness (with progressive gating over three sub-dimensions: Emotion Accuracy, Emotion Intensity, Emotion Transition)</em>, <em class=\"ltx_emph ltx_font_italic\">Character Consistency (i.e., Voice Identity Matching and Traits Embodiment)</em>, <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance (i.e., Local Scene Fit and Global Story Coherence)</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>. A prompt quality check and confidence ratings are also collected. Full scales and gating details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "matching",
                    "evaluation",
                    "prompt",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real-world Recoding.</span>\nAs noted in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S2\" title=\"2 Related Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, most prior efforts have relied heavily on synthetic data, mostly from TTS systems based on curated professional role-play voices. While such data can achieve high fidelity, it often lacks the diversity and spontaneity of real-world delivery, limiting the generalizability of evaluation models. To address this gap, we expand realism evaluation beyond synthetic and curated voices by incorporating newly recorded speech from both professional and amateur voice actors. This design introduces a broader spectrum of delivery styles, from polished professional acting to more natural and less controlled amateur performance.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "tts",
                    "design",
                    "systems",
                    "voice",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation Data Summary.</span>\nThe resulting realism evaluation dataset is constructed from a combination of curated media sources, including internal collections, public datasets (e.g., NCSSD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib40\" title=\"\">2024b</a>)</cite>), and original recordings collected for this project.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>All data were obtained under appropriate licenses and are legally available for research release.</span></span></span> We compile 15k annotated role-play samples: 9k in Mandarin and 5k in English. The dataset is divided into 12k samples for training, 2k mixed samples for the base test set, and 1k real-recording samples for the test set. The first test set supports general evaluation using our proposed pipeline with negative samples, while the second focuses exclusively on human speech performed by both professional and amateur voice actors. For benchmarking, 2,000 scenarios from the realism dataset are designated for realism role-play benchmarking. Detailed statistics and analyses are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11\" title=\"Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">K</span></a>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the two proposed datasets, we conduct experiments under three settings: zero-shot, few-shot, and finetuning, using pre-trained ALLMs. We formulate the task as a multi-metric estimation problem&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tjandra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib58\" title=\"\">2025</a>; Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib74\" title=\"\">2025</a>; Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib55\" title=\"\">2025b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib54\" title=\"\">a</a>)</cite>, where a single model is trained or prompted to predict multiple evaluation dimensions.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "settings",
                    "2025b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-shot and Few-shot ALLMs.</span>\nWe first evaluate the zero-shot capabilities of both proprietary and public models, including <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>, <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>, <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio</span>, and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>. All evaluations follow a unified prompting template that provides audio input alongside scene and character context.</p>\n\n",
                "matched_terms": [
                    "proprietary",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For proprietary models, we report the average of five independent runs to reduce variance and obtain more stable predictions. For open-source models, we observed weaker instruction-following ability and limited adaptation to unfamiliar tasks. To mitigate this, we did not request simultaneous prediction of multiple metrics. Instead, we queried one metric at a time. Following <cite class=\"ltx_cite ltx_citemacro_citet\">Song &amp; Bahri (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>); Lukasik et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>, we interpret model outputs through token distribution over discrete labels (<span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score.</p>\n\n",
                "matched_terms": [
                    "proprietary",
                    "opensource"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For few-shot experiments, we focus primarily on proprietary ALLMs, which demonstrate stronger instruction-following capabilities and have been widely used as judges in prior work. For each test query, we provide three additional annotated samples from the training set as in-context exemplars.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>Please find our detailed settings such as prompts, decoding algorithms in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS1\" title=\"L.1 Zeroshot/Fewshot Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.1</span></a> with additional ablation analysis in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS2\" title=\"L.2 Zeroshot/Fewshot Ablation Experiments &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.2</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "proprietary",
                    "used",
                    "settings"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Finetuning Model: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>.</span>\nIn addition to zero-shot and few-shot evaluations, we fine-tune a dedicated evaluation model, denoted as <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> for archetype and realism role-play evaluation, respectively.\nWe initialize from <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio-7B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib11\" title=\"\">2024</a>)</cite>, which provides a strong foundation for multi-modal audio-text reasoning.\nTo better leverage the multiple human annotations available in our datasets, we adopt a sampling-based training strategy: at each step, one annotator label is randomly selected, following practices in label distribution learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Geng, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib22\" title=\"\">2016</a>; Wen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib67\" title=\"\">2023</a>)</cite>.\nThis stochastic supervision implicitly exposes the model to the empirical annotation distribution, while during inference we output the full distribution over discrete labels (e.g., <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score, consistent with recent decoding-based regression methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Song &amp; Bahri, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>; Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib70\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib64\" title=\"\">2025b</a>; Lukasik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>. We apply LoRA fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib28\" title=\"\">2022</a>)</cite>. All implementation details, including optimizer settings, training schedule, and sampling policy, are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS3\" title=\"L.3 Fine-tuning Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "evaluation",
                    "settings",
                    "2025b",
                    "wang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further conduct ablation studies in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS4\" title=\"L.4 Fine-tuning Ablation Experiments &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.4</span></a>, examining: the choice of different base audio LLMs and full-parameter fine-tuning versus LoRA adaptation.\nThese analyses highlight the robustness of our sampling&#8211;expectation training strategy and clarify the trade-offs in model design.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "design"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype Evaluation.</span>\nThe results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T2\" title=\"Table 2 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reveal clear distinctions across evaluation settings. In the zero-shot condition, proprietary ALLMs such as <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> substantially outperform public models, achieving strong <span class=\"ltx_text ltx_font_italic\">content pass</span> accuracy (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>90%) and moderate correlations with other dimensions(<math alttext=\"0.43\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mn>0.43</mn><annotation encoding=\"application/x-tex\">0.43</annotation></semantics></math>&#8211;<math alttext=\"0.48\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mn>0.48</mn><annotation encoding=\"application/x-tex\">0.48</annotation></semantics></math>). By contrast, public models exhibit limited predictive ability, with correlations often below 0.2. Few-shot prompting provides modest gains for proprietary models, improving both accuracy and correlation (e.g., <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> rises from 0.480 to 0.493 average correlation). The most significant improvement arises from finetuning: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves an average correlation of 0.629, marking a large margin over zero-shot/few-shot models and demonstrating the effectiveness of task-specific supervision.</p>\n\n",
                "matched_terms": [
                    "proprietary",
                    "evaluation",
                    "settings"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T3\" title=\"Table 3 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> extends the analysis to fine-grained realism evaluation. Similar to archetype results, proprietary ALLMs lead the zero-shot setting, with <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> obtaining an average correlation of 0.390, compared to 0.284 for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>. Public models remain weak baselines. Few-shot prompting consistently improves performance: <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> gains +0.042 and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> gains +0.083 over their zero-shot averages, showing the benefit of in-context supervision. Once again, finetuning is most effective: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves 0.625 average correlation, higher than the best few-shot results. Importantly, the gains are consistent across all 10 realism dimensions, confirming that the sampling&#8211;expectation training strategy generalizes beyond simple classification to nuanced paralinguistic cues.</p>\n\n",
                "matched_terms": [
                    "proprietary",
                    "evaluation",
                    "realism",
                    "similar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real Recording Test Set.</span>\nThe real-recording evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T4\" title=\"Table 4 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> is notably more challenging, with all models suffering substantial performance drops. Proprietary models in the zero-shot setting achieve only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.04&#8211;0.08 average correlation, and public models often fall below zero. Few-shot prompting yields small but noticeable improvements, especially for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> (0.226 avg.), suggesting that exemplars partially mitigate domain mismatch between synthetic and real human speech. Interestingly, while finetuning achieves the highest overall average (0.247), its relative advantage over few-shot prompting is narrower than in the synthetic setting, and some dimensions (e.g., <span class=\"ltx_text ltx_font_italic\">Character Identity</span>) remain weak. We emphasize that this observed gap between synthetic and real recordings is not merely an artifact but an intended feature of Speech-DRAME, designed to expose the mismatch and drive future research into bridging synthetic-to-real generalization.</p>\n\n",
                "matched_terms": [
                    "proprietary",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall.</span> Across both archetype and realism evaluations, we observe three consistent trends: (i) zero-shot performance is limited, particularly for open-source models, (ii) few-shot prompting offers moderate but consistent gains, especially for proprietary ALLMs, and (iii) finetuning with task-specific supervision (<span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>) delivers the largest improvements, achieving state-of-the-art correlations across nearly all dimensions. Nevertheless, results on the real-recording test set underscore the gap between synthetic and real human role-play scenarios, motivating future work in data diversification and domain adaptation.</p>\n\n",
                "matched_terms": [
                    "proprietary",
                    "opensource",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on fine-tuned <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, we establish the DRAME-RoleBench to evaluate SFMs&#8217; role play ability. For archetype evaluation, 1,250 scenarios are constructed following the same data construction pipeline as the <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>. For realism evaluation, we reuse 2,000 scenarios in the <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> test set. We follow the same prompt strategy as outlined in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism",
                    "prompt",
                    "same"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, we consider a comprehensive set of candidate models, including six end-to-end models: <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib47\" title=\"\">2025b</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib75\" title=\"\">2024</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib71\" title=\"\">2025a</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Qwen3Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib72\" title=\"\">2025b</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">StepAudio2Mini</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib68\" title=\"\">2025</a>)</cite>; and ten cascaded models by combining two widely-used textual LLMs (i.e., <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib12\" title=\"\">2025</a>)</cite> and <span class=\"ltx_text ltx_font_typewriter\">Qwen3-30B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib73\" title=\"\">2025</a>)</cite>) and five recent TTS models (i.e., <span class=\"ltx_text ltx_font_typewriter\">F5TTS</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib8\" title=\"\">2024c</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">IndexTTS2</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib82\" title=\"\">2025b</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">MaskGCT</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib66\" title=\"\">2025d</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Vevo1.5</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib79\" title=\"\">2025b</a>)</cite>, and <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib17\" title=\"\">2024b</a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "openai",
                    "2025d",
                    "tts",
                    "zhang",
                    "maskgct",
                    "evaluation",
                    "vevo15",
                    "cosyvoice2",
                    "2025b",
                    "wang"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype evaluation.</span>\nOn archetype role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T5\" title=\"Table 5 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), cascaded pipelines consistently outperform end-to-end systems. The strongest cascaded systems, particularly <span class=\"ltx_text ltx_font_typewriter\">G-IndexTTS2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-IndexTTS2</span>, achieve both high content pass rates (above 94%) and balanced scores across audio quality, human likeness, and appropriateness. By contrast, end-to-end models such as <span class=\"ltx_text ltx_font_typewriter\">StepAudio2Mini</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> perform competitively in terms of content coverage (above 88%), but their expressive scores remain slightly lower than their cascaded counterparts. This gap suggests that separating reasoning (LLM) from speech rendering (TTS) remains advantageous when archetype fidelity is emphasized. Nevertheless, the fact that top end-to-end systems already surpass 3.0 in average rating indicates rapid closing of the performance gap.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "evaluation",
                    "tts",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism evaluation.</span>\nRealism role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T6\" title=\"Table 6 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) presents a different landscape. Here, performance differences between end-to-end and cascaded systems are less pronounced. Several end-to-end models, such as <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>, reach average scores of 3.24, matching or even exceeding the best cascaded pipelines. Importantly, realism scores are more evenly distributed across prosodic, emotional, character, and contextual dimensions, with most models clustering between 2.9&#8211;3.2. This convergence reflects the difficulty of realism-style evaluation: since the data preparation emphasized contrast with authentic human speech, models are judged under similar conditions, leading to smaller observed gaps.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "systems",
                    "matching",
                    "evaluation",
                    "similar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human alignment.</span>\nWe further conducted small-scale human studies with 10% of the benchmark data to verify the reliability of the benchmark. Archetype evaluation achieves a Spearman correlation of 0.706 between human judgments and <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, validating that the automatic evaluator closely tracks human preferences in this setting. Realism evaluation, however, achieves a more modest Spearman correlation of 0.375. This lower alignment is expected, since realism data construction did not prioritize fine-grained model separation but instead emphasized distinguishing human versus synthetic speech.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>We view this as evidence that realism presents a harder, less-saturated frontier. Rather than a limitation of DRAME-Eval, this highlights the challenge of designing evaluators that capture nuanced human perception, which motivates future work.</span></span></span> As a result, all models occupy a relatively narrow performance band, reducing sensitivity to model-level differences. We view this as evidence that realism evaluation highlights a harder, less saturated frontier where both SFMs and SEMs require further advances.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, cascaded pipelines retain an edge in archetype scenarios, especially in content robustness and expressive fidelity, but end-to-end models are catching up quickly. In realism scenarios, the field remains unsolved: existing systems exhibit similar performance bands and evaluators show weaker alignment with human judgments, underscoring the need for improved realism-grounded data and evaluation. Together, these findings establish <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> as both a reliable testbed for archetype fidelity and a challenging benchmark for realism fidelity.</p>\n\n",
                "matched_terms": [
                    "similar",
                    "evaluation",
                    "realism",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework for assessing spoken role-play through two complementary lenses: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em> (top-down) and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> (bottom-up, human-grounded). Our human-annotated <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> enables training and testing of SEMs, and our fine-tuned <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span> substantially outperforms zero-/few-shot ALLM judges. Building on this, <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> provides system-level comparison across various SFMs, revealing a consistent edge for cascaded pipelines on archetypes and a tighter spread under realism, where real human performance evaluation remain challenging. Together, these resources establish a reproducible, analyzable foundation for spoken role-play evaluation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>Limitations and future works are discussed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A13\" title=\"Appendix M Limitations and Future Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">M</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All data collection and annotation in this work followed strict ethical standards. Our bilingual datasets were constructed exclusively from legally accessible sources, including licensed media, open-source corpora, and original recordings conducted with informed consent. Annotators were recruited with appropriate compensation and provided with clear instructions, including the right to withdraw at any point. To minimize risks, no personally identifiable information was included in the released data, and all recordings were anonymized before annotation. We ensured cultural and linguistic diversity in annotator pools to mitigate potential bias across Mandarin and English role-play scenarios. While this study did not require formal IRB approval, we adhered to best practices in human-subject research, including transparency in task design and careful consideration of annotator well-being. Full details of our annotation protocol and ethical safeguards are presented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A9\" title=\"Appendix I Annotation Ethics &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</p>\n\n",
                "matched_terms": [
                    "opensource",
                    "design"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data generation:</span> LLMs were employed to expand prompts (e.g., character profiles, scene descriptions) and to produce negative samples in realism evaluation. All outputs were screened by human annotators to ensure validity and reduce bias.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline evaluators:</span> State-of-the-art audio LLMs (e.g., <span class=\"ltx_text ltx_font_typewriter\">GPT4o</span>, <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>) were included as zero-shot judges in our evaluation benchmark, enabling direct comparison with human-aligned SEMs.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation design and task construction:</span> <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> was used to assist with building instructions and summarizing story contexts. Drafted materials were subsequently refined by the authors and validated by human annotators.</p>\n\n",
                "matched_terms": [
                    "used",
                    "design"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Manuscript preparation:</span> LLMs were used for polishing, proofreading, and improving clarity of the paper. All conceptual design, analysis, and substantive writing remain the work of the authors, with LLMs serving only as language assistants.</p>\n\n",
                "matched_terms": [
                    "used",
                    "design"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Proprietary LLM outputs were used in a limited way (e.g., instruction building, story summarization, or baseline evaluations) and only for internal research purposes. Released datasets in Speech-DRAME are constructed from open-source models, licensed corpora, and original human recordings.</p>\n\n",
                "matched_terms": [
                    "proprietary",
                    "opensource",
                    "used",
                    "instruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our distinction between <span class=\"ltx_text ltx_font_italic\">Archetype Evaluation</span> and <span class=\"ltx_text ltx_font_italic\">Realism Evaluation</span> is not merely pragmatic, but is grounded in established theories of social identity, communication, and performance. The two perspectives capture complementary dimensions of how humans perceive and evaluate roles in interaction.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism as Situated Performance</span>\nIn contrast, realism highlights the fine-grained enactment of roles in situated interaction. Goffman&#8217;s dramaturgical sociology emphasizes that everyday social life is itself a performance, where believability depends on subtle paralinguistic and pragmatic cues such as prosody, hesitation, and emotional shading&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goffman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib23\" title=\"\">1949</a>)</cite>. Similarly, interactional sociolinguistics has demonstrated how micro-level features like intonation, pacing, and turn-taking patterns shape the perception of authenticity in communication&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gumperz, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib25\" title=\"\">1982</a>)</cite>. Realism Evaluation thus anchors role-play assessment in bottom-up sensitivity to authentic delivery, beyond schematic expectations.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Accordingly, Archetype Evaluation and Realism Evaluation are best viewed as complementary:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realism Evaluation captures micro-level cues of authenticity, ensuring sensitivity to nuanced delivery and interactional fit.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Pipeline overview.</span>\nAt a high level, archetype evaluation data are generated through a three-stage pipeline:\n(1) selecting keywords that specify role and scene;\n(2) expanding them into full prompts via structured templates; and\n(3) rendering prompts into speech with contemporary TTS systems.\nThis modular pipeline ensures both scalability and reproducibility, while keeping prompts vivid enough for reliable judgments. (4) Based on the generated data (paired scene and synthesized speech), expert human annotators are recruited for detailed annotation on semantic check (i.e., <span class=\"ltx_text ltx_font_italic\">Content Pass</span>) and three different spoken role-play dimensions (i.e., <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human-likeness</span>, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>).\nImplementation details, including template design, alternative prompting strategies, and annotation guidelines, are provided in the following subsections of this Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "design",
                    "tts",
                    "systems",
                    "audio",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, archetype evaluation emphasizes breadth and recognizability over subtlety.\nBy grounding role-play in socially recognizable categories and stereotypical situations, it provides a practical balance between scalability and interpretability, complementing the more fine-grained realism evaluation described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS2\" title=\"3.2 Role-play Evaluation with Two Strategies &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text generation.</span>\nWe followed a three-step expansion: keyword <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> scene description <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS3.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> full prompt with template. Some examples are shown as follows:</p>\n\n",
                "matched_terms": [
                    "generation",
                    "prompt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech generation.</span>\nPrompts were rendered into audio with CosyVoice2-0.5B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib17\" title=\"\">2024b</a>)</cite>. The prompts are selected from human-verified high-quality samples, where the selection is conducted from open-source datasets, such as LibriSpeech/Gigaspeech for English, and WenetSpeech for Chinese&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib49\" title=\"\">2015</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib5\" title=\"\">2021</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib77\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "generation",
                    "zhang",
                    "opensource"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate a diverse set of speech foundation models spanning both English and Mandarin. In total, we include 8 English and 7 Mandarin models, covering open-source and proprietary systems, and balancing end-to-end and cascaded architectures. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.T7\" title=\"Table 7 &#8227; D.4 Candidate Models for Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> provides the full list of models.</p>\n\n",
                "matched_terms": [
                    "proprietary",
                    "opensource",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Collection for Proprietary Models.</span> For proprietary systems, we adopt two collection strategies. For GPT-4o-realtime, responses are obtained directly via the <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-realtime</span> API. For Doubao and GPT-4o (Advanced Voice Mode), we collect outputs by recording from their official web interfaces. All recordings are captured through the interaction device itself, ensuring that no additional processing or distortion is introduced into the final audio.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "audio",
                    "voice",
                    "api",
                    "proprietary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">End-to-End Model Generation.</span>\nFor end-to-end systems, the audio prompt is directly used as input. The system prompt is set to default values to ensure consistent speech generation quality.</p>\n\n",
                "matched_terms": [
                    "systems",
                    "audio",
                    "prompt",
                    "generation",
                    "system",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cascaded Model Generation.</span>\nFor cascaded systems, we only use Cosyvoice2 as the TTS model in our collection of data. We select <span class=\"ltx_text ltx_font_typewriter\">Qwen3-30B-Instruct</span> and <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> as the text LLM candidates&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib73\" title=\"\">2025</a>; Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib12\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "cosyvoice2",
                    "tts",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the generation, we adopt a retrieval-augmented generation&#160;(RAG) strategy that combines text LLMs with zero-shot TTS models:</p>\n\n",
                "matched_terms": [
                    "generation",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Step 3: We retrieve acoustically similar samples from the database using <span class=\"ltx_text ltx_font_typewriter\">Qwen3-Embedding-7B</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib80\" title=\"\">2025c</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "2025c",
                    "zhang",
                    "similar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Step 4: The retrieved samples guide the zero-shot TTS system, which synthesizes speech conditioned on both the transcript and matched acoustic profile.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2\" title=\"Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the internal annotation interface used for <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>.\nThe platform provides an integrated environment for both playback and evaluation of model-generated role-play speech samples.\nAnnotators first interact with the audio player interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf1\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>), which visualizes waveforms and spectrograms for multiple versions of the same prompt, enabling side-by-side comparison under a shared role instruction (e.g., firefighter, teacher, or actor).\nThen, they proceed to the annotation interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf2\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a>), where they assign scores for multiple dimensions such as <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, along with reasoning and confidence inputs.\nThis structured interface supports high-consistency, fine-grained evaluation of role-play behaviors in speech models.</p>\n\n",
                "matched_terms": [
                    "same",
                    "audio",
                    "prompt",
                    "evaluation",
                    "used",
                    "instruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This appendix contains the full human annotation instructions used for archetype evaluation.</p>\n\n",
                "matched_terms": [
                    "used",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Instruction Failure.</span> Generated content does not adhere to the trait(s) or role described in the prompt (&#8220;question&#8221;).</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "instruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><em class=\"ltx_emph ltx_font_italic\">Example (reject):</em> Prompt: &#8220;Your personality is sharp-tongued. Your friend asks about their new haircut.&#8221; Audio says: &#8220;It looks great on you.&#8221; <math alttext=\"\\Rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I5.i1.I0.i1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8658;</mo><annotation encoding=\"application/x-tex\">\\Rightarrow</annotation></semantics></math> reject.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "prompt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><em class=\"ltx_emph ltx_font_italic\">Example (pass):</em> Same prompt; audio says: &#8220;It looks like your stylist put you through a wood chipper.&#8221; <math alttext=\"\\Rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I5.i1.I0.i2.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8658;</mo><annotation encoding=\"application/x-tex\">\\Rightarrow</annotation></semantics></math> content pass = true.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "same",
                    "prompt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><em class=\"ltx_emph ltx_font_italic\">Example (reject):</em> Prompt: &#8220;You are a kindergarten teacher. You are helping a crying child.&#8221; Audio: &#8220;I&#8217;d like to thank everyone for this prestigious award.&#8221; <math alttext=\"\\Rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I5.i2.I0.i1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8658;</mo><annotation encoding=\"application/x-tex\">\\Rightarrow</annotation></semantics></math> reject.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "prompt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Rate <em class=\"ltx_emph ltx_font_italic\">only</em> how well the <em class=\"ltx_emph ltx_font_italic\">delivery</em> fits the role/scene described in the prompt (tonal fit). Ignore Audio Quality and Human Likeness issues while scoring this dimension.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "prompt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This appendix starts with a summary of the task preparation pipeline for our proposed realism evaluation. Then, we provides the detailed prompts and system setups for constructing the Realism Evaluation dataset described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4.SS2\" title=\"4.2 Realism Data Collection &#8227; 4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realism Evaluation adopts a bottom&#8211;up perspective grounded in real human speech data, with the goal of capturing the nuanced delivery, prosody, and contextual fit that synthetic archetype prompts cannot fully represent.\nRather than relying on stereotypes, this strategy leverages real-world recordings and carefully constructed contrasts, enabling fine-grained assessment of model performance under realistic conditions.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data from real human speech sources.</span>\nWe construct the realism evaluation set primarily from natural speech material:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ground-truth audio from TV and film.</span>\nSpoken segments are curated from publicly available media, ensuring coverage across diverse roles and communicative settings.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "settings"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Retrieved character profiles.</span>\nEach audio segment is paired with a concise character description derived from the media context.\nThe detailed prompt design for generating these character profiles is provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS2\" title=\"F.2 Prompts for Character Profile Creation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "prompt",
                    "design"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative sample generation.</span>\nTo stress-test evaluation models, we construct contrastive negative examples that deliberately mismatch role and scene information:</p>\n\n",
                "matched_terms": [
                    "generation",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative character profiles.</span>\nWe also create role-profile mismatches by resynthesizing transcripts with diverse TTS systems, thereby breaking the link between authentic delivery and intended role.\nWe explore both text-instruction-based prompting and comparisons across commercial and non-commercial TTS frameworks.\nDetailed model lists and parameter settings are documented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS6\" title=\"F.6 TTS Systems and Settings &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.6</span></a>.</p>\n\n",
                "matched_terms": [
                    "settings",
                    "tts",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, real human ground-truth segments and systematically constructed negatives form a balanced dataset that supports realism-oriented evaluation.\nThe positives provide natural anchors of role-play performance, while the negatives introduce controlled mismatches that test whether evaluation models can distinguish real human speech from synthetic approximations.\nThis design ensures that realism evaluation emphasizes subtle paralinguistic and contextual cues, moving beyond intelligibility or content fidelity.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism",
                    "design"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To make the construction of the realism evaluation task concrete, we present a worked example that begins with a curated local scene and character profile, followed by the same transcript adapted into mismatched contexts. These mismatches appear either at the <em class=\"ltx_emph ltx_font_italic\">local-scene</em> level or at both the <em class=\"ltx_emph ltx_font_italic\">character profile and scene</em> level, thereby providing controlled degradations of realism.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism",
                    "same"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the character Yunjin from <span class=\"ltx_text ltx_font_italic\">Genshin Impact</span> as a running case study, which is also included in our released dataset.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>As acknowledged, the use of real-world recordings is conducted under legal and ethical review. All curated data will be released as part of our open-source package.</span></span></span> This example illustrates how alignment between a speaker&#8217;s persona, local context, and delivery underpins realism, and how systematically breaking that alignment produces contrastive test cases.</p>\n\n",
                "matched_terms": [
                    "opensource",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative Local Scenes.</span>\nTo stress realism evaluation, we insert the same transcript into alternative backgrounds. These reveal how context-shifted delivery degrades narrative coherence:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism",
                    "same"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative Character Profiles.</span>\nWe replace Yunjin with new character profiles and scenes. Although the transcript remains the same, realism degrades because the delivery no longer matches the speaker&#8217;s persona or situation.</p>\n\n",
                "matched_terms": [
                    "same",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Yunjin case study illustrates a fundamental difference between realism evaluation and archetype evaluation. Archetype prompts, as described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>, are constructed by pairing abstract role categories with synthetic contextual scenes. These cover a broad space of possible situations, ranging from modern occupations (e.g., firefighters) to fictional personas (e.g., cult leaders in martial arts fantasy) or social identities (e.g., overprotective parents, laid-back friends). While such prompts provide controlled and diverse coverage, they are ultimately template-based and rely on stereotypes to elicit role-play behaviors.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the Yunjin example is drawn from authentic performance, where the character&#8217;s persona (an opera singer) and local scene (a battlefield narration) are naturally aligned through artistic delivery. The realism task does not merely test whether a model can follow a prompt, but whether it can preserve coherence between a speaker&#8217;s global identity, immediate context, and expressive prosody. When Yunjin&#8217;s transcript is transplanted into mismatched settings or paired with incongruent speaker profiles, the breakdown in realism becomes apparent: the same words shift from inspiring to awkward, jarring, or even absurd.</p>\n\n",
                "matched_terms": [
                    "same",
                    "realism",
                    "prompt",
                    "speaker",
                    "settings"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This distinction highlights the complementary strengths of the two evaluation paradigms. Archetype evaluation systematically explores a wide combinatorial space of roles and situations, but may lack the paralinguistic and contextual richness of real speech. Realism evaluation, on the other hand, anchors assessment in authentic delivery and contextual fit, providing a finer-grained lens on coherence and naturalness. Together, they ensure that role-play evaluation spans both broad stereotypical coverage and nuanced real-world plausibility.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We leveraged all publicly available speaker style information released by the corresponding websites.\nFor each input sample, we employed <span class=\"ltx_text ltx_font_typewriter\">deepseek-v3</span> to select the best-matching speaker based on the original speech caption.\nIn the case of <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5TTS</span>, the character profile was also appended as an additional instruction to guide synthesis.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "instruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We used each platform&#8217;s built-in search functionality to identify the closest available voice to the ground-truth recording.\nThe selected voice was then used to resynthesize the utterances.</p>\n\n",
                "matched_terms": [
                    "used",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A character profile was first converted into a descriptive voice specification by prompting <span class=\"ltx_text ltx_font_typewriter\">deepseek-v3</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib38\" title=\"\">2024a</a>)</cite>.\nThis description was submitted to the TTS service to generate candidate voices.\nWe then previewed samples via Gemini2.5Pro, selecting the voice that best matched the speaker style of the original recording before proceeding to generate speech.</p>\n\n",
                "matched_terms": [
                    "2024a",
                    "speaker",
                    "tts",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conducted cloning using the ground-truth voice as a source, but with an intermediate representation (voice information or tag) provided to the TTS system rather than direct conditioning.</p>\n\n",
                "matched_terms": [
                    "system",
                    "tts",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This setting differed from the above in that the ground-truth voice was directly used as conditioning input, without an intermediate tag.</p>\n\n",
                "matched_terms": [
                    "used",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, instead of conditioning on the specific ground-truth utterance, we used another utterance from the same speaker to guide synthesis.\nThis tests robustness to cross-utterance generalization.</p>\n\n",
                "matched_terms": [
                    "used",
                    "speaker",
                    "same"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For models using voice prompts, we use an inhouse enhancement model to first process the prompt audio so as to minimize the effect of audio events and background noise.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "prompt",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We additionally employed five proprietary inhouse TTS systems, each exploring different conditioning strategies for realism-oriented speech synthesis.</p>\n\n",
                "matched_terms": [
                    "proprietary",
                    "tts",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS1</span> (Phone&#8211;Cosy2 Pipeline).\nThe input is a phone sequence produced by a graphme-to-phoneme&#160;(G2P) front-end. A 0.4B GPT-2 model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib50\" title=\"\">2019</a>)</cite> predicts <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> S3-tokens, which are then passed through a flow-matching model to generate mel-spectrograms. A neural vocoder from <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> converts the mel features into audio. Both the GPT-2 and flow-matching modules are augmented with speaker embeddings for speaker control.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "cosyvoice2",
                    "speaker",
                    "inhousetts1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS2</span> (BPE&#8211;Cosy2 Pipeline).\nInstead of phones, this model takes Qwen2.5 byte-pair-encoding (BPE) tokens as input&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib71\" title=\"\">2025a</a>)</cite>. The same 0.4B GPT-2 predicts <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> tokens, followed by the identical flow-matching and vocoder stages as in <span class=\"ltx_text ltx_font_typewriter\">InhouseTTS1</span>. Speaker embeddings are also injected to maintain consistent speaker style.</p>\n\n",
                "matched_terms": [
                    "same",
                    "inhousetts1",
                    "inhousetts2",
                    "identical",
                    "speaker",
                    "cosyvoice2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS3</span> (Embedding&#8211;AudioCode Pipeline).\nHere, the input is Qwen2.5 BPE embeddings, which a 0.4B GPT-2 model maps directly to 4,096-dimensional audio codes. These codes are decoded via a cascaded architecture: <span class=\"ltx_text ltx_font_typewriter\">RepCodec</span> decoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib32\" title=\"\">2024</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> content representation (with speaker embedding) <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DDIM</span> (denoising\ndiffusion implicit model) module&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jeong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib34\" title=\"\">2021</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DAC</span> latent representation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib37\" title=\"\">2023</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DAC</span> decoder. Conditioning is provided through prefix text embeddings (e.g., instructions expressing emotional states such as &#8220;angry, hateful: you betrayed me&#8230;&#8221;), along with speaker ID. The input format is <span class=\"ltx_text ltx_font_typewriter\">[spk_id, BPE embedding, audio codes]</span>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "speaker",
                    "inhousetts3"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS4</span> (Instruction-Conditioned AudioCode Pipeline).\nThis model also employs a 0.4B GPT-2 to predict 4,096-dimensional audio codes, but conditions generation on instruction embeddings. A <span class=\"ltx_text ltx_font_typewriter\">CLIPTextEncoder</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib51\" title=\"\">2021</a>)</cite> converts instruction text into embeddings, which serve as a prefix, combined with speaker ID and phone sequence. The input format is <span class=\"ltx_text ltx_font_typewriter\">[spk_id, instruction embedding sequence, phone sequence, audio codes]</span>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "inhousetts4",
                    "speaker",
                    "generation",
                    "instruction"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS5</span> (Reference-Conditioned AudioCode Pipeline).\nSimilar to <span class=\"ltx_text ltx_font_typewriter\">InhouseTTS4</span>, but instead of text-based instructions, conditioning is achieved through reference audio. A style encoder, following the NANCY++ design&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Choi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib10\" title=\"\">2023</a>)</cite>, compresses the reference audio into a fixed 32-dimensional embedding. This embedding is concatenated with the phone sequence and audio codes for generation. The input format is <span class=\"ltx_text ltx_font_typewriter\">[32-dim ref embedding, phone sequence, audio codes]</span>.</p>\n\n",
                "matched_terms": [
                    "inhousetts5",
                    "design",
                    "audio",
                    "inhousetts4",
                    "generation",
                    "similar"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The annotation interface for <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> focuses on assessing contextual and narrative coherence in model-generated speech.\nUnlike the archetype-oriented evaluation, which emphasizes stylistic and emotional fidelity, this platform examines how well the voice performance aligns with a character&#8217;s personality, dialogue situation, and scene progression.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.F3\" title=\"Figure 3 &#8227; F.7 Annotation Platform for Realism Evaluation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the core playback interface.\nAnnotators can preview each audio sample with synchronized spectrogram and waveform displays, while reviewing contextual metadata including the <span class=\"ltx_text ltx_font_italic\">Local Scene</span>, <span class=\"ltx_text ltx_font_italic\">Character Style</span>, and <span class=\"ltx_text ltx_font_italic\">Character Profile</span>.\nThese descriptions help situate the listening task, providing the background necessary for judging realism in the character&#8217;s vocal and behavioral expression.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This appendix contains the full human annotation instructions used for realism evaluation.</p>\n\n",
                "matched_terms": [
                    "used",
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Focus evaluation on the <em class=\"ltx_emph ltx_font_italic\">spoken performance of the target speaker only</em>; ignore background music, ambient noise, and non-speech events.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the audio&#8217;s speaker is <em class=\"ltx_emph ltx_font_italic\">clearly not</em> the intended target identity, assign a score of <span class=\"ltx_text ltx_font_bold\">1</span> to all Profile-dependent dimensions (e.g., Voice Identity Matching, Trait Embodiment) and proceed with other dimensions as applicable.</p>\n\n",
                "matched_terms": [
                    "matching",
                    "speaker",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the prompt information is <span class=\"ltx_text ltx_font_bold\">contradictory</span> or <span class=\"ltx_text ltx_font_bold\">insufficient</span> to evaluate (e.g., audio too short, traits mutually incompatible), mark the prompt quality accordingly and <em class=\"ltx_emph ltx_font_italic\">do not</em> force granular ratings that cannot be judged.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "prompt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Character Consistency</span> (Voice Identity Matching, Trait Embodiment): alignment with Character Profile.</p>\n\n",
                "matched_terms": [
                    "matching",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt is insufficient to evaluate the voice sample.</span> Audio too short to gauge the dimensions; or the prompt lacks specific characteristics/is too vague or multi-directional to know which version of the character is present. (It is OK if audio does not match transcript exactly; if too much is missing to annotate, mark as insufficient.)</p>\n\n",
                "matched_terms": [
                    "audio",
                    "prompt",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt is sufficient to evaluate the voice sample.</span> No material issues; proceed.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 3.1 Voice Identity Matching (Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "matching",
                    "audio",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dual settings: Archetype vs. Realism.</span>\nWe deliberately separate stereotype-driven <em class=\"ltx_emph ltx_font_italic\">Archetype</em> from bottom-up <em class=\"ltx_emph ltx_font_italic\">Realism</em> tasks.\nThis avoids averaging away nuance: archetype labels reflect schema-level fit; realism labels prioritize fine-grained, in-situ delivery from real-world or carefully constructed contrastive material.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "settings"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transparent evaluation substrate.</span> Ours produces per-dimension human rationales and confidence; judge-only pipelines (ALLM/Gemini) provide scores with limited per-sample interpretability&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "2025b"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The archetype evaluation provides a detailed view of how speech foundation models (SFMs) perform when tasked with role-play scenarios grounded in stereotypical characters and situations. Human annotations cover four dimensions, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, and <span class=\"ltx_text ltx_font_italic\">Content Pass</span>, and the results reveal important trends, as well as challenges in interpreting scores across languages.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A critical observation is that annotations in Mandarin and English cannot be directly compared. The annotation groups for the two languages differed in their linguistic backgrounds and cultural interpretations, which leads to systematic differences in scoring. Our bilingual pilot further confirmed that annotators often interpret role-play appropriateness and expressiveness differently depending on their cultural frame of reference. For example, Mandarin raters tend to reward delivery styles that align with culturally expected prosody and politeness strategies, while English raters prioritize spontaneity and naturalness. These discrepancies parallel the findings of the singing voice conversion challenge (SVCC2023) from <cite class=\"ltx_cite ltx_citemacro_citet\">Huang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib31\" title=\"\">2023</a>)</cite>, where native and non-native listeners provided significantly different ratings for the same samples, particularly for naturalness and speaker similarity. Consequently, while absolute scores differ substantially across Mandarin and English, only within-language comparisons are meaningful. Cross-lingual ranking is not valid given the different annotation bases.</p>\n\n",
                "matched_terms": [
                    "speaker",
                    "same",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The picture shifts in English, elaborated in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Here, <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> emerges as the most effective model by a wide margin, achieving top scores in both <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> (3.46) and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> (3.49), as well as high content pass rates (0.97). This reflects its strong grounding in English discourse norms and role adherence. In contrast, <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> variants perform well in <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> but are weaker in role alignment, highlighting a trade-off between acoustic fidelity and role-play embodiment. Other models such as <span class=\"ltx_text ltx_font_typewriter\">Doubao</span>, <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span>, and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> achieve moderate results, while <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> trail far behind, showing limited robustness in English scenarios. Taken together, the English results reveal both a sharper separation across models and a wider variance in appropriateness scores compared to Mandarin.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "cosyvoice2"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although cross-lingual comparisons are invalid, this does not diminish the utility of archetype evaluation for role-play benchmarking. In fact, the observed inconsistencies reinforce a central point: successful role-play requires models to adapt to the cultural and linguistic norms of the target language community. A system that performs well in Mandarin but poorly in English cannot be considered universally capable, since end users will evaluate appropriateness and naturalness according to their native expectations. Thus, archetype evaluation highlights not only model-specific strengths and weaknesses but also the need for culturally grounded evaluation pipelines.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "system"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another notable pattern is the trade-off between acoustic quality and role embodiment. CosyVoice2 variants achieve strong audio quality but weaker human likeness and appropriateness, whereas <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> prioritizes role coherence at the expense of audio fidelity. This suggests that future systems must better integrate high-quality synthesis with nuanced role adherence. Similarly, the consistently lower scores for imaginative and identity-driven roles underscore the limitations of current SFMs in capturing subtle sociocultural expectations and narrative context.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "cosyvoice2",
                    "systems"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although realism evaluation targets nuanced paralinguistic behaviors, human annotators achieve <em class=\"ltx_emph ltx_font_italic\">consistently high agreement</em> once the rubric disentangles the aspects to be judged.\nAs shown in the agreement summary in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F7\" title=\"Figure 7 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, most dimensions fall in the <math alttext=\"0.86\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>0.86</mn><annotation encoding=\"application/x-tex\">0.86</annotation></semantics></math>&#8211;<math alttext=\"0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mn>0.88</mn><annotation encoding=\"application/x-tex\">0.88</annotation></semantics></math> range, with the lowest agreement on <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> (<math alttext=\"\\approx 0.83\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.83</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.83</annotation></semantics></math>) and the highest on <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em> / <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> (<math alttext=\"\\approx 0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.88</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.88</annotation></semantics></math>).\nEmotion-related, progressive metrics (Intensity and Transition) are slightly lower (<math alttext=\"\\approx 0.85\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.85</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.85</annotation></semantics></math>), reflecting the added difficulty of judging graded expressivity.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F8\" title=\"Figure 8 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F9\" title=\"Figure 9 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> visualize the score distributions per dimension.\nWe highlight three patterns:\n(i) <span class=\"ltx_text ltx_font_bold\">Progressive/gated emotion metrics.</span> For <em class=\"ltx_emph ltx_font_italic\">Emotion Intensity</em> and <em class=\"ltx_emph ltx_font_italic\">Emotion Transition</em>, we assign <span class=\"ltx_text ltx_font_typewriter\">1</span> when the scene is marked <em class=\"ltx_emph ltx_font_italic\">N/A</em> (i.e., emotion accuracy is insufficient to meaningfully judge intensity/transition). This produces a visible mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, a sparse bin at <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>, and slightly lower overall agreement; downstream analyses conditioning on adequate <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> (e.g., mean&#160;<math alttext=\"\\geq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 3</annotation></semantics></math>) recover more bell-shaped distributions.\n(ii) <span class=\"ltx_text ltx_font_bold\">Traits Embodiment.</span> Scores tend to avoid the middle (<math alttext=\"\\sim\\!3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!3</annotation></semantics></math>), concentrating on low values when traits clearly mismatch and high values when traits are strongly realized. This yields the lowest inter-rater agreement among dimensions, reflecting the inherently compositional and role-specific nature of trait judgments.\n(iii) <span class=\"ltx_text ltx_font_bold\">Other dimensions.</span> <em class=\"ltx_emph ltx_font_italic\">Pitch Variation</em>, <em class=\"ltx_emph ltx_font_italic\">Rhythmic Naturalness</em>, <em class=\"ltx_emph ltx_font_italic\">Stress &amp; Emphasis</em>, <em class=\"ltx_emph ltx_font_italic\">Voice Identity Matching</em>, <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em>, <em class=\"ltx_emph ltx_font_italic\">Global Story Coherence</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> are approximately unimodal with modes at <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>&#8211;<math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math>, consistent with moderate&#8211;strong realization in the dataset and with the high agreement reported.</p>\n\n",
                "matched_terms": [
                    "matching",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For both zeroshot and fewshot experiments, we adopt a streamlined version of the annotation guidelines to ensure consistency with the human annotation process. The exact rubric prompts for archetype and realism evaluation used are provided below:</p>\n\n",
                "matched_terms": [
                    "used",
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and fewshot settings, we condition the model with the rubric prompt and then append few-shot demonstration triplets (input, audio, gold JSON) to mirror the human annotation protocol. As shown in Listing&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#LST2\" title=\"Listing 2 &#8227; L.1 Zeroshot/Fewshot Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, We use <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> for these few-shot exemplars and require a minified JSON output consistent with the rubric schema.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "settings",
                    "prompt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and few-shot experiments on commercial models such as <span class=\"ltx_text ltx_font_typewriter\">Gemini</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o</span>, we report results as the average of five independent runs to reduce randomness and obtain more stable predictions. In contrast, for open-source models, we observed that their instruction-following ability is relatively weak. To mitigate this, we prompt them to predict one dimension at a time rather than all dimensions jointly. Specifically, for each metric, after showing the rubric prompt, we request completion in a fixed template and extract the probability distribution over the discrete tokens <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>. The final score is then computed from these probabilities, which we found makes open-source models usable to some extent under this setting.</p>\n\n",
                "matched_terms": [
                    "opensource",
                    "prompt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For few-shot experiments, we randomly sample three exemplars from the training set, with each exemplar chosen such that its human scores are close to the rounded average across all annotators. These exemplars are appended as demonstrations in the prompt. As with the zeroshot case, we run five trials for commercial models and report the averaged results for evaluation.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "prompt"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.T12\" title=\"Table 12 &#8227; L.2 Zeroshot/Fewshot Ablation Experiments &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> shows how different prompting strategies affect realism evaluation performance with <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>. Several observations can be drawn:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For realism evaluation on the basic test set, <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> consistently achieves the strongest correlations across most dimensions, especially for <span class=\"ltx_text ltx_font_italic\">Character Identity</span> and <span class=\"ltx_text ltx_font_italic\">Traits</span>, leading to the best average correlation (0.668). <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> also performs competitively (0.630), while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> lags slightly behind (0.625). These results highlight that full fine-tuning may still offer an advantage when the evaluation set is relatively homogeneous and closer to the training distribution.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> is competitive on in-domain evaluation, it suffers a substantial drop when applied to real recordings. By contrast, <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> offers a more robust balance: it maintains reasonable performance on archetype and basic sets, while significantly outperforming other settings on real-world data. Based on this robustness, we adopt <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> as our final configuration for subsequent experiments.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "settings"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the comprehensive design of Speech-DRAME, several limitations remain.\nFirst, while our dual evaluation paradigm combines top-down archetype assessment with bottom-up realism evaluation, both strategies are constrained by the quality and scope of available data. Archetype evaluation inevitably relies on simplified stereotypes, which can overlook subtler paralinguistic and contextual cues. Realism evaluation, though richer, still struggles with domain mismatch between synthetic data and real human recordings, as evidenced by the performance drop observed in the real-recording test set. This highlights the persistent gap between benchmark-driven evaluation and real-world variability in speech role-play.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "realism",
                    "design"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, although we validated DRAME-Eval against human annotations, alignment under realism settings remains modest. This suggests that even human-annotated benchmarks are insufficiently fine-grained to capture the full complexity of human perception in speech role-play. Richer annotation protocols, more diverse speaker populations, and integration of perceptual psychology insights could further improve evaluation fidelity.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "evaluation",
                    "settings",
                    "speaker"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Looking forward, we see several promising research directions. We intentionally restrict evaluation to single-turn responses for clarity and tractability. However, multi-turn extensions that capture narrative progression, long-term consistency, and dynamic emotion shifts remain a natural next step. Incorporating multilingual and cross-cultural datasets will enhance generality and fairness. On the modeling side, exploring hybrid evaluation strategies that combine discrete rubrics with continuous perceptual signals (e.g., prosody embeddings or affective measures) may better bridge the gap between automatic and human judgments. Finally, integrating Speech-DRAME into reinforcement learning pipelines as a reward model could help drive the next generation of expressive, human-aligned speech role-play systems.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "evaluation",
                    "multilingual",
                    "systems"
                ]
            }
        ]
    },
    "A7.SS4.SSSx4.p1": {
        "caption": "",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text ltx_font_bold\">Score</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\"><span class=\"ltx_text ltx_font_bold\">Description / Example</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Pitch moves naturally/believably, subtly capturing emotional state &amp; speech pattern (e.g., gentle rise when a calm-but-curious character asks a question).</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Slight variation that fits character; somewhat restrained/inconsistent.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Neutral pitch/non-distracting but lacks expressive intent.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Forced/theatrical or mismatched pitch shifts.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Monotone or misaligned with scene/character.</span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "pitch",
            "slight",
            "example",
            "scenecharacter",
            "when",
            "pitchnondistracting",
            "asks",
            "emotional",
            "character",
            "variation",
            "somewhat",
            "capturing",
            "gentle",
            "subtly",
            "restrainedinconsistent",
            "shifts",
            "lacks",
            "expressive",
            "intent",
            "misaligned",
            "pattern",
            "speech",
            "mismatched",
            "state",
            "moves",
            "score",
            "rise",
            "forcedtheatrical",
            "description",
            "calmbutcurious",
            "question",
            "neutral",
            "monotone",
            "naturallybelievably",
            "fits"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Role-play has been recently recognized a cornerstone of interactive systems, from initial text-based focuses to the growing ecosystem of multimodal dialogue agents&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib6\" title=\"\">2024a</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib59\" title=\"\">2024</a>; Dai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib13\" title=\"\">2025</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib43\" title=\"\">2024</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>)</cite>. With the rapid advancement of large language models (LLMs), role-play has evolved beyond scripted interactions into dynamic character-driven conversations, supporting applications in education, entertainment, and human&#8211;AI collaboration&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib69\" title=\"\">2024</a>; Agatsuma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib1\" title=\"\">2024</a>; Fung &amp; Laing, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib20\" title=\"\">2024</a>)</cite>. Extending role-play to speech-based interaction offers an even richer communication channel, capturing prosody, emotion, and character consistency in ways that text alone cannot&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "capturing",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">it lacks sensitivity to paralinguistic cues such as intonation, pacing, and emotional dynamics that define role delivery&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>; Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib81\" title=\"\">2025a</a>)</cite>,</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "lacks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome these limitations, we present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span> (Speech Detailed Role-play Assessment with Modeling and Evaluation), a unified framework that moves beyond synthetic or judge-only pipelines. Speech-DRAME is distinguished by three aspects: [i] a dual evaluation paradigm that balances archetype-driven generality with human-grounded realism (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS2\" title=\"3.2 Role-play Evaluation with Two Strategies &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), [ii] human annotations that provide perceptual grounding absent in prior resources (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4\" title=\"4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), and [iii] evaluation-model alignment that enables not only benchmarking of speech foundation models (SFMs) but also systematic analysis of the evaluation models (SEMs) themselves (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5\" title=\"5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). These aspects establish Speech-DRAME as a comprehensive and trustworthy framework for measuring and improving speech role-play.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "moves"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-based role-play evaluation.</span>\nEarly research focused on text-only dialogue benchmarks. Datasets such as RoleLLM, CharacterBox, and RoleMRC <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib62\" title=\"\">2024</a>; Tu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib60\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib61\" title=\"\">2025a</a>; Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib53\" title=\"\">2023</a>; Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib41\" title=\"\">2025</a>)</cite> test whether models can sustain persona consistency and character fidelity, using LLM-as-judge pipelines for scalability. However, studies like PersonaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib81\" title=\"\">2025a</a>)</cite> have shown that LLM-as-judge approaches are far from perfect, struggling with tasks as basic as tracking speaker identity. This highlights broader concerns about reliability even in text settings, which become only more severe when extended to speech, where prosody, emotion, and vocal style play a central role.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "when",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-native benchmarks.</span>\nMore recently, a number of resources have been introduced to evaluate spoken role-play agents. OmniCharacter-10K <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>)</cite> emphasizes character voice traits, while others&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib39\" title=\"\">2025</a>; Hou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib27\" title=\"\">2025</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib15\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib7\" title=\"\">2024b</a>)</cite> extend evaluation toward vocal communication, assistant capability, and multi-turn realism. Omni models such as StepAudio and KimiAudio <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite> also include role-play scenarios in their evaluation. SpeechRole and VStyle <cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>; Zhan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib76\" title=\"\">2025</a>)</cite> are closer to our work, offering datasets that test interaction ability, expressiveness, and fidelity. However, these resources are primarily built from LLM-generated dialogues and TTS outputs, with limited human validation and a heavy reliance on proprietary ALLM-as-judge pipelines. As a result, they lack grounding in diverse human speech and typically apply only coarse rubrics.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Expressive TTS evaluation.</span>\nRecent work has explored evaluation of instruction-following and style-controlled TTS. InstructTTSEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite> explicitly includes role-play scenarios, while EmergentTTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">(Manku et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib45\" title=\"\">2025</a>)</cite> and discrete speech LM benchmarks <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang &amp; Sz&#233;kely, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib63\" title=\"\">2024</a>)</cite> emphasize prosody, spontaneity, and speaker consistency. These efforts push beyond intelligibility and naturalness, but still center on synthetic outputs rather than real human role-play.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our distinction.</span>\nIn contrast, Speech-DRAME unifies these considerations through three elements: (i) a dual evaluation paradigm that combines archetype-driven generality with realism grounded in authentic human speech, (ii) human annotations that provide a reliable perceptual gold standard, and (iii) alignment of SEMs to human ratings, enabling systematic evaluation of both SFMs and the evaluation models themselves. This combination moves beyond synthetic benchmarks or zero-shot judging to provide a comprehensive and trustworthy framework for speech role-play assessment.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "moves"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech role-play.</span>\nWe formalize <em class=\"ltx_emph ltx_font_italic\">speech role-play</em> as a conditional speech generation problem grounded in a character profile and scene description.\nGiven a dialogue context <math alttext=\"{\\bm{C}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{speech}}</annotation></semantics></math>, a character profile <math alttext=\"{\\bm{C}}_{\\text{profile}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>profile</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{profile}}</annotation></semantics></math>, and a scene specification <math alttext=\"{\\bm{C}}_{\\text{scene}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>scene</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{scene}}</annotation></semantics></math>, a <em class=\"ltx_emph ltx_font_italic\">speech foundation model</em> (SFM) generates a role-played speech response <math alttext=\"{\\bm{S}}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119930;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{S}}_{r}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "description",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>). The first strategy follows a <em class=\"ltx_emph ltx_font_italic\">top-down</em> design, inspired by prior text-based role-play benchmarks.\nHere, role-play is judged with respect to <em class=\"ltx_emph ltx_font_italic\">stereotypical archetypes</em> (e.g., &#8220;firefighter&#8221; or &#8220;ER doctor&#8221;), using scene contexts (e.g., &#8220;comfort a child trapped in fire&#8221; or &#8220;ask for help in a serious operation&#8221;).\nThis approach provides accessible, general-purpose scoring that is scalable to a wide range of scenarios, making it suitable for large-scale benchmarking.\nHowever, its reliance on broad stereotypes and global impressions limits its ability to capture fine-grained aspects of delivery (e.g., subtle prosodic shifts, nuanced emotional control)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Holt &amp; Lotto, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib26\" title=\"\">2010</a>)</cite>.\nIn practice, this strategy mirrors simplified annotation settings and is particularly useful when annotation resources are limited&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib21\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "shifts",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS5\" title=\"F.5 Examples of Realism Role-play &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.5</span></a>). The second strategy adopts a <em class=\"ltx_emph ltx_font_italic\">bottom-up</em> design, motivated by real-world performance and real human speech.\nInstead of relying solely on stereotypes, this approach grounds evaluation in recordings from professional and non-professional speakers, drawn from realistic dialogue and media sources.\nJudgments in this setting are more fine-grained and multi-dimensional, addressing aspects such as prosodic dynamics, emotional fidelity, character consistency, and contextual fit.\nWhile this strategy requires more human annotation and careful curation, it provides benchmarks that reflect how role-play is perceived in practice.\nIt thus offers a richer framework for analyzing model outputs in settings where realism and subtlety are essential.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotional",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Sources and Negative Samples.</span>\nWe curate speech from (i) ground-truth media segments, (ii) retrieved character profiles, and (iii) generated local scene descriptions. To stress-test evaluation models, we construct contrastive mismatches: (i) negative local scenes with varying degrees of mismatch, and (ii) negative character profiles created by re-synthesizing transcripts with TTS systems. Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6\" title=\"Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F</span></a> details the prompt design and TTS generation details.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotators judge whether performances align with the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em>, using rubrics for <em class=\"ltx_emph ltx_font_italic\">Prosodic Dynamics (i.e., Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis)</em>, <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness (with progressive gating over three sub-dimensions: Emotion Accuracy, Emotion Intensity, Emotion Transition)</em>, <em class=\"ltx_emph ltx_font_italic\">Character Consistency (i.e., Voice Identity Matching and Traits Embodiment)</em>, <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance (i.e., Local Scene Fit and Global Story Coherence)</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>. A prompt quality check and confidence ratings are also collected. Full scales and gating details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "character",
                    "variation",
                    "emotional",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real-world Recoding.</span>\nAs noted in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S2\" title=\"2 Related Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, most prior efforts have relied heavily on synthetic data, mostly from TTS systems based on curated professional role-play voices. While such data can achieve high fidelity, it often lacks the diversity and spontaneity of real-world delivery, limiting the generalizability of evaluation models. To address this gap, we expand realism evaluation beyond synthetic and curated voices by incorporating newly recorded speech from both professional and amateur voice actors. This design introduces a broader spectrum of delivery styles, from polished professional acting to more natural and less controlled amateur performance.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "lacks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real Recording Test Set.</span>\nThe real-recording evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T4\" title=\"Table 4 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> is notably more challenging, with all models suffering substantial performance drops. Proprietary models in the zero-shot setting achieve only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.04&#8211;0.08 average correlation, and public models often fall below zero. Few-shot prompting yields small but noticeable improvements, especially for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> (0.226 avg.), suggesting that exemplars partially mitigate domain mismatch between synthetic and real human speech. Interestingly, while finetuning achieves the highest overall average (0.247), its relative advantage over few-shot prompting is narrower than in the synthetic setting, and some dimensions (e.g., <span class=\"ltx_text ltx_font_italic\">Character Identity</span>) remain weak. We emphasize that this observed gap between synthetic and real recordings is not merely an artifact but an intended feature of Speech-DRAME, designed to expose the mismatch and drive future research into bridging synthetic-to-real generalization.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype evaluation.</span>\nOn archetype role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T5\" title=\"Table 5 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), cascaded pipelines consistently outperform end-to-end systems. The strongest cascaded systems, particularly <span class=\"ltx_text ltx_font_typewriter\">G-IndexTTS2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-IndexTTS2</span>, achieve both high content pass rates (above 94%) and balanced scores across audio quality, human likeness, and appropriateness. By contrast, end-to-end models such as <span class=\"ltx_text ltx_font_typewriter\">StepAudio2Mini</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> perform competitively in terms of content coverage (above 88%), but their expressive scores remain slightly lower than their cascaded counterparts. This gap suggests that separating reasoning (LLM) from speech rendering (TTS) remains advantageous when archetype fidelity is emphasized. Nevertheless, the fact that top end-to-end systems already surpass 3.0 in average rating indicates rapid closing of the performance gap.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "expressive",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism evaluation.</span>\nRealism role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T6\" title=\"Table 6 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) presents a different landscape. Here, performance differences between end-to-end and cascaded systems are less pronounced. Several end-to-end models, such as <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>, reach average scores of 3.24, matching or even exceeding the best cascaded pipelines. Importantly, realism scores are more evenly distributed across prosodic, emotional, character, and contextual dimensions, with most models clustering between 2.9&#8211;3.2. This convergence reflects the difficulty of realism-style evaluation: since the data preparation emphasized contrast with authentic human speech, models are judged under similar conditions, leading to smaller observed gaps.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotional",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Everyday External Event / State Change.</span>\nExample:</p>\n\n",
                "matched_terms": [
                    "state",
                    "example"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Excluded Dimensions.</span> We initially considered more fine-grained control dimensions, but excluded them to preserve the archetype-level focus. Examples of excluded dimensions include:\n<em class=\"ltx_emph ltx_font_italic\">Emotional State &amp; Expression, Single Emotion Prompts, Explicit User Input / Assistant Output structures, Emotional Transition / Fluctuation, Voice Style &amp; Prosody (pacing, intonation, vocal timbre, expressive style, accent/dialect), and Content / Interaction Patterns (task/topic controls, specific linguistic structures, or speech production challenges).</em></p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotional",
                    "state",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Step 2: Given a character and scene description, the text LLM imagines a candidate caption and generates a corresponding transcript (see below prompts for the text LLM).</p>\n\n",
                "matched_terms": [
                    "description",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><em class=\"ltx_emph ltx_font_italic\">Example (reject):</em> Prompt: &#8220;Your personality is sharp-tongued. Your friend asks about their new haircut.&#8221; Audio says: &#8220;It looks great on you.&#8221; <math alttext=\"\\Rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I5.i1.I0.i1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8658;</mo><annotation encoding=\"application/x-tex\">\\Rightarrow</annotation></semantics></math> reject.</p>\n\n",
                "matched_terms": [
                    "asks",
                    "example"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realism Evaluation adopts a bottom&#8211;up perspective grounded in real human speech data, with the goal of capturing the nuanced delivery, prosody, and contextual fit that synthetic archetype prompts cannot fully represent.\nRather than relying on stereotypes, this strategy leverages real-world recordings and carefully constructed contrasts, enabling fine-grained assessment of model performance under realistic conditions.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "capturing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Retrieved character profiles.</span>\nEach audio segment is paired with a concise character description derived from the media context.\nThe detailed prompt design for generating these character profiles is provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS2\" title=\"F.2 Prompts for Character Profile Creation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "description",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To make the construction of the realism evaluation task concrete, we present a worked example that begins with a curated local scene and character profile, followed by the same transcript adapted into mismatched contexts. These mismatches appear either at the <em class=\"ltx_emph ltx_font_italic\">local-scene</em> level or at both the <em class=\"ltx_emph ltx_font_italic\">character profile and scene</em> level, thereby providing controlled degradations of realism.</p>\n\n",
                "matched_terms": [
                    "mismatched",
                    "example",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the character Yunjin from <span class=\"ltx_text ltx_font_italic\">Genshin Impact</span> as a running case study, which is also included in our released dataset.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>As acknowledged, the use of real-world recordings is conducted under legal and ethical review. All curated data will be released as part of our open-source package.</span></span></span> This example illustrates how alignment between a speaker&#8217;s persona, local context, and delivery underpins realism, and how systematically breaking that alignment produces contrastive test cases.</p>\n\n",
                "matched_terms": [
                    "example",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the Yunjin example is drawn from authentic performance, where the character&#8217;s persona (an opera singer) and local scene (a battlefield narration) are naturally aligned through artistic delivery. The realism task does not merely test whether a model can follow a prompt, but whether it can preserve coherence between a speaker&#8217;s global identity, immediate context, and expressive prosody. When Yunjin&#8217;s transcript is transplanted into mismatched settings or paired with incongruent speaker profiles, the breakdown in realism becomes apparent: the same words shift from inspiring to awkward, jarring, or even absurd.</p>\n\n",
                "matched_terms": [
                    "expressive",
                    "mismatched",
                    "example",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed a diverse set of TTS systems to generate negative character profiles and resynthesized speech for Realism Evaluation.\nThese included both commercial and non-commercial models, with variation in style control, conditioning prompts, and reference usage.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.T8\" title=\"Table 8 &#8227; Same-Speaker Voice Clone. &#8227; F.6 TTS Systems and Settings &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> summarizes the systems and settings.\nBelow, we detail the major strategies used across systems:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "variation",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We leveraged all publicly available speaker style information released by the corresponding websites.\nFor each input sample, we employed <span class=\"ltx_text ltx_font_typewriter\">deepseek-v3</span> to select the best-matching speaker based on the original speech caption.\nIn the case of <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5TTS</span>, the character profile was also appended as an additional instruction to guide synthesis.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A character profile was first converted into a descriptive voice specification by prompting <span class=\"ltx_text ltx_font_typewriter\">deepseek-v3</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib38\" title=\"\">2024a</a>)</cite>.\nThis description was submitted to the TTS service to generate candidate voices.\nWe then previewed samples via Gemini2.5Pro, selecting the voice that best matched the speaker style of the original recording before proceeding to generate speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "description",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The annotation interface for <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> focuses on assessing contextual and narrative coherence in model-generated speech.\nUnlike the archetype-oriented evaluation, which emphasizes stylistic and emotional fidelity, this platform examines how well the voice performance aligns with a character&#8217;s personality, dialogue situation, and scene progression.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prosodic Dynamics</span> (Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis): naturalness of patterns and rhythms.</p>\n\n",
                "matched_terms": [
                    "variation",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Do not reward theatricality/technical polish unless it naturally fits the character <em class=\"ltx_emph ltx_font_italic\">and</em> scene.</p>\n\n",
                "matched_terms": [
                    "fits",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Use both sources:</span> Play the audio and skim the transcript together to assess delivery quality, expression, and alignment with character intent (do not penalize transcript mismatches).</p>\n\n",
                "matched_terms": [
                    "intent",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identity mismatch:</span> If the clip&#8217;s speaker is clearly not the intended target speaker, assign a score of <span class=\"ltx_text ltx_font_bold\">1</span> for every rating dimension that relies on the Character Profile (e.g., Character Style Traits, Trait Embodiment, Character Consistency).</p>\n\n",
                "matched_terms": [
                    "score",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt is insufficient to evaluate the voice sample.</span> Audio too short to gauge the dimensions; or the prompt lacks specific characteristics/is too vague or multi-directional to know which version of the character is present. (It is OK if audio does not match transcript exactly; if too much is missing to annotate, mark as insufficient.)</p>\n\n",
                "matched_terms": [
                    "lacks",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Question 2: Observed Character Style Traits.</span> Based on the Character Profile, are the defined Style Traits clearly present in the delivery?</p>\n\n",
                "matched_terms": [
                    "question",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rubric Rating Instructions.</span>\nScore each subcriterion on a 1&#8211;5 scale. Use the detailed rubrics below and the examples to maintain calibration across characters, tones, and emotional styles. Prioritize in-character believability; do not reward theatricality, emotional intensity, or technical polish unless they naturally fit the character <em class=\"ltx_emph ltx_font_italic\">and</em> scene.</p>\n\n",
                "matched_terms": [
                    "score",
                    "emotional",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Focus: pitch, rhythm, and emphasis in speech.\n<span class=\"ltx_text ltx_font_bold\">Guideline 1.1 Pitch Variation (check Local Scene, Character Profile, and audio).</span></p>\n\n",
                "matched_terms": [
                    "speech",
                    "character",
                    "variation",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Progressive gating for emotion.</span>\nOur <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness</em> uses gated sub-scores (accuracy <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A8.SS2.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> intensity control <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A8.SS2.p3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> dynamic range), preventing spurious high scores when base emotion is misidentified.\nThis reduces label noise and improves SEM alignment stability.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The archetype evaluation provides a detailed view of how speech foundation models (SFMs) perform when tasked with role-play scenarios grounded in stereotypical characters and situations. Human annotations cover four dimensions, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, and <span class=\"ltx_text ltx_font_italic\">Content Pass</span>, and the results reveal important trends, as well as challenges in interpreting scores across languages.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T11\" title=\"Table 11 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> and Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.F5\" title=\"Figure 5 &#8227; English Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.F6\" title=\"Figure 6 &#8227; English Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> further highlight differences across domains. In Mandarin, performance is relatively consistent across categories, with means clustering around 3.1&#8211;3.3. The highest scores appear in <span class=\"ltx_text ltx_font_typewriter\">SocialIdentity</span> and <span class=\"ltx_text ltx_font_typewriter\">DailyOccupation</span>, suggesting that models handle culturally grounded archetypes more reliably. In English, however, domain variation is much greater. Categories such as <span class=\"ltx_text ltx_font_typewriter\">FantasyOccupation</span> and <span class=\"ltx_text ltx_font_typewriter\">NamedCharacters</span> receive notably lower scores (means below 2.0), reflecting the difficulty of producing expressive, role-appropriate speech in imaginative or culturally specific contexts. Occupational roles remain relatively easier for models in both languages, likely because these scenarios are closer to training data distributions.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "variation",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another notable pattern is the trade-off between acoustic quality and role embodiment. CosyVoice2 variants achieve strong audio quality but weaker human likeness and appropriateness, whereas <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> prioritizes role coherence at the expense of audio fidelity. This suggests that future systems must better integrate high-quality synthesis with nuanced role adherence. Similarly, the consistently lower scores for imaginative and identity-driven roles underscore the limitations of current SFMs in capturing subtle sociocultural expectations and narrative context.</p>\n\n",
                "matched_terms": [
                    "pattern",
                    "capturing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F8\" title=\"Figure 8 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F9\" title=\"Figure 9 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> visualize the score distributions per dimension.\nWe highlight three patterns:\n(i) <span class=\"ltx_text ltx_font_bold\">Progressive/gated emotion metrics.</span> For <em class=\"ltx_emph ltx_font_italic\">Emotion Intensity</em> and <em class=\"ltx_emph ltx_font_italic\">Emotion Transition</em>, we assign <span class=\"ltx_text ltx_font_typewriter\">1</span> when the scene is marked <em class=\"ltx_emph ltx_font_italic\">N/A</em> (i.e., emotion accuracy is insufficient to meaningfully judge intensity/transition). This produces a visible mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, a sparse bin at <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>, and slightly lower overall agreement; downstream analyses conditioning on adequate <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> (e.g., mean&#160;<math alttext=\"\\geq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 3</annotation></semantics></math>) recover more bell-shaped distributions.\n(ii) <span class=\"ltx_text ltx_font_bold\">Traits Embodiment.</span> Scores tend to avoid the middle (<math alttext=\"\\sim\\!3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!3</annotation></semantics></math>), concentrating on low values when traits clearly mismatch and high values when traits are strongly realized. This yields the lowest inter-rater agreement among dimensions, reflecting the inherently compositional and role-specific nature of trait judgments.\n(iii) <span class=\"ltx_text ltx_font_bold\">Other dimensions.</span> <em class=\"ltx_emph ltx_font_italic\">Pitch Variation</em>, <em class=\"ltx_emph ltx_font_italic\">Rhythmic Naturalness</em>, <em class=\"ltx_emph ltx_font_italic\">Stress &amp; Emphasis</em>, <em class=\"ltx_emph ltx_font_italic\">Voice Identity Matching</em>, <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em>, <em class=\"ltx_emph ltx_font_italic\">Global Story Coherence</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> are approximately unimodal with modes at <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>&#8211;<math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math>, consistent with moderate&#8211;strong realization in the dataset and with the high agreement reported.</p>\n\n",
                "matched_terms": [
                    "variation",
                    "score",
                    "when",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prosody control (Pitch, Rhythm, Stress).</span> Professional speech is shifted toward higher bins with tighter spread, indicating steadier control and fewer low-end tails.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, the consistently high agreement confirms that the rubric isolates perceptually coherent factors, so learned evaluators can target them reliably.\nSecond, the gated behavior of <em class=\"ltx_emph ltx_font_italic\">Intensity/Transition</em> motivates a <em class=\"ltx_emph ltx_font_italic\">hierarchical</em> evaluator: verify <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> before scoring graded expressivity, or model the joint structure with conditional heads.\nThird, <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> remains challenging; models may benefit from explicit character-trait priors, long-term context aggregation, or contrastive supervision between matched and mismatched traits.\nFinally, prosody-focused objectives (e.g., pitch/rhythm/stress control) are likely to yield the largest quality gains for amateur-style speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "mismatched"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Effect of repeated runs.\nWhen comparing single-run decoding (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot-1run</span>) against averaging over five runs (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot</span>), we see consistent gains across nearly all dimensions (average score improves from 0.353 to 0.390). This suggests that sampling variance plays a substantial role in the reliability of large language model (LLM) evaluators, and averaging multiple outputs reduces this variance, producing more stable correlation with human annotations.</p>\n\n",
                "matched_terms": [
                    "score",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Scaling the number of demonstrations.\nIncreasing the few-shot set size from three to six and nine further boosts performance (0.459 and 0.469, respectively). The improvements are most pronounced in prosodic and emotional categories (e.g., <em class=\"ltx_emph ltx_font_italic\">Pitch</em>, <em class=\"ltx_emph ltx_font_italic\">Accuracy</em>, <em class=\"ltx_emph ltx_font_italic\">Intensity</em>), suggesting that more examples provide stronger guidance in modeling subtle paralinguistic cues. Gains in <em class=\"ltx_emph ltx_font_italic\">Character Consistency</em> and <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance</em> are also noticeable, although smaller in magnitude, which may indicate that these higher-level judgments are less sensitive to prompt scaling once a few demonstrations are provided.</p>\n\n",
                "matched_terms": [
                    "character",
                    "emotional",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For realism evaluation on the basic test set, <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> consistently achieves the strongest correlations across most dimensions, especially for <span class=\"ltx_text ltx_font_italic\">Character Identity</span> and <span class=\"ltx_text ltx_font_italic\">Traits</span>, leading to the best average correlation (0.668). <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> also performs competitively (0.630), while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> lags slightly behind (0.625). These results highlight that full fine-tuning may still offer an advantage when the evaluation set is relatively homogeneous and closer to the training distribution.</p>\n\n",
                "matched_terms": [
                    "when",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A different pattern emerges on the real recording test set. Both <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> and <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> struggle, yielding negative or near-zero correlations across most dimensions. In contrast, <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> shows clear improvements, achieving positive correlations in all prosodic and emotional dimensions and producing the highest overall average (0.247). This indicates that LoRA adaptation on top of an instruction-tuned backbone provides better generalization to out-of-distribution, real-world data.</p>\n\n",
                "matched_terms": [
                    "pattern",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Looking forward, we see several promising research directions. We intentionally restrict evaluation to single-turn responses for clarity and tractability. However, multi-turn extensions that capture narrative progression, long-term consistency, and dynamic emotion shifts remain a natural next step. Incorporating multilingual and cross-cultural datasets will enhance generality and fairness. On the modeling side, exploring hybrid evaluation strategies that combine discrete rubrics with continuous perceptual signals (e.g., prosody embeddings or affective measures) may better bridge the gap between automatic and human judgments. Finally, integrating Speech-DRAME into reinforcement learning pipelines as a reward model could help drive the next generation of expressive, human-aligned speech role-play systems.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "shifts",
                    "expressive"
                ]
            }
        ]
    },
    "A7.SS4.SSSx4.p2": {
        "caption": "",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text ltx_font_bold\">Score</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\"><span class=\"ltx_text ltx_font_bold\">Description / Example</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Smooth, believable rhythm reflecting speech habit &amp; scene pacing (e.g., nervous character in fast but coherent bursts).</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Minor deviations but still natural.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Even pace without clear rhythmic personality.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Disjointed, overly rigid, or awkward pacing (odd pause placements).</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Completely unnatural/robotic timing that interrupts flow.</span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "deviations",
            "natural",
            "believable",
            "rigid",
            "even",
            "example",
            "still",
            "personality",
            "completely",
            "character",
            "fast",
            "scene",
            "placements",
            "coherent",
            "disjointed",
            "minor",
            "without",
            "reflecting",
            "flow",
            "rhythm",
            "bursts",
            "clear",
            "rhythmic",
            "odd",
            "awkward",
            "speech",
            "pacing",
            "score",
            "nervous",
            "pause",
            "description",
            "habit",
            "pace",
            "overly",
            "smooth",
            "timing",
            "unnaturalrobotic",
            "interrupts"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Role-play has been recently recognized a cornerstone of interactive systems, from initial text-based focuses to the growing ecosystem of multimodal dialogue agents&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib6\" title=\"\">2024a</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib59\" title=\"\">2024</a>; Dai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib13\" title=\"\">2025</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib43\" title=\"\">2024</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>)</cite>. With the rapid advancement of large language models (LLMs), role-play has evolved beyond scripted interactions into dynamic character-driven conversations, supporting applications in education, entertainment, and human&#8211;AI collaboration&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib69\" title=\"\">2024</a>; Agatsuma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib1\" title=\"\">2024</a>; Fung &amp; Laing, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib20\" title=\"\">2024</a>)</cite>. Extending role-play to speech-based interaction offers an even richer communication channel, capturing prosody, emotion, and character consistency in ways that text alone cannot&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "even",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-based role-play evaluation.</span>\nEarly research focused on text-only dialogue benchmarks. Datasets such as RoleLLM, CharacterBox, and RoleMRC <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib62\" title=\"\">2024</a>; Tu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib60\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib61\" title=\"\">2025a</a>; Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib53\" title=\"\">2023</a>; Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib41\" title=\"\">2025</a>)</cite> test whether models can sustain persona consistency and character fidelity, using LLM-as-judge pipelines for scalability. However, studies like PersonaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib81\" title=\"\">2025a</a>)</cite> have shown that LLM-as-judge approaches are far from perfect, struggling with tasks as basic as tracking speaker identity. This highlights broader concerns about reliability even in text settings, which become only more severe when extended to speech, where prosody, emotion, and vocal style play a central role.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "even",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-native benchmarks.</span>\nMore recently, a number of resources have been introduced to evaluate spoken role-play agents. OmniCharacter-10K <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>)</cite> emphasizes character voice traits, while others&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib39\" title=\"\">2025</a>; Hou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib27\" title=\"\">2025</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib15\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib7\" title=\"\">2024b</a>)</cite> extend evaluation toward vocal communication, assistant capability, and multi-turn realism. Omni models such as StepAudio and KimiAudio <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite> also include role-play scenarios in their evaluation. SpeechRole and VStyle <cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>; Zhan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib76\" title=\"\">2025</a>)</cite> are closer to our work, offering datasets that test interaction ability, expressiveness, and fidelity. However, these resources are primarily built from LLM-generated dialogues and TTS outputs, with limited human validation and a heavy reliance on proprietary ALLM-as-judge pipelines. As a result, they lack grounding in diverse human speech and typically apply only coarse rubrics.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Expressive TTS evaluation.</span>\nRecent work has explored evaluation of instruction-following and style-controlled TTS. InstructTTSEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite> explicitly includes role-play scenarios, while EmergentTTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">(Manku et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib45\" title=\"\">2025</a>)</cite> and discrete speech LM benchmarks <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang &amp; Sz&#233;kely, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib63\" title=\"\">2024</a>)</cite> emphasize prosody, spontaneity, and speaker consistency. These efforts push beyond intelligibility and naturalness, but still center on synthetic outputs rather than real human role-play.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "still"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech role-play.</span>\nWe formalize <em class=\"ltx_emph ltx_font_italic\">speech role-play</em> as a conditional speech generation problem grounded in a character profile and scene description.\nGiven a dialogue context <math alttext=\"{\\bm{C}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{speech}}</annotation></semantics></math>, a character profile <math alttext=\"{\\bm{C}}_{\\text{profile}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>profile</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{profile}}</annotation></semantics></math>, and a scene specification <math alttext=\"{\\bm{C}}_{\\text{scene}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>scene</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{scene}}</annotation></semantics></math>, a <em class=\"ltx_emph ltx_font_italic\">speech foundation model</em> (SFM) generates a role-played speech response <math alttext=\"{\\bm{S}}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119930;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{S}}_{r}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "description",
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS5\" title=\"F.5 Examples of Realism Role-play &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.5</span></a>). The second strategy adopts a <em class=\"ltx_emph ltx_font_italic\">bottom-up</em> design, motivated by real-world performance and real human speech.\nInstead of relying solely on stereotypes, this approach grounds evaluation in recordings from professional and non-professional speakers, drawn from realistic dialogue and media sources.\nJudgments in this setting are more fine-grained and multi-dimensional, addressing aspects such as prosodic dynamics, emotional fidelity, character consistency, and contextual fit.\nWhile this strategy requires more human annotation and careful curation, it provides benchmarks that reflect how role-play is perceived in practice.\nIt thus offers a richer framework for analyzing model outputs in settings where realism and subtlety are essential.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Role and Scene Dimensions.</span>\nRoles are operationalized through seven categories: (1) functional occupations in daily life, (2) functional occupations in fictional or fantasy settings, (3) social identities, (4) named or specific characters, (5) personality traits, (6) relative relationship, (7) basic events. Scenes are anchored by everyday events or state changes that evoke stereotypical reactions. Details of template design and alternative prompting strategies are given in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>We note that some archetypes (e.g., firefighter) inevitably reflect cultural stereotypes. These are used solely for evaluation scalability and do not reflect normative judgments.</span></span></span></p>\n\n",
                "matched_terms": [
                    "scene",
                    "personality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Sources and Negative Samples.</span>\nWe curate speech from (i) ground-truth media segments, (ii) retrieved character profiles, and (iii) generated local scene descriptions. To stress-test evaluation models, we construct contrastive mismatches: (i) negative local scenes with varying degrees of mismatch, and (ii) negative character profiles created by re-synthesizing transcripts with TTS systems. Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6\" title=\"Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F</span></a> details the prompt design and TTS generation details.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotators judge whether performances align with the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em>, using rubrics for <em class=\"ltx_emph ltx_font_italic\">Prosodic Dynamics (i.e., Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis)</em>, <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness (with progressive gating over three sub-dimensions: Emotion Accuracy, Emotion Intensity, Emotion Transition)</em>, <em class=\"ltx_emph ltx_font_italic\">Character Consistency (i.e., Voice Identity Matching and Traits Embodiment)</em>, <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance (i.e., Local Scene Fit and Global Story Coherence)</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>. A prompt quality check and confidence ratings are also collected. Full scales and gating details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "rhythmic",
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real-world Recoding.</span>\nAs noted in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S2\" title=\"2 Related Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, most prior efforts have relied heavily on synthetic data, mostly from TTS systems based on curated professional role-play voices. While such data can achieve high fidelity, it often lacks the diversity and spontaneity of real-world delivery, limiting the generalizability of evaluation models. To address this gap, we expand realism evaluation beyond synthetic and curated voices by incorporating newly recorded speech from both professional and amateur voice actors. This design introduces a broader spectrum of delivery styles, from polished professional acting to more natural and less controlled amateur performance.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "natural"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-shot and Few-shot ALLMs.</span>\nWe first evaluate the zero-shot capabilities of both proprietary and public models, including <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>, <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>, <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio</span>, and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>. All evaluations follow a unified prompting template that provides audio input alongside scene and character context.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real Recording Test Set.</span>\nThe real-recording evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T4\" title=\"Table 4 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> is notably more challenging, with all models suffering substantial performance drops. Proprietary models in the zero-shot setting achieve only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.04&#8211;0.08 average correlation, and public models often fall below zero. Few-shot prompting yields small but noticeable improvements, especially for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> (0.226 avg.), suggesting that exemplars partially mitigate domain mismatch between synthetic and real human speech. Interestingly, while finetuning achieves the highest overall average (0.247), its relative advantage over few-shot prompting is narrower than in the synthetic setting, and some dimensions (e.g., <span class=\"ltx_text ltx_font_italic\">Character Identity</span>) remain weak. We emphasize that this observed gap between synthetic and real recordings is not merely an artifact but an intended feature of Speech-DRAME, designed to expose the mismatch and drive future research into bridging synthetic-to-real generalization.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism evaluation.</span>\nRealism role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T6\" title=\"Table 6 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) presents a different landscape. Here, performance differences between end-to-end and cascaded systems are less pronounced. Several end-to-end models, such as <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>, reach average scores of 3.24, matching or even exceeding the best cascaded pipelines. Importantly, realism scores are more evenly distributed across prosodic, emotional, character, and contextual dimensions, with most models clustering between 2.9&#8211;3.2. This convergence reflects the difficulty of realism-style evaluation: since the data preparation emphasized contrast with authentic human speech, models are judged under similar conditions, leading to smaller observed gaps.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "even",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data generation:</span> LLMs were employed to expand prompts (e.g., character profiles, scene descriptions) and to produce negative samples in realism evaluation. All outputs were screened by human annotators to ensure validity and reduce bias.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Pipeline overview.</span>\nAt a high level, archetype evaluation data are generated through a three-stage pipeline:\n(1) selecting keywords that specify role and scene;\n(2) expanding them into full prompts via structured templates; and\n(3) rendering prompts into speech with contemporary TTS systems.\nThis modular pipeline ensures both scalability and reproducibility, while keeping prompts vivid enough for reliable judgments. (4) Based on the generated data (paired scene and synthesized speech), expert human annotators are recruited for detailed annotation on semantic check (i.e., <span class=\"ltx_text ltx_font_italic\">Content Pass</span>) and three different spoken role-play dimensions (i.e., <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human-likeness</span>, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>).\nImplementation details, including template design, alternative prompting strategies, and annotation guidelines, are provided in the following subsections of this Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Personality Traits.</span>\nExample:</p>\n\n",
                "matched_terms": [
                    "example",
                    "personality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <em class=\"ltx_emph ltx_font_italic\">Your personality is defined as hot-blooded. Your team is losing a game, and in the final moments you must deliver the decisive score. You say:</em>\n</p>\n\n",
                "matched_terms": [
                    "score",
                    "personality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Excluded Dimensions.</span> We initially considered more fine-grained control dimensions, but excluded them to preserve the archetype-level focus. Examples of excluded dimensions include:\n<em class=\"ltx_emph ltx_font_italic\">Emotional State &amp; Expression, Single Emotion Prompts, Explicit User Input / Assistant Output structures, Emotional Transition / Fluctuation, Voice Style &amp; Prosody (pacing, intonation, vocal timbre, expressive style, accent/dialect), and Content / Interaction Patterns (task/topic controls, specific linguistic structures, or speech production challenges).</em></p>\n\n",
                "matched_terms": [
                    "speech",
                    "pacing"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text generation.</span>\nWe followed a three-step expansion: keyword <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> scene description <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS3.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> full prompt with template. Some examples are shown as follows:</p>\n\n",
                "matched_terms": [
                    "description",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Step 2: Given a character and scene description, the text LLM imagines a candidate caption and generates a corresponding transcript (see below prompts for the text LLM).</p>\n\n",
                "matched_terms": [
                    "description",
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Annotators also provide a confidence level (high, moderate, low) for their Appropriateness rating, reflecting familiarity with the role and scene. This provides additional granularity in analysis.</p>\n\n",
                "matched_terms": [
                    "reflecting",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><em class=\"ltx_emph ltx_font_italic\">Example (reject):</em> Prompt: &#8220;Your personality is sharp-tongued. Your friend asks about their new haircut.&#8221; Audio says: &#8220;It looks great on you.&#8221; <math alttext=\"\\Rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I5.i1.I0.i1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8658;</mo><annotation encoding=\"application/x-tex\">\\Rightarrow</annotation></semantics></math> reject.</p>\n\n",
                "matched_terms": [
                    "example",
                    "personality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data from real human speech sources.</span>\nWe construct the realism evaluation set primarily from natural speech material:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "natural"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Retrieved character profiles.</span>\nEach audio segment is paired with a concise character description derived from the media context.\nThe detailed prompt design for generating these character profiles is provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS2\" title=\"F.2 Prompts for Character Profile Creation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "description",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generated local scenes.</span>\nWe supplement retrieved character information with automatically generated local scene descriptions summarizing the surrounding plot or setting.\nAppendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS3\" title=\"F.3 Prompts for Local Scene Generation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.3</span></a> includes representative prompts and generation details.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative local scenes.</span>\nFor each original segment, we generate additional scene descriptions at different levels of mismatch, ranging from subtle alterations to fully unrelated scenarios.\nAppendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS4\" title=\"F.4 Prompts for Negative Scenes &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.4</span></a> outlines the prompting strategies and example outputs.</p>\n\n",
                "matched_terms": [
                    "example",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, real human ground-truth segments and systematically constructed negatives form a balanced dataset that supports realism-oriented evaluation.\nThe positives provide natural anchors of role-play performance, while the negatives introduce controlled mismatches that test whether evaluation models can distinguish real human speech from synthetic approximations.\nThis design ensures that realism evaluation emphasizes subtle paralinguistic and contextual cues, moving beyond intelligibility or content fidelity.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "natural"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During annotation, raters are presented with either: (i) ground-truth stories, (ii) stories with negative local scenes, or (iii) stories with negative character profile. Annotators assign realism scores without being exposed to the underlying mismatch reasons. Instead, these reasons are provided only to the quality-assurance team, who use them as reference for internal validation and consistency checks. While the mismatch reasons may not be perfect, they help streamline quality control procedures and maintain reliability across the dataset.</p>\n\n",
                "matched_terms": [
                    "without",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide consistent evaluation context, we generate local scene descriptions summarizing the situational background.\nThese prompts aim to capture the immediate narrative context without introducing extraneous detail.</p>\n\n",
                "matched_terms": [
                    "without",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To make the construction of the realism evaluation task concrete, we present a worked example that begins with a curated local scene and character profile, followed by the same transcript adapted into mismatched contexts. These mismatches appear either at the <em class=\"ltx_emph ltx_font_italic\">local-scene</em> level or at both the <em class=\"ltx_emph ltx_font_italic\">character profile and scene</em> level, thereby providing controlled degradations of realism.</p>\n\n",
                "matched_terms": [
                    "example",
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the character Yunjin from <span class=\"ltx_text ltx_font_italic\">Genshin Impact</span> as a running case study, which is also included in our released dataset.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>As acknowledged, the use of real-world recordings is conducted under legal and ethical review. All curated data will be released as part of our open-source package.</span></span></span> This example illustrates how alignment between a speaker&#8217;s persona, local context, and delivery underpins realism, and how systematically breaking that alignment produces contrastive test cases.</p>\n\n",
                "matched_terms": [
                    "example",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Level 0 (Mild Mismatch).</span>\n<em class=\"ltx_emph ltx_font_italic\">Fake Scene:</em> At a modern tech company&#8217;s annual party, a shy programmer is asked to perform. Under pressure, he recites opera lines he once saw online.\n<em class=\"ltx_emph ltx_font_italic\">Fake Profile:</em> Zhang Wei, an introverted software engineer with little artistic background, usually immersed in code and algorithms.\n<em class=\"ltx_emph ltx_font_italic\">Reason:</em> The heroic verses sound alien coming from someone unfamiliar with opera or traditional culture. The clash produces awkward humor but still feels like a coping attempt.</p>\n\n",
                "matched_terms": [
                    "awkward",
                    "still",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the Yunjin example is drawn from authentic performance, where the character&#8217;s persona (an opera singer) and local scene (a battlefield narration) are naturally aligned through artistic delivery. The realism task does not merely test whether a model can follow a prompt, but whether it can preserve coherence between a speaker&#8217;s global identity, immediate context, and expressive prosody. When Yunjin&#8217;s transcript is transplanted into mismatched settings or paired with incongruent speaker profiles, the breakdown in realism becomes apparent: the same words shift from inspiring to awkward, jarring, or even absurd.</p>\n\n",
                "matched_terms": [
                    "awkward",
                    "even",
                    "example",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed a diverse set of TTS systems to generate negative character profiles and resynthesized speech for Realism Evaluation.\nThese included both commercial and non-commercial models, with variation in style control, conditioning prompts, and reference usage.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.T8\" title=\"Table 8 &#8227; Same-Speaker Voice Clone. &#8227; F.6 TTS Systems and Settings &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> summarizes the systems and settings.\nBelow, we detail the major strategies used across systems:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We leveraged all publicly available speaker style information released by the corresponding websites.\nFor each input sample, we employed <span class=\"ltx_text ltx_font_typewriter\">deepseek-v3</span> to select the best-matching speaker based on the original speech caption.\nIn the case of <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5TTS</span>, the character profile was also appended as an additional instruction to guide synthesis.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A character profile was first converted into a descriptive voice specification by prompting <span class=\"ltx_text ltx_font_typewriter\">deepseek-v3</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib38\" title=\"\">2024a</a>)</cite>.\nThis description was submitted to the TTS service to generate candidate voices.\nWe then previewed samples via Gemini2.5Pro, selecting the voice that best matched the speaker style of the original recording before proceeding to generate speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "description",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The annotation interface for <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> focuses on assessing contextual and narrative coherence in model-generated speech.\nUnlike the archetype-oriented evaluation, which emphasizes stylistic and emotional fidelity, this platform examines how well the voice performance aligns with a character&#8217;s personality, dialogue situation, and scene progression.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "speech",
                    "personality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.F3\" title=\"Figure 3 &#8227; F.7 Annotation Platform for Realism Evaluation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the core playback interface.\nAnnotators can preview each audio sample with synchronized spectrogram and waveform displays, while reviewing contextual metadata including the <span class=\"ltx_text ltx_font_italic\">Local Scene</span>, <span class=\"ltx_text ltx_font_italic\">Character Style</span>, and <span class=\"ltx_text ltx_font_italic\">Character Profile</span>.\nThese descriptions help situate the listening task, providing the background necessary for judging realism in the character&#8217;s vocal and behavioral expression.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Read the <span class=\"ltx_text ltx_font_bold\">Character Profile</span> (long-term identity, style) and <span class=\"ltx_text ltx_font_bold\">Local Scene</span> (immediate context, goal/emotion) before listening.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Do not reward theatricality/technical polish unless it naturally fits the character <em class=\"ltx_emph ltx_font_italic\">and</em> scene.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Read first:</span> Carefully read the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em> to understand the long-term identity and immediate goal/emotion.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Focus only on human speech:</span> Do not factor in background music/soundtracks, environmental or ambient noise, or non-speech audio events; these may reflect the scene but should not influence scoring of the spoken performance.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identity mismatch:</span> If the clip&#8217;s speaker is clearly not the intended target speaker, assign a score of <span class=\"ltx_text ltx_font_bold\">1</span> for every rating dimension that relies on the Character Profile (e.g., Character Style Traits, Trait Embodiment, Character Consistency).</p>\n\n",
                "matched_terms": [
                    "score",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt contains contradictory information.</span> Conflicts make accurate annotation difficult or impossible. Examples: incompatible accent specifications; Local Scene drastically at odds with intended emotion (see <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>); Local and global scenes inconsistent; traits described as &#8220;evolving over time&#8221; without a timeline.</p>\n\n",
                "matched_terms": [
                    "without",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rubric Rating Instructions.</span>\nScore each subcriterion on a 1&#8211;5 scale. Use the detailed rubrics below and the examples to maintain calibration across characters, tones, and emotional styles. Prioritize in-character believability; do not reward theatricality, emotional intensity, or technical polish unless they naturally fit the character <em class=\"ltx_emph ltx_font_italic\">and</em> scene.</p>\n\n",
                "matched_terms": [
                    "score",
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Focus: pitch, rhythm, and emphasis in speech.\n<span class=\"ltx_text ltx_font_bold\">Guideline 1.1 Pitch Variation (check Local Scene, Character Profile, and audio).</span></p>\n\n",
                "matched_terms": [
                    "speech",
                    "rhythm",
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 1.2 Rhythmic Naturalness (check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "scene",
                    "rhythmic",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 1.3 Stress and Emphasis (check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 2.1 Emotion Accuracy (Required; check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 2.2 Emotion Intensity Control (Only if 2.1 <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"A7.SS4.SSSx5.p3.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math> 3; check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 2.3 Dynamic Range (Only if 2.2 <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"A7.SS4.SSSx5.p4.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math> 3; check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Understood the character and scene clearly,</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Interpreted speech alignment with intended personality and context.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "personality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All human annotation in this work was conducted in accordance with standard ethical practices for research involving human annotators. Annotators were recruited via public hiring for Mandarin and internal lab hiring for English. Prior to participation, all annotators were provided with a clear description of the task, the type of data they would encounter, and their rights as participants, including the ability to opt out at any time without penalty.</p>\n\n",
                "matched_terms": [
                    "without",
                    "description",
                    "clear"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Within Mandarin from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, the relative rankings of models are clear. <span class=\"ltx_text ltx_font_typewriter\">Doubao</span> stands out as the strongest performer across all four dimensions, reaching near-ceiling levels in both <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> (4.37) and <span class=\"ltx_text ltx_font_italic\">Content Pass</span> (0.98). It also leads in role-related dimensions such as <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, indicating balanced competence across linguistic accuracy, delivery, and expressiveness. <span class=\"ltx_text ltx_font_typewriter\">G-CosyVoice2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-CosyVoice2</span> follow closely, especially in audio quality, though they lag behind in role appropriateness. <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> achieve mid-range results, reflecting higher variance across samples, while <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> shows competitive content understanding but lower naturalness and appropriateness. <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> consistently ranks the lowest, with large variance, underscoring instability and weaker alignment with archetypal role expectations.</p>\n\n",
                "matched_terms": [
                    "reflecting",
                    "clear"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T11\" title=\"Table 11 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> and Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.F5\" title=\"Figure 5 &#8227; English Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.F6\" title=\"Figure 6 &#8227; English Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> further highlight differences across domains. In Mandarin, performance is relatively consistent across categories, with means clustering around 3.1&#8211;3.3. The highest scores appear in <span class=\"ltx_text ltx_font_typewriter\">SocialIdentity</span> and <span class=\"ltx_text ltx_font_typewriter\">DailyOccupation</span>, suggesting that models handle culturally grounded archetypes more reliably. In English, however, domain variation is much greater. Categories such as <span class=\"ltx_text ltx_font_typewriter\">FantasyOccupation</span> and <span class=\"ltx_text ltx_font_typewriter\">NamedCharacters</span> receive notably lower scores (means below 2.0), reflecting the difficulty of producing expressive, role-appropriate speech in imaginative or culturally specific contexts. Occupational roles remain relatively easier for models in both languages, likely because these scenarios are closer to training data distributions.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "reflecting"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although realism evaluation targets nuanced paralinguistic behaviors, human annotators achieve <em class=\"ltx_emph ltx_font_italic\">consistently high agreement</em> once the rubric disentangles the aspects to be judged.\nAs shown in the agreement summary in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F7\" title=\"Figure 7 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, most dimensions fall in the <math alttext=\"0.86\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>0.86</mn><annotation encoding=\"application/x-tex\">0.86</annotation></semantics></math>&#8211;<math alttext=\"0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mn>0.88</mn><annotation encoding=\"application/x-tex\">0.88</annotation></semantics></math> range, with the lowest agreement on <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> (<math alttext=\"\\approx 0.83\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.83</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.83</annotation></semantics></math>) and the highest on <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em> / <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> (<math alttext=\"\\approx 0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.88</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.88</annotation></semantics></math>).\nEmotion-related, progressive metrics (Intensity and Transition) are slightly lower (<math alttext=\"\\approx 0.85\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.85</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.85</annotation></semantics></math>), reflecting the added difficulty of judging graded expressivity.</p>\n\n",
                "matched_terms": [
                    "reflecting",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F8\" title=\"Figure 8 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F9\" title=\"Figure 9 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> visualize the score distributions per dimension.\nWe highlight three patterns:\n(i) <span class=\"ltx_text ltx_font_bold\">Progressive/gated emotion metrics.</span> For <em class=\"ltx_emph ltx_font_italic\">Emotion Intensity</em> and <em class=\"ltx_emph ltx_font_italic\">Emotion Transition</em>, we assign <span class=\"ltx_text ltx_font_typewriter\">1</span> when the scene is marked <em class=\"ltx_emph ltx_font_italic\">N/A</em> (i.e., emotion accuracy is insufficient to meaningfully judge intensity/transition). This produces a visible mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, a sparse bin at <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>, and slightly lower overall agreement; downstream analyses conditioning on adequate <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> (e.g., mean&#160;<math alttext=\"\\geq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 3</annotation></semantics></math>) recover more bell-shaped distributions.\n(ii) <span class=\"ltx_text ltx_font_bold\">Traits Embodiment.</span> Scores tend to avoid the middle (<math alttext=\"\\sim\\!3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!3</annotation></semantics></math>), concentrating on low values when traits clearly mismatch and high values when traits are strongly realized. This yields the lowest inter-rater agreement among dimensions, reflecting the inherently compositional and role-specific nature of trait judgments.\n(iii) <span class=\"ltx_text ltx_font_bold\">Other dimensions.</span> <em class=\"ltx_emph ltx_font_italic\">Pitch Variation</em>, <em class=\"ltx_emph ltx_font_italic\">Rhythmic Naturalness</em>, <em class=\"ltx_emph ltx_font_italic\">Stress &amp; Emphasis</em>, <em class=\"ltx_emph ltx_font_italic\">Voice Identity Matching</em>, <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em>, <em class=\"ltx_emph ltx_font_italic\">Global Story Coherence</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> are approximately unimodal with modes at <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>&#8211;<math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math>, consistent with moderate&#8211;strong realization in the dataset and with the high agreement reported.</p>\n\n",
                "matched_terms": [
                    "reflecting",
                    "score",
                    "rhythmic",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prosody control (Pitch, Rhythm, Stress).</span> Professional speech is shifted toward higher bins with tighter spread, indicating steadier control and fewer low-end tails.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "rhythm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identity and traits (Voice Identity, Traits Embodiment).</span> Professionals present clearer peaks in upper bins, while amateurs are broader or bimodal, reflecting inconsistent character anchoring.</p>\n\n",
                "matched_terms": [
                    "reflecting",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, the consistently high agreement confirms that the rubric isolates perceptually coherent factors, so learned evaluators can target them reliably.\nSecond, the gated behavior of <em class=\"ltx_emph ltx_font_italic\">Intensity/Transition</em> motivates a <em class=\"ltx_emph ltx_font_italic\">hierarchical</em> evaluator: verify <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> before scoring graded expressivity, or model the joint structure with conditional heads.\nThird, <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> remains challenging; models may benefit from explicit character-trait priors, long-term context aggregation, or contrastive supervision between matched and mismatched traits.\nFinally, prosody-focused objectives (e.g., pitch/rhythm/stress control) are likely to yield the largest quality gains for amateur-style speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "coherent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Impact of few-shot conditioning.\nIntroducing even three examples (<span class=\"ltx_text ltx_font_typewriter\">3-Shot</span>) yields a clear improvement over zeroshot (0.433 vs. 0.390). This indicates that providing explicit annotated demonstrations helps the model better align with the rubric. Interestingly, when using only a single run with few-shot demonstrations (<span class=\"ltx_text ltx_font_typewriter\">3-Shot-1run</span>), the performance is only marginally better than pure zeroshot, highlighting that both averaging and demonstrations are complementary: demonstrations improve grounding, while multiple runs mitigate stochasticity.</p>\n\n",
                "matched_terms": [
                    "even",
                    "clear"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For realism evaluation on the basic test set, <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> consistently achieves the strongest correlations across most dimensions, especially for <span class=\"ltx_text ltx_font_italic\">Character Identity</span> and <span class=\"ltx_text ltx_font_italic\">Traits</span>, leading to the best average correlation (0.668). <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> also performs competitively (0.630), while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> lags slightly behind (0.625). These results highlight that full fine-tuning may still offer an advantage when the evaluation set is relatively homogeneous and closer to the training distribution.</p>\n\n",
                "matched_terms": [
                    "still",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the comprehensive design of Speech-DRAME, several limitations remain.\nFirst, while our dual evaluation paradigm combines top-down archetype assessment with bottom-up realism evaluation, both strategies are constrained by the quality and scope of available data. Archetype evaluation inevitably relies on simplified stereotypes, which can overlook subtler paralinguistic and contextual cues. Realism evaluation, though richer, still struggles with domain mismatch between synthetic data and real human recordings, as evidenced by the performance drop observed in the real-recording test set. This highlights the persistent gap between benchmark-driven evaluation and real-world variability in speech role-play.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "still"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, although we validated DRAME-Eval against human annotations, alignment under realism settings remains modest. This suggests that even human-annotated benchmarks are insufficiently fine-grained to capture the full complexity of human perception in speech role-play. Richer annotation protocols, more diverse speaker populations, and integration of perceptual psychology insights could further improve evaluation fidelity.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "even"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Looking forward, we see several promising research directions. We intentionally restrict evaluation to single-turn responses for clarity and tractability. However, multi-turn extensions that capture narrative progression, long-term consistency, and dynamic emotion shifts remain a natural next step. Incorporating multilingual and cross-cultural datasets will enhance generality and fairness. On the modeling side, exploring hybrid evaluation strategies that combine discrete rubrics with continuous perceptual signals (e.g., prosody embeddings or affective measures) may better bridge the gap between automatic and human judgments. Finally, integrating Speech-DRAME into reinforcement learning pipelines as a reward model could help drive the next generation of expressive, human-aligned speech role-play systems.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "natural"
                ]
            }
        ]
    },
    "A7.SS4.SSSx4.p3": {
        "caption": "",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text ltx_font_bold\">Score</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\"><span class=\"ltx_text ltx_font_bold\">Description / Example</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Emphasis highlights key meanings/emotions in realistic ways (e.g., stress on &#8220;<em class=\"ltx_emph ltx_font_italic\">can&#8217;t</em>&#8221; in &#8220;We can&#8217;t leave them behind!&#8221;).</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Generally helpful emphasis with minor misalignment.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Mild/generic stress patterns (not distracting, limited impact).</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Over-emphasis or misplaced focus (e.g., stressing function words).</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Flat or confusing emphasis (all equal or no variation).</span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "realistic",
            "key",
            "example",
            "cant",
            "ways",
            "helpful",
            "words",
            "distracting",
            "not",
            "confusing",
            "meaningsemotions",
            "variation",
            "stress",
            "them",
            "behind",
            "overemphasis",
            "highlights",
            "misplaced",
            "stressing",
            "minor",
            "patterns",
            "emphasis",
            "flat",
            "impact",
            "score",
            "mildgeneric",
            "leave",
            "description",
            "all",
            "limited",
            "generally",
            "equal",
            "function",
            "we",
            "focus",
            "cant",
            "misalignment"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">However, in this work we focus on the <em class=\"ltx_emph ltx_font_italic\">single-turn setting</em>, where evaluation is defined at the response level and does not explicitly depend on the preceding dialogue context:</p>\n\n",
                "matched_terms": [
                    "focus",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech role-play evaluation can be framed in multiple ways depending on the available data and the desired granularity of analysis.\nWithin Speech-DRAME, we formalize two distinct strategies inspired by sociology theories<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Please refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A2\" title=\"Appendix B Theoretical Motivation for Dual Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> for detailed discussion of the dual evaluation design.</span></span></span>, including a top-down, archetype-oriented option using synthetic speech and a bottom-up, realism-oriented option from real human speech.\nRather than merging these into a single rubric, we treat them as separate perspectives, each with unique advantages and limitations.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarizes the key contrasts, while further implementation details are provided in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>-&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "ways",
                    "key",
                    "them"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotators judge whether performances align with the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em>, using rubrics for <em class=\"ltx_emph ltx_font_italic\">Prosodic Dynamics (i.e., Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis)</em>, <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness (with progressive gating over three sub-dimensions: Emotion Accuracy, Emotion Intensity, Emotion Transition)</em>, <em class=\"ltx_emph ltx_font_italic\">Character Consistency (i.e., Voice Identity Matching and Traits Embodiment)</em>, <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance (i.e., Local Scene Fit and Global Story Coherence)</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>. A prompt quality check and confidence ratings are also collected. Full scales and gating details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "variation",
                    "emphasis",
                    "stress"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For proprietary models, we report the average of five independent runs to reduce variance and obtain more stable predictions. For open-source models, we observed weaker instruction-following ability and limited adaptation to unfamiliar tasks. To mitigate this, we did not request simultaneous prediction of multiple metrics. Instead, we queried one metric at a time. Following <cite class=\"ltx_cite ltx_citemacro_citet\">Song &amp; Bahri (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>); Lukasik et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>, we interpret model outputs through token distribution over discrete labels (<span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score.</p>\n\n",
                "matched_terms": [
                    "score",
                    "not",
                    "limited"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Finetuning Model: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>.</span>\nIn addition to zero-shot and few-shot evaluations, we fine-tune a dedicated evaluation model, denoted as <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> for archetype and realism role-play evaluation, respectively.\nWe initialize from <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio-7B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib11\" title=\"\">2024</a>)</cite>, which provides a strong foundation for multi-modal audio-text reasoning.\nTo better leverage the multiple human annotations available in our datasets, we adopt a sampling-based training strategy: at each step, one annotator label is randomly selected, following practices in label distribution learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Geng, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib22\" title=\"\">2016</a>; Wen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib67\" title=\"\">2023</a>)</cite>.\nThis stochastic supervision implicitly exposes the model to the empirical annotation distribution, while during inference we output the full distribution over discrete labels (e.g., <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score, consistent with recent decoding-based regression methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Song &amp; Bahri, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>; Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib70\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib64\" title=\"\">2025b</a>; Lukasik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>. We apply LoRA fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib28\" title=\"\">2022</a>)</cite>. All implementation details, including optimizer settings, training schedule, and sampling policy, are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS3\" title=\"L.3 Fine-tuning Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "all",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real Recording Test Set.</span>\nThe real-recording evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T4\" title=\"Table 4 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> is notably more challenging, with all models suffering substantial performance drops. Proprietary models in the zero-shot setting achieve only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.04&#8211;0.08 average correlation, and public models often fall below zero. Few-shot prompting yields small but noticeable improvements, especially for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> (0.226 avg.), suggesting that exemplars partially mitigate domain mismatch between synthetic and real human speech. Interestingly, while finetuning achieves the highest overall average (0.247), its relative advantage over few-shot prompting is narrower than in the synthetic setting, and some dimensions (e.g., <span class=\"ltx_text ltx_font_italic\">Character Identity</span>) remain weak. We emphasize that this observed gap between synthetic and real recordings is not merely an artifact but an intended feature of Speech-DRAME, designed to expose the mismatch and drive future research into bridging synthetic-to-real generalization.</p>\n\n",
                "matched_terms": [
                    "all",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall.</span> Across both archetype and realism evaluations, we observe three consistent trends: (i) zero-shot performance is limited, particularly for open-source models, (ii) few-shot prompting offers moderate but consistent gains, especially for proprietary ALLMs, and (iii) finetuning with task-specific supervision (<span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>) delivers the largest improvements, achieving state-of-the-art correlations across nearly all dimensions. Nevertheless, results on the real-recording test set underscore the gap between synthetic and real human role-play scenarios, motivating future work in data diversification and domain adaptation.</p>\n\n",
                "matched_terms": [
                    "all",
                    "limited"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human alignment.</span>\nWe further conducted small-scale human studies with 10% of the benchmark data to verify the reliability of the benchmark. Archetype evaluation achieves a Spearman correlation of 0.706 between human judgments and <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, validating that the automatic evaluator closely tracks human preferences in this setting. Realism evaluation, however, achieves a more modest Spearman correlation of 0.375. This lower alignment is expected, since realism data construction did not prioritize fine-grained model separation but instead emphasized distinguishing human versus synthetic speech.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>We view this as evidence that realism presents a harder, less-saturated frontier. Rather than a limitation of DRAME-Eval, this highlights the challenge of designing evaluators that capture nuanced human perception, which motivates future work.</span></span></span> As a result, all models occupy a relatively narrow performance band, reducing sensitivity to model-level differences. We view this as evidence that realism evaluation highlights a harder, less saturated frontier where both SFMs and SEMs require further advances.</p>\n\n",
                "matched_terms": [
                    "highlights",
                    "not",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All data collection and annotation in this work followed strict ethical standards. Our bilingual datasets were constructed exclusively from legally accessible sources, including licensed media, open-source corpora, and original recordings conducted with informed consent. Annotators were recruited with appropriate compensation and provided with clear instructions, including the right to withdraw at any point. To minimize risks, no personally identifiable information was included in the released data, and all recordings were anonymized before annotation. We ensured cultural and linguistic diversity in annotator pools to mitigate potential bias across Mandarin and English role-play scenarios. While this study did not require formal IRB approval, we adhered to best practices in human-subject research, including transparency in task design and careful consideration of annotator well-being. Full details of our annotation protocol and ethical safeguards are presented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A9\" title=\"Appendix I Annotation Ethics &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</p>\n\n",
                "matched_terms": [
                    "all",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism as Situated Performance</span>\nIn contrast, realism highlights the fine-grained enactment of roles in situated interaction. Goffman&#8217;s dramaturgical sociology emphasizes that everyday social life is itself a performance, where believability depends on subtle paralinguistic and pragmatic cues such as prosody, hesitation, and emotional shading&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goffman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib23\" title=\"\">1949</a>)</cite>. Similarly, interactional sociolinguistics has demonstrated how micro-level features like intonation, pacing, and turn-taking patterns shape the perception of authenticity in communication&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gumperz, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib25\" title=\"\">1982</a>)</cite>. Realism Evaluation thus anchors role-play assessment in bottom-up sensitivity to authentic delivery, beyond schematic expectations.</p>\n\n",
                "matched_terms": [
                    "patterns",
                    "highlights"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <em class=\"ltx_emph ltx_font_italic\">You are an overprotective parent. Your child has not come home late at night, and you cannot reach them by phone. You say:</em>\n</p>\n\n",
                "matched_terms": [
                    "not",
                    "them"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Excluded Dimensions.</span> We initially considered more fine-grained control dimensions, but excluded them to preserve the archetype-level focus. Examples of excluded dimensions include:\n<em class=\"ltx_emph ltx_font_italic\">Emotional State &amp; Expression, Single Emotion Prompts, Explicit User Input / Assistant Output structures, Emotional Transition / Fluctuation, Voice Style &amp; Prosody (pacing, intonation, vocal timbre, expressive style, accent/dialect), and Content / Interaction Patterns (task/topic controls, specific linguistic structures, or speech production challenges).</em></p>\n\n",
                "matched_terms": [
                    "patterns",
                    "focus",
                    "them"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All steps are automatically processed by <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib12\" title=\"\">2025</a>)</cite>, but each output is manually verified to ensure high quality for downstream usage. We demonstrate an example for Roles: Occupation / Functional Role in Fictional or Fantasy as follows:</p>\n\n",
                "matched_terms": [
                    "all",
                    "example"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">30-second focus:</span> Assessment should focus on the first 30 seconds of the generation ONLY. Do not consider any content past 30s; disregard it entirely from your assessment and rubric ratings. (You may stop listening beyond 30s.)</p>\n\n",
                "matched_terms": [
                    "focus",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rubric Rating Instructions (for content pass = true).</span>\nWe shift focus from <em class=\"ltx_emph ltx_font_italic\">what</em> was said to <em class=\"ltx_emph ltx_font_italic\">how</em> it was said (vocal performance). Use gut intuition guided by rubric definitions. Score each audio <em class=\"ltx_emph ltx_font_italic\">in isolation</em> (no ranking across clips).</p>\n\n",
                "matched_terms": [
                    "score",
                    "focus"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">Appropriateness</span>: comment for <em class=\"ltx_emph ltx_font_italic\">all</em> scores 1&#8211;5. Explain <em class=\"ltx_emph ltx_font_italic\">why</em> the <em class=\"ltx_emph ltx_font_italic\">delivery</em> was/was not a good match to the in-context traits (role &amp; scene).</p>\n\n",
                "matched_terms": [
                    "all",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If any portion of the clip is ill-fitting, prefer the lower overall score; do not average across good and bad segments.</p>\n\n",
                "matched_terms": [
                    "score",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During annotation, raters are presented with either: (i) ground-truth stories, (ii) stories with negative local scenes, or (iii) stories with negative character profile. Annotators assign realism scores without being exposed to the underlying mismatch reasons. Instead, these reasons are provided only to the quality-assurance team, who use them as reference for internal validation and consistency checks. While the mismatch reasons may not be perfect, they help streamline quality control procedures and maintain reliability across the dataset.</p>\n\n",
                "matched_terms": [
                    "not",
                    "them"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the character Yunjin from <span class=\"ltx_text ltx_font_italic\">Genshin Impact</span> as a running case study, which is also included in our released dataset.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>As acknowledged, the use of real-world recordings is conducted under legal and ethical review. All curated data will be released as part of our open-source package.</span></span></span> This example illustrates how alignment between a speaker&#8217;s persona, local context, and delivery underpins realism, and how systematically breaking that alignment produces contrastive test cases.</p>\n\n",
                "matched_terms": [
                    "impact",
                    "all",
                    "example"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the Yunjin example is drawn from authentic performance, where the character&#8217;s persona (an opera singer) and local scene (a battlefield narration) are naturally aligned through artistic delivery. The realism task does not merely test whether a model can follow a prompt, but whether it can preserve coherence between a speaker&#8217;s global identity, immediate context, and expressive prosody. When Yunjin&#8217;s transcript is transplanted into mismatched settings or paired with incongruent speaker profiles, the breakdown in realism becomes apparent: the same words shift from inspiring to awkward, jarring, or even absurd.</p>\n\n",
                "matched_terms": [
                    "not",
                    "example",
                    "words"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the audio&#8217;s speaker is <em class=\"ltx_emph ltx_font_italic\">clearly not</em> the intended target identity, assign a score of <span class=\"ltx_text ltx_font_bold\">1</span> to all Profile-dependent dimensions (e.g., Voice Identity Matching, Trait Embodiment) and proceed with other dimensions as applicable.</p>\n\n",
                "matched_terms": [
                    "all",
                    "score",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prosodic Dynamics</span> (Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis): naturalness of patterns and rhythms.</p>\n\n",
                "matched_terms": [
                    "patterns",
                    "variation",
                    "emphasis",
                    "stress"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Focus only on human speech:</span> Do not factor in background music/soundtracks, environmental or ambient noise, or non-speech audio events; these may reflect the scene but should not influence scoring of the spoken performance.</p>\n\n",
                "matched_terms": [
                    "focus",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identity mismatch:</span> If the clip&#8217;s speaker is clearly not the intended target speaker, assign a score of <span class=\"ltx_text ltx_font_bold\">1</span> for every rating dimension that relies on the Character Profile (e.g., Character Style Traits, Trait Embodiment, Character Consistency).</p>\n\n",
                "matched_terms": [
                    "score",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Delete incorrect pre-populated traits; you do not need to replace them. If you delete many traits, <em class=\"ltx_emph ltx_font_italic\">Trait Embodiment</em> should likely be low.</p>\n\n",
                "matched_terms": [
                    "not",
                    "them"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rubric Rating Instructions.</span>\nScore each subcriterion on a 1&#8211;5 scale. Use the detailed rubrics below and the examples to maintain calibration across characters, tones, and emotional styles. Prioritize in-character believability; do not reward theatricality, emotional intensity, or technical polish unless they naturally fit the character <em class=\"ltx_emph ltx_font_italic\">and</em> scene.</p>\n\n",
                "matched_terms": [
                    "score",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Focus: pitch, rhythm, and emphasis in speech.\n<span class=\"ltx_text ltx_font_bold\">Guideline 1.1 Pitch Variation (check Local Scene, Character Profile, and audio).</span></p>\n\n",
                "matched_terms": [
                    "variation",
                    "focus",
                    "emphasis"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 1.3 Stress and Emphasis (check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "emphasis",
                    "stress"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Auditability and SEM alignment.</span>\nEvery score is traceable to a rubric with short rater rationales and confidence.\nWe then tune SEMs to these labels (rather than opaque judge outputs), enabling calibration studies and error analyses that would not be possible with black-box LLM judges.</p>\n\n",
                "matched_terms": [
                    "score",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All human annotation in this work was conducted in accordance with standard ethical practices for research involving human annotators. Annotators were recruited via public hiring for Mandarin and internal lab hiring for English. Prior to participation, all annotators were provided with a clear description of the task, the type of data they would encounter, and their rights as participants, including the ability to opt out at any time without penalty.</p>\n\n",
                "matched_terms": [
                    "description",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A critical observation is that annotations in Mandarin and English cannot be directly compared. The annotation groups for the two languages differed in their linguistic backgrounds and cultural interpretations, which leads to systematic differences in scoring. Our bilingual pilot further confirmed that annotators often interpret role-play appropriateness and expressiveness differently depending on their cultural frame of reference. For example, Mandarin raters tend to reward delivery styles that align with culturally expected prosody and politeness strategies, while English raters prioritize spontaneity and naturalness. These discrepancies parallel the findings of the singing voice conversion challenge (SVCC2023) from <cite class=\"ltx_cite ltx_citemacro_citet\">Huang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib31\" title=\"\">2023</a>)</cite>, where native and non-native listeners provided significantly different ratings for the same samples, particularly for naturalness and speaker similarity. Consequently, while absolute scores differ substantially across Mandarin and English, only within-language comparisons are meaningful. Cross-lingual ranking is not valid given the different annotation bases.</p>\n\n",
                "matched_terms": [
                    "not",
                    "example"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although cross-lingual comparisons are invalid, this does not diminish the utility of archetype evaluation for role-play benchmarking. In fact, the observed inconsistencies reinforce a central point: successful role-play requires models to adapt to the cultural and linguistic norms of the target language community. A system that performs well in Mandarin but poorly in English cannot be considered universally capable, since end users will evaluate appropriateness and naturalness according to their native expectations. Thus, archetype evaluation highlights not only model-specific strengths and weaknesses but also the need for culturally grounded evaluation pipelines.</p>\n\n",
                "matched_terms": [
                    "highlights",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F8\" title=\"Figure 8 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F9\" title=\"Figure 9 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> visualize the score distributions per dimension.\nWe highlight three patterns:\n(i) <span class=\"ltx_text ltx_font_bold\">Progressive/gated emotion metrics.</span> For <em class=\"ltx_emph ltx_font_italic\">Emotion Intensity</em> and <em class=\"ltx_emph ltx_font_italic\">Emotion Transition</em>, we assign <span class=\"ltx_text ltx_font_typewriter\">1</span> when the scene is marked <em class=\"ltx_emph ltx_font_italic\">N/A</em> (i.e., emotion accuracy is insufficient to meaningfully judge intensity/transition). This produces a visible mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, a sparse bin at <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>, and slightly lower overall agreement; downstream analyses conditioning on adequate <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> (e.g., mean&#160;<math alttext=\"\\geq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 3</annotation></semantics></math>) recover more bell-shaped distributions.\n(ii) <span class=\"ltx_text ltx_font_bold\">Traits Embodiment.</span> Scores tend to avoid the middle (<math alttext=\"\\sim\\!3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!3</annotation></semantics></math>), concentrating on low values when traits clearly mismatch and high values when traits are strongly realized. This yields the lowest inter-rater agreement among dimensions, reflecting the inherently compositional and role-specific nature of trait judgments.\n(iii) <span class=\"ltx_text ltx_font_bold\">Other dimensions.</span> <em class=\"ltx_emph ltx_font_italic\">Pitch Variation</em>, <em class=\"ltx_emph ltx_font_italic\">Rhythmic Naturalness</em>, <em class=\"ltx_emph ltx_font_italic\">Stress &amp; Emphasis</em>, <em class=\"ltx_emph ltx_font_italic\">Voice Identity Matching</em>, <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em>, <em class=\"ltx_emph ltx_font_italic\">Global Story Coherence</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> are approximately unimodal with modes at <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>&#8211;<math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math>, consistent with moderate&#8211;strong realization in the dataset and with the high agreement reported.</p>\n\n",
                "matched_terms": [
                    "score",
                    "patterns",
                    "variation",
                    "stress",
                    "emphasis"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and few-shot experiments on commercial models such as <span class=\"ltx_text ltx_font_typewriter\">Gemini</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o</span>, we report results as the average of five independent runs to reduce randomness and obtain more stable predictions. In contrast, for open-source models, we observed that their instruction-following ability is relatively weak. To mitigate this, we prompt them to predict one dimension at a time rather than all dimensions jointly. Specifically, for each metric, after showing the rubric prompt, we request completion in a fixed template and extract the probability distribution over the discrete tokens <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>. The final score is then computed from these probabilities, which we found makes open-source models usable to some extent under this setting.</p>\n\n",
                "matched_terms": [
                    "all",
                    "score",
                    "them"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Effect of repeated runs.\nWhen comparing single-run decoding (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot-1run</span>) against averaging over five runs (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot</span>), we see consistent gains across nearly all dimensions (average score improves from 0.353 to 0.390). This suggests that sampling variance plays a substantial role in the reliability of large language model (LLM) evaluators, and averaging multiple outputs reduces this variance, producing more stable correlation with human annotations.</p>\n\n",
                "matched_terms": [
                    "all",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While we utilize various prompts and sample them randomly during the fine-tuning experiments, we provide an example prompt as follows:</p>\n\n",
                "matched_terms": [
                    "them",
                    "example"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the archetype test set, all fine-tuned variants achieve strong <span class=\"ltx_text ltx_font_italic\">Content Pass</span> accuracy above 92%. Differences appear mainly in correlation-based metrics. Here, <span class=\"ltx_text ltx_font_typewriter\">Base-FT</span> yields the highest score on <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> delivers the strongest performance in <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> and the best overall average (0.629). This suggests that instruction-tuned checkpoints paired with parameter-efficient adaptation can better capture higher-level perceptual traits.</p>\n\n",
                "matched_terms": [
                    "all",
                    "score"
                ]
            }
        ]
    },
    "A7.SS4.SSSx5.p2": {
        "caption": "",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text ltx_font_bold\">Score</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\"><span class=\"ltx_text ltx_font_bold\">Description / Example</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Emotion matches personality and situation with subtlety (e.g., hesitant pause before apologizing for a reserved character).</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Emotionally grounded with light misalignment.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Too neutral/safe for the scene.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Out of sync with character or situation (e.g., cheerful during betrayal).</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Jarring/inappropriate (e.g., laughing/yelling without motivation).</span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "neutralsafe",
            "out",
            "example",
            "personality",
            "laughingyelling",
            "jarringinappropriate",
            "before",
            "character",
            "betrayal",
            "scene",
            "cheerful",
            "subtlety",
            "sync",
            "emotionally",
            "grounded",
            "without",
            "situation",
            "motivation",
            "matches",
            "hesitant",
            "emotion",
            "apologizing",
            "reserved",
            "score",
            "during",
            "light",
            "pause",
            "description",
            "too",
            "misalignment"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Role-play has become a key testbed for generative models, expanding from text-only dialogue to multimodal interaction. Extending role-play to speech captures prosody, emotion, and delivery, but also poses new evaluation challenges. Current pipelines often use audio large language models (ALLMs) as zero-shot judges, which miss paralinguistic cues, collapse multiple aspects into coarse scores, and rely on synthetic speech references that fail to reflect real-world roles. We present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework that contributes at three levels: (i) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-EvalBench</em>, an evaluation benchmark with bilingual human-annotated data and protocols for training and testing speech evaluation models&#160;(SEMs), (ii) <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, a fine-tuned evaluation model, which substantially outperforms zero-shot and few-shot ALLMs, and (iii) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-RoleBench</em>, a speech role-play benchmark that leverages <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> as an automatic judge to compare speech foundation models (SFMs). Speech-DRAME distinguishes between two complementary evaluation strategies: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>, a top-down approach measuring adherence to broad role archetypes, and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em>, a bottom-up approach grounded in real human speech that emphasizes nuanced role quality. Compared to zero-shot ALLM judges, <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves stronger agreement with human ratings (Pearson correlation from 0.480 to 0.629 in archetypes, and 0.390 to 0.625 in realism). By integrating transparent benchmark resources, modeling approaches, and system-level evaluation, Speech-DRAME provides the first comprehensive, reproducible foundation for assessing spoken role-play.</p>\n\n",
                "matched_terms": [
                    "grounded",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Role-play has been recently recognized a cornerstone of interactive systems, from initial text-based focuses to the growing ecosystem of multimodal dialogue agents&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib6\" title=\"\">2024a</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib59\" title=\"\">2024</a>; Dai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib13\" title=\"\">2025</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib43\" title=\"\">2024</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>)</cite>. With the rapid advancement of large language models (LLMs), role-play has evolved beyond scripted interactions into dynamic character-driven conversations, supporting applications in education, entertainment, and human&#8211;AI collaboration&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib69\" title=\"\">2024</a>; Agatsuma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib1\" title=\"\">2024</a>; Fung &amp; Laing, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib20\" title=\"\">2024</a>)</cite>. Extending role-play to speech-based interaction offers an even richer communication channel, capturing prosody, emotion, and character consistency in ways that text alone cannot&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-based role-play evaluation.</span>\nEarly research focused on text-only dialogue benchmarks. Datasets such as RoleLLM, CharacterBox, and RoleMRC <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib62\" title=\"\">2024</a>; Tu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib60\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib61\" title=\"\">2025a</a>; Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib53\" title=\"\">2023</a>; Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib41\" title=\"\">2025</a>)</cite> test whether models can sustain persona consistency and character fidelity, using LLM-as-judge pipelines for scalability. However, studies like PersonaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib81\" title=\"\">2025a</a>)</cite> have shown that LLM-as-judge approaches are far from perfect, struggling with tasks as basic as tracking speaker identity. This highlights broader concerns about reliability even in text settings, which become only more severe when extended to speech, where prosody, emotion, and vocal style play a central role.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech role-play.</span>\nWe formalize <em class=\"ltx_emph ltx_font_italic\">speech role-play</em> as a conditional speech generation problem grounded in a character profile and scene description.\nGiven a dialogue context <math alttext=\"{\\bm{C}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{speech}}</annotation></semantics></math>, a character profile <math alttext=\"{\\bm{C}}_{\\text{profile}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>profile</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{profile}}</annotation></semantics></math>, and a scene specification <math alttext=\"{\\bm{C}}_{\\text{scene}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>scene</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{scene}}</annotation></semantics></math>, a <em class=\"ltx_emph ltx_font_italic\">speech foundation model</em> (SFM) generates a role-played speech response <math alttext=\"{\\bm{S}}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119930;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{S}}_{r}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "description",
                    "grounded",
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here <math alttext=\"{\\bm{S}}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#119930;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{S}}_{r}</annotation></semantics></math> represents the spoken output (waveform or latent representation). This definition captures both the linguistic dimension (coherence with context and scene) and the paralinguistic dimension (prosody, style, emotion) expected of the role.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS5\" title=\"F.5 Examples of Realism Role-play &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.5</span></a>). The second strategy adopts a <em class=\"ltx_emph ltx_font_italic\">bottom-up</em> design, motivated by real-world performance and real human speech.\nInstead of relying solely on stereotypes, this approach grounds evaluation in recordings from professional and non-professional speakers, drawn from realistic dialogue and media sources.\nJudgments in this setting are more fine-grained and multi-dimensional, addressing aspects such as prosodic dynamics, emotional fidelity, character consistency, and contextual fit.\nWhile this strategy requires more human annotation and careful curation, it provides benchmarks that reflect how role-play is perceived in practice.\nIt thus offers a richer framework for analyzing model outputs in settings where realism and subtlety are essential.</p>\n\n",
                "matched_terms": [
                    "subtlety",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Role and Scene Dimensions.</span>\nRoles are operationalized through seven categories: (1) functional occupations in daily life, (2) functional occupations in fictional or fantasy settings, (3) social identities, (4) named or specific characters, (5) personality traits, (6) relative relationship, (7) basic events. Scenes are anchored by everyday events or state changes that evoke stereotypical reactions. Details of template design and alternative prompting strategies are given in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>We note that some archetypes (e.g., firefighter) inevitably reflect cultural stereotypes. These are used solely for evaluation scalability and do not reflect normative judgments.</span></span></span></p>\n\n",
                "matched_terms": [
                    "scene",
                    "personality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Sources and Negative Samples.</span>\nWe curate speech from (i) ground-truth media segments, (ii) retrieved character profiles, and (iii) generated local scene descriptions. To stress-test evaluation models, we construct contrastive mismatches: (i) negative local scenes with varying degrees of mismatch, and (ii) negative character profiles created by re-synthesizing transcripts with TTS systems. Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6\" title=\"Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F</span></a> details the prompt design and TTS generation details.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotators judge whether performances align with the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em>, using rubrics for <em class=\"ltx_emph ltx_font_italic\">Prosodic Dynamics (i.e., Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis)</em>, <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness (with progressive gating over three sub-dimensions: Emotion Accuracy, Emotion Intensity, Emotion Transition)</em>, <em class=\"ltx_emph ltx_font_italic\">Character Consistency (i.e., Voice Identity Matching and Traits Embodiment)</em>, <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance (i.e., Local Scene Fit and Global Story Coherence)</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>. A prompt quality check and confidence ratings are also collected. Full scales and gating details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-shot and Few-shot ALLMs.</span>\nWe first evaluate the zero-shot capabilities of both proprietary and public models, including <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>, <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>, <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio</span>, and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>. All evaluations follow a unified prompting template that provides audio input alongside scene and character context.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Finetuning Model: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>.</span>\nIn addition to zero-shot and few-shot evaluations, we fine-tune a dedicated evaluation model, denoted as <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> for archetype and realism role-play evaluation, respectively.\nWe initialize from <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio-7B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib11\" title=\"\">2024</a>)</cite>, which provides a strong foundation for multi-modal audio-text reasoning.\nTo better leverage the multiple human annotations available in our datasets, we adopt a sampling-based training strategy: at each step, one annotator label is randomly selected, following practices in label distribution learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Geng, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib22\" title=\"\">2016</a>; Wen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib67\" title=\"\">2023</a>)</cite>.\nThis stochastic supervision implicitly exposes the model to the empirical annotation distribution, while during inference we output the full distribution over discrete labels (e.g., <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score, consistent with recent decoding-based regression methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Song &amp; Bahri, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>; Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib70\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib64\" title=\"\">2025b</a>; Lukasik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>. We apply LoRA fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib28\" title=\"\">2022</a>)</cite>. All implementation details, including optimizer settings, training schedule, and sampling policy, are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS3\" title=\"L.3 Fine-tuning Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "score",
                    "during"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data generation:</span> LLMs were employed to expand prompts (e.g., character profiles, scene descriptions) and to produce negative samples in realism evaluation. All outputs were screened by human annotators to ensure validity and reduce bias.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Personality Traits.</span>\nExample:</p>\n\n",
                "matched_terms": [
                    "example",
                    "personality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <em class=\"ltx_emph ltx_font_italic\">Your personality is defined as hot-blooded. Your team is losing a game, and in the final moments you must deliver the decisive score. You say:</em>\n</p>\n\n",
                "matched_terms": [
                    "score",
                    "personality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text generation.</span>\nWe followed a three-step expansion: keyword <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> scene description <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS3.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> full prompt with template. Some examples are shown as follows:</p>\n\n",
                "matched_terms": [
                    "description",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Step 2: Given a character and scene description, the text LLM imagines a candidate caption and generates a corresponding transcript (see below prompts for the text LLM).</p>\n\n",
                "matched_terms": [
                    "description",
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><em class=\"ltx_emph ltx_font_italic\">Example (reject):</em> Prompt: &#8220;Your personality is sharp-tongued. Your friend asks about their new haircut.&#8221; Audio says: &#8220;It looks great on you.&#8221; <math alttext=\"\\Rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I5.i1.I0.i1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8658;</mo><annotation encoding=\"application/x-tex\">\\Rightarrow</annotation></semantics></math> reject.</p>\n\n",
                "matched_terms": [
                    "example",
                    "personality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Retrieved character profiles.</span>\nEach audio segment is paired with a concise character description derived from the media context.\nThe detailed prompt design for generating these character profiles is provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS2\" title=\"F.2 Prompts for Character Profile Creation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "description",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generated local scenes.</span>\nWe supplement retrieved character information with automatically generated local scene descriptions summarizing the surrounding plot or setting.\nAppendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS3\" title=\"F.3 Prompts for Local Scene Generation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.3</span></a> includes representative prompts and generation details.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative local scenes.</span>\nFor each original segment, we generate additional scene descriptions at different levels of mismatch, ranging from subtle alterations to fully unrelated scenarios.\nAppendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS4\" title=\"F.4 Prompts for Negative Scenes &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.4</span></a> outlines the prompting strategies and example outputs.</p>\n\n",
                "matched_terms": [
                    "example",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During annotation, raters are presented with either: (i) ground-truth stories, (ii) stories with negative local scenes, or (iii) stories with negative character profile. Annotators assign realism scores without being exposed to the underlying mismatch reasons. Instead, these reasons are provided only to the quality-assurance team, who use them as reference for internal validation and consistency checks. While the mismatch reasons may not be perfect, they help streamline quality control procedures and maintain reliability across the dataset.</p>\n\n",
                "matched_terms": [
                    "without",
                    "during",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide consistent evaluation context, we generate local scene descriptions summarizing the situational background.\nThese prompts aim to capture the immediate narrative context without introducing extraneous detail.</p>\n\n",
                "matched_terms": [
                    "without",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To make the construction of the realism evaluation task concrete, we present a worked example that begins with a curated local scene and character profile, followed by the same transcript adapted into mismatched contexts. These mismatches appear either at the <em class=\"ltx_emph ltx_font_italic\">local-scene</em> level or at both the <em class=\"ltx_emph ltx_font_italic\">character profile and scene</em> level, thereby providing controlled degradations of realism.</p>\n\n",
                "matched_terms": [
                    "example",
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the character Yunjin from <span class=\"ltx_text ltx_font_italic\">Genshin Impact</span> as a running case study, which is also included in our released dataset.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>As acknowledged, the use of real-world recordings is conducted under legal and ethical review. All curated data will be released as part of our open-source package.</span></span></span> This example illustrates how alignment between a speaker&#8217;s persona, local context, and delivery underpins realism, and how systematically breaking that alignment produces contrastive test cases.</p>\n\n",
                "matched_terms": [
                    "example",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Level 0 (Mild Mismatch).</span>\n<em class=\"ltx_emph ltx_font_italic\">Fake Scene:</em> A quiet teahouse where Yunjin casually chats with friends about recent weather. She suddenly delivers the heroic verses in full operatic style.\n<em class=\"ltx_emph ltx_font_italic\">Reason:</em> The shift from light conversation to intense battle imagery feels abrupt and unsupported. The mismatch is mild but noticeable.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "light"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Level 1 (Moderate Mismatch).</span>\n<em class=\"ltx_emph ltx_font_italic\">Fake Scene:</em> A teahouse with elderly patrons enjoying tea in a calm setting. Yunjin, dressed plainly, begins singing passionately.\n<em class=\"ltx_emph ltx_font_italic\">Reason:</em> The intense tone clashes with the subdued environment, startling listeners. Her heroic lines sound out of place, generating tonal dissonance instead of inspiration.</p>\n\n",
                "matched_terms": [
                    "out",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative Character Profiles.</span>\nWe replace Yunjin with new character profiles and scenes. Although the transcript remains the same, realism degrades because the delivery no longer matches the speaker&#8217;s persona or situation.</p>\n\n",
                "matched_terms": [
                    "situation",
                    "matches",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Level 1 (Moderate Mismatch).</span>\n<em class=\"ltx_emph ltx_font_italic\">Fake Scene:</em> In a futuristic laboratory, a robotic engineer celebrates a successful combat simulation. Overwhelmed with excitement, he blurts out the opera verses.\n<em class=\"ltx_emph ltx_font_italic\">Fake Profile:</em> Xiao&#160;A, a calm and rational roboticist, normally concise and analytical, not prone to poetic expression.\n<em class=\"ltx_emph ltx_font_italic\">Reason:</em> The dramatic, metaphorical verses conflict with his logical persona and sterile environment. The mismatch is sharper: the heroic rhetoric feels uncharacteristic and jarring.</p>\n\n",
                "matched_terms": [
                    "out",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the Yunjin example is drawn from authentic performance, where the character&#8217;s persona (an opera singer) and local scene (a battlefield narration) are naturally aligned through artistic delivery. The realism task does not merely test whether a model can follow a prompt, but whether it can preserve coherence between a speaker&#8217;s global identity, immediate context, and expressive prosody. When Yunjin&#8217;s transcript is transplanted into mismatched settings or paired with incongruent speaker profiles, the breakdown in realism becomes apparent: the same words shift from inspiring to awkward, jarring, or even absurd.</p>\n\n",
                "matched_terms": [
                    "example",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A character profile was first converted into a descriptive voice specification by prompting <span class=\"ltx_text ltx_font_typewriter\">deepseek-v3</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib38\" title=\"\">2024a</a>)</cite>.\nThis description was submitted to the TTS service to generate candidate voices.\nWe then previewed samples via Gemini2.5Pro, selecting the voice that best matched the speaker style of the original recording before proceeding to generate speech.</p>\n\n",
                "matched_terms": [
                    "description",
                    "before",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The annotation interface for <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> focuses on assessing contextual and narrative coherence in model-generated speech.\nUnlike the archetype-oriented evaluation, which emphasizes stylistic and emotional fidelity, this platform examines how well the voice performance aligns with a character&#8217;s personality, dialogue situation, and scene progression.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "situation",
                    "personality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.F3\" title=\"Figure 3 &#8227; F.7 Annotation Platform for Realism Evaluation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the core playback interface.\nAnnotators can preview each audio sample with synchronized spectrogram and waveform displays, while reviewing contextual metadata including the <span class=\"ltx_text ltx_font_italic\">Local Scene</span>, <span class=\"ltx_text ltx_font_italic\">Character Style</span>, and <span class=\"ltx_text ltx_font_italic\">Character Profile</span>.\nThese descriptions help situate the listening task, providing the background necessary for judging realism in the character&#8217;s vocal and behavioral expression.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Read the <span class=\"ltx_text ltx_font_bold\">Character Profile</span> (long-term identity, style) and <span class=\"ltx_text ltx_font_bold\">Local Scene</span> (immediate context, goal/emotion) before listening.</p>\n\n",
                "matched_terms": [
                    "before",
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Do not reward theatricality/technical polish unless it naturally fits the character <em class=\"ltx_emph ltx_font_italic\">and</em> scene.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Read first:</span> Carefully read the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em> to understand the long-term identity and immediate goal/emotion.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identity mismatch:</span> If the clip&#8217;s speaker is clearly not the intended target speaker, assign a score of <span class=\"ltx_text ltx_font_bold\">1</span> for every rating dimension that relies on the Character Profile (e.g., Character Style Traits, Trait Embodiment, Character Consistency).</p>\n\n",
                "matched_terms": [
                    "score",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt contains contradictory information.</span> Conflicts make accurate annotation difficult or impossible. Examples: incompatible accent specifications; Local Scene drastically at odds with intended emotion (see <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>); Local and global scenes inconsistent; traits described as &#8220;evolving over time&#8221; without a timeline.</p>\n\n",
                "matched_terms": [
                    "without",
                    "scene",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt is insufficient to evaluate the voice sample.</span> Audio too short to gauge the dimensions; or the prompt lacks specific characteristics/is too vague or multi-directional to know which version of the character is present. (It is OK if audio does not match transcript exactly; if too much is missing to annotate, mark as insufficient.)</p>\n\n",
                "matched_terms": [
                    "too",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rubric Rating Instructions.</span>\nScore each subcriterion on a 1&#8211;5 scale. Use the detailed rubrics below and the examples to maintain calibration across characters, tones, and emotional styles. Prioritize in-character believability; do not reward theatricality, emotional intensity, or technical polish unless they naturally fit the character <em class=\"ltx_emph ltx_font_italic\">and</em> scene.</p>\n\n",
                "matched_terms": [
                    "score",
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Focus: pitch, rhythm, and emphasis in speech.\n<span class=\"ltx_text ltx_font_bold\">Guideline 1.1 Pitch Variation (check Local Scene, Character Profile, and audio).</span></p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 1.2 Rhythmic Naturalness (check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 1.3 Stress and Emphasis (check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Focus: alignment of expressed emotion with character style and situational demand. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Gating:</span> Only rate 2.2 if 2.1 <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"A7.SS4.SSSx5.p1.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math> 3; only rate 2.3 if 2.2 <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"A7.SS4.SSSx5.p1.m2\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math> 3.</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 2.1 Emotion Accuracy (Required; check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 2.2 Emotion Intensity Control (Only if 2.1 <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"A7.SS4.SSSx5.p3.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math> 3; check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "emotion",
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 2.3 Dynamic Range (Only if 2.2 <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"A7.SS4.SSSx5.p4.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math> 3; check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Understood the character and scene clearly,</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All human annotation in this work was conducted in accordance with standard ethical practices for research involving human annotators. Annotators were recruited via public hiring for Mandarin and internal lab hiring for English. Prior to participation, all annotators were provided with a clear description of the task, the type of data they would encounter, and their rights as participants, including the ability to opt out at any time without penalty.</p>\n\n",
                "matched_terms": [
                    "without",
                    "description",
                    "out"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F8\" title=\"Figure 8 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F9\" title=\"Figure 9 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> visualize the score distributions per dimension.\nWe highlight three patterns:\n(i) <span class=\"ltx_text ltx_font_bold\">Progressive/gated emotion metrics.</span> For <em class=\"ltx_emph ltx_font_italic\">Emotion Intensity</em> and <em class=\"ltx_emph ltx_font_italic\">Emotion Transition</em>, we assign <span class=\"ltx_text ltx_font_typewriter\">1</span> when the scene is marked <em class=\"ltx_emph ltx_font_italic\">N/A</em> (i.e., emotion accuracy is insufficient to meaningfully judge intensity/transition). This produces a visible mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, a sparse bin at <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>, and slightly lower overall agreement; downstream analyses conditioning on adequate <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> (e.g., mean&#160;<math alttext=\"\\geq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 3</annotation></semantics></math>) recover more bell-shaped distributions.\n(ii) <span class=\"ltx_text ltx_font_bold\">Traits Embodiment.</span> Scores tend to avoid the middle (<math alttext=\"\\sim\\!3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!3</annotation></semantics></math>), concentrating on low values when traits clearly mismatch and high values when traits are strongly realized. This yields the lowest inter-rater agreement among dimensions, reflecting the inherently compositional and role-specific nature of trait judgments.\n(iii) <span class=\"ltx_text ltx_font_bold\">Other dimensions.</span> <em class=\"ltx_emph ltx_font_italic\">Pitch Variation</em>, <em class=\"ltx_emph ltx_font_italic\">Rhythmic Naturalness</em>, <em class=\"ltx_emph ltx_font_italic\">Stress &amp; Emphasis</em>, <em class=\"ltx_emph ltx_font_italic\">Voice Identity Matching</em>, <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em>, <em class=\"ltx_emph ltx_font_italic\">Global Story Coherence</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> are approximately unimodal with modes at <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>&#8211;<math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math>, consistent with moderate&#8211;strong realization in the dataset and with the high agreement reported.</p>\n\n",
                "matched_terms": [
                    "score",
                    "scene",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, the consistently high agreement confirms that the rubric isolates perceptually coherent factors, so learned evaluators can target them reliably.\nSecond, the gated behavior of <em class=\"ltx_emph ltx_font_italic\">Intensity/Transition</em> motivates a <em class=\"ltx_emph ltx_font_italic\">hierarchical</em> evaluator: verify <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> before scoring graded expressivity, or model the joint structure with conditional heads.\nThird, <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> remains challenging; models may benefit from explicit character-trait priors, long-term context aggregation, or contrastive supervision between matched and mismatched traits.\nFinally, prosody-focused objectives (e.g., pitch/rhythm/stress control) are likely to yield the largest quality gains for amateur-style speech.</p>\n\n",
                "matched_terms": [
                    "before",
                    "emotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While we utilize various prompts and sample them randomly during the fine-tuning experiments, we provide an example prompt as follows:</p>\n\n",
                "matched_terms": [
                    "during",
                    "example"
                ]
            }
        ]
    },
    "A7.SS4.SSSx5.p3": {
        "caption": "",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text ltx_font_bold\">Score</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\"><span class=\"ltx_text ltx_font_bold\">Description / Example</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Intensity calibrated perfectly (e.g., whispered anger in a tense standoff).</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Mostly appropriate (slight over-/under-shoot).</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Slightly exaggerated or dull.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Overacted or underplayed (e.g., screaming in a reflective moment).</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Wildly off-scale (e.g., hysterical laughter in a death scene).</span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "slight",
            "example",
            "hysterical",
            "slightly",
            "anger",
            "overacted",
            "offscale",
            "death",
            "scene",
            "wildly",
            "appropriate",
            "screaming",
            "tense",
            "mostly",
            "moment",
            "dull",
            "intensity",
            "underplayed",
            "overundershoot",
            "perfectly",
            "score",
            "standoff",
            "laughter",
            "calibrated",
            "description",
            "exaggerated",
            "whispered",
            "reflective"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech role-play.</span>\nWe formalize <em class=\"ltx_emph ltx_font_italic\">speech role-play</em> as a conditional speech generation problem grounded in a character profile and scene description.\nGiven a dialogue context <math alttext=\"{\\bm{C}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{speech}}</annotation></semantics></math>, a character profile <math alttext=\"{\\bm{C}}_{\\text{profile}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>profile</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{profile}}</annotation></semantics></math>, and a scene specification <math alttext=\"{\\bm{C}}_{\\text{scene}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>scene</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{scene}}</annotation></semantics></math>, a <em class=\"ltx_emph ltx_font_italic\">speech foundation model</em> (SFM) generates a role-played speech response <math alttext=\"{\\bm{S}}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119930;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{S}}_{r}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "description",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotators judge whether performances align with the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em>, using rubrics for <em class=\"ltx_emph ltx_font_italic\">Prosodic Dynamics (i.e., Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis)</em>, <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness (with progressive gating over three sub-dimensions: Emotion Accuracy, Emotion Intensity, Emotion Transition)</em>, <em class=\"ltx_emph ltx_font_italic\">Character Consistency (i.e., Voice Identity Matching and Traits Embodiment)</em>, <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance (i.e., Local Scene Fit and Global Story Coherence)</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>. A prompt quality check and confidence ratings are also collected. Full scales and gating details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "intensity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text generation.</span>\nWe followed a three-step expansion: keyword <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> scene description <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS3.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> full prompt with template. Some examples are shown as follows:</p>\n\n",
                "matched_terms": [
                    "description",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Step 2: Given a character and scene description, the text LLM imagines a candidate caption and generates a corresponding transcript (see below prompts for the text LLM).</p>\n\n",
                "matched_terms": [
                    "description",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative local scenes.</span>\nFor each original segment, we generate additional scene descriptions at different levels of mismatch, ranging from subtle alterations to fully unrelated scenarios.\nAppendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS4\" title=\"F.4 Prompts for Negative Scenes &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.4</span></a> outlines the prompting strategies and example outputs.</p>\n\n",
                "matched_terms": [
                    "example",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To make the construction of the realism evaluation task concrete, we present a worked example that begins with a curated local scene and character profile, followed by the same transcript adapted into mismatched contexts. These mismatches appear either at the <em class=\"ltx_emph ltx_font_italic\">local-scene</em> level or at both the <em class=\"ltx_emph ltx_font_italic\">character profile and scene</em> level, thereby providing controlled degradations of realism.</p>\n\n",
                "matched_terms": [
                    "example",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Curated Local Scene.</span>\nYunjin is narrating a fierce battle through operatic singing. Her verses depict a hero charging into enemy lines, slaying the enemy commander, and securing peace. The atmosphere is filled with tension and intensity, highlighting the hero&#8217;s extraordinary bravery and determination.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "intensity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the Yunjin example is drawn from authentic performance, where the character&#8217;s persona (an opera singer) and local scene (a battlefield narration) are naturally aligned through artistic delivery. The realism task does not merely test whether a model can follow a prompt, but whether it can preserve coherence between a speaker&#8217;s global identity, immediate context, and expressive prosody. When Yunjin&#8217;s transcript is transplanted into mismatched settings or paired with incongruent speaker profiles, the breakdown in realism becomes apparent: the same words shift from inspiring to awkward, jarring, or even absurd.</p>\n\n",
                "matched_terms": [
                    "example",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rubric Rating Instructions.</span>\nScore each subcriterion on a 1&#8211;5 scale. Use the detailed rubrics below and the examples to maintain calibration across characters, tones, and emotional styles. Prioritize in-character believability; do not reward theatricality, emotional intensity, or technical polish unless they naturally fit the character <em class=\"ltx_emph ltx_font_italic\">and</em> scene.</p>\n\n",
                "matched_terms": [
                    "score",
                    "scene",
                    "intensity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 2.2 Emotion Intensity Control (Only if 2.1 <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"A7.SS4.SSSx5.p3.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math> 3; check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "scene",
                    "intensity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although realism evaluation targets nuanced paralinguistic behaviors, human annotators achieve <em class=\"ltx_emph ltx_font_italic\">consistently high agreement</em> once the rubric disentangles the aspects to be judged.\nAs shown in the agreement summary in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F7\" title=\"Figure 7 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, most dimensions fall in the <math alttext=\"0.86\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>0.86</mn><annotation encoding=\"application/x-tex\">0.86</annotation></semantics></math>&#8211;<math alttext=\"0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mn>0.88</mn><annotation encoding=\"application/x-tex\">0.88</annotation></semantics></math> range, with the lowest agreement on <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> (<math alttext=\"\\approx 0.83\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.83</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.83</annotation></semantics></math>) and the highest on <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em> / <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> (<math alttext=\"\\approx 0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.88</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.88</annotation></semantics></math>).\nEmotion-related, progressive metrics (Intensity and Transition) are slightly lower (<math alttext=\"\\approx 0.85\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.85</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.85</annotation></semantics></math>), reflecting the added difficulty of judging graded expressivity.</p>\n\n",
                "matched_terms": [
                    "slightly",
                    "scene",
                    "intensity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F8\" title=\"Figure 8 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F9\" title=\"Figure 9 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> visualize the score distributions per dimension.\nWe highlight three patterns:\n(i) <span class=\"ltx_text ltx_font_bold\">Progressive/gated emotion metrics.</span> For <em class=\"ltx_emph ltx_font_italic\">Emotion Intensity</em> and <em class=\"ltx_emph ltx_font_italic\">Emotion Transition</em>, we assign <span class=\"ltx_text ltx_font_typewriter\">1</span> when the scene is marked <em class=\"ltx_emph ltx_font_italic\">N/A</em> (i.e., emotion accuracy is insufficient to meaningfully judge intensity/transition). This produces a visible mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, a sparse bin at <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>, and slightly lower overall agreement; downstream analyses conditioning on adequate <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> (e.g., mean&#160;<math alttext=\"\\geq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 3</annotation></semantics></math>) recover more bell-shaped distributions.\n(ii) <span class=\"ltx_text ltx_font_bold\">Traits Embodiment.</span> Scores tend to avoid the middle (<math alttext=\"\\sim\\!3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!3</annotation></semantics></math>), concentrating on low values when traits clearly mismatch and high values when traits are strongly realized. This yields the lowest inter-rater agreement among dimensions, reflecting the inherently compositional and role-specific nature of trait judgments.\n(iii) <span class=\"ltx_text ltx_font_bold\">Other dimensions.</span> <em class=\"ltx_emph ltx_font_italic\">Pitch Variation</em>, <em class=\"ltx_emph ltx_font_italic\">Rhythmic Naturalness</em>, <em class=\"ltx_emph ltx_font_italic\">Stress &amp; Emphasis</em>, <em class=\"ltx_emph ltx_font_italic\">Voice Identity Matching</em>, <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em>, <em class=\"ltx_emph ltx_font_italic\">Global Story Coherence</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> are approximately unimodal with modes at <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>&#8211;<math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math>, consistent with moderate&#8211;strong realization in the dataset and with the high agreement reported.</p>\n\n",
                "matched_terms": [
                    "score",
                    "slightly",
                    "scene",
                    "intensity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scene and semantics (Local Scene Fit, Global Coherence, Semantic Match).</span> Differences are present but smaller; both groups cluster in the mid&#8211;high range, with professionals exhibiting slightly narrower distributions.</p>\n\n",
                "matched_terms": [
                    "slightly",
                    "scene"
                ]
            }
        ]
    },
    "A7.SS4.SSSx5.p4": {
        "caption": "",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text ltx_font_bold\">Score</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\"><span class=\"ltx_text ltx_font_bold\">Description / Example</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Emotional shifts, if present, are natural and in-character (e.g., mentor calmly grows firmer while warning a student).</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Appropriate shifts present but mild/slow to emerge.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Mostly steady; slight modulation; no strong arc.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Flat/monotone or unnaturally varied with unjustified jumps.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Significant out-of-character fluctuations without triggers.</span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "student",
            "present",
            "natural",
            "while",
            "significant",
            "slight",
            "example",
            "mildslow",
            "jumps",
            "flatmonotone",
            "varied",
            "triggers",
            "strong",
            "emotional",
            "incharacter",
            "appropriate",
            "shifts",
            "mostly",
            "without",
            "unnaturally",
            "warning",
            "calmly",
            "unjustified",
            "mentor",
            "modulation",
            "firmer",
            "score",
            "arc",
            "fluctuations",
            "steady",
            "outofcharacter",
            "description",
            "emerge",
            "grows"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>). The first strategy follows a <em class=\"ltx_emph ltx_font_italic\">top-down</em> design, inspired by prior text-based role-play benchmarks.\nHere, role-play is judged with respect to <em class=\"ltx_emph ltx_font_italic\">stereotypical archetypes</em> (e.g., &#8220;firefighter&#8221; or &#8220;ER doctor&#8221;), using scene contexts (e.g., &#8220;comfort a child trapped in fire&#8221; or &#8220;ask for help in a serious operation&#8221;).\nThis approach provides accessible, general-purpose scoring that is scalable to a wide range of scenarios, making it suitable for large-scale benchmarking.\nHowever, its reliance on broad stereotypes and global impressions limits its ability to capture fine-grained aspects of delivery (e.g., subtle prosodic shifts, nuanced emotional control)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Holt &amp; Lotto, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib26\" title=\"\">2010</a>)</cite>.\nIn practice, this strategy mirrors simplified annotation settings and is particularly useful when annotation resources are limited&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib21\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "shifts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS5\" title=\"F.5 Examples of Realism Role-play &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.5</span></a>). The second strategy adopts a <em class=\"ltx_emph ltx_font_italic\">bottom-up</em> design, motivated by real-world performance and real human speech.\nInstead of relying solely on stereotypes, this approach grounds evaluation in recordings from professional and non-professional speakers, drawn from realistic dialogue and media sources.\nJudgments in this setting are more fine-grained and multi-dimensional, addressing aspects such as prosodic dynamics, emotional fidelity, character consistency, and contextual fit.\nWhile this strategy requires more human annotation and careful curation, it provides benchmarks that reflect how role-play is perceived in practice.\nIt thus offers a richer framework for analyzing model outputs in settings where realism and subtlety are essential.</p>\n\n",
                "matched_terms": [
                    "while",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real-world Recoding.</span>\nAs noted in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S2\" title=\"2 Related Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, most prior efforts have relied heavily on synthetic data, mostly from TTS systems based on curated professional role-play voices. While such data can achieve high fidelity, it often lacks the diversity and spontaneity of real-world delivery, limiting the generalizability of evaluation models. To address this gap, we expand realism evaluation beyond synthetic and curated voices by incorporating newly recorded speech from both professional and amateur voice actors. This design introduces a broader spectrum of delivery styles, from polished professional acting to more natural and less controlled amateur performance.</p>\n\n",
                "matched_terms": [
                    "mostly",
                    "while",
                    "natural"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation Data Summary.</span>\nThe resulting realism evaluation dataset is constructed from a combination of curated media sources, including internal collections, public datasets (e.g., NCSSD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib40\" title=\"\">2024b</a>)</cite>), and original recordings collected for this project.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>All data were obtained under appropriate licenses and are legally available for research release.</span></span></span> We compile 15k annotated role-play samples: 9k in Mandarin and 5k in English. The dataset is divided into 12k samples for training, 2k mixed samples for the base test set, and 1k real-recording samples for the test set. The first test set supports general evaluation using our proposed pipeline with negative samples, while the second focuses exclusively on human speech performed by both professional and amateur voice actors. For benchmarking, 2,000 scenarios from the realism dataset are designated for realism role-play benchmarking. Detailed statistics and analyses are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11\" title=\"Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">K</span></a>.</p>\n\n",
                "matched_terms": [
                    "while",
                    "appropriate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Finetuning Model: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>.</span>\nIn addition to zero-shot and few-shot evaluations, we fine-tune a dedicated evaluation model, denoted as <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> for archetype and realism role-play evaluation, respectively.\nWe initialize from <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio-7B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib11\" title=\"\">2024</a>)</cite>, which provides a strong foundation for multi-modal audio-text reasoning.\nTo better leverage the multiple human annotations available in our datasets, we adopt a sampling-based training strategy: at each step, one annotator label is randomly selected, following practices in label distribution learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Geng, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib22\" title=\"\">2016</a>; Wen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib67\" title=\"\">2023</a>)</cite>.\nThis stochastic supervision implicitly exposes the model to the empirical annotation distribution, while during inference we output the full distribution over discrete labels (e.g., <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score, consistent with recent decoding-based regression methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Song &amp; Bahri, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>; Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib70\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib64\" title=\"\">2025b</a>; Lukasik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>. We apply LoRA fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib28\" title=\"\">2022</a>)</cite>. All implementation details, including optimizer settings, training schedule, and sampling policy, are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS3\" title=\"L.3 Fine-tuning Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "strong",
                    "score",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype Evaluation.</span>\nThe results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T2\" title=\"Table 2 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reveal clear distinctions across evaluation settings. In the zero-shot condition, proprietary ALLMs such as <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> substantially outperform public models, achieving strong <span class=\"ltx_text ltx_font_italic\">content pass</span> accuracy (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>90%) and moderate correlations with other dimensions(<math alttext=\"0.43\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mn>0.43</mn><annotation encoding=\"application/x-tex\">0.43</annotation></semantics></math>&#8211;<math alttext=\"0.48\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mn>0.48</mn><annotation encoding=\"application/x-tex\">0.48</annotation></semantics></math>). By contrast, public models exhibit limited predictive ability, with correlations often below 0.2. Few-shot prompting provides modest gains for proprietary models, improving both accuracy and correlation (e.g., <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> rises from 0.480 to 0.493 average correlation). The most significant improvement arises from finetuning: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves an average correlation of 0.629, marking a large margin over zero-shot/few-shot models and demonstrating the effectiveness of task-specific supervision.</p>\n\n",
                "matched_terms": [
                    "strong",
                    "significant"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All data collection and annotation in this work followed strict ethical standards. Our bilingual datasets were constructed exclusively from legally accessible sources, including licensed media, open-source corpora, and original recordings conducted with informed consent. Annotators were recruited with appropriate compensation and provided with clear instructions, including the right to withdraw at any point. To minimize risks, no personally identifiable information was included in the released data, and all recordings were anonymized before annotation. We ensured cultural and linguistic diversity in annotator pools to mitigate potential bias across Mandarin and English role-play scenarios. While this study did not require formal IRB approval, we adhered to best practices in human-subject research, including transparency in task design and careful consideration of annotator well-being. Full details of our annotation protocol and ethical safeguards are presented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A9\" title=\"Appendix I Annotation Ethics &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</p>\n\n",
                "matched_terms": [
                    "while",
                    "appropriate"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, real human ground-truth segments and systematically constructed negatives form a balanced dataset that supports realism-oriented evaluation.\nThe positives provide natural anchors of role-play performance, while the negatives introduce controlled mismatches that test whether evaluation models can distinguish real human speech from synthetic approximations.\nThis design ensures that realism evaluation emphasizes subtle paralinguistic and contextual cues, moving beyond intelligibility or content fidelity.</p>\n\n",
                "matched_terms": [
                    "while",
                    "natural"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During annotation, raters are presented with either: (i) ground-truth stories, (ii) stories with negative local scenes, or (iii) stories with negative character profile. Annotators assign realism scores without being exposed to the underlying mismatch reasons. Instead, these reasons are provided only to the quality-assurance team, who use them as reference for internal validation and consistency checks. While the mismatch reasons may not be perfect, they help streamline quality control procedures and maintain reliability across the dataset.</p>\n\n",
                "matched_terms": [
                    "without",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To make the construction of the realism evaluation task concrete, we present a worked example that begins with a curated local scene and character profile, followed by the same transcript adapted into mismatched contexts. These mismatches appear either at the <em class=\"ltx_emph ltx_font_italic\">local-scene</em> level or at both the <em class=\"ltx_emph ltx_font_italic\">character profile and scene</em> level, thereby providing controlled degradations of realism.</p>\n\n",
                "matched_terms": [
                    "present",
                    "example"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rubric Rating Instructions.</span>\nScore each subcriterion on a 1&#8211;5 scale. Use the detailed rubrics below and the examples to maintain calibration across characters, tones, and emotional styles. Prioritize in-character believability; do not reward theatricality, emotional intensity, or technical polish unless they naturally fit the character <em class=\"ltx_emph ltx_font_italic\">and</em> scene.</p>\n\n",
                "matched_terms": [
                    "score",
                    "emotional",
                    "incharacter"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All human annotation in this work was conducted in accordance with standard ethical practices for research involving human annotators. Annotators were recruited via public hiring for Mandarin and internal lab hiring for English. Prior to participation, all annotators were provided with a clear description of the task, the type of data they would encounter, and their rights as participants, including the ability to opt out at any time without penalty.</p>\n\n",
                "matched_terms": [
                    "without",
                    "description"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A critical observation is that annotations in Mandarin and English cannot be directly compared. The annotation groups for the two languages differed in their linguistic backgrounds and cultural interpretations, which leads to systematic differences in scoring. Our bilingual pilot further confirmed that annotators often interpret role-play appropriateness and expressiveness differently depending on their cultural frame of reference. For example, Mandarin raters tend to reward delivery styles that align with culturally expected prosody and politeness strategies, while English raters prioritize spontaneity and naturalness. These discrepancies parallel the findings of the singing voice conversion challenge (SVCC2023) from <cite class=\"ltx_cite ltx_citemacro_citet\">Huang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib31\" title=\"\">2023</a>)</cite>, where native and non-native listeners provided significantly different ratings for the same samples, particularly for naturalness and speaker similarity. Consequently, while absolute scores differ substantially across Mandarin and English, only within-language comparisons are meaningful. Cross-lingual ranking is not valid given the different annotation bases.</p>\n\n",
                "matched_terms": [
                    "while",
                    "example"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The picture shifts in English, elaborated in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Here, <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> emerges as the most effective model by a wide margin, achieving top scores in both <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> (3.46) and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> (3.49), as well as high content pass rates (0.97). This reflects its strong grounding in English discourse norms and role adherence. In contrast, <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> variants perform well in <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> but are weaker in role alignment, highlighting a trade-off between acoustic fidelity and role-play embodiment. Other models such as <span class=\"ltx_text ltx_font_typewriter\">Doubao</span>, <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span>, and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> achieve moderate results, while <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> trail far behind, showing limited robustness in English scenarios. Taken together, the English results reveal both a sharper separation across models and a wider variance in appropriateness scores compared to Mandarin.</p>\n\n",
                "matched_terms": [
                    "strong",
                    "while",
                    "shifts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In sum, archetype evaluation reveals three major findings:\n(1) Mandarin and English annotations cannot be directly compared, due to linguistic and cultural annotation inconsistencies, consistent with prior evidence from SVCC 2023.\n(2) Within each language, clear model rankings emerge, <span class=\"ltx_text ltx_font_typewriter\">Doubao</span> dominates in Mandarin, while <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> excels in English.\n(3) Domain sensitivity remains a challenge, particularly for imaginative or culturally rich roles, where scores are substantially lower and inter-model variance is higher.</p>\n\n",
                "matched_terms": [
                    "while",
                    "emerge"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identity and traits (Voice Identity, Traits Embodiment).</span> Professionals present clearer peaks in upper bins, while amateurs are broader or bimodal, reflecting inconsistent character anchoring.</p>\n\n",
                "matched_terms": [
                    "while",
                    "present"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While we utilize various prompts and sample them randomly during the fine-tuning experiments, we provide an example prompt as follows:</p>\n\n",
                "matched_terms": [
                    "while",
                    "example"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the archetype test set, all fine-tuned variants achieve strong <span class=\"ltx_text ltx_font_italic\">Content Pass</span> accuracy above 92%. Differences appear mainly in correlation-based metrics. Here, <span class=\"ltx_text ltx_font_typewriter\">Base-FT</span> yields the highest score on <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> delivers the strongest performance in <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> and the best overall average (0.629). This suggests that instruction-tuned checkpoints paired with parameter-efficient adaptation can better capture higher-level perceptual traits.</p>\n\n",
                "matched_terms": [
                    "strong",
                    "score",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Third, while DRAME-Eval substantially outperforms zero-shot and few-shot ALLMs, its reliance on fine-tuning from a strong base model raises questions of accessibility and scalability. Future work should explore lightweight adaptation techniques, semi-supervised training with partially annotated data, and leveraging emergent capabilities of open-source multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "strong",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Looking forward, we see several promising research directions. We intentionally restrict evaluation to single-turn responses for clarity and tractability. However, multi-turn extensions that capture narrative progression, long-term consistency, and dynamic emotion shifts remain a natural next step. Incorporating multilingual and cross-cultural datasets will enhance generality and fairness. On the modeling side, exploring hybrid evaluation strategies that combine discrete rubrics with continuous perceptual signals (e.g., prosody embeddings or affective measures) may better bridge the gap between automatic and human judgments. Finally, integrating Speech-DRAME into reinforcement learning pipelines as a reward model could help drive the next generation of expressive, human-aligned speech role-play systems.</p>\n\n",
                "matched_terms": [
                    "shifts",
                    "natural"
                ]
            }
        ]
    },
    "A7.SS4.SSSx6.p2": {
        "caption": "",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text ltx_font_bold\">Score</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\"><span class=\"ltx_text ltx_font_bold\">Description / Example</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Voice clearly matches the expected identity (e.g., youthful, curious tone for energetic sidekick).</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Acceptable match with minor mismatches.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Neutral/not character-specific.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Somewhat inconsistent (e.g., deep voice for high-pitched character).</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Clearly mismatched identity (e.g., gritty adult voice for bubbly child).</span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "youthful",
            "identity",
            "example",
            "neutralnot",
            "inconsistent",
            "bubbly",
            "mismatches",
            "adult",
            "highpitched",
            "match",
            "voice",
            "character",
            "somewhat",
            "characterspecific",
            "curious",
            "energetic",
            "minor",
            "deep",
            "tone",
            "acceptable",
            "matches",
            "mismatched",
            "score",
            "child",
            "gritty",
            "description",
            "clearly",
            "sidekick",
            "expected"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-based role-play evaluation.</span>\nEarly research focused on text-only dialogue benchmarks. Datasets such as RoleLLM, CharacterBox, and RoleMRC <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib62\" title=\"\">2024</a>; Tu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib60\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib61\" title=\"\">2025a</a>; Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib53\" title=\"\">2023</a>; Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib41\" title=\"\">2025</a>)</cite> test whether models can sustain persona consistency and character fidelity, using LLM-as-judge pipelines for scalability. However, studies like PersonaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib81\" title=\"\">2025a</a>)</cite> have shown that LLM-as-judge approaches are far from perfect, struggling with tasks as basic as tracking speaker identity. This highlights broader concerns about reliability even in text settings, which become only more severe when extended to speech, where prosody, emotion, and vocal style play a central role.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-native benchmarks.</span>\nMore recently, a number of resources have been introduced to evaluate spoken role-play agents. OmniCharacter-10K <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>)</cite> emphasizes character voice traits, while others&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib39\" title=\"\">2025</a>; Hou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib27\" title=\"\">2025</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib15\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib7\" title=\"\">2024b</a>)</cite> extend evaluation toward vocal communication, assistant capability, and multi-turn realism. Omni models such as StepAudio and KimiAudio <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite> also include role-play scenarios in their evaluation. SpeechRole and VStyle <cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>; Zhan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib76\" title=\"\">2025</a>)</cite> are closer to our work, offering datasets that test interaction ability, expressiveness, and fidelity. However, these resources are primarily built from LLM-generated dialogues and TTS outputs, with limited human validation and a heavy reliance on proprietary ALLM-as-judge pipelines. As a result, they lack grounding in diverse human speech and typically apply only coarse rubrics.</p>\n\n",
                "matched_terms": [
                    "character",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech role-play.</span>\nWe formalize <em class=\"ltx_emph ltx_font_italic\">speech role-play</em> as a conditional speech generation problem grounded in a character profile and scene description.\nGiven a dialogue context <math alttext=\"{\\bm{C}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{speech}}</annotation></semantics></math>, a character profile <math alttext=\"{\\bm{C}}_{\\text{profile}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>profile</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{profile}}</annotation></semantics></math>, and a scene specification <math alttext=\"{\\bm{C}}_{\\text{scene}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>scene</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{scene}}</annotation></semantics></math>, a <em class=\"ltx_emph ltx_font_italic\">speech foundation model</em> (SFM) generates a role-played speech response <math alttext=\"{\\bm{S}}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119930;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{S}}_{r}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "description",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Sources and Negative Samples.</span>\nWe curate speech from (i) ground-truth media segments, (ii) retrieved character profiles, and (iii) generated local scene descriptions. To stress-test evaluation models, we construct contrastive mismatches: (i) negative local scenes with varying degrees of mismatch, and (ii) negative character profiles created by re-synthesizing transcripts with TTS systems. Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6\" title=\"Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F</span></a> details the prompt design and TTS generation details.</p>\n\n",
                "matched_terms": [
                    "mismatches",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotators judge whether performances align with the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em>, using rubrics for <em class=\"ltx_emph ltx_font_italic\">Prosodic Dynamics (i.e., Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis)</em>, <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness (with progressive gating over three sub-dimensions: Emotion Accuracy, Emotion Intensity, Emotion Transition)</em>, <em class=\"ltx_emph ltx_font_italic\">Character Consistency (i.e., Voice Identity Matching and Traits Embodiment)</em>, <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance (i.e., Local Scene Fit and Global Story Coherence)</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>. A prompt quality check and confidence ratings are also collected. Full scales and gating details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character",
                    "match",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real Recording Test Set.</span>\nThe real-recording evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T4\" title=\"Table 4 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> is notably more challenging, with all models suffering substantial performance drops. Proprietary models in the zero-shot setting achieve only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.04&#8211;0.08 average correlation, and public models often fall below zero. Few-shot prompting yields small but noticeable improvements, especially for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> (0.226 avg.), suggesting that exemplars partially mitigate domain mismatch between synthetic and real human speech. Interestingly, while finetuning achieves the highest overall average (0.247), its relative advantage over few-shot prompting is narrower than in the synthetic setting, and some dimensions (e.g., <span class=\"ltx_text ltx_font_italic\">Character Identity</span>) remain weak. We emphasize that this observed gap between synthetic and real recordings is not merely an artifact but an intended feature of Speech-DRAME, designed to expose the mismatch and drive future research into bridging synthetic-to-real generalization.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Role dimensions.</span>\nWe operationalize archetypes through five principal categories, each representing a stereotypical dimension of role identity or behavior, including (1) <span class=\"ltx_text ltx_font_bold\">Occupation / Functional Role (Modern Daily Life)</span> (e.g., a firefighter rescuing a trapped civilian). (2) <span class=\"ltx_text ltx_font_bold\">Occupation / Functional Role (Fictional or Fantasy)</span> (e.g., a martial arts cult leader confronting orthodox rivals). (3)&#160;<span class=\"ltx_text ltx_font_bold\">Social Identity</span> (e.g., a doting parent worrying about a child, or a &#8220;Buddhist youth&#8221; eschewing competition). (4) <span class=\"ltx_text ltx_font_bold\">Named / Specific Characters</span> (e.g., Sun Wukong mocking celestial authorities in <em class=\"ltx_emph ltx_font_italic\">Journey to the West</em>). (5) <span class=\"ltx_text ltx_font_bold\">Personality Traits</span> (e.g., &#8220;hot-blooded&#8221; in a competitive setting, or &#8220;complaining&#8221; in response to poor service). (6) <span class=\"ltx_text ltx_font_bold\">Relative Roles</span> (e.g., parent or stranger).</p>\n\n",
                "matched_terms": [
                    "identity",
                    "child"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Social Identity.</span> Example:</p>\n\n",
                "matched_terms": [
                    "identity",
                    "example"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Step 2: Given a character and scene description, the text LLM imagines a candidate caption and generates a corresponding transcript (see below prompts for the text LLM).</p>\n\n",
                "matched_terms": [
                    "description",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Retrieved character profiles.</span>\nEach audio segment is paired with a concise character description derived from the media context.\nThe detailed prompt design for generating these character profiles is provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS2\" title=\"F.2 Prompts for Character Profile Creation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "description",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative character profiles.</span>\nWe also create role-profile mismatches by resynthesizing transcripts with diverse TTS systems, thereby breaking the link between authentic delivery and intended role.\nWe explore both text-instruction-based prompting and comparisons across commercial and non-commercial TTS frameworks.\nDetailed model lists and parameter settings are documented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS6\" title=\"F.6 TTS Systems and Settings &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.6</span></a>.</p>\n\n",
                "matched_terms": [
                    "mismatches",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To make the construction of the realism evaluation task concrete, we present a worked example that begins with a curated local scene and character profile, followed by the same transcript adapted into mismatched contexts. These mismatches appear either at the <em class=\"ltx_emph ltx_font_italic\">local-scene</em> level or at both the <em class=\"ltx_emph ltx_font_italic\">character profile and scene</em> level, thereby providing controlled degradations of realism.</p>\n\n",
                "matched_terms": [
                    "mismatches",
                    "mismatched",
                    "example",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the character Yunjin from <span class=\"ltx_text ltx_font_italic\">Genshin Impact</span> as a running case study, which is also included in our released dataset.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>As acknowledged, the use of real-world recordings is conducted under legal and ethical review. All curated data will be released as part of our open-source package.</span></span></span> This example illustrates how alignment between a speaker&#8217;s persona, local context, and delivery underpins realism, and how systematically breaking that alignment produces contrastive test cases.</p>\n\n",
                "matched_terms": [
                    "example",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Character Profile.</span>\nYunjin is a passionate opera performer with deep knowledge of traditional singing art. Her lyrics vividly portray battlefield struggles and heroic deeds, conveying admiration for those who stand firm against overwhelming odds. She always seeks to inspire her audience with courage, justice, and hope, channeling her artistry into both entertainment and moral elevation.</p>\n\n",
                "matched_terms": [
                    "deep",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative Character Profiles.</span>\nWe replace Yunjin with new character profiles and scenes. Although the transcript remains the same, realism degrades because the delivery no longer matches the speaker&#8217;s persona or situation.</p>\n\n",
                "matched_terms": [
                    "matches",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the Yunjin example is drawn from authentic performance, where the character&#8217;s persona (an opera singer) and local scene (a battlefield narration) are naturally aligned through artistic delivery. The realism task does not merely test whether a model can follow a prompt, but whether it can preserve coherence between a speaker&#8217;s global identity, immediate context, and expressive prosody. When Yunjin&#8217;s transcript is transplanted into mismatched settings or paired with incongruent speaker profiles, the breakdown in realism becomes apparent: the same words shift from inspiring to awkward, jarring, or even absurd.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "mismatched",
                    "example"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A character profile was first converted into a descriptive voice specification by prompting <span class=\"ltx_text ltx_font_typewriter\">deepseek-v3</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib38\" title=\"\">2024a</a>)</cite>.\nThis description was submitted to the TTS service to generate candidate voices.\nWe then previewed samples via Gemini2.5Pro, selecting the voice that best matched the speaker style of the original recording before proceeding to generate speech.</p>\n\n",
                "matched_terms": [
                    "description",
                    "character",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Read the <span class=\"ltx_text ltx_font_bold\">Character Profile</span> (long-term identity, style) and <span class=\"ltx_text ltx_font_bold\">Local Scene</span> (immediate context, goal/emotion) before listening.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the audio&#8217;s speaker is <em class=\"ltx_emph ltx_font_italic\">clearly not</em> the intended target identity, assign a score of <span class=\"ltx_text ltx_font_bold\">1</span> to all Profile-dependent dimensions (e.g., Voice Identity Matching, Trait Embodiment) and proceed with other dimensions as applicable.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "score",
                    "clearly",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Character Consistency</span> (Voice Identity Matching, Trait Embodiment): alignment with Character Profile.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Read first:</span> Carefully read the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em> to understand the long-term identity and immediate goal/emotion.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Use both sources:</span> Play the audio and skim the transcript together to assess delivery quality, expression, and alignment with character intent (do not penalize transcript mismatches).</p>\n\n",
                "matched_terms": [
                    "mismatches",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identity mismatch:</span> If the clip&#8217;s speaker is clearly not the intended target speaker, assign a score of <span class=\"ltx_text ltx_font_bold\">1</span> for every rating dimension that relies on the Character Profile (e.g., Character Style Traits, Trait Embodiment, Character Consistency).</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character",
                    "score",
                    "clearly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt contains contradictory information.</span> Conflicts make accurate annotation difficult or impossible. Examples: incompatible accent specifications; Local Scene drastically at odds with intended emotion (see <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>); Local and global scenes inconsistent; traits described as &#8220;evolving over time&#8221; without a timeline.</p>\n\n",
                "matched_terms": [
                    "match",
                    "inconsistent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt is insufficient to evaluate the voice sample.</span> Audio too short to gauge the dimensions; or the prompt lacks specific characteristics/is too vague or multi-directional to know which version of the character is present. (It is OK if audio does not match transcript exactly; if too much is missing to annotate, mark as insufficient.)</p>\n\n",
                "matched_terms": [
                    "character",
                    "match",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Question 2: Observed Character Style Traits.</span> Based on the Character Profile, are the defined Style Traits clearly present in the delivery?</p>\n\n",
                "matched_terms": [
                    "clearly",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rubric Rating Instructions.</span>\nScore each subcriterion on a 1&#8211;5 scale. Use the detailed rubrics below and the examples to maintain calibration across characters, tones, and emotional styles. Prioritize in-character believability; do not reward theatricality, emotional intensity, or technical polish unless they naturally fit the character <em class=\"ltx_emph ltx_font_italic\">and</em> scene.</p>\n\n",
                "matched_terms": [
                    "score",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Focus: how well the voice and language align with the character&#8217;s core identity.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 3.1 Voice Identity Matching (Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Understood the character and scene clearly,</p>\n\n",
                "matched_terms": [
                    "clearly",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A critical observation is that annotations in Mandarin and English cannot be directly compared. The annotation groups for the two languages differed in their linguistic backgrounds and cultural interpretations, which leads to systematic differences in scoring. Our bilingual pilot further confirmed that annotators often interpret role-play appropriateness and expressiveness differently depending on their cultural frame of reference. For example, Mandarin raters tend to reward delivery styles that align with culturally expected prosody and politeness strategies, while English raters prioritize spontaneity and naturalness. These discrepancies parallel the findings of the singing voice conversion challenge (SVCC2023) from <cite class=\"ltx_cite ltx_citemacro_citet\">Huang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib31\" title=\"\">2023</a>)</cite>, where native and non-native listeners provided significantly different ratings for the same samples, particularly for naturalness and speaker similarity. Consequently, while absolute scores differ substantially across Mandarin and English, only within-language comparisons are meaningful. Cross-lingual ranking is not valid given the different annotation bases.</p>\n\n",
                "matched_terms": [
                    "expected",
                    "example",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F8\" title=\"Figure 8 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F9\" title=\"Figure 9 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> visualize the score distributions per dimension.\nWe highlight three patterns:\n(i) <span class=\"ltx_text ltx_font_bold\">Progressive/gated emotion metrics.</span> For <em class=\"ltx_emph ltx_font_italic\">Emotion Intensity</em> and <em class=\"ltx_emph ltx_font_italic\">Emotion Transition</em>, we assign <span class=\"ltx_text ltx_font_typewriter\">1</span> when the scene is marked <em class=\"ltx_emph ltx_font_italic\">N/A</em> (i.e., emotion accuracy is insufficient to meaningfully judge intensity/transition). This produces a visible mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, a sparse bin at <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>, and slightly lower overall agreement; downstream analyses conditioning on adequate <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> (e.g., mean&#160;<math alttext=\"\\geq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 3</annotation></semantics></math>) recover more bell-shaped distributions.\n(ii) <span class=\"ltx_text ltx_font_bold\">Traits Embodiment.</span> Scores tend to avoid the middle (<math alttext=\"\\sim\\!3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!3</annotation></semantics></math>), concentrating on low values when traits clearly mismatch and high values when traits are strongly realized. This yields the lowest inter-rater agreement among dimensions, reflecting the inherently compositional and role-specific nature of trait judgments.\n(iii) <span class=\"ltx_text ltx_font_bold\">Other dimensions.</span> <em class=\"ltx_emph ltx_font_italic\">Pitch Variation</em>, <em class=\"ltx_emph ltx_font_italic\">Rhythmic Naturalness</em>, <em class=\"ltx_emph ltx_font_italic\">Stress &amp; Emphasis</em>, <em class=\"ltx_emph ltx_font_italic\">Voice Identity Matching</em>, <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em>, <em class=\"ltx_emph ltx_font_italic\">Global Story Coherence</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> are approximately unimodal with modes at <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>&#8211;<math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math>, consistent with moderate&#8211;strong realization in the dataset and with the high agreement reported.</p>\n\n",
                "matched_terms": [
                    "score",
                    "identity",
                    "match",
                    "voice",
                    "clearly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identity and traits (Voice Identity, Traits Embodiment).</span> Professionals present clearer peaks in upper bins, while amateurs are broader or bimodal, reflecting inconsistent character anchoring.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character",
                    "inconsistent",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For realism evaluation on the basic test set, <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> consistently achieves the strongest correlations across most dimensions, especially for <span class=\"ltx_text ltx_font_italic\">Character Identity</span> and <span class=\"ltx_text ltx_font_italic\">Traits</span>, leading to the best average correlation (0.668). <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> also performs competitively (0.630), while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> lags slightly behind (0.625). These results highlight that full fine-tuning may still offer an advantage when the evaluation set is relatively homogeneous and closer to the training distribution.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character"
                ]
            }
        ]
    },
    "A7.SS4.SSSx6.p3": {
        "caption": "",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text ltx_font_bold\">Score</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\"><span class=\"ltx_text ltx_font_bold\">Description / Example</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Defining traits subtly but clearly conveyed (e.g., insecure character hesitates/lowers voice).</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Most traits conveyed; minor inconsistencies.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Traits weak but not disruptive.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Missing or conflicting traits (e.g., arrogant tone for a humble character).</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">No identifiable traits or completely off.</span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "example",
            "completely",
            "not",
            "conflicting",
            "defining",
            "arrogant",
            "most",
            "character",
            "voice",
            "conveyed",
            "inconsistencies",
            "hesitateslowers",
            "subtly",
            "off",
            "minor",
            "tone",
            "missing",
            "identifiable",
            "score",
            "insecure",
            "weak",
            "humble",
            "description",
            "clearly",
            "traits",
            "disruptive"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-native benchmarks.</span>\nMore recently, a number of resources have been introduced to evaluate spoken role-play agents. OmniCharacter-10K <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>)</cite> emphasizes character voice traits, while others&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib39\" title=\"\">2025</a>; Hou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib27\" title=\"\">2025</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib15\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib7\" title=\"\">2024b</a>)</cite> extend evaluation toward vocal communication, assistant capability, and multi-turn realism. Omni models such as StepAudio and KimiAudio <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite> also include role-play scenarios in their evaluation. SpeechRole and VStyle <cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>; Zhan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib76\" title=\"\">2025</a>)</cite> are closer to our work, offering datasets that test interaction ability, expressiveness, and fidelity. However, these resources are primarily built from LLM-generated dialogues and TTS outputs, with limited human validation and a heavy reliance on proprietary ALLM-as-judge pipelines. As a result, they lack grounding in diverse human speech and typically apply only coarse rubrics.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "voice",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech role-play.</span>\nWe formalize <em class=\"ltx_emph ltx_font_italic\">speech role-play</em> as a conditional speech generation problem grounded in a character profile and scene description.\nGiven a dialogue context <math alttext=\"{\\bm{C}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{speech}}</annotation></semantics></math>, a character profile <math alttext=\"{\\bm{C}}_{\\text{profile}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>profile</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{profile}}</annotation></semantics></math>, and a scene specification <math alttext=\"{\\bm{C}}_{\\text{scene}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>scene</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{scene}}</annotation></semantics></math>, a <em class=\"ltx_emph ltx_font_italic\">speech foundation model</em> (SFM) generates a role-played speech response <math alttext=\"{\\bm{S}}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119930;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{S}}_{r}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "description",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Role and Scene Dimensions.</span>\nRoles are operationalized through seven categories: (1) functional occupations in daily life, (2) functional occupations in fictional or fantasy settings, (3) social identities, (4) named or specific characters, (5) personality traits, (6) relative relationship, (7) basic events. Scenes are anchored by everyday events or state changes that evoke stereotypical reactions. Details of template design and alternative prompting strategies are given in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>We note that some archetypes (e.g., firefighter) inevitably reflect cultural stereotypes. These are used solely for evaluation scalability and do not reflect normative judgments.</span></span></span></p>\n\n",
                "matched_terms": [
                    "traits",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotators judge whether performances align with the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em>, using rubrics for <em class=\"ltx_emph ltx_font_italic\">Prosodic Dynamics (i.e., Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis)</em>, <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness (with progressive gating over three sub-dimensions: Emotion Accuracy, Emotion Intensity, Emotion Transition)</em>, <em class=\"ltx_emph ltx_font_italic\">Character Consistency (i.e., Voice Identity Matching and Traits Embodiment)</em>, <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance (i.e., Local Scene Fit and Global Story Coherence)</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>. A prompt quality check and confidence ratings are also collected. Full scales and gating details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "voice",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real-world Recoding.</span>\nAs noted in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S2\" title=\"2 Related Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, most prior efforts have relied heavily on synthetic data, mostly from TTS systems based on curated professional role-play voices. While such data can achieve high fidelity, it often lacks the diversity and spontaneity of real-world delivery, limiting the generalizability of evaluation models. To address this gap, we expand realism evaluation beyond synthetic and curated voices by incorporating newly recorded speech from both professional and amateur voice actors. This design introduces a broader spectrum of delivery styles, from polished professional acting to more natural and less controlled amateur performance.</p>\n\n",
                "matched_terms": [
                    "most",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For proprietary models, we report the average of five independent runs to reduce variance and obtain more stable predictions. For open-source models, we observed weaker instruction-following ability and limited adaptation to unfamiliar tasks. To mitigate this, we did not request simultaneous prediction of multiple metrics. Instead, we queried one metric at a time. Following <cite class=\"ltx_cite ltx_citemacro_citet\">Song &amp; Bahri (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>); Lukasik et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>, we interpret model outputs through token distribution over discrete labels (<span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score.</p>\n\n",
                "matched_terms": [
                    "score",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T3\" title=\"Table 3 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> extends the analysis to fine-grained realism evaluation. Similar to archetype results, proprietary ALLMs lead the zero-shot setting, with <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> obtaining an average correlation of 0.390, compared to 0.284 for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>. Public models remain weak baselines. Few-shot prompting consistently improves performance: <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> gains +0.042 and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> gains +0.083 over their zero-shot averages, showing the benefit of in-context supervision. Once again, finetuning is most effective: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves 0.625 average correlation, higher than the best few-shot results. Importantly, the gains are consistent across all 10 realism dimensions, confirming that the sampling&#8211;expectation training strategy generalizes beyond simple classification to nuanced paralinguistic cues.</p>\n\n",
                "matched_terms": [
                    "weak",
                    "most"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real Recording Test Set.</span>\nThe real-recording evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T4\" title=\"Table 4 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> is notably more challenging, with all models suffering substantial performance drops. Proprietary models in the zero-shot setting achieve only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.04&#8211;0.08 average correlation, and public models often fall below zero. Few-shot prompting yields small but noticeable improvements, especially for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> (0.226 avg.), suggesting that exemplars partially mitigate domain mismatch between synthetic and real human speech. Interestingly, while finetuning achieves the highest overall average (0.247), its relative advantage over few-shot prompting is narrower than in the synthetic setting, and some dimensions (e.g., <span class=\"ltx_text ltx_font_italic\">Character Identity</span>) remain weak. We emphasize that this observed gap between synthetic and real recordings is not merely an artifact but an intended feature of Speech-DRAME, designed to expose the mismatch and drive future research into bridging synthetic-to-real generalization.</p>\n\n",
                "matched_terms": [
                    "weak",
                    "not",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism evaluation.</span>\nRealism role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T6\" title=\"Table 6 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) presents a different landscape. Here, performance differences between end-to-end and cascaded systems are less pronounced. Several end-to-end models, such as <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>, reach average scores of 3.24, matching or even exceeding the best cascaded pipelines. Importantly, realism scores are more evenly distributed across prosodic, emotional, character, and contextual dimensions, with most models clustering between 2.9&#8211;3.2. This convergence reflects the difficulty of realism-style evaluation: since the data preparation emphasized contrast with authentic human speech, models are judged under similar conditions, leading to smaller observed gaps.</p>\n\n",
                "matched_terms": [
                    "most",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All data collection and annotation in this work followed strict ethical standards. Our bilingual datasets were constructed exclusively from legally accessible sources, including licensed media, open-source corpora, and original recordings conducted with informed consent. Annotators were recruited with appropriate compensation and provided with clear instructions, including the right to withdraw at any point. To minimize risks, no personally identifiable information was included in the released data, and all recordings were anonymized before annotation. We ensured cultural and linguistic diversity in annotator pools to mitigate potential bias across Mandarin and English role-play scenarios. While this study did not require formal IRB approval, we adhered to best practices in human-subject research, including transparency in task design and careful consideration of annotator well-being. Full details of our annotation protocol and ethical safeguards are presented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A9\" title=\"Appendix I Annotation Ethics &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</p>\n\n",
                "matched_terms": [
                    "not",
                    "identifiable"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Personality Traits.</span>\nExample:</p>\n\n",
                "matched_terms": [
                    "traits",
                    "example"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Step 2: Given a character and scene description, the text LLM imagines a candidate caption and generates a corresponding transcript (see below prompts for the text LLM).</p>\n\n",
                "matched_terms": [
                    "description",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Instruction Failure.</span> Generated content does not adhere to the trait(s) or role described in the prompt (&#8220;question&#8221;).</p>\n\n",
                "matched_terms": [
                    "traits",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">Appropriateness</span>: comment for <em class=\"ltx_emph ltx_font_italic\">all</em> scores 1&#8211;5. Explain <em class=\"ltx_emph ltx_font_italic\">why</em> the <em class=\"ltx_emph ltx_font_italic\">delivery</em> was/was not a good match to the in-context traits (role &amp; scene).</p>\n\n",
                "matched_terms": [
                    "traits",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If any portion of the clip is ill-fitting, prefer the lower overall score; do not average across good and bad segments.</p>\n\n",
                "matched_terms": [
                    "score",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Retrieved character profiles.</span>\nEach audio segment is paired with a concise character description derived from the media context.\nThe detailed prompt design for generating these character profiles is provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS2\" title=\"F.2 Prompts for Character Profile Creation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "description",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During annotation, raters are presented with either: (i) ground-truth stories, (ii) stories with negative local scenes, or (iii) stories with negative character profile. Annotators assign realism scores without being exposed to the underlying mismatch reasons. Instead, these reasons are provided only to the quality-assurance team, who use them as reference for internal validation and consistency checks. While the mismatch reasons may not be perfect, they help streamline quality control procedures and maintain reliability across the dataset.</p>\n\n",
                "matched_terms": [
                    "not",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To make the construction of the realism evaluation task concrete, we present a worked example that begins with a curated local scene and character profile, followed by the same transcript adapted into mismatched contexts. These mismatches appear either at the <em class=\"ltx_emph ltx_font_italic\">local-scene</em> level or at both the <em class=\"ltx_emph ltx_font_italic\">character profile and scene</em> level, thereby providing controlled degradations of realism.</p>\n\n",
                "matched_terms": [
                    "example",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the character Yunjin from <span class=\"ltx_text ltx_font_italic\">Genshin Impact</span> as a running case study, which is also included in our released dataset.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>As acknowledged, the use of real-world recordings is conducted under legal and ethical review. All curated data will be released as part of our open-source package.</span></span></span> This example illustrates how alignment between a speaker&#8217;s persona, local context, and delivery underpins realism, and how systematically breaking that alignment produces contrastive test cases.</p>\n\n",
                "matched_terms": [
                    "example",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the Yunjin example is drawn from authentic performance, where the character&#8217;s persona (an opera singer) and local scene (a battlefield narration) are naturally aligned through artistic delivery. The realism task does not merely test whether a model can follow a prompt, but whether it can preserve coherence between a speaker&#8217;s global identity, immediate context, and expressive prosody. When Yunjin&#8217;s transcript is transplanted into mismatched settings or paired with incongruent speaker profiles, the breakdown in realism becomes apparent: the same words shift from inspiring to awkward, jarring, or even absurd.</p>\n\n",
                "matched_terms": [
                    "not",
                    "example"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A character profile was first converted into a descriptive voice specification by prompting <span class=\"ltx_text ltx_font_typewriter\">deepseek-v3</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib38\" title=\"\">2024a</a>)</cite>.\nThis description was submitted to the TTS service to generate candidate voices.\nWe then previewed samples via Gemini2.5Pro, selecting the voice that best matched the speaker style of the original recording before proceeding to generate speech.</p>\n\n",
                "matched_terms": [
                    "description",
                    "voice",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the audio&#8217;s speaker is <em class=\"ltx_emph ltx_font_italic\">clearly not</em> the intended target identity, assign a score of <span class=\"ltx_text ltx_font_bold\">1</span> to all Profile-dependent dimensions (e.g., Voice Identity Matching, Trait Embodiment) and proceed with other dimensions as applicable.</p>\n\n",
                "matched_terms": [
                    "score",
                    "clearly",
                    "not",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the prompt information is <span class=\"ltx_text ltx_font_bold\">contradictory</span> or <span class=\"ltx_text ltx_font_bold\">insufficient</span> to evaluate (e.g., audio too short, traits mutually incompatible), mark the prompt quality accordingly and <em class=\"ltx_emph ltx_font_italic\">do not</em> force granular ratings that cannot be judged.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Character Consistency</span> (Voice Identity Matching, Trait Embodiment): alignment with Character Profile.</p>\n\n",
                "matched_terms": [
                    "character",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Do not reward theatricality/technical polish unless it naturally fits the character <em class=\"ltx_emph ltx_font_italic\">and</em> scene.</p>\n\n",
                "matched_terms": [
                    "not",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Use both sources:</span> Play the audio and skim the transcript together to assess delivery quality, expression, and alignment with character intent (do not penalize transcript mismatches).</p>\n\n",
                "matched_terms": [
                    "not",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identity mismatch:</span> If the clip&#8217;s speaker is clearly not the intended target speaker, assign a score of <span class=\"ltx_text ltx_font_bold\">1</span> for every rating dimension that relies on the Character Profile (e.g., Character Style Traits, Trait Embodiment, Character Consistency).</p>\n\n",
                "matched_terms": [
                    "score",
                    "character",
                    "clearly",
                    "traits",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt is insufficient to evaluate the voice sample.</span> Audio too short to gauge the dimensions; or the prompt lacks specific characteristics/is too vague or multi-directional to know which version of the character is present. (It is OK if audio does not match transcript exactly; if too much is missing to annotate, mark as insufficient.)</p>\n\n",
                "matched_terms": [
                    "voice",
                    "missing",
                    "not",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Question 2: Observed Character Style Traits.</span> Based on the Character Profile, are the defined Style Traits clearly present in the delivery?</p>\n\n",
                "matched_terms": [
                    "traits",
                    "clearly",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Delete incorrect pre-populated traits; you do not need to replace them. If you delete many traits, <em class=\"ltx_emph ltx_font_italic\">Trait Embodiment</em> should likely be low.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rubric Rating Instructions.</span>\nScore each subcriterion on a 1&#8211;5 scale. Use the detailed rubrics below and the examples to maintain calibration across characters, tones, and emotional styles. Prioritize in-character believability; do not reward theatricality, emotional intensity, or technical polish unless they naturally fit the character <em class=\"ltx_emph ltx_font_italic\">and</em> scene.</p>\n\n",
                "matched_terms": [
                    "score",
                    "not",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 3.1 Voice Identity Matching (Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "character",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Understood the character and scene clearly,</p>\n\n",
                "matched_terms": [
                    "clearly",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Auditability and SEM alignment.</span>\nEvery score is traceable to a rubric with short rater rationales and confidence.\nWe then tune SEMs to these labels (rather than opaque judge outputs), enabling calibration studies and error analyses that would not be possible with black-box LLM judges.</p>\n\n",
                "matched_terms": [
                    "score",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A critical observation is that annotations in Mandarin and English cannot be directly compared. The annotation groups for the two languages differed in their linguistic backgrounds and cultural interpretations, which leads to systematic differences in scoring. Our bilingual pilot further confirmed that annotators often interpret role-play appropriateness and expressiveness differently depending on their cultural frame of reference. For example, Mandarin raters tend to reward delivery styles that align with culturally expected prosody and politeness strategies, while English raters prioritize spontaneity and naturalness. These discrepancies parallel the findings of the singing voice conversion challenge (SVCC2023) from <cite class=\"ltx_cite ltx_citemacro_citet\">Huang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib31\" title=\"\">2023</a>)</cite>, where native and non-native listeners provided significantly different ratings for the same samples, particularly for naturalness and speaker similarity. Consequently, while absolute scores differ substantially across Mandarin and English, only within-language comparisons are meaningful. Cross-lingual ranking is not valid given the different annotation bases.</p>\n\n",
                "matched_terms": [
                    "not",
                    "example",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although cross-lingual comparisons are invalid, this does not diminish the utility of archetype evaluation for role-play benchmarking. In fact, the observed inconsistencies reinforce a central point: successful role-play requires models to adapt to the cultural and linguistic norms of the target language community. A system that performs well in Mandarin but poorly in English cannot be considered universally capable, since end users will evaluate appropriateness and naturalness according to their native expectations. Thus, archetype evaluation highlights not only model-specific strengths and weaknesses but also the need for culturally grounded evaluation pipelines.</p>\n\n",
                "matched_terms": [
                    "inconsistencies",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although realism evaluation targets nuanced paralinguistic behaviors, human annotators achieve <em class=\"ltx_emph ltx_font_italic\">consistently high agreement</em> once the rubric disentangles the aspects to be judged.\nAs shown in the agreement summary in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F7\" title=\"Figure 7 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, most dimensions fall in the <math alttext=\"0.86\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>0.86</mn><annotation encoding=\"application/x-tex\">0.86</annotation></semantics></math>&#8211;<math alttext=\"0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mn>0.88</mn><annotation encoding=\"application/x-tex\">0.88</annotation></semantics></math> range, with the lowest agreement on <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> (<math alttext=\"\\approx 0.83\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.83</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.83</annotation></semantics></math>) and the highest on <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em> / <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> (<math alttext=\"\\approx 0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.88</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.88</annotation></semantics></math>).\nEmotion-related, progressive metrics (Intensity and Transition) are slightly lower (<math alttext=\"\\approx 0.85\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.85</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.85</annotation></semantics></math>), reflecting the added difficulty of judging graded expressivity.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "most"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F8\" title=\"Figure 8 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F9\" title=\"Figure 9 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> visualize the score distributions per dimension.\nWe highlight three patterns:\n(i) <span class=\"ltx_text ltx_font_bold\">Progressive/gated emotion metrics.</span> For <em class=\"ltx_emph ltx_font_italic\">Emotion Intensity</em> and <em class=\"ltx_emph ltx_font_italic\">Emotion Transition</em>, we assign <span class=\"ltx_text ltx_font_typewriter\">1</span> when the scene is marked <em class=\"ltx_emph ltx_font_italic\">N/A</em> (i.e., emotion accuracy is insufficient to meaningfully judge intensity/transition). This produces a visible mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, a sparse bin at <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>, and slightly lower overall agreement; downstream analyses conditioning on adequate <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> (e.g., mean&#160;<math alttext=\"\\geq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 3</annotation></semantics></math>) recover more bell-shaped distributions.\n(ii) <span class=\"ltx_text ltx_font_bold\">Traits Embodiment.</span> Scores tend to avoid the middle (<math alttext=\"\\sim\\!3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!3</annotation></semantics></math>), concentrating on low values when traits clearly mismatch and high values when traits are strongly realized. This yields the lowest inter-rater agreement among dimensions, reflecting the inherently compositional and role-specific nature of trait judgments.\n(iii) <span class=\"ltx_text ltx_font_bold\">Other dimensions.</span> <em class=\"ltx_emph ltx_font_italic\">Pitch Variation</em>, <em class=\"ltx_emph ltx_font_italic\">Rhythmic Naturalness</em>, <em class=\"ltx_emph ltx_font_italic\">Stress &amp; Emphasis</em>, <em class=\"ltx_emph ltx_font_italic\">Voice Identity Matching</em>, <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em>, <em class=\"ltx_emph ltx_font_italic\">Global Story Coherence</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> are approximately unimodal with modes at <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>&#8211;<math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math>, consistent with moderate&#8211;strong realization in the dataset and with the high agreement reported.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "score",
                    "clearly",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identity and traits (Voice Identity, Traits Embodiment).</span> Professionals present clearer peaks in upper bins, while amateurs are broader or bimodal, reflecting inconsistent character anchoring.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "character",
                    "voice"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and few-shot experiments on commercial models such as <span class=\"ltx_text ltx_font_typewriter\">Gemini</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o</span>, we report results as the average of five independent runs to reduce randomness and obtain more stable predictions. In contrast, for open-source models, we observed that their instruction-following ability is relatively weak. To mitigate this, we prompt them to predict one dimension at a time rather than all dimensions jointly. Specifically, for each metric, after showing the rubric prompt, we request completion in a fixed template and extract the probability distribution over the discrete tokens <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>. The final score is then computed from these probabilities, which we found makes open-source models usable to some extent under this setting.</p>\n\n",
                "matched_terms": [
                    "weak",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Scaling the number of demonstrations.\nIncreasing the few-shot set size from three to six and nine further boosts performance (0.459 and 0.469, respectively). The improvements are most pronounced in prosodic and emotional categories (e.g., <em class=\"ltx_emph ltx_font_italic\">Pitch</em>, <em class=\"ltx_emph ltx_font_italic\">Accuracy</em>, <em class=\"ltx_emph ltx_font_italic\">Intensity</em>), suggesting that more examples provide stronger guidance in modeling subtle paralinguistic cues. Gains in <em class=\"ltx_emph ltx_font_italic\">Character Consistency</em> and <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance</em> are also noticeable, although smaller in magnitude, which may indicate that these higher-level judgments are less sensitive to prompt scaling once a few demonstrations are provided.</p>\n\n",
                "matched_terms": [
                    "most",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the archetype test set, all fine-tuned variants achieve strong <span class=\"ltx_text ltx_font_italic\">Content Pass</span> accuracy above 92%. Differences appear mainly in correlation-based metrics. Here, <span class=\"ltx_text ltx_font_typewriter\">Base-FT</span> yields the highest score on <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> delivers the strongest performance in <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> and the best overall average (0.629). This suggests that instruction-tuned checkpoints paired with parameter-efficient adaptation can better capture higher-level perceptual traits.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For realism evaluation on the basic test set, <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> consistently achieves the strongest correlations across most dimensions, especially for <span class=\"ltx_text ltx_font_italic\">Character Identity</span> and <span class=\"ltx_text ltx_font_italic\">Traits</span>, leading to the best average correlation (0.668). <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> also performs competitively (0.630), while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> lags slightly behind (0.625). These results highlight that full fine-tuning may still offer an advantage when the evaluation set is relatively homogeneous and closer to the training distribution.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "most",
                    "character"
                ]
            }
        ]
    },
    "A7.SS4.SSSx7.p2": {
        "caption": "",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text ltx_font_bold\">Score</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\"><span class=\"ltx_text ltx_font_bold\">Description / Example</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Matches the scene&#8217;s emotional/narrative context (e.g., urgent whisper during stealth).</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Mostly coherent with minor mismatch (e.g., calm tone while others panic).</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Neutral: neither reinforces nor harms the scene.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Confusing or slightly inappropriate tone (e.g., laughing during a tense standoff).</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Directly contradicts scene intent (e.g., casual joke while a character is dying).</span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "while",
            "laughing",
            "urgent",
            "example",
            "inappropriate",
            "emotionalnarrative",
            "slightly",
            "panic",
            "context",
            "confusing",
            "directly",
            "character",
            "neither",
            "joke",
            "scene",
            "calm",
            "contradicts",
            "dying",
            "coherent",
            "stealth",
            "minor",
            "mostly",
            "tense",
            "tone",
            "reinforces",
            "scenes",
            "intent",
            "others",
            "casual",
            "matches",
            "mismatch",
            "harms",
            "score",
            "standoff",
            "during",
            "nor",
            "description",
            "neutral",
            "whisper"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-native benchmarks.</span>\nMore recently, a number of resources have been introduced to evaluate spoken role-play agents. OmniCharacter-10K <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>)</cite> emphasizes character voice traits, while others&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib39\" title=\"\">2025</a>; Hou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib27\" title=\"\">2025</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib15\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib7\" title=\"\">2024b</a>)</cite> extend evaluation toward vocal communication, assistant capability, and multi-turn realism. Omni models such as StepAudio and KimiAudio <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite> also include role-play scenarios in their evaluation. SpeechRole and VStyle <cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>; Zhan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib76\" title=\"\">2025</a>)</cite> are closer to our work, offering datasets that test interaction ability, expressiveness, and fidelity. However, these resources are primarily built from LLM-generated dialogues and TTS outputs, with limited human validation and a heavy reliance on proprietary ALLM-as-judge pipelines. As a result, they lack grounding in diverse human speech and typically apply only coarse rubrics.</p>\n\n",
                "matched_terms": [
                    "others",
                    "while",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech role-play.</span>\nWe formalize <em class=\"ltx_emph ltx_font_italic\">speech role-play</em> as a conditional speech generation problem grounded in a character profile and scene description.\nGiven a dialogue context <math alttext=\"{\\bm{C}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{speech}}</annotation></semantics></math>, a character profile <math alttext=\"{\\bm{C}}_{\\text{profile}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>profile</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{profile}}</annotation></semantics></math>, and a scene specification <math alttext=\"{\\bm{C}}_{\\text{scene}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>scene</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{scene}}</annotation></semantics></math>, a <em class=\"ltx_emph ltx_font_italic\">speech foundation model</em> (SFM) generates a role-played speech response <math alttext=\"{\\bm{S}}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119930;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{S}}_{r}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "description",
                    "context",
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here <math alttext=\"{\\bm{S}}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#119930;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{S}}_{r}</annotation></semantics></math> represents the spoken output (waveform or latent representation). This definition captures both the linguistic dimension (coherence with context and scene) and the paralinguistic dimension (prosody, style, emotion) expected of the role.</p>\n\n",
                "matched_terms": [
                    "context",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS5\" title=\"F.5 Examples of Realism Role-play &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.5</span></a>). The second strategy adopts a <em class=\"ltx_emph ltx_font_italic\">bottom-up</em> design, motivated by real-world performance and real human speech.\nInstead of relying solely on stereotypes, this approach grounds evaluation in recordings from professional and non-professional speakers, drawn from realistic dialogue and media sources.\nJudgments in this setting are more fine-grained and multi-dimensional, addressing aspects such as prosodic dynamics, emotional fidelity, character consistency, and contextual fit.\nWhile this strategy requires more human annotation and careful curation, it provides benchmarks that reflect how role-play is perceived in practice.\nIt thus offers a richer framework for analyzing model outputs in settings where realism and subtlety are essential.</p>\n\n",
                "matched_terms": [
                    "while",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Sources and Negative Samples.</span>\nWe curate speech from (i) ground-truth media segments, (ii) retrieved character profiles, and (iii) generated local scene descriptions. To stress-test evaluation models, we construct contrastive mismatches: (i) negative local scenes with varying degrees of mismatch, and (ii) negative character profiles created by re-synthesizing transcripts with TTS systems. Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6\" title=\"Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F</span></a> details the prompt design and TTS generation details.</p>\n\n",
                "matched_terms": [
                    "mismatch",
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotators judge whether performances align with the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em>, using rubrics for <em class=\"ltx_emph ltx_font_italic\">Prosodic Dynamics (i.e., Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis)</em>, <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness (with progressive gating over three sub-dimensions: Emotion Accuracy, Emotion Intensity, Emotion Transition)</em>, <em class=\"ltx_emph ltx_font_italic\">Character Consistency (i.e., Voice Identity Matching and Traits Embodiment)</em>, <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance (i.e., Local Scene Fit and Global Story Coherence)</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>. A prompt quality check and confidence ratings are also collected. Full scales and gating details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real-world Recoding.</span>\nAs noted in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S2\" title=\"2 Related Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, most prior efforts have relied heavily on synthetic data, mostly from TTS systems based on curated professional role-play voices. While such data can achieve high fidelity, it often lacks the diversity and spontaneity of real-world delivery, limiting the generalizability of evaluation models. To address this gap, we expand realism evaluation beyond synthetic and curated voices by incorporating newly recorded speech from both professional and amateur voice actors. This design introduces a broader spectrum of delivery styles, from polished professional acting to more natural and less controlled amateur performance.</p>\n\n",
                "matched_terms": [
                    "mostly",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-shot and Few-shot ALLMs.</span>\nWe first evaluate the zero-shot capabilities of both proprietary and public models, including <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>, <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>, <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio</span>, and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>. All evaluations follow a unified prompting template that provides audio input alongside scene and character context.</p>\n\n",
                "matched_terms": [
                    "context",
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Finetuning Model: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>.</span>\nIn addition to zero-shot and few-shot evaluations, we fine-tune a dedicated evaluation model, denoted as <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> for archetype and realism role-play evaluation, respectively.\nWe initialize from <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio-7B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib11\" title=\"\">2024</a>)</cite>, which provides a strong foundation for multi-modal audio-text reasoning.\nTo better leverage the multiple human annotations available in our datasets, we adopt a sampling-based training strategy: at each step, one annotator label is randomly selected, following practices in label distribution learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Geng, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib22\" title=\"\">2016</a>; Wen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib67\" title=\"\">2023</a>)</cite>.\nThis stochastic supervision implicitly exposes the model to the empirical annotation distribution, while during inference we output the full distribution over discrete labels (e.g., <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score, consistent with recent decoding-based regression methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Song &amp; Bahri, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>; Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib70\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib64\" title=\"\">2025b</a>; Lukasik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>. We apply LoRA fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib28\" title=\"\">2022</a>)</cite>. All implementation details, including optimizer settings, training schedule, and sampling policy, are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS3\" title=\"L.3 Fine-tuning Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "score",
                    "while",
                    "during"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real Recording Test Set.</span>\nThe real-recording evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T4\" title=\"Table 4 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> is notably more challenging, with all models suffering substantial performance drops. Proprietary models in the zero-shot setting achieve only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.04&#8211;0.08 average correlation, and public models often fall below zero. Few-shot prompting yields small but noticeable improvements, especially for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> (0.226 avg.), suggesting that exemplars partially mitigate domain mismatch between synthetic and real human speech. Interestingly, while finetuning achieves the highest overall average (0.247), its relative advantage over few-shot prompting is narrower than in the synthetic setting, and some dimensions (e.g., <span class=\"ltx_text ltx_font_italic\">Character Identity</span>) remain weak. We emphasize that this observed gap between synthetic and real recordings is not merely an artifact but an intended feature of Speech-DRAME, designed to expose the mismatch and drive future research into bridging synthetic-to-real generalization.</p>\n\n",
                "matched_terms": [
                    "mismatch",
                    "while",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data generation:</span> LLMs were employed to expand prompts (e.g., character profiles, scene descriptions) and to produce negative samples in realism evaluation. All outputs were screened by human annotators to ensure validity and reduce bias.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multi-turn generation.</span>\nConsider a dialogue of <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> turns, with context <math alttext=\"\\{{\\bm{C}}_{\\text{speech}}^{(t)}\\}_{t=1}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m2\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>&#119914;</mi><mtext>speech</mtext><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\{{\\bm{C}}_{\\text{speech}}^{(t)}\\}_{t=1}^{T}</annotation></semantics></math>, profile <math alttext=\"{\\bm{C}}_{\\text{profile}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m3\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>profile</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{profile}}</annotation></semantics></math>, and scene <math alttext=\"{\\bm{C}}_{\\text{scene}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m4\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>scene</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{scene}}</annotation></semantics></math>.\nAt each turn <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m5\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, the SFM produces</p>\n\n",
                "matched_terms": [
                    "context",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Pipeline overview.</span>\nAt a high level, archetype evaluation data are generated through a three-stage pipeline:\n(1) selecting keywords that specify role and scene;\n(2) expanding them into full prompts via structured templates; and\n(3) rendering prompts into speech with contemporary TTS systems.\nThis modular pipeline ensures both scalability and reproducibility, while keeping prompts vivid enough for reliable judgments. (4) Based on the generated data (paired scene and synthesized speech), expert human annotators are recruited for detailed annotation on semantic check (i.e., <span class=\"ltx_text ltx_font_italic\">Content Pass</span>) and three different spoken role-play dimensions (i.e., <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human-likeness</span>, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>).\nImplementation details, including template design, alternative prompting strategies, and annotation guidelines, are provided in the following subsections of this Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "while",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text generation.</span>\nWe followed a three-step expansion: keyword <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> scene description <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS3.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> full prompt with template. Some examples are shown as follows:</p>\n\n",
                "matched_terms": [
                    "description",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Step 2: Given a character and scene description, the text LLM imagines a candidate caption and generates a corresponding transcript (see below prompts for the text LLM).</p>\n\n",
                "matched_terms": [
                    "description",
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Appropriateness (Context Considered):</span> tonal fit of the vocal performance to the described role and scene.</p>\n\n",
                "matched_terms": [
                    "context",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Retrieved character profiles.</span>\nEach audio segment is paired with a concise character description derived from the media context.\nThe detailed prompt design for generating these character profiles is provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS2\" title=\"F.2 Prompts for Character Profile Creation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "description",
                    "context",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generated local scenes.</span>\nWe supplement retrieved character information with automatically generated local scene descriptions summarizing the surrounding plot or setting.\nAppendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS3\" title=\"F.3 Prompts for Local Scene Generation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.3</span></a> includes representative prompts and generation details.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative sample generation.</span>\nTo stress-test evaluation models, we construct contrastive negative examples that deliberately mismatch role and scene information:</p>\n\n",
                "matched_terms": [
                    "mismatch",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative local scenes.</span>\nFor each original segment, we generate additional scene descriptions at different levels of mismatch, ranging from subtle alterations to fully unrelated scenarios.\nAppendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS4\" title=\"F.4 Prompts for Negative Scenes &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.4</span></a> outlines the prompting strategies and example outputs.</p>\n\n",
                "matched_terms": [
                    "mismatch",
                    "example",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During annotation, raters are presented with either: (i) ground-truth stories, (ii) stories with negative local scenes, or (iii) stories with negative character profile. Annotators assign realism scores without being exposed to the underlying mismatch reasons. Instead, these reasons are provided only to the quality-assurance team, who use them as reference for internal validation and consistency checks. While the mismatch reasons may not be perfect, they help streamline quality control procedures and maintain reliability across the dataset.</p>\n\n",
                "matched_terms": [
                    "mismatch",
                    "while",
                    "during",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide consistent evaluation context, we generate local scene descriptions summarizing the situational background.\nThese prompts aim to capture the immediate narrative context without introducing extraneous detail.</p>\n\n",
                "matched_terms": [
                    "context",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To make the construction of the realism evaluation task concrete, we present a worked example that begins with a curated local scene and character profile, followed by the same transcript adapted into mismatched contexts. These mismatches appear either at the <em class=\"ltx_emph ltx_font_italic\">local-scene</em> level or at both the <em class=\"ltx_emph ltx_font_italic\">character profile and scene</em> level, thereby providing controlled degradations of realism.</p>\n\n",
                "matched_terms": [
                    "example",
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the character Yunjin from <span class=\"ltx_text ltx_font_italic\">Genshin Impact</span> as a running case study, which is also included in our released dataset.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>As acknowledged, the use of real-world recordings is conducted under legal and ethical review. All curated data will be released as part of our open-source package.</span></span></span> This example illustrates how alignment between a speaker&#8217;s persona, local context, and delivery underpins realism, and how systematically breaking that alignment produces contrastive test cases.</p>\n\n",
                "matched_terms": [
                    "character",
                    "example",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Level 0 (Mild Mismatch).</span>\n<em class=\"ltx_emph ltx_font_italic\">Fake Scene:</em> A quiet teahouse where Yunjin casually chats with friends about recent weather. She suddenly delivers the heroic verses in full operatic style.\n<em class=\"ltx_emph ltx_font_italic\">Reason:</em> The shift from light conversation to intense battle imagery feels abrupt and unsupported. The mismatch is mild but noticeable.</p>\n\n",
                "matched_terms": [
                    "mismatch",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Level 1 (Moderate Mismatch).</span>\n<em class=\"ltx_emph ltx_font_italic\">Fake Scene:</em> A teahouse with elderly patrons enjoying tea in a calm setting. Yunjin, dressed plainly, begins singing passionately.\n<em class=\"ltx_emph ltx_font_italic\">Reason:</em> The intense tone clashes with the subdued environment, startling listeners. Her heroic lines sound out of place, generating tonal dissonance instead of inspiration.</p>\n\n",
                "matched_terms": [
                    "calm",
                    "tone",
                    "mismatch",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Level 2 (Severe Mismatch).</span>\n<em class=\"ltx_emph ltx_font_italic\">Fake Scene:</em> A modern caf&#233; with soft jazz music and casual chatter. Yunjin sips a latte while reading a magazine, then recites the battle verses.\n<em class=\"ltx_emph ltx_font_italic\">Reason:</em> The dramatic imagery of warriors and swords is wholly incompatible with the relaxed, contemporary caf&#233; ambience. The mismatch is extreme, rendering the delivery absurd.</p>\n\n",
                "matched_terms": [
                    "mismatch",
                    "while",
                    "casual",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative Character Profiles.</span>\nWe replace Yunjin with new character profiles and scenes. Although the transcript remains the same, realism degrades because the delivery no longer matches the speaker&#8217;s persona or situation.</p>\n\n",
                "matched_terms": [
                    "matches",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Level 0 (Mild Mismatch).</span>\n<em class=\"ltx_emph ltx_font_italic\">Fake Scene:</em> At a modern tech company&#8217;s annual party, a shy programmer is asked to perform. Under pressure, he recites opera lines he once saw online.\n<em class=\"ltx_emph ltx_font_italic\">Fake Profile:</em> Zhang Wei, an introverted software engineer with little artistic background, usually immersed in code and algorithms.\n<em class=\"ltx_emph ltx_font_italic\">Reason:</em> The heroic verses sound alien coming from someone unfamiliar with opera or traditional culture. The clash produces awkward humor but still feels like a coping attempt.</p>\n\n",
                "matched_terms": [
                    "mismatch",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Level 1 (Moderate Mismatch).</span>\n<em class=\"ltx_emph ltx_font_italic\">Fake Scene:</em> In a futuristic laboratory, a robotic engineer celebrates a successful combat simulation. Overwhelmed with excitement, he blurts out the opera verses.\n<em class=\"ltx_emph ltx_font_italic\">Fake Profile:</em> Xiao&#160;A, a calm and rational roboticist, normally concise and analytical, not prone to poetic expression.\n<em class=\"ltx_emph ltx_font_italic\">Reason:</em> The dramatic, metaphorical verses conflict with his logical persona and sterile environment. The mismatch is sharper: the heroic rhetoric feels uncharacteristic and jarring.</p>\n\n",
                "matched_terms": [
                    "calm",
                    "mismatch",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Level 2 (Severe Mismatch).</span>\n<em class=\"ltx_emph ltx_font_italic\">Fake Scene:</em> On a modern city street, a rebellious street performer in jeans and a rock-band T-shirt strums an electric guitar, suddenly shouting the verses in parody.\n<em class=\"ltx_emph ltx_font_italic\">Fake Profile:</em> Mark, an anti-traditionalist performer who mocks classical art and heroic ideals, favoring satire and irony.\n<em class=\"ltx_emph ltx_font_italic\">Reason:</em> The verses, originally meant to inspire reverence, become ironic and absurd when delivered by someone who rejects those values. The clash is extreme, generating intentional ridicule and total loss of realism.</p>\n\n",
                "matched_terms": [
                    "mismatch",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the Yunjin example is drawn from authentic performance, where the character&#8217;s persona (an opera singer) and local scene (a battlefield narration) are naturally aligned through artistic delivery. The realism task does not merely test whether a model can follow a prompt, but whether it can preserve coherence between a speaker&#8217;s global identity, immediate context, and expressive prosody. When Yunjin&#8217;s transcript is transplanted into mismatched settings or paired with incongruent speaker profiles, the breakdown in realism becomes apparent: the same words shift from inspiring to awkward, jarring, or even absurd.</p>\n\n",
                "matched_terms": [
                    "example",
                    "scene",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A character profile was first converted into a descriptive voice specification by prompting <span class=\"ltx_text ltx_font_typewriter\">deepseek-v3</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib38\" title=\"\">2024a</a>)</cite>.\nThis description was submitted to the TTS service to generate candidate voices.\nWe then previewed samples via Gemini2.5Pro, selecting the voice that best matched the speaker style of the original recording before proceeding to generate speech.</p>\n\n",
                "matched_terms": [
                    "description",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.F3\" title=\"Figure 3 &#8227; F.7 Annotation Platform for Realism Evaluation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the core playback interface.\nAnnotators can preview each audio sample with synchronized spectrogram and waveform displays, while reviewing contextual metadata including the <span class=\"ltx_text ltx_font_italic\">Local Scene</span>, <span class=\"ltx_text ltx_font_italic\">Character Style</span>, and <span class=\"ltx_text ltx_font_italic\">Character Profile</span>.\nThese descriptions help situate the listening task, providing the background necessary for judging realism in the character&#8217;s vocal and behavioral expression.</p>\n\n",
                "matched_terms": [
                    "while",
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Read the <span class=\"ltx_text ltx_font_bold\">Character Profile</span> (long-term identity, style) and <span class=\"ltx_text ltx_font_bold\">Local Scene</span> (immediate context, goal/emotion) before listening.</p>\n\n",
                "matched_terms": [
                    "context",
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Do not reward theatricality/technical polish unless it naturally fits the character <em class=\"ltx_emph ltx_font_italic\">and</em> scene.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Read first:</span> Carefully read the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em> to understand the long-term identity and immediate goal/emotion.</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Use both sources:</span> Play the audio and skim the transcript together to assess delivery quality, expression, and alignment with character intent (do not penalize transcript mismatches).</p>\n\n",
                "matched_terms": [
                    "intent",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Target speaker only:</span> If multiple speakers are present, evaluate only the target character&#8217;s lines (consider others only as context for turn-taking).</p>\n\n",
                "matched_terms": [
                    "others",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identity mismatch:</span> If the clip&#8217;s speaker is clearly not the intended target speaker, assign a score of <span class=\"ltx_text ltx_font_bold\">1</span> for every rating dimension that relies on the Character Profile (e.g., Character Style Traits, Trait Embodiment, Character Consistency).</p>\n\n",
                "matched_terms": [
                    "score",
                    "mismatch",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rubric Rating Instructions.</span>\nScore each subcriterion on a 1&#8211;5 scale. Use the detailed rubrics below and the examples to maintain calibration across characters, tones, and emotional styles. Prioritize in-character believability; do not reward theatricality, emotional intensity, or technical polish unless they naturally fit the character <em class=\"ltx_emph ltx_font_italic\">and</em> scene.</p>\n\n",
                "matched_terms": [
                    "score",
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Focus: pitch, rhythm, and emphasis in speech.\n<span class=\"ltx_text ltx_font_bold\">Guideline 1.1 Pitch Variation (check Local Scene, Character Profile, and audio).</span></p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 1.2 Rhythmic Naturalness (check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 1.3 Stress and Emphasis (check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 2.1 Emotion Accuracy (Required; check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 2.2 Emotion Intensity Control (Only if 2.1 <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"A7.SS4.SSSx5.p3.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math> 3; check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 2.3 Dynamic Range (Only if 2.2 <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"A7.SS4.SSSx5.p4.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math> 3; check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Understood the character and scene clearly,</p>\n\n",
                "matched_terms": [
                    "scene",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A critical observation is that annotations in Mandarin and English cannot be directly compared. The annotation groups for the two languages differed in their linguistic backgrounds and cultural interpretations, which leads to systematic differences in scoring. Our bilingual pilot further confirmed that annotators often interpret role-play appropriateness and expressiveness differently depending on their cultural frame of reference. For example, Mandarin raters tend to reward delivery styles that align with culturally expected prosody and politeness strategies, while English raters prioritize spontaneity and naturalness. These discrepancies parallel the findings of the singing voice conversion challenge (SVCC2023) from <cite class=\"ltx_cite ltx_citemacro_citet\">Huang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib31\" title=\"\">2023</a>)</cite>, where native and non-native listeners provided significantly different ratings for the same samples, particularly for naturalness and speaker similarity. Consequently, while absolute scores differ substantially across Mandarin and English, only within-language comparisons are meaningful. Cross-lingual ranking is not valid given the different annotation bases.</p>\n\n",
                "matched_terms": [
                    "while",
                    "example",
                    "directly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In sum, archetype evaluation reveals three major findings:\n(1) Mandarin and English annotations cannot be directly compared, due to linguistic and cultural annotation inconsistencies, consistent with prior evidence from SVCC 2023.\n(2) Within each language, clear model rankings emerge, <span class=\"ltx_text ltx_font_typewriter\">Doubao</span> dominates in Mandarin, while <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> excels in English.\n(3) Domain sensitivity remains a challenge, particularly for imaginative or culturally rich roles, where scores are substantially lower and inter-model variance is higher.</p>\n\n",
                "matched_terms": [
                    "while",
                    "directly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although realism evaluation targets nuanced paralinguistic behaviors, human annotators achieve <em class=\"ltx_emph ltx_font_italic\">consistently high agreement</em> once the rubric disentangles the aspects to be judged.\nAs shown in the agreement summary in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F7\" title=\"Figure 7 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, most dimensions fall in the <math alttext=\"0.86\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>0.86</mn><annotation encoding=\"application/x-tex\">0.86</annotation></semantics></math>&#8211;<math alttext=\"0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mn>0.88</mn><annotation encoding=\"application/x-tex\">0.88</annotation></semantics></math> range, with the lowest agreement on <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> (<math alttext=\"\\approx 0.83\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.83</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.83</annotation></semantics></math>) and the highest on <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em> / <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> (<math alttext=\"\\approx 0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.88</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.88</annotation></semantics></math>).\nEmotion-related, progressive metrics (Intensity and Transition) are slightly lower (<math alttext=\"\\approx 0.85\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.85</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.85</annotation></semantics></math>), reflecting the added difficulty of judging graded expressivity.</p>\n\n",
                "matched_terms": [
                    "slightly",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F8\" title=\"Figure 8 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F9\" title=\"Figure 9 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> visualize the score distributions per dimension.\nWe highlight three patterns:\n(i) <span class=\"ltx_text ltx_font_bold\">Progressive/gated emotion metrics.</span> For <em class=\"ltx_emph ltx_font_italic\">Emotion Intensity</em> and <em class=\"ltx_emph ltx_font_italic\">Emotion Transition</em>, we assign <span class=\"ltx_text ltx_font_typewriter\">1</span> when the scene is marked <em class=\"ltx_emph ltx_font_italic\">N/A</em> (i.e., emotion accuracy is insufficient to meaningfully judge intensity/transition). This produces a visible mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, a sparse bin at <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>, and slightly lower overall agreement; downstream analyses conditioning on adequate <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> (e.g., mean&#160;<math alttext=\"\\geq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 3</annotation></semantics></math>) recover more bell-shaped distributions.\n(ii) <span class=\"ltx_text ltx_font_bold\">Traits Embodiment.</span> Scores tend to avoid the middle (<math alttext=\"\\sim\\!3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!3</annotation></semantics></math>), concentrating on low values when traits clearly mismatch and high values when traits are strongly realized. This yields the lowest inter-rater agreement among dimensions, reflecting the inherently compositional and role-specific nature of trait judgments.\n(iii) <span class=\"ltx_text ltx_font_bold\">Other dimensions.</span> <em class=\"ltx_emph ltx_font_italic\">Pitch Variation</em>, <em class=\"ltx_emph ltx_font_italic\">Rhythmic Naturalness</em>, <em class=\"ltx_emph ltx_font_italic\">Stress &amp; Emphasis</em>, <em class=\"ltx_emph ltx_font_italic\">Voice Identity Matching</em>, <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em>, <em class=\"ltx_emph ltx_font_italic\">Global Story Coherence</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> are approximately unimodal with modes at <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>&#8211;<math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math>, consistent with moderate&#8211;strong realization in the dataset and with the high agreement reported.</p>\n\n",
                "matched_terms": [
                    "score",
                    "mismatch",
                    "slightly",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identity and traits (Voice Identity, Traits Embodiment).</span> Professionals present clearer peaks in upper bins, while amateurs are broader or bimodal, reflecting inconsistent character anchoring.</p>\n\n",
                "matched_terms": [
                    "while",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scene and semantics (Local Scene Fit, Global Coherence, Semantic Match).</span> Differences are present but smaller; both groups cluster in the mid&#8211;high range, with professionals exhibiting slightly narrower distributions.</p>\n\n",
                "matched_terms": [
                    "slightly",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, the consistently high agreement confirms that the rubric isolates perceptually coherent factors, so learned evaluators can target them reliably.\nSecond, the gated behavior of <em class=\"ltx_emph ltx_font_italic\">Intensity/Transition</em> motivates a <em class=\"ltx_emph ltx_font_italic\">hierarchical</em> evaluator: verify <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> before scoring graded expressivity, or model the joint structure with conditional heads.\nThird, <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> remains challenging; models may benefit from explicit character-trait priors, long-term context aggregation, or contrastive supervision between matched and mismatched traits.\nFinally, prosody-focused objectives (e.g., pitch/rhythm/stress control) are likely to yield the largest quality gains for amateur-style speech.</p>\n\n",
                "matched_terms": [
                    "coherent",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While we utilize various prompts and sample them randomly during the fine-tuning experiments, we provide an example prompt as follows:</p>\n\n",
                "matched_terms": [
                    "while",
                    "during",
                    "example"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the archetype test set, all fine-tuned variants achieve strong <span class=\"ltx_text ltx_font_italic\">Content Pass</span> accuracy above 92%. Differences appear mainly in correlation-based metrics. Here, <span class=\"ltx_text ltx_font_typewriter\">Base-FT</span> yields the highest score on <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> delivers the strongest performance in <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> and the best overall average (0.629). This suggests that instruction-tuned checkpoints paired with parameter-efficient adaptation can better capture higher-level perceptual traits.</p>\n\n",
                "matched_terms": [
                    "score",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For realism evaluation on the basic test set, <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> consistently achieves the strongest correlations across most dimensions, especially for <span class=\"ltx_text ltx_font_italic\">Character Identity</span> and <span class=\"ltx_text ltx_font_italic\">Traits</span>, leading to the best average correlation (0.668). <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> also performs competitively (0.630), while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> lags slightly behind (0.625). These results highlight that full fine-tuning may still offer an advantage when the evaluation set is relatively homogeneous and closer to the training distribution.</p>\n\n",
                "matched_terms": [
                    "while",
                    "slightly",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the comprehensive design of Speech-DRAME, several limitations remain.\nFirst, while our dual evaluation paradigm combines top-down archetype assessment with bottom-up realism evaluation, both strategies are constrained by the quality and scope of available data. Archetype evaluation inevitably relies on simplified stereotypes, which can overlook subtler paralinguistic and contextual cues. Realism evaluation, though richer, still struggles with domain mismatch between synthetic data and real human recordings, as evidenced by the performance drop observed in the real-recording test set. This highlights the persistent gap between benchmark-driven evaluation and real-world variability in speech role-play.</p>\n\n",
                "matched_terms": [
                    "mismatch",
                    "while"
                ]
            }
        ]
    },
    "A7.SS4.SSSx7.p3": {
        "caption": "",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text ltx_font_bold\">Score</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\"><span class=\"ltx_text ltx_font_bold\">Description / Example</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Perfectly consistent with background and arc (e.g., wise mentor advice aligned with past scenes).</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Slightly off-style but plausible (e.g., cheerful tone for usually sarcastic character).</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Generic/filler delivery (<em class=\"ltx_emph ltx_font_italic\">could be anyone</em>).</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Contradicts growth or habits (e.g., arrogance after humility arc).</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Breaks established identity (e.g., cold anger from consistently empathetic healer).</span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "growth",
            "identity",
            "example",
            "genericfiller",
            "humility",
            "slightly",
            "anger",
            "advice",
            "background",
            "character",
            "wise",
            "sarcastic",
            "delivery",
            "past",
            "after",
            "from",
            "consistently",
            "cheerful",
            "contradicts",
            "offstyle",
            "could",
            "tone",
            "plausible",
            "cold",
            "breaks",
            "arrogance",
            "mentor",
            "empathetic",
            "anyone",
            "perfectly",
            "score",
            "arc",
            "scenes",
            "habits",
            "established",
            "aligned",
            "description",
            "consistent",
            "usually",
            "healer"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Role-play has become a key testbed for generative models, expanding from text-only dialogue to multimodal interaction. Extending role-play to speech captures prosody, emotion, and delivery, but also poses new evaluation challenges. Current pipelines often use audio large language models (ALLMs) as zero-shot judges, which miss paralinguistic cues, collapse multiple aspects into coarse scores, and rely on synthetic speech references that fail to reflect real-world roles. We present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework that contributes at three levels: (i) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-EvalBench</em>, an evaluation benchmark with bilingual human-annotated data and protocols for training and testing speech evaluation models&#160;(SEMs), (ii) <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, a fine-tuned evaluation model, which substantially outperforms zero-shot and few-shot ALLMs, and (iii) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-RoleBench</em>, a speech role-play benchmark that leverages <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> as an automatic judge to compare speech foundation models (SFMs). Speech-DRAME distinguishes between two complementary evaluation strategies: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>, a top-down approach measuring adherence to broad role archetypes, and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em>, a bottom-up approach grounded in real human speech that emphasizes nuanced role quality. Compared to zero-shot ALLM judges, <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves stronger agreement with human ratings (Pearson correlation from 0.480 to 0.629 in archetypes, and 0.390 to 0.625 in realism). By integrating transparent benchmark resources, modeling approaches, and system-level evaluation, Speech-DRAME provides the first comprehensive, reproducible foundation for assessing spoken role-play.</p>\n\n",
                "matched_terms": [
                    "from",
                    "delivery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Role-play has been recently recognized a cornerstone of interactive systems, from initial text-based focuses to the growing ecosystem of multimodal dialogue agents&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib6\" title=\"\">2024a</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib59\" title=\"\">2024</a>; Dai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib13\" title=\"\">2025</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib43\" title=\"\">2024</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>)</cite>. With the rapid advancement of large language models (LLMs), role-play has evolved beyond scripted interactions into dynamic character-driven conversations, supporting applications in education, entertainment, and human&#8211;AI collaboration&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib69\" title=\"\">2024</a>; Agatsuma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib1\" title=\"\">2024</a>; Fung &amp; Laing, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib20\" title=\"\">2024</a>)</cite>. Extending role-play to speech-based interaction offers an even richer communication channel, capturing prosody, emotion, and character consistency in ways that text alone cannot&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "from",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-based role-play evaluation.</span>\nEarly research focused on text-only dialogue benchmarks. Datasets such as RoleLLM, CharacterBox, and RoleMRC <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib62\" title=\"\">2024</a>; Tu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib60\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib61\" title=\"\">2025a</a>; Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib53\" title=\"\">2023</a>; Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib41\" title=\"\">2025</a>)</cite> test whether models can sustain persona consistency and character fidelity, using LLM-as-judge pipelines for scalability. However, studies like PersonaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib81\" title=\"\">2025a</a>)</cite> have shown that LLM-as-judge approaches are far from perfect, struggling with tasks as basic as tracking speaker identity. This highlights broader concerns about reliability even in text settings, which become only more severe when extended to speech, where prosody, emotion, and vocal style play a central role.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "from",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-native benchmarks.</span>\nMore recently, a number of resources have been introduced to evaluate spoken role-play agents. OmniCharacter-10K <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>)</cite> emphasizes character voice traits, while others&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib39\" title=\"\">2025</a>; Hou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib27\" title=\"\">2025</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib15\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib7\" title=\"\">2024b</a>)</cite> extend evaluation toward vocal communication, assistant capability, and multi-turn realism. Omni models such as StepAudio and KimiAudio <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite> also include role-play scenarios in their evaluation. SpeechRole and VStyle <cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>; Zhan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib76\" title=\"\">2025</a>)</cite> are closer to our work, offering datasets that test interaction ability, expressiveness, and fidelity. However, these resources are primarily built from LLM-generated dialogues and TTS outputs, with limited human validation and a heavy reliance on proprietary ALLM-as-judge pipelines. As a result, they lack grounding in diverse human speech and typically apply only coarse rubrics.</p>\n\n",
                "matched_terms": [
                    "from",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Automatic judging with ALLMs.</span>\nIn parallel, advances in automatic judging highlight both opportunities and limitations. Audio-capable LLMs such as AudioJudge and Audio-Aware LLMs as Judges <cite class=\"ltx_cite ltx_citemacro_citep\">(Manakul et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib44\" title=\"\">2025</a>; Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>)</cite> can assess style, emotion, and delivery, while systems like Auto-ATT <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib65\" title=\"\">2025c</a>)</cite> scale this to Audio Turing Tests. Yet studies consistently reveal high prompt sensitivity and shallow capture of paralinguistic cues, suggesting that zero-shot judging remains insufficient for nuanced role-play evaluation.</p>\n\n",
                "matched_terms": [
                    "consistently",
                    "delivery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech role-play.</span>\nWe formalize <em class=\"ltx_emph ltx_font_italic\">speech role-play</em> as a conditional speech generation problem grounded in a character profile and scene description.\nGiven a dialogue context <math alttext=\"{\\bm{C}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{speech}}</annotation></semantics></math>, a character profile <math alttext=\"{\\bm{C}}_{\\text{profile}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>profile</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{profile}}</annotation></semantics></math>, and a scene specification <math alttext=\"{\\bm{C}}_{\\text{scene}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>scene</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{scene}}</annotation></semantics></math>, a <em class=\"ltx_emph ltx_font_italic\">speech foundation model</em> (SFM) generates a role-played speech response <math alttext=\"{\\bm{S}}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119930;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{S}}_{r}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "description",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS5\" title=\"F.5 Examples of Realism Role-play &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.5</span></a>). The second strategy adopts a <em class=\"ltx_emph ltx_font_italic\">bottom-up</em> design, motivated by real-world performance and real human speech.\nInstead of relying solely on stereotypes, this approach grounds evaluation in recordings from professional and non-professional speakers, drawn from realistic dialogue and media sources.\nJudgments in this setting are more fine-grained and multi-dimensional, addressing aspects such as prosodic dynamics, emotional fidelity, character consistency, and contextual fit.\nWhile this strategy requires more human annotation and careful curation, it provides benchmarks that reflect how role-play is perceived in practice.\nIt thus offers a richer framework for analyzing model outputs in settings where realism and subtlety are essential.</p>\n\n",
                "matched_terms": [
                    "from",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Sources and Negative Samples.</span>\nWe curate speech from (i) ground-truth media segments, (ii) retrieved character profiles, and (iii) generated local scene descriptions. To stress-test evaluation models, we construct contrastive mismatches: (i) negative local scenes with varying degrees of mismatch, and (ii) negative character profiles created by re-synthesizing transcripts with TTS systems. Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6\" title=\"Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F</span></a> details the prompt design and TTS generation details.</p>\n\n",
                "matched_terms": [
                    "scenes",
                    "from",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotators judge whether performances align with the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em>, using rubrics for <em class=\"ltx_emph ltx_font_italic\">Prosodic Dynamics (i.e., Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis)</em>, <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness (with progressive gating over three sub-dimensions: Emotion Accuracy, Emotion Intensity, Emotion Transition)</em>, <em class=\"ltx_emph ltx_font_italic\">Character Consistency (i.e., Voice Identity Matching and Traits Embodiment)</em>, <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance (i.e., Local Scene Fit and Global Story Coherence)</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>. A prompt quality check and confidence ratings are also collected. Full scales and gating details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real-world Recoding.</span>\nAs noted in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S2\" title=\"2 Related Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, most prior efforts have relied heavily on synthetic data, mostly from TTS systems based on curated professional role-play voices. While such data can achieve high fidelity, it often lacks the diversity and spontaneity of real-world delivery, limiting the generalizability of evaluation models. To address this gap, we expand realism evaluation beyond synthetic and curated voices by incorporating newly recorded speech from both professional and amateur voice actors. This design introduces a broader spectrum of delivery styles, from polished professional acting to more natural and less controlled amateur performance.</p>\n\n",
                "matched_terms": [
                    "from",
                    "delivery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Finetuning Model: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>.</span>\nIn addition to zero-shot and few-shot evaluations, we fine-tune a dedicated evaluation model, denoted as <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> for archetype and realism role-play evaluation, respectively.\nWe initialize from <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio-7B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib11\" title=\"\">2024</a>)</cite>, which provides a strong foundation for multi-modal audio-text reasoning.\nTo better leverage the multiple human annotations available in our datasets, we adopt a sampling-based training strategy: at each step, one annotator label is randomly selected, following practices in label distribution learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Geng, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib22\" title=\"\">2016</a>; Wen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib67\" title=\"\">2023</a>)</cite>.\nThis stochastic supervision implicitly exposes the model to the empirical annotation distribution, while during inference we output the full distribution over discrete labels (e.g., <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score, consistent with recent decoding-based regression methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Song &amp; Bahri, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>; Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib70\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib64\" title=\"\">2025b</a>; Lukasik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>. We apply LoRA fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib28\" title=\"\">2022</a>)</cite>. All implementation details, including optimizer settings, training schedule, and sampling policy, are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS3\" title=\"L.3 Fine-tuning Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "consistent",
                    "score",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T3\" title=\"Table 3 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> extends the analysis to fine-grained realism evaluation. Similar to archetype results, proprietary ALLMs lead the zero-shot setting, with <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> obtaining an average correlation of 0.390, compared to 0.284 for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>. Public models remain weak baselines. Few-shot prompting consistently improves performance: <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> gains +0.042 and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> gains +0.083 over their zero-shot averages, showing the benefit of in-context supervision. Once again, finetuning is most effective: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves 0.625 average correlation, higher than the best few-shot results. Importantly, the gains are consistent across all 10 realism dimensions, confirming that the sampling&#8211;expectation training strategy generalizes beyond simple classification to nuanced paralinguistic cues.</p>\n\n",
                "matched_terms": [
                    "consistently",
                    "consistent"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real Recording Test Set.</span>\nThe real-recording evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T4\" title=\"Table 4 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> is notably more challenging, with all models suffering substantial performance drops. Proprietary models in the zero-shot setting achieve only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.04&#8211;0.08 average correlation, and public models often fall below zero. Few-shot prompting yields small but noticeable improvements, especially for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> (0.226 avg.), suggesting that exemplars partially mitigate domain mismatch between synthetic and real human speech. Interestingly, while finetuning achieves the highest overall average (0.247), its relative advantage over few-shot prompting is narrower than in the synthetic setting, and some dimensions (e.g., <span class=\"ltx_text ltx_font_italic\">Character Identity</span>) remain weak. We emphasize that this observed gap between synthetic and real recordings is not merely an artifact but an intended feature of Speech-DRAME, designed to expose the mismatch and drive future research into bridging synthetic-to-real generalization.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype evaluation.</span>\nOn archetype role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T5\" title=\"Table 5 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), cascaded pipelines consistently outperform end-to-end systems. The strongest cascaded systems, particularly <span class=\"ltx_text ltx_font_typewriter\">G-IndexTTS2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-IndexTTS2</span>, achieve both high content pass rates (above 94%) and balanced scores across audio quality, human likeness, and appropriateness. By contrast, end-to-end models such as <span class=\"ltx_text ltx_font_typewriter\">StepAudio2Mini</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> perform competitively in terms of content coverage (above 88%), but their expressive scores remain slightly lower than their cascaded counterparts. This gap suggests that separating reasoning (LLM) from speech rendering (TTS) remains advantageous when archetype fidelity is emphasized. Nevertheless, the fact that top end-to-end systems already surpass 3.0 in average rating indicates rapid closing of the performance gap.</p>\n\n",
                "matched_terms": [
                    "consistently",
                    "slightly",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our distinction between <span class=\"ltx_text ltx_font_italic\">Archetype Evaluation</span> and <span class=\"ltx_text ltx_font_italic\">Realism Evaluation</span> is not merely pragmatic, but is grounded in established theories of social identity, communication, and performance. The two perspectives capture complementary dimensions of how humans perceive and evaluate roles in interaction.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "established"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetypes as Collective Schemas</span>\nThe notion of archetypes originates from Jungian psychology, where they represent recurrent, culturally embedded character prototypes in the collective unconscious&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jung, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib36\" title=\"\">2014</a>)</cite>. In media and performance studies, archetypes have been shown to guide audience expectations, providing top-down interpretive frames for evaluating whether a character &#8220;fits&#8221; within familiar categories such as hero, mentor, or trickster&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Campbell, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib4\" title=\"\">2008</a>)</cite>. Archetype-based reasoning allows for scalable, general-purpose judgments because it leverages culturally shared schemas rather than idiosyncratic details.</p>\n\n",
                "matched_terms": [
                    "mentor",
                    "from",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Social Identity.</span> Example:</p>\n\n",
                "matched_terms": [
                    "identity",
                    "example"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Step 2: Given a character and scene description, the text LLM imagines a candidate caption and generates a corresponding transcript (see below prompts for the text LLM).</p>\n\n",
                "matched_terms": [
                    "description",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">30-second focus:</span> Assessment should focus on the first 30 seconds of the generation ONLY. Do not consider any content past 30s; disregard it entirely from your assessment and rubric ratings. (You may stop listening beyond 30s.)</p>\n\n",
                "matched_terms": [
                    "past",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rubric Rating Instructions (for content pass = true).</span>\nWe shift focus from <em class=\"ltx_emph ltx_font_italic\">what</em> was said to <em class=\"ltx_emph ltx_font_italic\">how</em> it was said (vocal performance). Use gut intuition guided by rubric definitions. Score each audio <em class=\"ltx_emph ltx_font_italic\">in isolation</em> (no ranking across clips).</p>\n\n",
                "matched_terms": [
                    "score",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">After each 1&#8211;5 rating, add a short comment explaining your score.</p>\n\n",
                "matched_terms": [
                    "score",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Retrieved character profiles.</span>\nEach audio segment is paired with a concise character description derived from the media context.\nThe detailed prompt design for generating these character profiles is provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS2\" title=\"F.2 Prompts for Character Profile Creation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "description",
                    "from",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generated local scenes.</span>\nWe supplement retrieved character information with automatically generated local scene descriptions summarizing the surrounding plot or setting.\nAppendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS3\" title=\"F.3 Prompts for Local Scene Generation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.3</span></a> includes representative prompts and generation details.</p>\n\n",
                "matched_terms": [
                    "scenes",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative local scenes.</span>\nFor each original segment, we generate additional scene descriptions at different levels of mismatch, ranging from subtle alterations to fully unrelated scenarios.\nAppendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS4\" title=\"F.4 Prompts for Negative Scenes &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.4</span></a> outlines the prompting strategies and example outputs.</p>\n\n",
                "matched_terms": [
                    "scenes",
                    "from",
                    "example"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative character profiles.</span>\nWe also create role-profile mismatches by resynthesizing transcripts with diverse TTS systems, thereby breaking the link between authentic delivery and intended role.\nWe explore both text-instruction-based prompting and comparisons across commercial and non-commercial TTS frameworks.\nDetailed model lists and parameter settings are documented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS6\" title=\"F.6 TTS Systems and Settings &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.6</span></a>.</p>\n\n",
                "matched_terms": [
                    "delivery",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During annotation, raters are presented with either: (i) ground-truth stories, (ii) stories with negative local scenes, or (iii) stories with negative character profile. Annotators assign realism scores without being exposed to the underlying mismatch reasons. Instead, these reasons are provided only to the quality-assurance team, who use them as reference for internal validation and consistency checks. While the mismatch reasons may not be perfect, they help streamline quality control procedures and maintain reliability across the dataset.</p>\n\n",
                "matched_terms": [
                    "scenes",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide consistent evaluation context, we generate local scene descriptions summarizing the situational background.\nThese prompts aim to capture the immediate narrative context without introducing extraneous detail.</p>\n\n",
                "matched_terms": [
                    "consistent",
                    "background"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To make the construction of the realism evaluation task concrete, we present a worked example that begins with a curated local scene and character profile, followed by the same transcript adapted into mismatched contexts. These mismatches appear either at the <em class=\"ltx_emph ltx_font_italic\">local-scene</em> level or at both the <em class=\"ltx_emph ltx_font_italic\">character profile and scene</em> level, thereby providing controlled degradations of realism.</p>\n\n",
                "matched_terms": [
                    "example",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the character Yunjin from <span class=\"ltx_text ltx_font_italic\">Genshin Impact</span> as a running case study, which is also included in our released dataset.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>As acknowledged, the use of real-world recordings is conducted under legal and ethical review. All curated data will be released as part of our open-source package.</span></span></span> This example illustrates how alignment between a speaker&#8217;s persona, local context, and delivery underpins realism, and how systematically breaking that alignment produces contrastive test cases.</p>\n\n",
                "matched_terms": [
                    "from",
                    "example",
                    "delivery",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative Local Scenes.</span>\nTo stress realism evaluation, we insert the same transcript into alternative backgrounds. These reveal how context-shifted delivery degrades narrative coherence:</p>\n\n",
                "matched_terms": [
                    "scenes",
                    "delivery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative Character Profiles.</span>\nWe replace Yunjin with new character profiles and scenes. Although the transcript remains the same, realism degrades because the delivery no longer matches the speaker&#8217;s persona or situation.</p>\n\n",
                "matched_terms": [
                    "scenes",
                    "delivery",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Level 0 (Mild Mismatch).</span>\n<em class=\"ltx_emph ltx_font_italic\">Fake Scene:</em> At a modern tech company&#8217;s annual party, a shy programmer is asked to perform. Under pressure, he recites opera lines he once saw online.\n<em class=\"ltx_emph ltx_font_italic\">Fake Profile:</em> Zhang Wei, an introverted software engineer with little artistic background, usually immersed in code and algorithms.\n<em class=\"ltx_emph ltx_font_italic\">Reason:</em> The heroic verses sound alien coming from someone unfamiliar with opera or traditional culture. The clash produces awkward humor but still feels like a coping attempt.</p>\n\n",
                "matched_terms": [
                    "usually",
                    "from",
                    "background"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Yunjin case study illustrates a fundamental difference between realism evaluation and archetype evaluation. Archetype prompts, as described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>, are constructed by pairing abstract role categories with synthetic contextual scenes. These cover a broad space of possible situations, ranging from modern occupations (e.g., firefighters) to fictional personas (e.g., cult leaders in martial arts fantasy) or social identities (e.g., overprotective parents, laid-back friends). While such prompts provide controlled and diverse coverage, they are ultimately template-based and rely on stereotypes to elicit role-play behaviors.</p>\n\n",
                "matched_terms": [
                    "scenes",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the Yunjin example is drawn from authentic performance, where the character&#8217;s persona (an opera singer) and local scene (a battlefield narration) are naturally aligned through artistic delivery. The realism task does not merely test whether a model can follow a prompt, but whether it can preserve coherence between a speaker&#8217;s global identity, immediate context, and expressive prosody. When Yunjin&#8217;s transcript is transplanted into mismatched settings or paired with incongruent speaker profiles, the breakdown in realism becomes apparent: the same words shift from inspiring to awkward, jarring, or even absurd.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "example",
                    "delivery",
                    "aligned",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A character profile was first converted into a descriptive voice specification by prompting <span class=\"ltx_text ltx_font_typewriter\">deepseek-v3</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib38\" title=\"\">2024a</a>)</cite>.\nThis description was submitted to the TTS service to generate candidate voices.\nWe then previewed samples via Gemini2.5Pro, selecting the voice that best matched the speaker style of the original recording before proceeding to generate speech.</p>\n\n",
                "matched_terms": [
                    "description",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.F3\" title=\"Figure 3 &#8227; F.7 Annotation Platform for Realism Evaluation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the core playback interface.\nAnnotators can preview each audio sample with synchronized spectrogram and waveform displays, while reviewing contextual metadata including the <span class=\"ltx_text ltx_font_italic\">Local Scene</span>, <span class=\"ltx_text ltx_font_italic\">Character Style</span>, and <span class=\"ltx_text ltx_font_italic\">Character Profile</span>.\nThese descriptions help situate the listening task, providing the background necessary for judging realism in the character&#8217;s vocal and behavioral expression.</p>\n\n",
                "matched_terms": [
                    "background",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Read the <span class=\"ltx_text ltx_font_bold\">Character Profile</span> (long-term identity, style) and <span class=\"ltx_text ltx_font_bold\">Local Scene</span> (immediate context, goal/emotion) before listening.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the audio&#8217;s speaker is <em class=\"ltx_emph ltx_font_italic\">clearly not</em> the intended target identity, assign a score of <span class=\"ltx_text ltx_font_bold\">1</span> to all Profile-dependent dimensions (e.g., Voice Identity Matching, Trait Embodiment) and proceed with other dimensions as applicable.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Character Consistency</span> (Voice Identity Matching, Trait Embodiment): alignment with Character Profile.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Read first:</span> Carefully read the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em> to understand the long-term identity and immediate goal/emotion.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Use both sources:</span> Play the audio and skim the transcript together to assess delivery quality, expression, and alignment with character intent (do not penalize transcript mismatches).</p>\n\n",
                "matched_terms": [
                    "delivery",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identity mismatch:</span> If the clip&#8217;s speaker is clearly not the intended target speaker, assign a score of <span class=\"ltx_text ltx_font_bold\">1</span> for every rating dimension that relies on the Character Profile (e.g., Character Style Traits, Trait Embodiment, Character Consistency).</p>\n\n",
                "matched_terms": [
                    "identity",
                    "score",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Question 2: Observed Character Style Traits.</span> Based on the Character Profile, are the defined Style Traits clearly present in the delivery?</p>\n\n",
                "matched_terms": [
                    "delivery",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rubric Rating Instructions.</span>\nScore each subcriterion on a 1&#8211;5 scale. Use the detailed rubrics below and the examples to maintain calibration across characters, tones, and emotional styles. Prioritize in-character believability; do not reward theatricality, emotional intensity, or technical polish unless they naturally fit the character <em class=\"ltx_emph ltx_font_italic\">and</em> scene.</p>\n\n",
                "matched_terms": [
                    "score",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 3.1 Voice Identity Matching (Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dual settings: Archetype vs. Realism.</span>\nWe deliberately separate stereotype-driven <em class=\"ltx_emph ltx_font_italic\">Archetype</em> from bottom-up <em class=\"ltx_emph ltx_font_italic\">Realism</em> tasks.\nThis avoids averaging away nuance: archetype labels reflect schema-level fit; realism labels prioritize fine-grained, in-situ delivery from real-world or carefully constructed contrastive material.</p>\n\n",
                "matched_terms": [
                    "from",
                    "delivery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A critical observation is that annotations in Mandarin and English cannot be directly compared. The annotation groups for the two languages differed in their linguistic backgrounds and cultural interpretations, which leads to systematic differences in scoring. Our bilingual pilot further confirmed that annotators often interpret role-play appropriateness and expressiveness differently depending on their cultural frame of reference. For example, Mandarin raters tend to reward delivery styles that align with culturally expected prosody and politeness strategies, while English raters prioritize spontaneity and naturalness. These discrepancies parallel the findings of the singing voice conversion challenge (SVCC2023) from <cite class=\"ltx_cite ltx_citemacro_citet\">Huang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib31\" title=\"\">2023</a>)</cite>, where native and non-native listeners provided significantly different ratings for the same samples, particularly for naturalness and speaker similarity. Consequently, while absolute scores differ substantially across Mandarin and English, only within-language comparisons are meaningful. Cross-lingual ranking is not valid given the different annotation bases.</p>\n\n",
                "matched_terms": [
                    "from",
                    "example",
                    "delivery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Within Mandarin from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, the relative rankings of models are clear. <span class=\"ltx_text ltx_font_typewriter\">Doubao</span> stands out as the strongest performer across all four dimensions, reaching near-ceiling levels in both <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> (4.37) and <span class=\"ltx_text ltx_font_italic\">Content Pass</span> (0.98). It also leads in role-related dimensions such as <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, indicating balanced competence across linguistic accuracy, delivery, and expressiveness. <span class=\"ltx_text ltx_font_typewriter\">G-CosyVoice2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-CosyVoice2</span> follow closely, especially in audio quality, though they lag behind in role appropriateness. <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> achieve mid-range results, reflecting higher variance across samples, while <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> shows competitive content understanding but lower naturalness and appropriateness. <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> consistently ranks the lowest, with large variance, underscoring instability and weaker alignment with archetypal role expectations.</p>\n\n",
                "matched_terms": [
                    "consistently",
                    "from",
                    "delivery"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In sum, archetype evaluation reveals three major findings:\n(1) Mandarin and English annotations cannot be directly compared, due to linguistic and cultural annotation inconsistencies, consistent with prior evidence from SVCC 2023.\n(2) Within each language, clear model rankings emerge, <span class=\"ltx_text ltx_font_typewriter\">Doubao</span> dominates in Mandarin, while <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> excels in English.\n(3) Domain sensitivity remains a challenge, particularly for imaginative or culturally rich roles, where scores are substantially lower and inter-model variance is higher.</p>\n\n",
                "matched_terms": [
                    "consistent",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although realism evaluation targets nuanced paralinguistic behaviors, human annotators achieve <em class=\"ltx_emph ltx_font_italic\">consistently high agreement</em> once the rubric disentangles the aspects to be judged.\nAs shown in the agreement summary in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F7\" title=\"Figure 7 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, most dimensions fall in the <math alttext=\"0.86\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>0.86</mn><annotation encoding=\"application/x-tex\">0.86</annotation></semantics></math>&#8211;<math alttext=\"0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mn>0.88</mn><annotation encoding=\"application/x-tex\">0.88</annotation></semantics></math> range, with the lowest agreement on <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> (<math alttext=\"\\approx 0.83\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.83</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.83</annotation></semantics></math>) and the highest on <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em> / <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> (<math alttext=\"\\approx 0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.88</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.88</annotation></semantics></math>).\nEmotion-related, progressive metrics (Intensity and Transition) are slightly lower (<math alttext=\"\\approx 0.85\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.85</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.85</annotation></semantics></math>), reflecting the added difficulty of judging graded expressivity.</p>\n\n",
                "matched_terms": [
                    "consistently",
                    "slightly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F8\" title=\"Figure 8 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F9\" title=\"Figure 9 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> visualize the score distributions per dimension.\nWe highlight three patterns:\n(i) <span class=\"ltx_text ltx_font_bold\">Progressive/gated emotion metrics.</span> For <em class=\"ltx_emph ltx_font_italic\">Emotion Intensity</em> and <em class=\"ltx_emph ltx_font_italic\">Emotion Transition</em>, we assign <span class=\"ltx_text ltx_font_typewriter\">1</span> when the scene is marked <em class=\"ltx_emph ltx_font_italic\">N/A</em> (i.e., emotion accuracy is insufficient to meaningfully judge intensity/transition). This produces a visible mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, a sparse bin at <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>, and slightly lower overall agreement; downstream analyses conditioning on adequate <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> (e.g., mean&#160;<math alttext=\"\\geq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 3</annotation></semantics></math>) recover more bell-shaped distributions.\n(ii) <span class=\"ltx_text ltx_font_bold\">Traits Embodiment.</span> Scores tend to avoid the middle (<math alttext=\"\\sim\\!3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!3</annotation></semantics></math>), concentrating on low values when traits clearly mismatch and high values when traits are strongly realized. This yields the lowest inter-rater agreement among dimensions, reflecting the inherently compositional and role-specific nature of trait judgments.\n(iii) <span class=\"ltx_text ltx_font_bold\">Other dimensions.</span> <em class=\"ltx_emph ltx_font_italic\">Pitch Variation</em>, <em class=\"ltx_emph ltx_font_italic\">Rhythmic Naturalness</em>, <em class=\"ltx_emph ltx_font_italic\">Stress &amp; Emphasis</em>, <em class=\"ltx_emph ltx_font_italic\">Voice Identity Matching</em>, <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em>, <em class=\"ltx_emph ltx_font_italic\">Global Story Coherence</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> are approximately unimodal with modes at <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>&#8211;<math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math>, consistent with moderate&#8211;strong realization in the dataset and with the high agreement reported.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "consistent",
                    "score",
                    "slightly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identity and traits (Voice Identity, Traits Embodiment).</span> Professionals present clearer peaks in upper bins, while amateurs are broader or bimodal, reflecting inconsistent character anchoring.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, the consistently high agreement confirms that the rubric isolates perceptually coherent factors, so learned evaluators can target them reliably.\nSecond, the gated behavior of <em class=\"ltx_emph ltx_font_italic\">Intensity/Transition</em> motivates a <em class=\"ltx_emph ltx_font_italic\">hierarchical</em> evaluator: verify <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> before scoring graded expressivity, or model the joint structure with conditional heads.\nThird, <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> remains challenging; models may benefit from explicit character-trait priors, long-term context aggregation, or contrastive supervision between matched and mismatched traits.\nFinally, prosody-focused objectives (e.g., pitch/rhythm/stress control) are likely to yield the largest quality gains for amateur-style speech.</p>\n\n",
                "matched_terms": [
                    "consistently",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and few-shot experiments on commercial models such as <span class=\"ltx_text ltx_font_typewriter\">Gemini</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o</span>, we report results as the average of five independent runs to reduce randomness and obtain more stable predictions. In contrast, for open-source models, we observed that their instruction-following ability is relatively weak. To mitigate this, we prompt them to predict one dimension at a time rather than all dimensions jointly. Specifically, for each metric, after showing the rubric prompt, we request completion in a fixed template and extract the probability distribution over the discrete tokens <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>. The final score is then computed from these probabilities, which we found makes open-source models usable to some extent under this setting.</p>\n\n",
                "matched_terms": [
                    "score",
                    "from",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Effect of repeated runs.\nWhen comparing single-run decoding (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot-1run</span>) against averaging over five runs (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot</span>), we see consistent gains across nearly all dimensions (average score improves from 0.353 to 0.390). This suggests that sampling variance plays a substantial role in the reliability of large language model (LLM) evaluators, and averaging multiple outputs reduces this variance, producing more stable correlation with human annotations.</p>\n\n",
                "matched_terms": [
                    "consistent",
                    "score",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Scaling the number of demonstrations.\nIncreasing the few-shot set size from three to six and nine further boosts performance (0.459 and 0.469, respectively). The improvements are most pronounced in prosodic and emotional categories (e.g., <em class=\"ltx_emph ltx_font_italic\">Pitch</em>, <em class=\"ltx_emph ltx_font_italic\">Accuracy</em>, <em class=\"ltx_emph ltx_font_italic\">Intensity</em>), suggesting that more examples provide stronger guidance in modeling subtle paralinguistic cues. Gains in <em class=\"ltx_emph ltx_font_italic\">Character Consistency</em> and <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance</em> are also noticeable, although smaller in magnitude, which may indicate that these higher-level judgments are less sensitive to prompt scaling once a few demonstrations are provided.</p>\n\n",
                "matched_terms": [
                    "from",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For realism evaluation on the basic test set, <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> consistently achieves the strongest correlations across most dimensions, especially for <span class=\"ltx_text ltx_font_italic\">Character Identity</span> and <span class=\"ltx_text ltx_font_italic\">Traits</span>, leading to the best average correlation (0.668). <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> also performs competitively (0.630), while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> lags slightly behind (0.625). These results highlight that full fine-tuning may still offer an advantage when the evaluation set is relatively homogeneous and closer to the training distribution.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "consistently",
                    "slightly",
                    "character"
                ]
            }
        ]
    },
    "A7.SS4.SSSx8.p2": {
        "caption": "",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\"><span class=\"ltx_text ltx_font_bold\">Score</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\"><span class=\"ltx_text ltx_font_bold\">Description / Example</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">5</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Content perfectly advances the scene goal/emotion (e.g., rescue: &#8220;Secure the exit; I&#8217;ll get the hostages.&#8221;).</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">4</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Minor nuance mismatch but still scene-appropriate.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">3</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Neutral filler that neither helps nor contradicts (plausible bridge).</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">2</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Partially conflicts or confuses context; requires <em class=\"ltx_emph ltx_font_italic\">stretch</em> explanation.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:34.1pt;\">1</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:335.7pt;\">Directly contradicts/derails scene intent; no reasonable bridge.</span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "contradictsderails",
            "confuses",
            "nuance",
            "rescue",
            "content",
            "ill",
            "example",
            "still",
            "get",
            "sceneappropriate",
            "helps",
            "explanation",
            "bridge",
            "advances",
            "reasonable",
            "filler",
            "context",
            "requires",
            "directly",
            "neither",
            "secure",
            "scene",
            "partially",
            "contradicts",
            "conflicts",
            "minor",
            "stretch",
            "plausible",
            "intent",
            "goalemotion",
            "hostages",
            "mismatch",
            "perfectly",
            "score",
            "nor",
            "description",
            "neutral",
            "exit"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech role-play.</span>\nWe formalize <em class=\"ltx_emph ltx_font_italic\">speech role-play</em> as a conditional speech generation problem grounded in a character profile and scene description.\nGiven a dialogue context <math alttext=\"{\\bm{C}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{speech}}</annotation></semantics></math>, a character profile <math alttext=\"{\\bm{C}}_{\\text{profile}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>profile</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{profile}}</annotation></semantics></math>, and a scene specification <math alttext=\"{\\bm{C}}_{\\text{scene}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>scene</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{scene}}</annotation></semantics></math>, a <em class=\"ltx_emph ltx_font_italic\">speech foundation model</em> (SFM) generates a role-played speech response <math alttext=\"{\\bm{S}}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119930;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{S}}_{r}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "description",
                    "context",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here <math alttext=\"{\\bm{S}}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><msub><mi>&#119930;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{S}}_{r}</annotation></semantics></math> represents the spoken output (waveform or latent representation). This definition captures both the linguistic dimension (coherence with context and scene) and the paralinguistic dimension (prosody, style, emotion) expected of the role.</p>\n\n",
                "matched_terms": [
                    "context",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Sources and Negative Samples.</span>\nWe curate speech from (i) ground-truth media segments, (ii) retrieved character profiles, and (iii) generated local scene descriptions. To stress-test evaluation models, we construct contrastive mismatches: (i) negative local scenes with varying degrees of mismatch, and (ii) negative character profiles created by re-synthesizing transcripts with TTS systems. Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6\" title=\"Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F</span></a> details the prompt design and TTS generation details.</p>\n\n",
                "matched_terms": [
                    "mismatch",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-shot and Few-shot ALLMs.</span>\nWe first evaluate the zero-shot capabilities of both proprietary and public models, including <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>, <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>, <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio</span>, and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>. All evaluations follow a unified prompting template that provides audio input alongside scene and character context.</p>\n\n",
                "matched_terms": [
                    "context",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real Recording Test Set.</span>\nThe real-recording evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T4\" title=\"Table 4 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> is notably more challenging, with all models suffering substantial performance drops. Proprietary models in the zero-shot setting achieve only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.04&#8211;0.08 average correlation, and public models often fall below zero. Few-shot prompting yields small but noticeable improvements, especially for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> (0.226 avg.), suggesting that exemplars partially mitigate domain mismatch between synthetic and real human speech. Interestingly, while finetuning achieves the highest overall average (0.247), its relative advantage over few-shot prompting is narrower than in the synthetic setting, and some dimensions (e.g., <span class=\"ltx_text ltx_font_italic\">Character Identity</span>) remain weak. We emphasize that this observed gap between synthetic and real recordings is not merely an artifact but an intended feature of Speech-DRAME, designed to expose the mismatch and drive future research into bridging synthetic-to-real generalization.</p>\n\n",
                "matched_terms": [
                    "mismatch",
                    "partially"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multi-turn generation.</span>\nConsider a dialogue of <math alttext=\"T\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m1\" intent=\":literal\"><semantics><mi>T</mi><annotation encoding=\"application/x-tex\">T</annotation></semantics></math> turns, with context <math alttext=\"\\{{\\bm{C}}_{\\text{speech}}^{(t)}\\}_{t=1}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m2\" intent=\":literal\"><semantics><msubsup><mrow><mo stretchy=\"false\">{</mo><msubsup><mi>&#119914;</mi><mtext>speech</mtext><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">}</mo></mrow><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></msubsup><annotation encoding=\"application/x-tex\">\\{{\\bm{C}}_{\\text{speech}}^{(t)}\\}_{t=1}^{T}</annotation></semantics></math>, profile <math alttext=\"{\\bm{C}}_{\\text{profile}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m3\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>profile</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{profile}}</annotation></semantics></math>, and scene <math alttext=\"{\\bm{C}}_{\\text{scene}}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m4\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>scene</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{scene}}</annotation></semantics></math>.\nAt each turn <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p2.m5\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, the SFM produces</p>\n\n",
                "matched_terms": [
                    "context",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Pipeline overview.</span>\nAt a high level, archetype evaluation data are generated through a three-stage pipeline:\n(1) selecting keywords that specify role and scene;\n(2) expanding them into full prompts via structured templates; and\n(3) rendering prompts into speech with contemporary TTS systems.\nThis modular pipeline ensures both scalability and reproducibility, while keeping prompts vivid enough for reliable judgments. (4) Based on the generated data (paired scene and synthesized speech), expert human annotators are recruited for detailed annotation on semantic check (i.e., <span class=\"ltx_text ltx_font_italic\">Content Pass</span>) and three different spoken role-play dimensions (i.e., <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human-likeness</span>, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>).\nImplementation details, including template design, alternative prompting strategies, and annotation guidelines, are provided in the following subsections of this Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "content",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text generation.</span>\nWe followed a three-step expansion: keyword <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> scene description <math alttext=\"\\to\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS3.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\to</annotation></semantics></math> full prompt with template. Some examples are shown as follows:</p>\n\n",
                "matched_terms": [
                    "description",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Step 2: Given a character and scene description, the text LLM imagines a candidate caption and generates a corresponding transcript (see below prompts for the text LLM).</p>\n\n",
                "matched_terms": [
                    "description",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Quality (No Context):</span> independence from content, focus on distortions or synthesis artifacts.</p>\n\n",
                "matched_terms": [
                    "content",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Appropriateness (Context Considered):</span> tonal fit of the vocal performance to the described role and scene.</p>\n\n",
                "matched_terms": [
                    "context",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><em class=\"ltx_emph ltx_font_italic\">Example (pass):</em> Same prompt; audio says: &#8220;It looks like your stylist put you through a wood chipper.&#8221; <math alttext=\"\\Rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I5.i1.I0.i2.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8658;</mo><annotation encoding=\"application/x-tex\">\\Rightarrow</annotation></semantics></math> content pass = true.</p>\n\n",
                "matched_terms": [
                    "content",
                    "example"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><em class=\"ltx_emph ltx_font_italic\">Example (pass):</em> &#8220;Oh no. Are you hurt? Don&#8217;t worry. We&#8217;ll get you taken care of, little one.&#8221; <math alttext=\"\\Rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I5.i2.I0.i2.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8658;</mo><annotation encoding=\"application/x-tex\">\\Rightarrow</annotation></semantics></math> content pass = true.</p>\n\n",
                "matched_terms": [
                    "content",
                    "get",
                    "example"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rubric Rating Instructions (for content pass = true).</span>\nWe shift focus from <em class=\"ltx_emph ltx_font_italic\">what</em> was said to <em class=\"ltx_emph ltx_font_italic\">how</em> it was said (vocal performance). Use gut intuition guided by rubric definitions. Score each audio <em class=\"ltx_emph ltx_font_italic\">in isolation</em> (no ranking across clips).</p>\n\n",
                "matched_terms": [
                    "content",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Retrieved character profiles.</span>\nEach audio segment is paired with a concise character description derived from the media context.\nThe detailed prompt design for generating these character profiles is provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS2\" title=\"F.2 Prompts for Character Profile Creation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "description",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative sample generation.</span>\nTo stress-test evaluation models, we construct contrastive negative examples that deliberately mismatch role and scene information:</p>\n\n",
                "matched_terms": [
                    "mismatch",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative local scenes.</span>\nFor each original segment, we generate additional scene descriptions at different levels of mismatch, ranging from subtle alterations to fully unrelated scenarios.\nAppendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS4\" title=\"F.4 Prompts for Negative Scenes &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.4</span></a> outlines the prompting strategies and example outputs.</p>\n\n",
                "matched_terms": [
                    "mismatch",
                    "example",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide consistent evaluation context, we generate local scene descriptions summarizing the situational background.\nThese prompts aim to capture the immediate narrative context without introducing extraneous detail.</p>\n\n",
                "matched_terms": [
                    "context",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To make the construction of the realism evaluation task concrete, we present a worked example that begins with a curated local scene and character profile, followed by the same transcript adapted into mismatched contexts. These mismatches appear either at the <em class=\"ltx_emph ltx_font_italic\">local-scene</em> level or at both the <em class=\"ltx_emph ltx_font_italic\">character profile and scene</em> level, thereby providing controlled degradations of realism.</p>\n\n",
                "matched_terms": [
                    "example",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the character Yunjin from <span class=\"ltx_text ltx_font_italic\">Genshin Impact</span> as a running case study, which is also included in our released dataset.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>As acknowledged, the use of real-world recordings is conducted under legal and ethical review. All curated data will be released as part of our open-source package.</span></span></span> This example illustrates how alignment between a speaker&#8217;s persona, local context, and delivery underpins realism, and how systematically breaking that alignment produces contrastive test cases.</p>\n\n",
                "matched_terms": [
                    "example",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Level 0 (Mild Mismatch).</span>\n<em class=\"ltx_emph ltx_font_italic\">Fake Scene:</em> A quiet teahouse where Yunjin casually chats with friends about recent weather. She suddenly delivers the heroic verses in full operatic style.\n<em class=\"ltx_emph ltx_font_italic\">Reason:</em> The shift from light conversation to intense battle imagery feels abrupt and unsupported. The mismatch is mild but noticeable.</p>\n\n",
                "matched_terms": [
                    "mismatch",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Level 1 (Moderate Mismatch).</span>\n<em class=\"ltx_emph ltx_font_italic\">Fake Scene:</em> A teahouse with elderly patrons enjoying tea in a calm setting. Yunjin, dressed plainly, begins singing passionately.\n<em class=\"ltx_emph ltx_font_italic\">Reason:</em> The intense tone clashes with the subdued environment, startling listeners. Her heroic lines sound out of place, generating tonal dissonance instead of inspiration.</p>\n\n",
                "matched_terms": [
                    "mismatch",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Level 2 (Severe Mismatch).</span>\n<em class=\"ltx_emph ltx_font_italic\">Fake Scene:</em> A modern caf&#233; with soft jazz music and casual chatter. Yunjin sips a latte while reading a magazine, then recites the battle verses.\n<em class=\"ltx_emph ltx_font_italic\">Reason:</em> The dramatic imagery of warriors and swords is wholly incompatible with the relaxed, contemporary caf&#233; ambience. The mismatch is extreme, rendering the delivery absurd.</p>\n\n",
                "matched_terms": [
                    "mismatch",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Level 0 (Mild Mismatch).</span>\n<em class=\"ltx_emph ltx_font_italic\">Fake Scene:</em> At a modern tech company&#8217;s annual party, a shy programmer is asked to perform. Under pressure, he recites opera lines he once saw online.\n<em class=\"ltx_emph ltx_font_italic\">Fake Profile:</em> Zhang Wei, an introverted software engineer with little artistic background, usually immersed in code and algorithms.\n<em class=\"ltx_emph ltx_font_italic\">Reason:</em> The heroic verses sound alien coming from someone unfamiliar with opera or traditional culture. The clash produces awkward humor but still feels like a coping attempt.</p>\n\n",
                "matched_terms": [
                    "mismatch",
                    "still",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Level 1 (Moderate Mismatch).</span>\n<em class=\"ltx_emph ltx_font_italic\">Fake Scene:</em> In a futuristic laboratory, a robotic engineer celebrates a successful combat simulation. Overwhelmed with excitement, he blurts out the opera verses.\n<em class=\"ltx_emph ltx_font_italic\">Fake Profile:</em> Xiao&#160;A, a calm and rational roboticist, normally concise and analytical, not prone to poetic expression.\n<em class=\"ltx_emph ltx_font_italic\">Reason:</em> The dramatic, metaphorical verses conflict with his logical persona and sterile environment. The mismatch is sharper: the heroic rhetoric feels uncharacteristic and jarring.</p>\n\n",
                "matched_terms": [
                    "mismatch",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Level 2 (Severe Mismatch).</span>\n<em class=\"ltx_emph ltx_font_italic\">Fake Scene:</em> On a modern city street, a rebellious street performer in jeans and a rock-band T-shirt strums an electric guitar, suddenly shouting the verses in parody.\n<em class=\"ltx_emph ltx_font_italic\">Fake Profile:</em> Mark, an anti-traditionalist performer who mocks classical art and heroic ideals, favoring satire and irony.\n<em class=\"ltx_emph ltx_font_italic\">Reason:</em> The verses, originally meant to inspire reverence, become ironic and absurd when delivered by someone who rejects those values. The clash is extreme, generating intentional ridicule and total loss of realism.</p>\n\n",
                "matched_terms": [
                    "mismatch",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the Yunjin example is drawn from authentic performance, where the character&#8217;s persona (an opera singer) and local scene (a battlefield narration) are naturally aligned through artistic delivery. The realism task does not merely test whether a model can follow a prompt, but whether it can preserve coherence between a speaker&#8217;s global identity, immediate context, and expressive prosody. When Yunjin&#8217;s transcript is transplanted into mismatched settings or paired with incongruent speaker profiles, the breakdown in realism becomes apparent: the same words shift from inspiring to awkward, jarring, or even absurd.</p>\n\n",
                "matched_terms": [
                    "example",
                    "scene",
                    "context"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS3</span> (Embedding&#8211;AudioCode Pipeline).\nHere, the input is Qwen2.5 BPE embeddings, which a 0.4B GPT-2 model maps directly to 4,096-dimensional audio codes. These codes are decoded via a cascaded architecture: <span class=\"ltx_text ltx_font_typewriter\">RepCodec</span> decoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib32\" title=\"\">2024</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> content representation (with speaker embedding) <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DDIM</span> (denoising\ndiffusion implicit model) module&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jeong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib34\" title=\"\">2021</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DAC</span> latent representation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib37\" title=\"\">2023</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DAC</span> decoder. Conditioning is provided through prefix text embeddings (e.g., instructions expressing emotional states such as &#8220;angry, hateful: you betrayed me&#8230;&#8221;), along with speaker ID. The input format is <span class=\"ltx_text ltx_font_typewriter\">[spk_id, BPE embedding, audio codes]</span>.</p>\n\n",
                "matched_terms": [
                    "content",
                    "directly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Read the <span class=\"ltx_text ltx_font_bold\">Character Profile</span> (long-term identity, style) and <span class=\"ltx_text ltx_font_bold\">Local Scene</span> (immediate context, goal/emotion) before listening.</p>\n\n",
                "matched_terms": [
                    "context",
                    "scene",
                    "goalemotion"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Semantic Match</span> (spoken content vs. Local Scene goal/emotion/action).</p>\n\n",
                "matched_terms": [
                    "content",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Read first:</span> Carefully read the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em> to understand the long-term identity and immediate goal/emotion.</p>\n\n",
                "matched_terms": [
                    "goalemotion",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identity mismatch:</span> If the clip&#8217;s speaker is clearly not the intended target speaker, assign a score of <span class=\"ltx_text ltx_font_bold\">1</span> for every rating dimension that relies on the Character Profile (e.g., Character Style Traits, Trait Embodiment, Character Consistency).</p>\n\n",
                "matched_terms": [
                    "score",
                    "mismatch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt contains contradictory information.</span> Conflicts make accurate annotation difficult or impossible. Examples: incompatible accent specifications; Local Scene drastically at odds with intended emotion (see <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>); Local and global scenes inconsistent; traits described as &#8220;evolving over time&#8221; without a timeline.</p>\n\n",
                "matched_terms": [
                    "conflicts",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rubric Rating Instructions.</span>\nScore each subcriterion on a 1&#8211;5 scale. Use the detailed rubrics below and the examples to maintain calibration across characters, tones, and emotional styles. Prioritize in-character believability; do not reward theatricality, emotional intensity, or technical polish unless they naturally fit the character <em class=\"ltx_emph ltx_font_italic\">and</em> scene.</p>\n\n",
                "matched_terms": [
                    "score",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A critical observation is that annotations in Mandarin and English cannot be directly compared. The annotation groups for the two languages differed in their linguistic backgrounds and cultural interpretations, which leads to systematic differences in scoring. Our bilingual pilot further confirmed that annotators often interpret role-play appropriateness and expressiveness differently depending on their cultural frame of reference. For example, Mandarin raters tend to reward delivery styles that align with culturally expected prosody and politeness strategies, while English raters prioritize spontaneity and naturalness. These discrepancies parallel the findings of the singing voice conversion challenge (SVCC2023) from <cite class=\"ltx_cite ltx_citemacro_citet\">Huang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib31\" title=\"\">2023</a>)</cite>, where native and non-native listeners provided significantly different ratings for the same samples, particularly for naturalness and speaker similarity. Consequently, while absolute scores differ substantially across Mandarin and English, only within-language comparisons are meaningful. Cross-lingual ranking is not valid given the different annotation bases.</p>\n\n",
                "matched_terms": [
                    "example",
                    "directly"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F8\" title=\"Figure 8 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F9\" title=\"Figure 9 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> visualize the score distributions per dimension.\nWe highlight three patterns:\n(i) <span class=\"ltx_text ltx_font_bold\">Progressive/gated emotion metrics.</span> For <em class=\"ltx_emph ltx_font_italic\">Emotion Intensity</em> and <em class=\"ltx_emph ltx_font_italic\">Emotion Transition</em>, we assign <span class=\"ltx_text ltx_font_typewriter\">1</span> when the scene is marked <em class=\"ltx_emph ltx_font_italic\">N/A</em> (i.e., emotion accuracy is insufficient to meaningfully judge intensity/transition). This produces a visible mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, a sparse bin at <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>, and slightly lower overall agreement; downstream analyses conditioning on adequate <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> (e.g., mean&#160;<math alttext=\"\\geq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 3</annotation></semantics></math>) recover more bell-shaped distributions.\n(ii) <span class=\"ltx_text ltx_font_bold\">Traits Embodiment.</span> Scores tend to avoid the middle (<math alttext=\"\\sim\\!3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!3</annotation></semantics></math>), concentrating on low values when traits clearly mismatch and high values when traits are strongly realized. This yields the lowest inter-rater agreement among dimensions, reflecting the inherently compositional and role-specific nature of trait judgments.\n(iii) <span class=\"ltx_text ltx_font_bold\">Other dimensions.</span> <em class=\"ltx_emph ltx_font_italic\">Pitch Variation</em>, <em class=\"ltx_emph ltx_font_italic\">Rhythmic Naturalness</em>, <em class=\"ltx_emph ltx_font_italic\">Stress &amp; Emphasis</em>, <em class=\"ltx_emph ltx_font_italic\">Voice Identity Matching</em>, <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em>, <em class=\"ltx_emph ltx_font_italic\">Global Story Coherence</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> are approximately unimodal with modes at <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>&#8211;<math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math>, consistent with moderate&#8211;strong realization in the dataset and with the high agreement reported.</p>\n\n",
                "matched_terms": [
                    "score",
                    "mismatch",
                    "scene"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the archetype test set, all fine-tuned variants achieve strong <span class=\"ltx_text ltx_font_italic\">Content Pass</span> accuracy above 92%. Differences appear mainly in correlation-based metrics. Here, <span class=\"ltx_text ltx_font_typewriter\">Base-FT</span> yields the highest score on <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> delivers the strongest performance in <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> and the best overall average (0.629). This suggests that instruction-tuned checkpoints paired with parameter-efficient adaptation can better capture higher-level perceptual traits.</p>\n\n",
                "matched_terms": [
                    "content",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the comprehensive design of Speech-DRAME, several limitations remain.\nFirst, while our dual evaluation paradigm combines top-down archetype assessment with bottom-up realism evaluation, both strategies are constrained by the quality and scope of available data. Archetype evaluation inevitably relies on simplified stereotypes, which can overlook subtler paralinguistic and contextual cues. Realism evaluation, though richer, still struggles with domain mismatch between synthetic data and real human recordings, as evidenced by the performance drop observed in the real-recording test set. This highlights the persistent gap between benchmark-driven evaluation and real-world variability in speech role-play.</p>\n\n",
                "matched_terms": [
                    "mismatch",
                    "still"
                ]
            }
        ]
    },
    "A8.T9": {
        "caption": "Table 9: Annotation pipeline comparison across three representative frameworks.",
        "body": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_middle ltx_border_r ltx_border_tt\" style=\"width:78.0pt;padding-top:2pt;padding-bottom:2pt;\"/>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_tt\" style=\"padding-top:2pt;padding-bottom:2pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SpeechRole</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_tt\" style=\"padding-top:2pt;padding-bottom:2pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio-Aware LLMs as Judges</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_tt\" style=\"padding-top:2pt;padding-bottom:2pt;\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">InstructTTSEval</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#FFFFFF;\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t\" style=\"width:78.0pt;padding-top:2pt;padding-bottom:2pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data basis</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:2pt;padding-bottom:2pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p\">LLM-generated dialogues (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"A8.T9.m1\" intent=\":literal\"><semantics><mo mathbackground=\"#FFFFFF\" style=\"--ltx-bg-color:#FFFFFF;\">&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>112k) for 98 roles; speech via collection/synthesis; limited manual verification</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:2pt;padding-bottom:2pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p\">Model outputs evaluated by audio-aware LLMs (ALLM-as-judge) on voice-style IF and role-play</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-top:2pt;padding-bottom:2pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p\">Expressive clips mined from movies/TV; reverse-generated style instructions (EN/ZH); Gemini-as-judge</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t\" style=\"width:78.0pt;padding-top:2pt;padding-bottom:2pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Label source</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:2pt;padding-bottom:2pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p\">Mixture: automatic pipelines + manual checks; benchmark dims: interaction/expressiveness/role fidelity</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:2pt;padding-bottom:2pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p\">Automatic labels from ALLMs; no human rubric for each clip during evaluation</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-top:2pt;padding-bottom:2pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p\">Automatic labels from Gemini; human annotations used for agreement calibration</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#FFFFFF;\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t\" style=\"width:78.0pt;padding-top:2pt;padding-bottom:2pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Primary goal</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:2pt;padding-bottom:2pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p\">Large-scale SRPA dataset &amp; benchmark across paradigms (cascaded/E2E)</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\" style=\"padding-top:2pt;padding-bottom:2pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p\">Feasibility of ALLM-as-judge for speaking style/role-play</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-top:2pt;padding-bottom:2pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#FFFFFF;\">\n<span class=\"ltx_p\">Automatic benchmarking of instruction-following TTS across APS/DSD/RP</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#F2F2F2;\">\n<td class=\"ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_r ltx_border_t\" style=\"width:78.0pt;padding-top:2pt;padding-bottom:2pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Key limitation (for DRAME&#8217;s aims)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:2pt;padding-bottom:2pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p\">Synthetic bias from LLM/TTS; conflates generator priors with evaluation targets</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_r ltx_border_t\" style=\"padding-top:2pt;padding-bottom:2pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p\">Judge sensitivity to prompts; opaque criteria; limited human verifiability</span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_t\" style=\"padding-top:2pt;padding-bottom:2pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" style=\"--ltx-bg-color:#F2F2F2;\">\n<span class=\"ltx_p\">Judge-driven scores; limited per-dimension human rationale; coarse control over rater bias</span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "representative",
            "apsdsdrp",
            "prompts",
            "collectionsynthesis",
            "agreement",
            "evaluation",
            "each",
            "geminiasjudge",
            "feasibility",
            "tts",
            "annotations",
            "opaque",
            "primary",
            "from",
            "verifiability",
            "over",
            "expressive",
            "llmtts",
            "sim112k",
            "instructionfollowing",
            "calibration",
            "roleplay",
            "mined",
            "label",
            "annotation",
            "speaking",
            "judgedriven",
            "used",
            "scores",
            "verification",
            "across",
            "labels",
            "perdimension",
            "speechrole",
            "dialogues",
            "interactionexpressivenessrole",
            "sensitivity",
            "cascadede2e",
            "llms",
            "benchmark",
            "aims",
            "frameworks",
            "roles",
            "limited",
            "human",
            "clips",
            "targets",
            "bias",
            "paradigms",
            "key",
            "rater",
            "fidelity",
            "control",
            "srpa",
            "instructions",
            "mixture",
            "instructttseval",
            "moviestv",
            "goal",
            "coarse",
            "manual",
            "priors",
            "llmgenerated",
            "rubric",
            "speech",
            "limitation",
            "via",
            "rationale",
            "outputs",
            "model",
            "data",
            "largescale",
            "judges",
            "audioaware",
            "evaluated",
            "automatic",
            "conflates",
            "reversegenerated",
            "style",
            "source",
            "comparison",
            "clip",
            "dims",
            "voicestyle",
            "styleroleplay",
            "criteria",
            "benchmarking",
            "judge",
            "pipelines",
            "allms",
            "checks",
            "pipeline",
            "basis",
            "generator",
            "allmasjudge",
            "drames",
            "during",
            "enzh",
            "gemini",
            "three",
            "dataset",
            "synthetic"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A8.T9\" title=\"Table 9 &#8227; H.1 Scope and Label Source &#8227; Appendix H Annotation Methodology: Comparison to Contemporary Work &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> contrasts how recent resources obtain labels and what they optimize for.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Role-play has become a key testbed for generative models, expanding from text-only dialogue to multimodal interaction. Extending role-play to speech captures prosody, emotion, and delivery, but also poses new evaluation challenges. Current pipelines often use audio large language models (ALLMs) as zero-shot judges, which miss paralinguistic cues, collapse multiple aspects into coarse scores, and rely on synthetic speech references that fail to reflect real-world roles. We present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework that contributes at three levels: (i) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-EvalBench</em>, an evaluation benchmark with bilingual human-annotated data and protocols for training and testing speech evaluation models&#160;(SEMs), (ii) <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, a fine-tuned evaluation model, which substantially outperforms zero-shot and few-shot ALLMs, and (iii) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-RoleBench</em>, a speech role-play benchmark that leverages <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> as an automatic judge to compare speech foundation models (SFMs). Speech-DRAME distinguishes between two complementary evaluation strategies: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>, a top-down approach measuring adherence to broad role archetypes, and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em>, a bottom-up approach grounded in real human speech that emphasizes nuanced role quality. Compared to zero-shot ALLM judges, <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves stronger agreement with human ratings (Pearson correlation from 0.480 to 0.629 in archetypes, and 0.390 to 0.625 in realism). By integrating transparent benchmark resources, modeling approaches, and system-level evaluation, Speech-DRAME provides the first comprehensive, reproducible foundation for assessing spoken role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "automatic",
                    "key",
                    "agreement",
                    "evaluation",
                    "from",
                    "judge",
                    "scores",
                    "coarse",
                    "pipelines",
                    "allms",
                    "speech",
                    "benchmark",
                    "roles",
                    "three",
                    "model",
                    "data",
                    "human",
                    "judges",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Role-play has been recently recognized a cornerstone of interactive systems, from initial text-based focuses to the growing ecosystem of multimodal dialogue agents&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib6\" title=\"\">2024a</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib59\" title=\"\">2024</a>; Dai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib13\" title=\"\">2025</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib43\" title=\"\">2024</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>)</cite>. With the rapid advancement of large language models (LLMs), role-play has evolved beyond scripted interactions into dynamic character-driven conversations, supporting applications in education, entertainment, and human&#8211;AI collaboration&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib69\" title=\"\">2024</a>; Agatsuma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib1\" title=\"\">2024</a>; Fung &amp; Laing, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib20\" title=\"\">2024</a>)</cite>. Extending role-play to speech-based interaction offers an even richer communication channel, capturing prosody, emotion, and character consistency in ways that text alone cannot&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "roleplay",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Evaluating speech role-play, however, remains a major challenge. Current pipelines often rely on ALLMs as judges, typically in a zero-shot setting&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>; Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>)</cite>. While this strategy can provide rough ratings, it falls short for speech role-play:</p>\n\n",
                "matched_terms": [
                    "pipelines",
                    "roleplay",
                    "allms",
                    "judges",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">it collapses diverse aspects of performance into limited coarse dimensions, obscuring critical dimensions of role-play quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite>, and</p>\n\n",
                "matched_terms": [
                    "coarse",
                    "roleplay",
                    "limited"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">it depends on synthetic speech references rather than real human speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>)</cite>, limiting its ability to reflect real-world diversity in speech role-play.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "roleplay",
                    "synthetic",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome these limitations, we present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span> (Speech Detailed Role-play Assessment with Modeling and Evaluation), a unified framework that moves beyond synthetic or judge-only pipelines. Speech-DRAME is distinguished by three aspects: [i] a dual evaluation paradigm that balances archetype-driven generality with human-grounded realism (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS2\" title=\"3.2 Role-play Evaluation with Two Strategies &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), [ii] human annotations that provide perceptual grounding absent in prior resources (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4\" title=\"4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), and [iii] evaluation-model alignment that enables not only benchmarking of speech foundation models (SFMs) but also systematic analysis of the evaluation models (SEMs) themselves (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5\" title=\"5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). These aspects establish Speech-DRAME as a comprehensive and trustworthy framework for measuring and improving speech role-play.</p>\n\n",
                "matched_terms": [
                    "pipelines",
                    "roleplay",
                    "annotations",
                    "three",
                    "evaluation",
                    "human",
                    "benchmarking",
                    "speech",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, Speech-DRAME contributes at three levels: (i) <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>, an evaluation benchmark consisting of human-annotated datasets for archetype and realism role-play; (ii) <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span>, a fine-tuned SEM trained on EvalBench that surpasses zero-shot and few-shot ALLMs; and (iii) <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span>, a speech role-play benchmark that leverages DRAME-Eval as an automatic judge to systematically compare SFMs. By combining dual evaluation strategies, human-anchored supervision, and evaluation-model alignment, Speech-DRAME delivers systematic insights into both role-play systems and evaluation models themselves.\nOur contributions are four-fold:</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "roleplay",
                    "allms",
                    "automatic",
                    "three",
                    "evaluation",
                    "speech",
                    "judge"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a <span class=\"ltx_text ltx_font_bold\">formal task definition</span> of speech role-play and its evaluation via speech foundation models (SFMs) for generation and speech evaluation models (SEMs) for assessment, unifying linguistic and paralinguistic dimensions.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "roleplay",
                    "evaluation",
                    "via"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>, the first evaluation benchmark with human annotations under both archetype- and realism-based strategies, supporting the training and testing of SEMs.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "annotations",
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span>, a fine-tuned SEM trained on EvalBench that substantially outperforms zero-shot and few-shot ALLMs, demonstrating the importance of human supervision for evaluation models.</p>\n\n",
                "matched_terms": [
                    "allms",
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We establish <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span>, a speech role-play benchmark that leverages DRAME-Eval as an automatic judge to systematically compare both end-to-end and cascaded SFMs across multiple role quality dimensions.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Relevant codes, pre-trained models, and datasets will be open-sourced at \n<br class=\"ltx_break\"/><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/Anuttacon/speech_drame\" title=\"\">https://github.com/Anuttacon/speech_drame</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "roleplay",
                    "across",
                    "automatic",
                    "speech",
                    "judge"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through this work, we aim to move from acting in text to acting in speech, equipping the community with the models, evaluation strategies, and benchmarks needed to measure and improve speech role-play.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "roleplay",
                    "evaluation",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-based role-play evaluation.</span>\nEarly research focused on text-only dialogue benchmarks. Datasets such as RoleLLM, CharacterBox, and RoleMRC <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib62\" title=\"\">2024</a>; Tu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib60\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib61\" title=\"\">2025a</a>; Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib53\" title=\"\">2023</a>; Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib41\" title=\"\">2025</a>)</cite> test whether models can sustain persona consistency and character fidelity, using LLM-as-judge pipelines for scalability. However, studies like PersonaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib81\" title=\"\">2025a</a>)</cite> have shown that LLM-as-judge approaches are far from perfect, struggling with tasks as basic as tracking speaker identity. This highlights broader concerns about reliability even in text settings, which become only more severe when extended to speech, where prosody, emotion, and vocal style play a central role.</p>\n\n",
                "matched_terms": [
                    "pipelines",
                    "roleplay",
                    "style",
                    "fidelity",
                    "evaluation",
                    "from",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-native benchmarks.</span>\nMore recently, a number of resources have been introduced to evaluate spoken role-play agents. OmniCharacter-10K <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>)</cite> emphasizes character voice traits, while others&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib39\" title=\"\">2025</a>; Hou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib27\" title=\"\">2025</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib15\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib7\" title=\"\">2024b</a>)</cite> extend evaluation toward vocal communication, assistant capability, and multi-turn realism. Omni models such as StepAudio and KimiAudio <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite> also include role-play scenarios in their evaluation. SpeechRole and VStyle <cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>; Zhan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib76\" title=\"\">2025</a>)</cite> are closer to our work, offering datasets that test interaction ability, expressiveness, and fidelity. However, these resources are primarily built from LLM-generated dialogues and TTS outputs, with limited human validation and a heavy reliance on proprietary ALLM-as-judge pipelines. As a result, they lack grounding in diverse human speech and typically apply only coarse rubrics.</p>\n\n",
                "matched_terms": [
                    "coarse",
                    "roleplay",
                    "pipelines",
                    "outputs",
                    "tts",
                    "speechrole",
                    "dialogues",
                    "fidelity",
                    "evaluation",
                    "limited",
                    "from",
                    "human",
                    "llmgenerated",
                    "allmasjudge",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Automatic judging with ALLMs.</span>\nIn parallel, advances in automatic judging highlight both opportunities and limitations. Audio-capable LLMs such as AudioJudge and Audio-Aware LLMs as Judges <cite class=\"ltx_cite ltx_citemacro_citep\">(Manakul et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib44\" title=\"\">2025</a>; Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>)</cite> can assess style, emotion, and delivery, while systems like Auto-ATT <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib65\" title=\"\">2025c</a>)</cite> scale this to Audio Turing Tests. Yet studies consistently reveal high prompt sensitivity and shallow capture of paralinguistic cues, suggesting that zero-shot judging remains insufficient for nuanced role-play evaluation.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "roleplay",
                    "allms",
                    "audioaware",
                    "automatic",
                    "style",
                    "evaluation",
                    "sensitivity",
                    "judges"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Expressive TTS evaluation.</span>\nRecent work has explored evaluation of instruction-following and style-controlled TTS. InstructTTSEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite> explicitly includes role-play scenarios, while EmergentTTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">(Manku et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib45\" title=\"\">2025</a>)</cite> and discrete speech LM benchmarks <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang &amp; Sz&#233;kely, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib63\" title=\"\">2024</a>)</cite> emphasize prosody, spontaneity, and speaker consistency. These efforts push beyond intelligibility and naturalness, but still center on synthetic outputs rather than real human role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "instructionfollowing",
                    "outputs",
                    "tts",
                    "expressive",
                    "evaluation",
                    "instructttseval",
                    "human",
                    "speech",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our distinction.</span>\nIn contrast, Speech-DRAME unifies these considerations through three elements: (i) a dual evaluation paradigm that combines archetype-driven generality with realism grounded in authentic human speech, (ii) human annotations that provide a reliable perceptual gold standard, and (iii) alignment of SEMs to human ratings, enabling systematic evaluation of both SFMs and the evaluation models themselves. This combination moves beyond synthetic benchmarks or zero-shot judging to provide a comprehensive and trustworthy framework for speech role-play assessment.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "annotations",
                    "three",
                    "evaluation",
                    "human",
                    "speech",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech role-play.</span>\nWe formalize <em class=\"ltx_emph ltx_font_italic\">speech role-play</em> as a conditional speech generation problem grounded in a character profile and scene description.\nGiven a dialogue context <math alttext=\"{\\bm{C}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{speech}}</annotation></semantics></math>, a character profile <math alttext=\"{\\bm{C}}_{\\text{profile}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>profile</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{profile}}</annotation></semantics></math>, and a scene specification <math alttext=\"{\\bm{C}}_{\\text{scene}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>scene</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{scene}}</annotation></semantics></math>, a <em class=\"ltx_emph ltx_font_italic\">speech foundation model</em> (SFM) generates a role-played speech response <math alttext=\"{\\bm{S}}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119930;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{S}}_{r}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "roleplay",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation.</span>\nIn principle, a <em class=\"ltx_emph ltx_font_italic\">speech evaluation model</em> (SEM) can assess a generated response using the same conditioning signals:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "evaluation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This formulation is intentionally modular: a dialogue-level evaluation can be obtained by aggregating single-turn scores across turns (e.g., averaging, recency-weighted pooling). We leave the detailed discussion of this multi-turn extension to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A3\" title=\"Appendix C Extension to Multi-turn Role-play Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "evaluation",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech role-play evaluation can be framed in multiple ways depending on the available data and the desired granularity of analysis.\nWithin Speech-DRAME, we formalize two distinct strategies inspired by sociology theories<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Please refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A2\" title=\"Appendix B Theoretical Motivation for Dual Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> for detailed discussion of the dual evaluation design.</span></span></span>, including a top-down, archetype-oriented option using synthetic speech and a bottom-up, realism-oriented option from real human speech.\nRather than merging these into a single rubric, we treat them as separate perspectives, each with unique advantages and limitations.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarizes the key contrasts, while further implementation details are provided in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>-&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "key",
                    "evaluation",
                    "data",
                    "each",
                    "from",
                    "rubric",
                    "human",
                    "speech",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>). The first strategy follows a <em class=\"ltx_emph ltx_font_italic\">top-down</em> design, inspired by prior text-based role-play benchmarks.\nHere, role-play is judged with respect to <em class=\"ltx_emph ltx_font_italic\">stereotypical archetypes</em> (e.g., &#8220;firefighter&#8221; or &#8220;ER doctor&#8221;), using scene contexts (e.g., &#8220;comfort a child trapped in fire&#8221; or &#8220;ask for help in a serious operation&#8221;).\nThis approach provides accessible, general-purpose scoring that is scalable to a wide range of scenarios, making it suitable for large-scale benchmarking.\nHowever, its reliance on broad stereotypes and global impressions limits its ability to capture fine-grained aspects of delivery (e.g., subtle prosodic shifts, nuanced emotional control)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Holt &amp; Lotto, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib26\" title=\"\">2010</a>)</cite>.\nIn practice, this strategy mirrors simplified annotation settings and is particularly useful when annotation resources are limited&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib21\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "annotation",
                    "evaluation",
                    "control",
                    "limited",
                    "largescale",
                    "benchmarking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS5\" title=\"F.5 Examples of Realism Role-play &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.5</span></a>). The second strategy adopts a <em class=\"ltx_emph ltx_font_italic\">bottom-up</em> design, motivated by real-world performance and real human speech.\nInstead of relying solely on stereotypes, this approach grounds evaluation in recordings from professional and non-professional speakers, drawn from realistic dialogue and media sources.\nJudgments in this setting are more fine-grained and multi-dimensional, addressing aspects such as prosodic dynamics, emotional fidelity, character consistency, and contextual fit.\nWhile this strategy requires more human annotation and careful curation, it provides benchmarks that reflect how role-play is perceived in practice.\nIt thus offers a richer framework for analyzing model outputs in settings where realism and subtlety are essential.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "outputs",
                    "model",
                    "fidelity",
                    "annotation",
                    "evaluation",
                    "from",
                    "human",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section details the <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> data curation process. Our data curation methodology follows a two&#8211;stage design: (i) <em class=\"ltx_emph ltx_font_italic\">task preparation</em>, which specifies the roles, scenes, and evaluation settings used to elicit speech; and (ii) <em class=\"ltx_emph ltx_font_italic\">human annotation</em>, which provides reliable ground-truth judgments on generated or real human speech.</p>\n\n",
                "matched_terms": [
                    "roles",
                    "annotation",
                    "evaluation",
                    "data",
                    "human",
                    "speech",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task Preparation.</span>\nWe employ complementary strategies: a <em class=\"ltx_emph ltx_font_italic\">top&#8211;down</em> archetype-based approach, rooted in stereotypes and generalized role expectations, and a <em class=\"ltx_emph ltx_font_italic\">bottom&#8211;up</em> realism-based approach, derived from real human speech. Both follow a modular pipeline of role/scene definition, prompt expansion, and speech rendering or retrieval, ensuring scalability and validity.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "pipeline",
                    "from",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human Annotation.</span>\nAll collected audio undergoes a standardized annotation protocol: (1) an initial semantically-based screening to exclude invalid clips; (2) rubric-based scoring across multiple perceptual dimensions; and (3) meta-annotations such as confidence or prompt-quality judgments. Full rubrics are provided in the appendices&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A5\" title=\"Appendix E Annotation Guidelines for Archetype Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">E</span></a> &amp; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>All data collection follows in high standard ethical consideration as outlined in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A9\" title=\"Appendix I Annotation Ethics &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "across",
                    "annotation",
                    "data",
                    "human",
                    "clips"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Archetype Data Collection &#8227; 4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents a summary of the curated <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> dataset, where we further elaborate the details of archetype and realism evaluation data collection in following subsections, respectively. For benchmark purpose, we provide an official train-test split for the human annotated dataset for <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> so that following researchers can use the dataset to train and evaluate their SEMs for speech role-play.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "roleplay",
                    "evaluation",
                    "data",
                    "human",
                    "speech",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Archetype evaluation adopts a top&#8211;down perspective from sociology and role theory&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goffman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib23\" title=\"\">1949</a>; Campbell, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib4\" title=\"\">2008</a>)</cite>, abstracting role-play into broad and recognizable categories.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "evaluation",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Role and Scene Dimensions.</span>\nRoles are operationalized through seven categories: (1) functional occupations in daily life, (2) functional occupations in fictional or fantasy settings, (3) social identities, (4) named or specific characters, (5) personality traits, (6) relative relationship, (7) basic events. Scenes are anchored by everyday events or state changes that evoke stereotypical reactions. Details of template design and alternative prompting strategies are given in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>We note that some archetypes (e.g., firefighter) inevitably reflect cultural stereotypes. These are used solely for evaluation scalability and do not reflect normative judgments.</span></span></span></p>\n\n",
                "matched_terms": [
                    "used",
                    "evaluation",
                    "roles"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Sources.</span> The data is collected from a range of speech foundation models, including both cascaded and end-to-end models. We limit that only audio prompts are used as the input but adopt different prompt strategies for best quality with regards to different models. Details about the selected models and prompting details are discussed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS4\" title=\"D.4 Candidate Models for Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "data",
                    "from",
                    "speech",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotation follows two stages: an initial semantic check (i.e., <span class=\"ltx_text ltx_font_italic\">Content Pass</span>) to filter out instruction non-compliance or audio defects, followed by rubric-based scoring on three dimensions: <em class=\"ltx_emph ltx_font_italic\">Audio Quality</em>, <em class=\"ltx_emph ltx_font_italic\">Human-likeness</em>, and <em class=\"ltx_emph ltx_font_italic\">Appropriateness</em>. Confidence judgments are also recorded. Detailed rubric scales are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A5\" title=\"Appendix E Annotation Guidelines for Archetype Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "three",
                    "rubric",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype Evaluation Data Summary.</span>\nThe resulting archetype evaluation dataset comprises 552 distinct scenarios for both Mandarin and English, evaluated across eight models,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>The model list is discussed in Appdenix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS4\" title=\"D.4 Candidate Models for Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.4</span></a>. Note we only include seven models for English due to the deactivation of ChatGPT-4O advanced voice model.</span></span></span> yielding 8,280 samples. In total, the dataset are split into 6,780 for training and 1,500 for testing. For benchmarking, an additional 1,250 scenarios spanning seven role/scene categories are prepared to facilitate archetype role-play benchmarking. More detailed statistics and analyses are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10\" title=\"Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">J</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "across",
                    "evaluated",
                    "model",
                    "evaluation",
                    "data",
                    "benchmarking",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Sources and Negative Samples.</span>\nWe curate speech from (i) ground-truth media segments, (ii) retrieved character profiles, and (iii) generated local scene descriptions. To stress-test evaluation models, we construct contrastive mismatches: (i) negative local scenes with varying degrees of mismatch, and (ii) negative character profiles created by re-synthesizing transcripts with TTS systems. Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6\" title=\"Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F</span></a> details the prompt design and TTS generation details.</p>\n\n",
                "matched_terms": [
                    "tts",
                    "evaluation",
                    "data",
                    "from",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotators judge whether performances align with the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em>, using rubrics for <em class=\"ltx_emph ltx_font_italic\">Prosodic Dynamics (i.e., Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis)</em>, <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness (with progressive gating over three sub-dimensions: Emotion Accuracy, Emotion Intensity, Emotion Transition)</em>, <em class=\"ltx_emph ltx_font_italic\">Character Consistency (i.e., Voice Identity Matching and Traits Embodiment)</em>, <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance (i.e., Local Scene Fit and Global Story Coherence)</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>. A prompt quality check and confidence ratings are also collected. Full scales and gating details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "over",
                    "three",
                    "annotation",
                    "evaluation",
                    "judge"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real-world Recoding.</span>\nAs noted in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S2\" title=\"2 Related Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, most prior efforts have relied heavily on synthetic data, mostly from TTS systems based on curated professional role-play voices. While such data can achieve high fidelity, it often lacks the diversity and spontaneity of real-world delivery, limiting the generalizability of evaluation models. To address this gap, we expand realism evaluation beyond synthetic and curated voices by incorporating newly recorded speech from both professional and amateur voice actors. This design introduces a broader spectrum of delivery styles, from polished professional acting to more natural and less controlled amateur performance.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "tts",
                    "fidelity",
                    "evaluation",
                    "data",
                    "from",
                    "speech",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation Data Summary.</span>\nThe resulting realism evaluation dataset is constructed from a combination of curated media sources, including internal collections, public datasets (e.g., NCSSD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib40\" title=\"\">2024b</a>)</cite>), and original recordings collected for this project.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>All data were obtained under appropriate licenses and are legally available for research release.</span></span></span> We compile 15k annotated role-play samples: 9k in Mandarin and 5k in English. The dataset is divided into 12k samples for training, 2k mixed samples for the base test set, and 1k real-recording samples for the test set. The first test set supports general evaluation using our proposed pipeline with negative samples, while the second focuses exclusively on human speech performed by both professional and amateur voice actors. For benchmarking, 2,000 scenarios from the realism dataset are designated for realism role-play benchmarking. Detailed statistics and analyses are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11\" title=\"Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">K</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "pipeline",
                    "evaluation",
                    "data",
                    "from",
                    "human",
                    "benchmarking",
                    "speech",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the two proposed datasets, we conduct experiments under three settings: zero-shot, few-shot, and finetuning, using pre-trained ALLMs. We formulate the task as a multi-metric estimation problem&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tjandra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib58\" title=\"\">2025</a>; Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib74\" title=\"\">2025</a>; Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib55\" title=\"\">2025b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib54\" title=\"\">a</a>)</cite>, where a single model is trained or prompted to predict multiple evaluation dimensions.</p>\n\n",
                "matched_terms": [
                    "allms",
                    "evaluation",
                    "three",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For proprietary models, we report the average of five independent runs to reduce variance and obtain more stable predictions. For open-source models, we observed weaker instruction-following ability and limited adaptation to unfamiliar tasks. To mitigate this, we did not request simultaneous prediction of multiple metrics. Instead, we queried one metric at a time. Following <cite class=\"ltx_cite ltx_citemacro_citet\">Song &amp; Bahri (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>); Lukasik et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>, we interpret model outputs through token distribution over discrete labels (<span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score.</p>\n\n",
                "matched_terms": [
                    "over",
                    "outputs",
                    "labels",
                    "instructionfollowing",
                    "model",
                    "limited"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For few-shot experiments, we focus primarily on proprietary ALLMs, which demonstrate stronger instruction-following capabilities and have been widely used as judges in prior work. For each test query, we provide three additional annotated samples from the training set as in-context exemplars.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>Please find our detailed settings such as prompts, decoding algorithms in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS1\" title=\"L.1 Zeroshot/Fewshot Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.1</span></a> with additional ablation analysis in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS2\" title=\"L.2 Zeroshot/Fewshot Ablation Experiments &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.2</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "allms",
                    "instructionfollowing",
                    "prompts",
                    "three",
                    "each",
                    "from",
                    "judges",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Finetuning Model: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>.</span>\nIn addition to zero-shot and few-shot evaluations, we fine-tune a dedicated evaluation model, denoted as <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> for archetype and realism role-play evaluation, respectively.\nWe initialize from <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio-7B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib11\" title=\"\">2024</a>)</cite>, which provides a strong foundation for multi-modal audio-text reasoning.\nTo better leverage the multiple human annotations available in our datasets, we adopt a sampling-based training strategy: at each step, one annotator label is randomly selected, following practices in label distribution learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Geng, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib22\" title=\"\">2016</a>; Wen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib67\" title=\"\">2023</a>)</cite>.\nThis stochastic supervision implicitly exposes the model to the empirical annotation distribution, while during inference we output the full distribution over discrete labels (e.g., <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score, consistent with recent decoding-based regression methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Song &amp; Bahri, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>; Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib70\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib64\" title=\"\">2025b</a>; Lukasik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>. We apply LoRA fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib28\" title=\"\">2022</a>)</cite>. All implementation details, including optimizer settings, training schedule, and sampling policy, are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS3\" title=\"L.3 Fine-tuning Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "over",
                    "roleplay",
                    "labels",
                    "during",
                    "label",
                    "annotations",
                    "model",
                    "annotation",
                    "evaluation",
                    "each",
                    "from",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further conduct ablation studies in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS4\" title=\"L.4 Fine-tuning Ablation Experiments &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.4</span></a>, examining: the choice of different base audio LLMs and full-parameter fine-tuning versus LoRA adaptation.\nThese analyses highlight the robustness of our sampling&#8211;expectation training strategy and clarify the trade-offs in model design.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics for Role-play Evaluation Model.</span> For most dimensions with 1-5 scale, we report Pearson correlation. For <span class=\"ltx_text ltx_font_italic\">Content Pass</span> estimation, we report accuracy.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "evaluation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype Evaluation.</span>\nThe results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T2\" title=\"Table 2 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reveal clear distinctions across evaluation settings. In the zero-shot condition, proprietary ALLMs such as <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> substantially outperform public models, achieving strong <span class=\"ltx_text ltx_font_italic\">content pass</span> accuracy (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>90%) and moderate correlations with other dimensions(<math alttext=\"0.43\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mn>0.43</mn><annotation encoding=\"application/x-tex\">0.43</annotation></semantics></math>&#8211;<math alttext=\"0.48\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mn>0.48</mn><annotation encoding=\"application/x-tex\">0.48</annotation></semantics></math>). By contrast, public models exhibit limited predictive ability, with correlations often below 0.2. Few-shot prompting provides modest gains for proprietary models, improving both accuracy and correlation (e.g., <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> rises from 0.480 to 0.493 average correlation). The most significant improvement arises from finetuning: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves an average correlation of 0.629, marking a large margin over zero-shot/few-shot models and demonstrating the effectiveness of task-specific supervision.</p>\n\n",
                "matched_terms": [
                    "over",
                    "allms",
                    "across",
                    "evaluation",
                    "limited",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T3\" title=\"Table 3 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> extends the analysis to fine-grained realism evaluation. Similar to archetype results, proprietary ALLMs lead the zero-shot setting, with <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> obtaining an average correlation of 0.390, compared to 0.284 for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>. Public models remain weak baselines. Few-shot prompting consistently improves performance: <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> gains +0.042 and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> gains +0.083 over their zero-shot averages, showing the benefit of in-context supervision. Once again, finetuning is most effective: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves 0.625 average correlation, higher than the best few-shot results. Importantly, the gains are consistent across all 10 realism dimensions, confirming that the sampling&#8211;expectation training strategy generalizes beyond simple classification to nuanced paralinguistic cues.</p>\n\n",
                "matched_terms": [
                    "over",
                    "evaluation",
                    "allms",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real Recording Test Set.</span>\nThe real-recording evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T4\" title=\"Table 4 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> is notably more challenging, with all models suffering substantial performance drops. Proprietary models in the zero-shot setting achieve only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.04&#8211;0.08 average correlation, and public models often fall below zero. Few-shot prompting yields small but noticeable improvements, especially for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> (0.226 avg.), suggesting that exemplars partially mitigate domain mismatch between synthetic and real human speech. Interestingly, while finetuning achieves the highest overall average (0.247), its relative advantage over few-shot prompting is narrower than in the synthetic setting, and some dimensions (e.g., <span class=\"ltx_text ltx_font_italic\">Character Identity</span>) remain weak. We emphasize that this observed gap between synthetic and real recordings is not merely an artifact but an intended feature of Speech-DRAME, designed to expose the mismatch and drive future research into bridging synthetic-to-real generalization.</p>\n\n",
                "matched_terms": [
                    "over",
                    "evaluation",
                    "human",
                    "speech",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall.</span> Across both archetype and realism evaluations, we observe three consistent trends: (i) zero-shot performance is limited, particularly for open-source models, (ii) few-shot prompting offers moderate but consistent gains, especially for proprietary ALLMs, and (iii) finetuning with task-specific supervision (<span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>) delivers the largest improvements, achieving state-of-the-art correlations across nearly all dimensions. Nevertheless, results on the real-recording test set underscore the gap between synthetic and real human role-play scenarios, motivating future work in data diversification and domain adaptation.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "across",
                    "allms",
                    "three",
                    "limited",
                    "data",
                    "human",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on fine-tuned <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, we establish the DRAME-RoleBench to evaluate SFMs&#8217; role play ability. For archetype evaluation, 1,250 scenarios are constructed following the same data construction pipeline as the <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>. For realism evaluation, we reuse 2,000 scenarios in the <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> test set. We follow the same prompt strategy as outlined in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "pipeline",
                    "evaluation",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, we consider a comprehensive set of candidate models, including six end-to-end models: <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib47\" title=\"\">2025b</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib75\" title=\"\">2024</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib71\" title=\"\">2025a</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Qwen3Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib72\" title=\"\">2025b</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">StepAudio2Mini</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib68\" title=\"\">2025</a>)</cite>; and ten cascaded models by combining two widely-used textual LLMs (i.e., <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib12\" title=\"\">2025</a>)</cite> and <span class=\"ltx_text ltx_font_typewriter\">Qwen3-30B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib73\" title=\"\">2025</a>)</cite>) and five recent TTS models (i.e., <span class=\"ltx_text ltx_font_typewriter\">F5TTS</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib8\" title=\"\">2024c</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">IndexTTS2</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib82\" title=\"\">2025b</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">MaskGCT</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib66\" title=\"\">2025d</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Vevo1.5</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib79\" title=\"\">2025b</a>)</cite>, and <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib17\" title=\"\">2024b</a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "llms",
                    "evaluation",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype evaluation.</span>\nOn archetype role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T5\" title=\"Table 5 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), cascaded pipelines consistently outperform end-to-end systems. The strongest cascaded systems, particularly <span class=\"ltx_text ltx_font_typewriter\">G-IndexTTS2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-IndexTTS2</span>, achieve both high content pass rates (above 94%) and balanced scores across audio quality, human likeness, and appropriateness. By contrast, end-to-end models such as <span class=\"ltx_text ltx_font_typewriter\">StepAudio2Mini</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> perform competitively in terms of content coverage (above 88%), but their expressive scores remain slightly lower than their cascaded counterparts. This gap suggests that separating reasoning (LLM) from speech rendering (TTS) remains advantageous when archetype fidelity is emphasized. Nevertheless, the fact that top end-to-end systems already surpass 3.0 in average rating indicates rapid closing of the performance gap.</p>\n\n",
                "matched_terms": [
                    "pipelines",
                    "roleplay",
                    "across",
                    "tts",
                    "fidelity",
                    "expressive",
                    "evaluation",
                    "from",
                    "human",
                    "speech",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism evaluation.</span>\nRealism role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T6\" title=\"Table 6 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) presents a different landscape. Here, performance differences between end-to-end and cascaded systems are less pronounced. Several end-to-end models, such as <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>, reach average scores of 3.24, matching or even exceeding the best cascaded pipelines. Importantly, realism scores are more evenly distributed across prosodic, emotional, character, and contextual dimensions, with most models clustering between 2.9&#8211;3.2. This convergence reflects the difficulty of realism-style evaluation: since the data preparation emphasized contrast with authentic human speech, models are judged under similar conditions, leading to smaller observed gaps.</p>\n\n",
                "matched_terms": [
                    "pipelines",
                    "roleplay",
                    "across",
                    "evaluation",
                    "data",
                    "human",
                    "speech",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human alignment.</span>\nWe further conducted small-scale human studies with 10% of the benchmark data to verify the reliability of the benchmark. Archetype evaluation achieves a Spearman correlation of 0.706 between human judgments and <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, validating that the automatic evaluator closely tracks human preferences in this setting. Realism evaluation, however, achieves a more modest Spearman correlation of 0.375. This lower alignment is expected, since realism data construction did not prioritize fine-grained model separation but instead emphasized distinguishing human versus synthetic speech.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>We view this as evidence that realism presents a harder, less-saturated frontier. Rather than a limitation of DRAME-Eval, this highlights the challenge of designing evaluators that capture nuanced human perception, which motivates future work.</span></span></span> As a result, all models occupy a relatively narrow performance band, reducing sensitivity to model-level differences. We view this as evidence that realism evaluation highlights a harder, less saturated frontier where both SFMs and SEMs require further advances.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "automatic",
                    "model",
                    "sensitivity",
                    "evaluation",
                    "data",
                    "human",
                    "limitation",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, cascaded pipelines retain an edge in archetype scenarios, especially in content robustness and expressive fidelity, but end-to-end models are catching up quickly. In realism scenarios, the field remains unsolved: existing systems exhibit similar performance bands and evaluators show weaker alignment with human judgments, underscoring the need for improved realism-grounded data and evaluation. Together, these findings establish <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> as both a reliable testbed for archetype fidelity and a challenging benchmark for realism fidelity.</p>\n\n",
                "matched_terms": [
                    "pipelines",
                    "benchmark",
                    "expressive",
                    "fidelity",
                    "evaluation",
                    "data",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework for assessing spoken role-play through two complementary lenses: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em> (top-down) and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> (bottom-up, human-grounded). Our human-annotated <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> enables training and testing of SEMs, and our fine-tuned <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span> substantially outperforms zero-/few-shot ALLM judges. Building on this, <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> provides system-level comparison across various SFMs, revealing a consistent edge for cascaded pipelines on archetypes and a tighter spread under realism, where real human performance evaluation remain challenging. Together, these resources establish a reproducible, analyzable foundation for spoken role-play evaluation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>Limitations and future works are discussed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A13\" title=\"Appendix M Limitations and Future Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">M</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "pipelines",
                    "roleplay",
                    "across",
                    "evaluation",
                    "human",
                    "judges",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All data collection and annotation in this work followed strict ethical standards. Our bilingual datasets were constructed exclusively from legally accessible sources, including licensed media, open-source corpora, and original recordings conducted with informed consent. Annotators were recruited with appropriate compensation and provided with clear instructions, including the right to withdraw at any point. To minimize risks, no personally identifiable information was included in the released data, and all recordings were anonymized before annotation. We ensured cultural and linguistic diversity in annotator pools to mitigate potential bias across Mandarin and English role-play scenarios. While this study did not require formal IRB approval, we adhered to best practices in human-subject research, including transparency in task design and careful consideration of annotator well-being. Full details of our annotation protocol and ethical safeguards are presented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A9\" title=\"Appendix I Annotation Ethics &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "across",
                    "annotation",
                    "data",
                    "from",
                    "instructions",
                    "bias"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Large language models (LLMs) and audio-capable LLMs were used in four capacities in this work:</p>\n\n",
                "matched_terms": [
                    "llms",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data generation:</span> LLMs were employed to expand prompts (e.g., character profiles, scene descriptions) and to produce negative samples in realism evaluation. All outputs were screened by human annotators to ensure validity and reduce bias.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "outputs",
                    "prompts",
                    "evaluation",
                    "data",
                    "human",
                    "bias"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline evaluators:</span> State-of-the-art audio LLMs (e.g., <span class=\"ltx_text ltx_font_typewriter\">GPT4o</span>, <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>) were included as zero-shot judges in our evaluation benchmark, enabling direct comparison with human-aligned SEMs.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "benchmark",
                    "evaluation",
                    "judges",
                    "comparison"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation design and task construction:</span> <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> was used to assist with building instructions and summarizing story contexts. Drafted materials were subsequently refined by the authors and validated by human annotators.</p>\n\n",
                "matched_terms": [
                    "used",
                    "instructions",
                    "human",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Manuscript preparation:</span> LLMs were used for polishing, proofreading, and improving clarity of the paper. All conceptual design, analysis, and substantive writing remain the work of the authors, with LLMs serving only as language assistants.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Proprietary LLM outputs were used in a limited way (e.g., instruction building, story summarization, or baseline evaluations) and only for internal research purposes. Released datasets in Speech-DRAME are constructed from open-source models, licensed corpora, and original human recordings.</p>\n\n",
                "matched_terms": [
                    "outputs",
                    "limited",
                    "from",
                    "human",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our distinction between <span class=\"ltx_text ltx_font_italic\">Archetype Evaluation</span> and <span class=\"ltx_text ltx_font_italic\">Realism Evaluation</span> is not merely pragmatic, but is grounded in established theories of social identity, communication, and performance. The two perspectives capture complementary dimensions of how humans perceive and evaluate roles in interaction.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "roles"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism as Situated Performance</span>\nIn contrast, realism highlights the fine-grained enactment of roles in situated interaction. Goffman&#8217;s dramaturgical sociology emphasizes that everyday social life is itself a performance, where believability depends on subtle paralinguistic and pragmatic cues such as prosody, hesitation, and emotional shading&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goffman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib23\" title=\"\">1949</a>)</cite>. Similarly, interactional sociolinguistics has demonstrated how micro-level features like intonation, pacing, and turn-taking patterns shape the perception of authenticity in communication&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gumperz, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib25\" title=\"\">1982</a>)</cite>. Realism Evaluation thus anchors role-play assessment in bottom-up sensitivity to authentic delivery, beyond schematic expectations.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "evaluation",
                    "roles",
                    "sensitivity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realism Evaluation captures micro-level cues of authenticity, ensuring sensitivity to nuanced delivery and interactional fit.</p>\n\n",
                "matched_terms": [
                    "sensitivity",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, they mirror the dual nature of human role assessment, balancing scalability and comparability with depth and nuance. This theoretical grounding reinforces the design of Speech-DRAME as a framework that does not privilege one perspective over the other, but instead integrates both to reflect the full spectrum of how roles are evaluated in speech-based interaction.</p>\n\n",
                "matched_terms": [
                    "over",
                    "roles",
                    "evaluated",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our main formulation (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS1\" title=\"3.1 Problem Formulation &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) focuses on the <em class=\"ltx_emph ltx_font_italic\">single-turn</em> setting, where each response <math alttext=\"{\\bm{S}}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119930;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{S}}_{r}</annotation></semantics></math> is generated and evaluated independently.\nThis simplification enables clearer analysis of speech role-play quality and aligns with our bilingual annotation protocol.\nNevertheless, the same formulation extends naturally to multi-turn dialogues.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "evaluated",
                    "dialogues",
                    "annotation",
                    "each",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Multi-turn evaluation.</span>\nEach generated speech segment is scored by an SEM:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "evaluation",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dialogue-level aggregation.</span>\nA dialogue-level evaluation can be obtained by applying an aggregation operator <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p4.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> over turn-level scores:</p>\n\n",
                "matched_terms": [
                    "over",
                    "evaluation",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p4.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> may be a simple mean, a recency-weighted average, or a learned temporal model that captures consistency across turns.\nThe choice of <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p4.m3\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> reflects the intended evaluation focus, e.g., stability of persona traits, adaptation to scene progression, or coherence across multiple speakers.</p>\n\n",
                "matched_terms": [
                    "across",
                    "evaluation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Discussion.</span>\nWhile our released dataset and experiments concentrate on single-turn evaluation, the modular nature of the formulation ensures that extending to multi-turn is straightforward.\nFuture work can leverage <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p5.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> to study richer aspects of speech role-play, such as persona persistence, interaction dynamics, and role consistency across extended dialogues.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "across",
                    "dialogues",
                    "evaluation",
                    "speech",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This appendix first discusses the summary of task preparation steps for archetype evaluation. Then we provide concrete examples and extended discussion of the archetype evaluation annotation pipeline that are omitted in the main text for clarity.</p>\n\n",
                "matched_terms": [
                    "pipeline",
                    "evaluation",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Archetype Evaluation follows a top&#8211;down perspective rooted in sociology and role theory, where social behavior is interpreted through shared stereotypes and generalized expectations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goffman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib23\" title=\"\">1949</a>; Campbell, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib4\" title=\"\">2008</a>)</cite>.\nThis strategy abstracts role-play into broad and easily recognizable archetypes, enabling scalable evaluation of whether a generated response aligns with salient role features.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Role dimensions.</span>\nWe operationalize archetypes through five principal categories, each representing a stereotypical dimension of role identity or behavior, including (1) <span class=\"ltx_text ltx_font_bold\">Occupation / Functional Role (Modern Daily Life)</span> (e.g., a firefighter rescuing a trapped civilian). (2) <span class=\"ltx_text ltx_font_bold\">Occupation / Functional Role (Fictional or Fantasy)</span> (e.g., a martial arts cult leader confronting orthodox rivals). (3)&#160;<span class=\"ltx_text ltx_font_bold\">Social Identity</span> (e.g., a doting parent worrying about a child, or a &#8220;Buddhist youth&#8221; eschewing competition). (4) <span class=\"ltx_text ltx_font_bold\">Named / Specific Characters</span> (e.g., Sun Wukong mocking celestial authorities in <em class=\"ltx_emph ltx_font_italic\">Journey to the West</em>). (5) <span class=\"ltx_text ltx_font_bold\">Personality Traits</span> (e.g., &#8220;hot-blooded&#8221; in a competitive setting, or &#8220;complaining&#8221; in response to poor service). (6) <span class=\"ltx_text ltx_font_bold\">Relative Roles</span> (e.g., parent or stranger).</p>\n\n",
                "matched_terms": [
                    "roles",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scene dimensions.</span>\nIn addition to roles, we incorporate contextual triggers that anchor role-play in stereotypical situations.\nA central focus is on <span class=\"ltx_text ltx_font_bold\">everyday external events or state changes</span> (e.g., receiving a promotion, being caught in gossip), which provide concrete scenarios for testing archetypical responses.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "roles"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Exclusion of fine-grained controls.</span>\nWe deliberately excluded explicit emotional states, prosodic instructions, or linguistic constraints.\nThese dimensions are better suited to realism-oriented or style-controlled evaluation, whereas archetype evaluation benefits from concrete, stereotype-level scenarios that sharpen judgments and improve human agreement.</p>\n\n",
                "matched_terms": [
                    "agreement",
                    "evaluation",
                    "from",
                    "human",
                    "instructions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Pipeline overview.</span>\nAt a high level, archetype evaluation data are generated through a three-stage pipeline:\n(1) selecting keywords that specify role and scene;\n(2) expanding them into full prompts via structured templates; and\n(3) rendering prompts into speech with contemporary TTS systems.\nThis modular pipeline ensures both scalability and reproducibility, while keeping prompts vivid enough for reliable judgments. (4) Based on the generated data (paired scene and synthesized speech), expert human annotators are recruited for detailed annotation on semantic check (i.e., <span class=\"ltx_text ltx_font_italic\">Content Pass</span>) and three different spoken role-play dimensions (i.e., <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human-likeness</span>, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>).\nImplementation details, including template design, alternative prompting strategies, and annotation guidelines, are provided in the following subsections of this Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "tts",
                    "prompts",
                    "pipeline",
                    "three",
                    "annotation",
                    "evaluation",
                    "data",
                    "human",
                    "speech",
                    "via"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, archetype evaluation emphasizes breadth and recognizability over subtlety.\nBy grounding role-play in socially recognizable categories and stereotypical situations, it provides a practical balance between scalability and interpretability, complementing the more fine-grained realism evaluation described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS2\" title=\"3.2 Role-play Evaluation with Two Strategies &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "over",
                    "roleplay",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We constructed archetype evaluation prompts by combining role categories with contextual scenes.\nThe following illustrates the core categories with representative examples.</p>\n\n",
                "matched_terms": [
                    "representative",
                    "evaluation",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Excluded Dimensions.</span> We initially considered more fine-grained control dimensions, but excluded them to preserve the archetype-level focus. Examples of excluded dimensions include:\n<em class=\"ltx_emph ltx_font_italic\">Emotional State &amp; Expression, Single Emotion Prompts, Explicit User Input / Assistant Output structures, Emotional Transition / Fluctuation, Voice Style &amp; Prosody (pacing, intonation, vocal timbre, expressive style, accent/dialect), and Content / Interaction Patterns (task/topic controls, specific linguistic structures, or speech production challenges).</em></p>\n\n",
                "matched_terms": [
                    "prompts",
                    "expressive",
                    "style",
                    "control",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These dimensions are more closely aligned with explicit instruction-following or style transfer rather than broad stereotype judgment, and they reduced clarity in human evaluations.</p>\n\n",
                "matched_terms": [
                    "instructionfollowing",
                    "human",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">The Choice of Prompts.</span> We also experimented with broader, less specific templates that allowed more freedom for generative models but produced less discriminative evaluation results:</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While LLMs often responded fluently to these broad templates, human annotators found judgments to be more ambiguous and tolerant, making scores less sharp. The final design therefore emphasizes more concrete, vivid prompts, as elaborated in the above examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "prompts",
                    "human",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All steps are automatically processed by <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib12\" title=\"\">2025</a>)</cite>, but each output is manually verified to ensure high quality for downstream usage. We demonstrate an example for Roles: Occupation / Functional Role in Fictional or Fantasy as follows:</p>\n\n",
                "matched_terms": [
                    "roles",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech generation.</span>\nPrompts were rendered into audio with CosyVoice2-0.5B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib17\" title=\"\">2024b</a>)</cite>. The prompts are selected from human-verified high-quality samples, where the selection is conducted from open-source datasets, such as LibriSpeech/Gigaspeech for English, and WenetSpeech for Chinese&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib49\" title=\"\">2015</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib5\" title=\"\">2021</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib77\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "prompts",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Collection for Proprietary Models.</span> For proprietary systems, we adopt two collection strategies. For GPT-4o-realtime, responses are obtained directly via the <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-realtime</span> API. For Doubao and GPT-4o (Advanced Voice Mode), we collect outputs by recording from their official web interfaces. All recordings are captured through the interaction device itself, ensuring that no additional processing or distortion is introduced into the final audio.</p>\n\n",
                "matched_terms": [
                    "outputs",
                    "from",
                    "via"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">End-to-End Model Generation.</span>\nFor end-to-end systems, the audio prompt is directly used as input. The system prompt is set to default values to ensure consistent speech generation quality.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "used",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cascaded Model Generation.</span>\nFor cascaded systems, we only use Cosyvoice2 as the TTS model in our collection of data. We select <span class=\"ltx_text ltx_font_typewriter\">Qwen3-30B-Instruct</span> and <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> as the text LLM candidates&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib73\" title=\"\">2025</a>; Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib12\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "data",
                    "model",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the generation, we adopt a retrieval-augmented generation&#160;(RAG) strategy that combines text LLMs with zero-shot TTS models:</p>\n\n",
                "matched_terms": [
                    "llms",
                    "tts",
                    "during"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Step 1: We first build a speech database (LibriSpeech and GigaSpeech for English, WenetSpeech for Mandarin&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib49\" title=\"\">2015</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib5\" title=\"\">2021</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib77\" title=\"\">2022</a>)</cite>). Each sample is annotated with Gemini-based speech captions describing salient acoustic attributes.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Step 4: The retrieved samples guide the zero-shot TTS system, which synthesizes speech conditioned on both the transcript and matched acoustic profile.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2\" title=\"Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the internal annotation interface used for <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>.\nThe platform provides an integrated environment for both playback and evaluation of model-generated role-play speech samples.\nAnnotators first interact with the audio player interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf1\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>), which visualizes waveforms and spectrograms for multiple versions of the same prompt, enabling side-by-side comparison under a shared role instruction (e.g., firefighter, teacher, or actor).\nThen, they proceed to the annotation interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf2\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a>), where they assign scores for multiple dimensions such as <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, along with reasoning and confidence inputs.\nThis structured interface supports high-consistency, fine-grained evaluation of role-play behaviors in speech models.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "annotation",
                    "evaluation",
                    "human",
                    "speech",
                    "used",
                    "comparison",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This appendix contains the full human annotation instructions used for archetype evaluation.</p>\n\n",
                "matched_terms": [
                    "annotation",
                    "evaluation",
                    "human",
                    "used",
                    "instructions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt three scales, each rated from 1 (worst) to 5 (best):</p>\n\n",
                "matched_terms": [
                    "three",
                    "each",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comments for Audio Quality and Human Likeness are only required for scores <math alttext=\"\\leq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I3.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8804;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\leq 3</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "human",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Initial Assessment &amp; Rejection Instructions.</span>\nListen to each audio <em class=\"ltx_emph ltx_font_italic\">in full</em>, paying special attention to the following caveats and rejection criteria.</p>\n\n",
                "matched_terms": [
                    "criteria",
                    "instructions",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">30-second focus:</span> Assessment should focus on the first 30 seconds of the generation ONLY. Do not consider any content past 30s; disregard it entirely from your assessment and rubric ratings. (You may stop listening beyond 30s.)</p>\n\n",
                "matched_terms": [
                    "from",
                    "rubric"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Early breakdowns:</span> If repetition, restarts, or obvious breakdowns occur <em class=\"ltx_emph ltx_font_italic\">before</em> 30s (or for total clips shorter than 30s), the clip may be rejected.</p>\n\n",
                "matched_terms": [
                    "clip",
                    "clips"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Excessively short clips:</span> If a clip is only 1&#8211;2 seconds, reject due to insufficient information for reliable rubric ratings.</p>\n\n",
                "matched_terms": [
                    "clip",
                    "clips",
                    "rubric"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rubric Rating Instructions (for content pass = true).</span>\nWe shift focus from <em class=\"ltx_emph ltx_font_italic\">what</em> was said to <em class=\"ltx_emph ltx_font_italic\">how</em> it was said (vocal performance). Use gut intuition guided by rubric definitions. Score each audio <em class=\"ltx_emph ltx_font_italic\">in isolation</em> (no ranking across clips).</p>\n\n",
                "matched_terms": [
                    "across",
                    "each",
                    "from",
                    "rubric",
                    "clips",
                    "instructions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the prompt indicates a specific gender, flag mismatches in comments (e.g., &#8220;GENDER MISMATCH&#8221;). Do <em class=\"ltx_emph ltx_font_italic\">not</em> penalize rubric scores for gender mismatch; rate vocal performance as-is.</p>\n\n",
                "matched_terms": [
                    "rubric",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Judge within traditional <em class=\"ltx_emph ltx_font_italic\">stereotypes</em> for the prompt; when in doubt between two scores, choose the <em class=\"ltx_emph ltx_font_italic\">lower</em>.</p>\n\n",
                "matched_terms": [
                    "judge",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If any portion of the clip is ill-fitting, prefer the lower overall score; do not average across good and bad segments.</p>\n\n",
                "matched_terms": [
                    "clip",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This appendix starts with a summary of the task preparation pipeline for our proposed realism evaluation. Then, we provides the detailed prompts and system setups for constructing the Realism Evaluation dataset described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4.SS2\" title=\"4.2 Realism Data Collection &#8227; 4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "pipeline",
                    "prompts",
                    "evaluation",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realism Evaluation adopts a bottom&#8211;up perspective grounded in real human speech data, with the goal of capturing the nuanced delivery, prosody, and contextual fit that synthetic archetype prompts cannot fully represent.\nRather than relying on stereotypes, this strategy leverages real-world recordings and carefully constructed contrasts, enabling fine-grained assessment of model performance under realistic conditions.</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "model",
                    "evaluation",
                    "data",
                    "goal",
                    "human",
                    "speech",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data from real human speech sources.</span>\nWe construct the realism evaluation set primarily from natural speech material:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "data",
                    "from",
                    "human",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Ground-truth audio from TV and film.</span>\nSpoken segments are curated from publicly available media, ensuring coverage across diverse roles and communicative settings.</p>\n\n",
                "matched_terms": [
                    "across",
                    "roles",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Retrieved character profiles.</span>\nEach audio segment is paired with a concise character description derived from the media context.\nThe detailed prompt design for generating these character profiles is provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS2\" title=\"F.2 Prompts for Character Profile Creation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generated local scenes.</span>\nWe supplement retrieved character information with automatically generated local scene descriptions summarizing the surrounding plot or setting.\nAppendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS3\" title=\"F.3 Prompts for Local Scene Generation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.3</span></a> includes representative prompts and generation details.</p>\n\n",
                "matched_terms": [
                    "representative",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative local scenes.</span>\nFor each original segment, we generate additional scene descriptions at different levels of mismatch, ranging from subtle alterations to fully unrelated scenarios.\nAppendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS4\" title=\"F.4 Prompts for Negative Scenes &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.4</span></a> outlines the prompting strategies and example outputs.</p>\n\n",
                "matched_terms": [
                    "outputs",
                    "each",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative character profiles.</span>\nWe also create role-profile mismatches by resynthesizing transcripts with diverse TTS systems, thereby breaking the link between authentic delivery and intended role.\nWe explore both text-instruction-based prompting and comparisons across commercial and non-commercial TTS frameworks.\nDetailed model lists and parameter settings are documented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS6\" title=\"F.6 TTS Systems and Settings &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.6</span></a>.</p>\n\n",
                "matched_terms": [
                    "across",
                    "model",
                    "tts",
                    "frameworks"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, real human ground-truth segments and systematically constructed negatives form a balanced dataset that supports realism-oriented evaluation.\nThe positives provide natural anchors of role-play performance, while the negatives introduce controlled mismatches that test whether evaluation models can distinguish real human speech from synthetic approximations.\nThis design ensures that realism evaluation emphasizes subtle paralinguistic and contextual cues, moving beyond intelligibility or content fidelity.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "fidelity",
                    "evaluation",
                    "from",
                    "human",
                    "speech",
                    "dataset",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During annotation, raters are presented with either: (i) ground-truth stories, (ii) stories with negative local scenes, or (iii) stories with negative character profile. Annotators assign realism scores without being exposed to the underlying mismatch reasons. Instead, these reasons are provided only to the quality-assurance team, who use them as reference for internal validation and consistency checks. While the mismatch reasons may not be perfect, they help streamline quality control procedures and maintain reliability across the dataset.</p>\n\n",
                "matched_terms": [
                    "across",
                    "during",
                    "checks",
                    "annotation",
                    "control",
                    "dataset",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To provide consistent evaluation context, we generate local scene descriptions summarizing the situational background.\nThese prompts aim to capture the immediate narrative context without introducing extraneous detail.</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Negative scenes are created by altering or mismatching the original context at varying levels of severity.\nThis section provides the prompting templates and representative outputs.</p>\n\n",
                "matched_terms": [
                    "representative",
                    "outputs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the character Yunjin from <span class=\"ltx_text ltx_font_italic\">Genshin Impact</span> as a running case study, which is also included in our released dataset.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>As acknowledged, the use of real-world recordings is conducted under legal and ethical review. All curated data will be released as part of our open-source package.</span></span></span> This example illustrates how alignment between a speaker&#8217;s persona, local context, and delivery underpins realism, and how systematically breaking that alignment produces contrastive test cases.</p>\n\n",
                "matched_terms": [
                    "data",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Level 0 (Mild Mismatch).</span>\n<em class=\"ltx_emph ltx_font_italic\">Fake Scene:</em> A quiet teahouse where Yunjin casually chats with friends about recent weather. She suddenly delivers the heroic verses in full operatic style.\n<em class=\"ltx_emph ltx_font_italic\">Reason:</em> The shift from light conversation to intense battle imagery feels abrupt and unsupported. The mismatch is mild but noticeable.</p>\n\n",
                "matched_terms": [
                    "from",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Yunjin case study illustrates a fundamental difference between realism evaluation and archetype evaluation. Archetype prompts, as described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>, are constructed by pairing abstract role categories with synthetic contextual scenes. These cover a broad space of possible situations, ranging from modern occupations (e.g., firefighters) to fictional personas (e.g., cult leaders in martial arts fantasy) or social identities (e.g., overprotective parents, laid-back friends). While such prompts provide controlled and diverse coverage, they are ultimately template-based and rely on stereotypes to elicit role-play behaviors.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "prompts",
                    "evaluation",
                    "from",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the Yunjin example is drawn from authentic performance, where the character&#8217;s persona (an opera singer) and local scene (a battlefield narration) are naturally aligned through artistic delivery. The realism task does not merely test whether a model can follow a prompt, but whether it can preserve coherence between a speaker&#8217;s global identity, immediate context, and expressive prosody. When Yunjin&#8217;s transcript is transplanted into mismatched settings or paired with incongruent speaker profiles, the breakdown in realism becomes apparent: the same words shift from inspiring to awkward, jarring, or even absurd.</p>\n\n",
                "matched_terms": [
                    "model",
                    "from",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This distinction highlights the complementary strengths of the two evaluation paradigms. Archetype evaluation systematically explores a wide combinatorial space of roles and situations, but may lack the paralinguistic and contextual richness of real speech. Realism evaluation, on the other hand, anchors assessment in authentic delivery and contextual fit, providing a finer-grained lens on coherence and naturalness. Together, they ensure that role-play evaluation spans both broad stereotypical coverage and nuanced real-world plausibility.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "paradigms",
                    "roles",
                    "evaluation",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed a diverse set of TTS systems to generate negative character profiles and resynthesized speech for Realism Evaluation.\nThese included both commercial and non-commercial models, with variation in style control, conditioning prompts, and reference usage.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.T8\" title=\"Table 8 &#8227; Same-Speaker Voice Clone. &#8227; F.6 TTS Systems and Settings &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> summarizes the systems and settings.\nBelow, we detail the major strategies used across systems:</p>\n\n",
                "matched_terms": [
                    "across",
                    "tts",
                    "prompts",
                    "style",
                    "evaluation",
                    "control",
                    "speech",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We leveraged all publicly available speaker style information released by the corresponding websites.\nFor each input sample, we employed <span class=\"ltx_text ltx_font_typewriter\">deepseek-v3</span> to select the best-matching speaker based on the original speech caption.\nIn the case of <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5TTS</span>, the character profile was also appended as an additional instruction to guide synthesis.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "each",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We used each platform&#8217;s built-in search functionality to identify the closest available voice to the ground-truth recording.\nThe selected voice was then used to resynthesize the utterances.</p>\n\n",
                "matched_terms": [
                    "used",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A character profile was first converted into a descriptive voice specification by prompting <span class=\"ltx_text ltx_font_typewriter\">deepseek-v3</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib38\" title=\"\">2024a</a>)</cite>.\nThis description was submitted to the TTS service to generate candidate voices.\nWe then previewed samples via Gemini2.5Pro, selecting the voice that best matched the speaker style of the original recording before proceeding to generate speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "tts",
                    "style",
                    "via"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We conducted cloning using the ground-truth voice as a source, but with an intermediate representation (voice information or tag) provided to the TTS system rather than direct conditioning.</p>\n\n",
                "matched_terms": [
                    "source",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This setting differed from the above in that the ground-truth voice was directly used as conditioning input, without an intermediate tag.</p>\n\n",
                "matched_terms": [
                    "used",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Here, instead of conditioning on the specific ground-truth utterance, we used another utterance from the same speaker to guide synthesis.\nThis tests robustness to cross-utterance generalization.</p>\n\n",
                "matched_terms": [
                    "used",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For models using voice prompts, we use an inhouse enhancement model to first process the prompt audio so as to minimize the effect of audio events and background noise.</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We additionally employed five proprietary inhouse TTS systems, each exploring different conditioning strategies for realism-oriented speech synthesis.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "each",
                    "tts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS1</span> (Phone&#8211;Cosy2 Pipeline).\nThe input is a phone sequence produced by a graphme-to-phoneme&#160;(G2P) front-end. A 0.4B GPT-2 model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib50\" title=\"\">2019</a>)</cite> predicts <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> S3-tokens, which are then passed through a flow-matching model to generate mel-spectrograms. A neural vocoder from <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> converts the mel features into audio. Both the GPT-2 and flow-matching modules are augmented with speaker embeddings for speaker control.</p>\n\n",
                "matched_terms": [
                    "model",
                    "pipeline",
                    "control",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS2</span> (BPE&#8211;Cosy2 Pipeline).\nInstead of phones, this model takes Qwen2.5 byte-pair-encoding (BPE) tokens as input&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib71\" title=\"\">2025a</a>)</cite>. The same 0.4B GPT-2 predicts <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> tokens, followed by the identical flow-matching and vocoder stages as in <span class=\"ltx_text ltx_font_typewriter\">InhouseTTS1</span>. Speaker embeddings are also injected to maintain consistent speaker style.</p>\n\n",
                "matched_terms": [
                    "pipeline",
                    "model",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS3</span> (Embedding&#8211;AudioCode Pipeline).\nHere, the input is Qwen2.5 BPE embeddings, which a 0.4B GPT-2 model maps directly to 4,096-dimensional audio codes. These codes are decoded via a cascaded architecture: <span class=\"ltx_text ltx_font_typewriter\">RepCodec</span> decoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib32\" title=\"\">2024</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> content representation (with speaker embedding) <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DDIM</span> (denoising\ndiffusion implicit model) module&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jeong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib34\" title=\"\">2021</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DAC</span> latent representation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib37\" title=\"\">2023</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DAC</span> decoder. Conditioning is provided through prefix text embeddings (e.g., instructions expressing emotional states such as &#8220;angry, hateful: you betrayed me&#8230;&#8221;), along with speaker ID. The input format is <span class=\"ltx_text ltx_font_typewriter\">[spk_id, BPE embedding, audio codes]</span>.</p>\n\n",
                "matched_terms": [
                    "pipeline",
                    "instructions",
                    "model",
                    "via"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS4</span> (Instruction-Conditioned AudioCode Pipeline).\nThis model also employs a 0.4B GPT-2 to predict 4,096-dimensional audio codes, but conditions generation on instruction embeddings. A <span class=\"ltx_text ltx_font_typewriter\">CLIPTextEncoder</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib51\" title=\"\">2021</a>)</cite> converts instruction text into embeddings, which serve as a prefix, combined with speaker ID and phone sequence. The input format is <span class=\"ltx_text ltx_font_typewriter\">[spk_id, instruction embedding sequence, phone sequence, audio codes]</span>.</p>\n\n",
                "matched_terms": [
                    "pipeline",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS5</span> (Reference-Conditioned AudioCode Pipeline).\nSimilar to <span class=\"ltx_text ltx_font_typewriter\">InhouseTTS4</span>, but instead of text-based instructions, conditioning is achieved through reference audio. A style encoder, following the NANCY++ design&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Choi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib10\" title=\"\">2023</a>)</cite>, compresses the reference audio into a fixed 32-dimensional embedding. This embedding is concatenated with the phone sequence and audio codes for generation. The input format is <span class=\"ltx_text ltx_font_typewriter\">[32-dim ref embedding, phone sequence, audio codes]</span>.</p>\n\n",
                "matched_terms": [
                    "pipeline",
                    "instructions",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The annotation interface for <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> focuses on assessing contextual and narrative coherence in model-generated speech.\nUnlike the archetype-oriented evaluation, which emphasizes stylistic and emotional fidelity, this platform examines how well the voice performance aligns with a character&#8217;s personality, dialogue situation, and scene progression.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "evaluation",
                    "fidelity",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.F3\" title=\"Figure 3 &#8227; F.7 Annotation Platform for Realism Evaluation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the core playback interface.\nAnnotators can preview each audio sample with synchronized spectrogram and waveform displays, while reviewing contextual metadata including the <span class=\"ltx_text ltx_font_italic\">Local Scene</span>, <span class=\"ltx_text ltx_font_italic\">Character Style</span>, and <span class=\"ltx_text ltx_font_italic\">Character Profile</span>.\nThese descriptions help situate the listening task, providing the background necessary for judging realism in the character&#8217;s vocal and behavioral expression.</p>\n\n",
                "matched_terms": [
                    "each",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This appendix contains the full human annotation instructions used for realism evaluation.</p>\n\n",
                "matched_terms": [
                    "annotation",
                    "evaluation",
                    "human",
                    "used",
                    "instructions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt five dimensions, each rated from 1 (worst) to 5 (best). Emotional Expressiveness uses <em class=\"ltx_emph ltx_font_italic\">progressive gating</em>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Focus only on human speech:</span> Do not factor in background music/soundtracks, environmental or ambient noise, or non-speech audio events; these may reflect the scene but should not influence scoring of the spoken performance.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt contains contradictory information.</span> Conflicts make accurate annotation difficult or impossible. Examples: incompatible accent specifications; Local Scene drastically at odds with intended emotion (see <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>); Local and global scenes inconsistent; traits described as &#8220;evolving over time&#8221; without a timeline.</p>\n\n",
                "matched_terms": [
                    "over",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rubric Rating Instructions.</span>\nScore each subcriterion on a 1&#8211;5 scale. Use the detailed rubrics below and the examples to maintain calibration across characters, tones, and emotional styles. Prioritize in-character believability; do not reward theatricality, emotional intensity, or technical polish unless they naturally fit the character <em class=\"ltx_emph ltx_font_italic\">and</em> scene.</p>\n\n",
                "matched_terms": [
                    "across",
                    "calibration",
                    "each",
                    "rubric",
                    "instructions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human-grounded, bilingual labels.</span>\nWe annotate in English <em class=\"ltx_emph ltx_font_italic\">and</em> Mandarin with trained raters, capturing cross-lingual prosody and style, while recording rater confidence to qualify downstream usage.</p>\n\n",
                "matched_terms": [
                    "labels",
                    "rater",
                    "style"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dual settings: Archetype vs. Realism.</span>\nWe deliberately separate stereotype-driven <em class=\"ltx_emph ltx_font_italic\">Archetype</em> from bottom-up <em class=\"ltx_emph ltx_font_italic\">Realism</em> tasks.\nThis avoids averaging away nuance: archetype labels reflect schema-level fit; realism labels prioritize fine-grained, in-situ delivery from real-world or carefully constructed contrastive material.</p>\n\n",
                "matched_terms": [
                    "labels",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Progressive gating for emotion.</span>\nOur <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness</em> uses gated sub-scores (accuracy <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A8.SS2.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> intensity control <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A8.SS2.p3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> dynamic range), preventing spurious high scores when base emotion is misidentified.\nThis reduces label noise and improves SEM alignment stability.</p>\n\n",
                "matched_terms": [
                    "label",
                    "control",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Auditability and SEM alignment.</span>\nEvery score is traceable to a rubric with short rater rationales and confidence.\nWe then tune SEMs to these labels (rather than opaque judge outputs), enabling calibration studies and error analyses that would not be possible with black-box LLM judges.</p>\n\n",
                "matched_terms": [
                    "labels",
                    "outputs",
                    "calibration",
                    "opaque",
                    "rater",
                    "rubric",
                    "judges",
                    "judge"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Sensitivity to delivery.</span> Human raters, bilingual scope, and gated rubrics capture intonation, pacing, and subtle emotional control that judge-only pipelines often miss or compress into single numbers&#160;(see Appdendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>).</p>\n\n",
                "matched_terms": [
                    "pipelines",
                    "sensitivity",
                    "control",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Reduced synthetic bias.</span> Unlike pipelines built from LLM-generated dialogues or TTS-only references, we anchor annotations to real-world/contrastive human recordings and explicitly stress-test mismatches&#160;(Appdendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS4\" title=\"F.4 Prompts for Negative Scenes &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.4</span></a>, Appdendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS6\" title=\"F.6 TTS Systems and Settings &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.6</span></a>).</p>\n\n",
                "matched_terms": [
                    "pipelines",
                    "dialogues",
                    "annotations",
                    "llmgenerated",
                    "from",
                    "human",
                    "synthetic",
                    "bias"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transparent evaluation substrate.</span> Ours produces per-dimension human rationales and confidence; judge-only pipelines (ALLM/Gemini) provide scores with limited per-sample interpretability&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "pipelines",
                    "perdimension",
                    "evaluation",
                    "limited",
                    "human",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Better for SEM training.</span> Rubric-aligned labels let us <em class=\"ltx_emph ltx_font_italic\">train</em> and <em class=\"ltx_emph ltx_font_italic\">audit</em> SEMs, measure human alignment, and analyze error modes; judge outputs are harder to trust long-term as they change with model versions and prompts.</p>\n\n",
                "matched_terms": [
                    "outputs",
                    "labels",
                    "prompts",
                    "model",
                    "human",
                    "judge"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All human annotation in this work was conducted in accordance with standard ethical practices for research involving human annotators. Annotators were recruited via public hiring for Mandarin and internal lab hiring for English. Prior to participation, all annotators were provided with a clear description of the task, the type of data they would encounter, and their rights as participants, including the ability to opt out at any time without penalty.</p>\n\n",
                "matched_terms": [
                    "data",
                    "human",
                    "via",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The data presented to annotators consisted of synthetic speech samples, anonymized human speech, or recordings from public corpora, and contained no personally identifiable or sensitive information. To mitigate potential risks, annotators were allowed to skip any item that they found uncomfortable or inappropriate.</p>\n\n",
                "matched_terms": [
                    "data",
                    "from",
                    "human",
                    "speech",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure annotation reliability, annotators were provided with written guidelines, examples, and trial tasks before beginning. Quality control procedures included inter-annotator agreement checks, and comprehensive screening tests. Each speech sample is evaluated by at least 3 annotators. This study did not involve vulnerable populations and posed minimal risk to participants.</p>\n\n",
                "matched_terms": [
                    "evaluated",
                    "checks",
                    "annotation",
                    "agreement",
                    "control",
                    "each",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The archetype evaluation provides a detailed view of how speech foundation models (SFMs) perform when tasked with role-play scenarios grounded in stereotypical characters and situations. Human annotations cover four dimensions, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, and <span class=\"ltx_text ltx_font_italic\">Content Pass</span>, and the results reveal important trends, as well as challenges in interpreting scores across languages.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "across",
                    "annotations",
                    "evaluation",
                    "human",
                    "speech",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A critical observation is that annotations in Mandarin and English cannot be directly compared. The annotation groups for the two languages differed in their linguistic backgrounds and cultural interpretations, which leads to systematic differences in scoring. Our bilingual pilot further confirmed that annotators often interpret role-play appropriateness and expressiveness differently depending on their cultural frame of reference. For example, Mandarin raters tend to reward delivery styles that align with culturally expected prosody and politeness strategies, while English raters prioritize spontaneity and naturalness. These discrepancies parallel the findings of the singing voice conversion challenge (SVCC2023) from <cite class=\"ltx_cite ltx_citemacro_citet\">Huang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib31\" title=\"\">2023</a>)</cite>, where native and non-native listeners provided significantly different ratings for the same samples, particularly for naturalness and speaker similarity. Consequently, while absolute scores differ substantially across Mandarin and English, only within-language comparisons are meaningful. Cross-lingual ranking is not valid given the different annotation bases.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "across",
                    "annotations",
                    "annotation",
                    "from",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Within Mandarin from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, the relative rankings of models are clear. <span class=\"ltx_text ltx_font_typewriter\">Doubao</span> stands out as the strongest performer across all four dimensions, reaching near-ceiling levels in both <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> (4.37) and <span class=\"ltx_text ltx_font_italic\">Content Pass</span> (0.98). It also leads in role-related dimensions such as <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, indicating balanced competence across linguistic accuracy, delivery, and expressiveness. <span class=\"ltx_text ltx_font_typewriter\">G-CosyVoice2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-CosyVoice2</span> follow closely, especially in audio quality, though they lag behind in role appropriateness. <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> achieve mid-range results, reflecting higher variance across samples, while <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> shows competitive content understanding but lower naturalness and appropriateness. <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> consistently ranks the lowest, with large variance, underscoring instability and weaker alignment with archetypal role expectations.</p>\n\n",
                "matched_terms": [
                    "across",
                    "from",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The picture shifts in English, elaborated in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Here, <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> emerges as the most effective model by a wide margin, achieving top scores in both <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> (3.46) and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> (3.49), as well as high content pass rates (0.97). This reflects its strong grounding in English discourse norms and role adherence. In contrast, <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> variants perform well in <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> but are weaker in role alignment, highlighting a trade-off between acoustic fidelity and role-play embodiment. Other models such as <span class=\"ltx_text ltx_font_typewriter\">Doubao</span>, <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span>, and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> achieve moderate results, while <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> trail far behind, showing limited robustness in English scenarios. Taken together, the English results reveal both a sharper separation across models and a wider variance in appropriateness scores compared to Mandarin.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "across",
                    "model",
                    "fidelity",
                    "limited",
                    "human",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T11\" title=\"Table 11 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> and Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.F5\" title=\"Figure 5 &#8227; English Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.F6\" title=\"Figure 6 &#8227; English Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> further highlight differences across domains. In Mandarin, performance is relatively consistent across categories, with means clustering around 3.1&#8211;3.3. The highest scores appear in <span class=\"ltx_text ltx_font_typewriter\">SocialIdentity</span> and <span class=\"ltx_text ltx_font_typewriter\">DailyOccupation</span>, suggesting that models handle culturally grounded archetypes more reliably. In English, however, domain variation is much greater. Categories such as <span class=\"ltx_text ltx_font_typewriter\">FantasyOccupation</span> and <span class=\"ltx_text ltx_font_typewriter\">NamedCharacters</span> receive notably lower scores (means below 2.0), reflecting the difficulty of producing expressive, role-appropriate speech in imaginative or culturally specific contexts. Occupational roles remain relatively easier for models in both languages, likely because these scenarios are closer to training data distributions.</p>\n\n",
                "matched_terms": [
                    "across",
                    "roles",
                    "expressive",
                    "data",
                    "speech",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although cross-lingual comparisons are invalid, this does not diminish the utility of archetype evaluation for role-play benchmarking. In fact, the observed inconsistencies reinforce a central point: successful role-play requires models to adapt to the cultural and linguistic norms of the target language community. A system that performs well in Mandarin but poorly in English cannot be considered universally capable, since end users will evaluate appropriateness and naturalness according to their native expectations. Thus, archetype evaluation highlights not only model-specific strengths and weaknesses but also the need for culturally grounded evaluation pipelines.</p>\n\n",
                "matched_terms": [
                    "pipelines",
                    "roleplay",
                    "evaluation",
                    "benchmarking"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another notable pattern is the trade-off between acoustic quality and role embodiment. CosyVoice2 variants achieve strong audio quality but weaker human likeness and appropriateness, whereas <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> prioritizes role coherence at the expense of audio fidelity. This suggests that future systems must better integrate high-quality synthesis with nuanced role adherence. Similarly, the consistently lower scores for imaginative and identity-driven roles underscore the limitations of current SFMs in capturing subtle sociocultural expectations and narrative context.</p>\n\n",
                "matched_terms": [
                    "roles",
                    "human",
                    "fidelity",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In sum, archetype evaluation reveals three major findings:\n(1) Mandarin and English annotations cannot be directly compared, due to linguistic and cultural annotation inconsistencies, consistent with prior evidence from SVCC 2023.\n(2) Within each language, clear model rankings emerge, <span class=\"ltx_text ltx_font_typewriter\">Doubao</span> dominates in Mandarin, while <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> excels in English.\n(3) Domain sensitivity remains a challenge, particularly for imaginative or culturally rich roles, where scores are substantially lower and inter-model variance is higher.</p>\n\n",
                "matched_terms": [
                    "annotations",
                    "roles",
                    "three",
                    "model",
                    "annotation",
                    "evaluation",
                    "sensitivity",
                    "each",
                    "from",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although realism evaluation targets nuanced paralinguistic behaviors, human annotators achieve <em class=\"ltx_emph ltx_font_italic\">consistently high agreement</em> once the rubric disentangles the aspects to be judged.\nAs shown in the agreement summary in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F7\" title=\"Figure 7 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, most dimensions fall in the <math alttext=\"0.86\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>0.86</mn><annotation encoding=\"application/x-tex\">0.86</annotation></semantics></math>&#8211;<math alttext=\"0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mn>0.88</mn><annotation encoding=\"application/x-tex\">0.88</annotation></semantics></math> range, with the lowest agreement on <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> (<math alttext=\"\\approx 0.83\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.83</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.83</annotation></semantics></math>) and the highest on <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em> / <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> (<math alttext=\"\\approx 0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.88</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.88</annotation></semantics></math>).\nEmotion-related, progressive metrics (Intensity and Transition) are slightly lower (<math alttext=\"\\approx 0.85\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.85</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.85</annotation></semantics></math>), reflecting the added difficulty of judging graded expressivity.</p>\n\n",
                "matched_terms": [
                    "agreement",
                    "evaluation",
                    "human",
                    "rubric",
                    "targets"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Let <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math> denote the possible score range (e.g., <math alttext=\"R=4\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px2.p1.m2\" intent=\":literal\"><semantics><mrow><mi>R</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">R=4</annotation></semantics></math> for a <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px2.p1.m3\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>&#8211;<math alttext=\"5\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px2.p1.m4\" intent=\":literal\"><semantics><mn>5</mn><annotation encoding=\"application/x-tex\">5</annotation></semantics></math> scale).\nFor a sample <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px2.p1.m5\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> with annotator scores <math alttext=\"\\{s_{ij}\\}_{j}\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px2.p1.m6\" intent=\":literal\"><semantics><msub><mrow><mo stretchy=\"false\">{</mo><msub><mi>s</mi><mrow><mi>i</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>j</mi></mrow></msub><mo stretchy=\"false\">}</mo></mrow><mi>j</mi></msub><annotation encoding=\"application/x-tex\">\\{s_{ij}\\}_{j}</annotation></semantics></math>, we compute the sample standard deviation <math alttext=\"\\sigma_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px2.p1.m7\" intent=\":literal\"><semantics><msub><mi>&#963;</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">\\sigma_{i}</annotation></semantics></math> and define the per-sample agreement</p>\n\n",
                "matched_terms": [
                    "agreement",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then report the overall agreement <math alttext=\"A=1-\\bar{\\sigma}/R\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px2.p1.m8\" intent=\":literal\"><semantics><mrow><mi>A</mi><mo>=</mo><mrow><mn>1</mn><mo>&#8722;</mo><mrow><mover accent=\"true\"><mi>&#963;</mi><mo>&#175;</mo></mover><mo>/</mo><mi>R</mi></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">A=1-\\bar{\\sigma}/R</annotation></semantics></math>, where <math alttext=\"\\bar{\\sigma}\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px2.p1.m9\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>&#963;</mi><mo>&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\bar{\\sigma}</annotation></semantics></math> is the mean of <math alttext=\"\\{\\sigma_{i}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px2.p1.m10\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>&#963;</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\sigma_{i}\\}</annotation></semantics></math> over samples with at least two annotations.\nWe also clamp <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px2.p1.m11\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> to <math alttext=\"[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px2.p1.m12\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0,1]</annotation></semantics></math> and ignore single-annotator cases.\nA minimal, reproducible implementation is:</p>\n\n",
                "matched_terms": [
                    "agreement",
                    "over",
                    "annotations"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F8\" title=\"Figure 8 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F9\" title=\"Figure 9 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> visualize the score distributions per dimension.\nWe highlight three patterns:\n(i) <span class=\"ltx_text ltx_font_bold\">Progressive/gated emotion metrics.</span> For <em class=\"ltx_emph ltx_font_italic\">Emotion Intensity</em> and <em class=\"ltx_emph ltx_font_italic\">Emotion Transition</em>, we assign <span class=\"ltx_text ltx_font_typewriter\">1</span> when the scene is marked <em class=\"ltx_emph ltx_font_italic\">N/A</em> (i.e., emotion accuracy is insufficient to meaningfully judge intensity/transition). This produces a visible mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, a sparse bin at <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>, and slightly lower overall agreement; downstream analyses conditioning on adequate <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> (e.g., mean&#160;<math alttext=\"\\geq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 3</annotation></semantics></math>) recover more bell-shaped distributions.\n(ii) <span class=\"ltx_text ltx_font_bold\">Traits Embodiment.</span> Scores tend to avoid the middle (<math alttext=\"\\sim\\!3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!3</annotation></semantics></math>), concentrating on low values when traits clearly mismatch and high values when traits are strongly realized. This yields the lowest inter-rater agreement among dimensions, reflecting the inherently compositional and role-specific nature of trait judgments.\n(iii) <span class=\"ltx_text ltx_font_bold\">Other dimensions.</span> <em class=\"ltx_emph ltx_font_italic\">Pitch Variation</em>, <em class=\"ltx_emph ltx_font_italic\">Rhythmic Naturalness</em>, <em class=\"ltx_emph ltx_font_italic\">Stress &amp; Emphasis</em>, <em class=\"ltx_emph ltx_font_italic\">Voice Identity Matching</em>, <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em>, <em class=\"ltx_emph ltx_font_italic\">Global Story Coherence</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> are approximately unimodal with modes at <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>&#8211;<math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math>, consistent with moderate&#8211;strong realization in the dataset and with the high agreement reported.</p>\n\n",
                "matched_terms": [
                    "three",
                    "agreement",
                    "dataset",
                    "judge",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prosody control (Pitch, Rhythm, Stress).</span> Professional speech is shifted toward higher bins with tighter spread, indicating steadier control and fewer low-end tails.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "control"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, the consistently high agreement confirms that the rubric isolates perceptually coherent factors, so learned evaluators can target them reliably.\nSecond, the gated behavior of <em class=\"ltx_emph ltx_font_italic\">Intensity/Transition</em> motivates a <em class=\"ltx_emph ltx_font_italic\">hierarchical</em> evaluator: verify <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> before scoring graded expressivity, or model the joint structure with conditional heads.\nThird, <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> remains challenging; models may benefit from explicit character-trait priors, long-term context aggregation, or contrastive supervision between matched and mismatched traits.\nFinally, prosody-focused objectives (e.g., pitch/rhythm/stress control) are likely to yield the largest quality gains for amateur-style speech.</p>\n\n",
                "matched_terms": [
                    "priors",
                    "model",
                    "agreement",
                    "control",
                    "from",
                    "rubric",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For both zeroshot and fewshot experiments, we adopt a streamlined version of the annotation guidelines to ensure consistency with the human annotation process. The exact rubric prompts for archetype and realism evaluation used are provided below:</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "annotation",
                    "evaluation",
                    "human",
                    "rubric",
                    "used"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and fewshot settings, we condition the model with the rubric prompt and then append few-shot demonstration triplets (input, audio, gold JSON) to mirror the human annotation protocol. As shown in Listing&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#LST2\" title=\"Listing 2 &#8227; L.1 Zeroshot/Fewshot Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, We use <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> for these few-shot exemplars and require a minified JSON output consistent with the rubric schema.</p>\n\n",
                "matched_terms": [
                    "model",
                    "human",
                    "rubric",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and few-shot experiments on commercial models such as <span class=\"ltx_text ltx_font_typewriter\">Gemini</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o</span>, we report results as the average of five independent runs to reduce randomness and obtain more stable predictions. In contrast, for open-source models, we observed that their instruction-following ability is relatively weak. To mitigate this, we prompt them to predict one dimension at a time rather than all dimensions jointly. Specifically, for each metric, after showing the rubric prompt, we request completion in a fixed template and extract the probability distribution over the discrete tokens <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>. The final score is then computed from these probabilities, which we found makes open-source models usable to some extent under this setting.</p>\n\n",
                "matched_terms": [
                    "over",
                    "instructionfollowing",
                    "gemini",
                    "each",
                    "from",
                    "rubric"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For few-shot experiments, we randomly sample three exemplars from the training set, with each exemplar chosen such that its human scores are close to the rounded average across all annotators. These exemplars are appended as demonstrations in the prompt. As with the zeroshot case, we run five trials for commercial models and report the averaged results for evaluation.</p>\n\n",
                "matched_terms": [
                    "across",
                    "three",
                    "evaluation",
                    "each",
                    "from",
                    "human",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Effect of repeated runs.\nWhen comparing single-run decoding (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot-1run</span>) against averaging over five runs (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot</span>), we see consistent gains across nearly all dimensions (average score improves from 0.353 to 0.390). This suggests that sampling variance plays a substantial role in the reliability of large language model (LLM) evaluators, and averaging multiple outputs reduces this variance, producing more stable correlation with human annotations.</p>\n\n",
                "matched_terms": [
                    "over",
                    "across",
                    "outputs",
                    "annotations",
                    "model",
                    "from",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Impact of few-shot conditioning.\nIntroducing even three examples (<span class=\"ltx_text ltx_font_typewriter\">3-Shot</span>) yields a clear improvement over zeroshot (0.433 vs. 0.390). This indicates that providing explicit annotated demonstrations helps the model better align with the rubric. Interestingly, when using only a single run with few-shot demonstrations (<span class=\"ltx_text ltx_font_typewriter\">3-Shot-1run</span>), the performance is only marginally better than pure zeroshot, highlighting that both averaging and demonstrations are complementary: demonstrations improve grounding, while multiple runs mitigate stochasticity.</p>\n\n",
                "matched_terms": [
                    "over",
                    "three",
                    "model",
                    "rubric"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Scaling the number of demonstrations.\nIncreasing the few-shot set size from three to six and nine further boosts performance (0.459 and 0.469, respectively). The improvements are most pronounced in prosodic and emotional categories (e.g., <em class=\"ltx_emph ltx_font_italic\">Pitch</em>, <em class=\"ltx_emph ltx_font_italic\">Accuracy</em>, <em class=\"ltx_emph ltx_font_italic\">Intensity</em>), suggesting that more examples provide stronger guidance in modeling subtle paralinguistic cues. Gains in <em class=\"ltx_emph ltx_font_italic\">Character Consistency</em> and <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance</em> are also noticeable, although smaller in magnitude, which may indicate that these higher-level judgments are less sensitive to prompt scaling once a few demonstrations are provided.</p>\n\n",
                "matched_terms": [
                    "three",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In general, the results demonstrate two complementary strategies for stabilizing LLM-based evaluators: (i) averaging multiple runs to reduce randomness, and (ii) enriching prompts with more few-shot exemplars to improve adherence to rubric-based evaluation. Combining both leads to the strongest overall performance, with the nine-shot condition achieving the highest average correlation&#160;(0.469).</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For supervised fine-tuning, we primarily adopted a parameter-efficient strategy using LoRA, applied to the <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio-7B-Instruct</span> model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib11\" title=\"\">2024</a>)</cite>. LoRA was configured with a rank of 16, <math alttext=\"\\alpha=32\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS3.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#945;</mi><mo>=</mo><mn>32</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=32</annotation></semantics></math>, and a dropout of 0.1, targeting the major projection layers, and trained for 10,000 steps to ensure stable convergence. The training pipeline employed a per-device batch size of 4 with gradient accumulation of 4 steps (effective batch size 32), AdamW optimization with cosine scheduling, a peak learning rate of <math alttext=\"5\\times 10^{-5}\" class=\"ltx_Math\" display=\"inline\" id=\"A12.SS3.p1.m2\" intent=\":literal\"><semantics><mrow><mn>5</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>5</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">5\\times 10^{-5}</annotation></semantics></math>, weight decay of 0.01, and a warmup strategy defined by 500 steps. Gradient clipping with a norm of 1.0, <span class=\"ltx_text ltx_font_typewriter\">bf16</span> precision, and gradient checkpointing were enabled to stabilize training and reduce memory overhead. Distributed training was performed on 2xH100 GPUs using DeepSpeed (ZeRO stage-1).</p>\n\n",
                "matched_terms": [
                    "pipeline",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While we utilize various prompts and sample them randomly during the fine-tuning experiments, we provide an example prompt as follows:</p>\n\n",
                "matched_terms": [
                    "prompts",
                    "during"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For realism evaluation on the basic test set, <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> consistently achieves the strongest correlations across most dimensions, especially for <span class=\"ltx_text ltx_font_italic\">Character Identity</span> and <span class=\"ltx_text ltx_font_italic\">Traits</span>, leading to the best average correlation (0.668). <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> also performs competitively (0.630), while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> lags slightly behind (0.625). These results highlight that full fine-tuning may still offer an advantage when the evaluation set is relatively homogeneous and closer to the training distribution.</p>\n\n",
                "matched_terms": [
                    "across",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A different pattern emerges on the real recording test set. Both <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> and <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> struggle, yielding negative or near-zero correlations across most dimensions. In contrast, <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> shows clear improvements, achieving positive correlations in all prosodic and emotional dimensions and producing the highest overall average (0.247). This indicates that LoRA adaptation on top of an instruction-tuned backbone provides better generalization to out-of-distribution, real-world data.</p>\n\n",
                "matched_terms": [
                    "across",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> is competitive on in-domain evaluation, it suffers a substantial drop when applied to real recordings. By contrast, <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> offers a more robust balance: it maintains reasonable performance on archetype and basic sets, while significantly outperforming other settings on real-world data. Based on this robustness, we adopt <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> as our final configuration for subsequent experiments.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "data"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the comprehensive design of Speech-DRAME, several limitations remain.\nFirst, while our dual evaluation paradigm combines top-down archetype assessment with bottom-up realism evaluation, both strategies are constrained by the quality and scope of available data. Archetype evaluation inevitably relies on simplified stereotypes, which can overlook subtler paralinguistic and contextual cues. Realism evaluation, though richer, still struggles with domain mismatch between synthetic data and real human recordings, as evidenced by the performance drop observed in the real-recording test set. This highlights the persistent gap between benchmark-driven evaluation and real-world variability in speech role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "evaluation",
                    "data",
                    "human",
                    "speech",
                    "synthetic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, the present framework emphasizes bilingual evaluation (Mandarin and English), but linguistic and cultural diversity is far broader. Cross-lingual generalization and culturally sensitive evaluation are still open challenges, particularly since we observed annotation discrepancies across language groups. This limits the extent to which Speech-DRAME can claim universal coverage.</p>\n\n",
                "matched_terms": [
                    "across",
                    "evaluation",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Third, while DRAME-Eval substantially outperforms zero-shot and few-shot ALLMs, its reliance on fine-tuning from a strong base model raises questions of accessibility and scalability. Future work should explore lightweight adaptation techniques, semi-supervised training with partially annotated data, and leveraging emergent capabilities of open-source multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "llms",
                    "allms",
                    "model",
                    "data",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, although we validated DRAME-Eval against human annotations, alignment under realism settings remains modest. This suggests that even human-annotated benchmarks are insufficiently fine-grained to capture the full complexity of human perception in speech role-play. Richer annotation protocols, more diverse speaker populations, and integration of perceptual psychology insights could further improve evaluation fidelity.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "annotations",
                    "fidelity",
                    "annotation",
                    "evaluation",
                    "human",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Looking forward, we see several promising research directions. We intentionally restrict evaluation to single-turn responses for clarity and tractability. However, multi-turn extensions that capture narrative progression, long-term consistency, and dynamic emotion shifts remain a natural next step. Incorporating multilingual and cross-cultural datasets will enhance generality and fairness. On the modeling side, exploring hybrid evaluation strategies that combine discrete rubrics with continuous perceptual signals (e.g., prosody embeddings or affective measures) may better bridge the gap between automatic and human judgments. Finally, integrating Speech-DRAME into reinforcement learning pipelines as a reward model could help drive the next generation of expressive, human-aligned speech role-play systems.</p>\n\n",
                "matched_terms": [
                    "pipelines",
                    "roleplay",
                    "automatic",
                    "model",
                    "expressive",
                    "evaluation",
                    "human",
                    "speech"
                ]
            }
        ]
    },
    "A10.T10": {
        "caption": "Table 10: Overall human annotation results on Mandarin and English archetype evaluation dimensions (mean \\pm std). Best per column in bold.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model (Mandarin)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Appropriateness</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Human Likeness</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Audio Quality</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Content Pass</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">Doubao</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">3.8236 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m3\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.6658</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">3.6572 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m4\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.6427</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">4.3743 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m5\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.6991</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.9848 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m6\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.1224</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">G-CosyVoice2</span></td>\n<td class=\"ltx_td ltx_align_center\">3.4243 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m7\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.7389</td>\n<td class=\"ltx_td ltx_align_center\">3.3156 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m8\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.7215</td>\n<td class=\"ltx_td ltx_align_center\">4.0406 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m9\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.7298</td>\n<td class=\"ltx_td ltx_align_center\">0.9725 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m10\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.1637</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Q-CosyVoice2</span></td>\n<td class=\"ltx_td ltx_align_center\">3.4228 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m11\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.7788</td>\n<td class=\"ltx_td ltx_align_center\">3.2750 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m12\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.7335</td>\n<td class=\"ltx_td ltx_align_center\">4.0236 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m13\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.7661</td>\n<td class=\"ltx_td ltx_align_center\">0.9674 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m14\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.1776</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span></td>\n<td class=\"ltx_td ltx_align_center\">3.3866 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m15\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 1.1086</td>\n<td class=\"ltx_td ltx_align_center\">3.3391 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m16\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 1.0591</td>\n<td class=\"ltx_td ltx_align_center\">3.9525 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m17\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 1.2801</td>\n<td class=\"ltx_td ltx_align_center\">0.8623 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m18\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.3446</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span></td>\n<td class=\"ltx_td ltx_align_center\">3.2022 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m19\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.9597</td>\n<td class=\"ltx_td ltx_align_center\">3.0275 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m20\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.8687</td>\n<td class=\"ltx_td ltx_align_center\">3.9246 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m21\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 1.1105</td>\n<td class=\"ltx_td ltx_align_center\">0.8949 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m22\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.3067</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span></td>\n<td class=\"ltx_td ltx_align_center\">3.0228 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m23\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.8581</td>\n<td class=\"ltx_td ltx_align_center\">2.6536 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m24\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.7151</td>\n<td class=\"ltx_td ltx_align_center\">3.7297 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m25\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 1.0418</td>\n<td class=\"ltx_td ltx_align_center\">0.9098 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m26\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.2865</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span></td>\n<td class=\"ltx_td ltx_align_center\">2.4942 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m27\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 1.4242</td>\n<td class=\"ltx_td ltx_align_center\">2.3830 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m28\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 1.3201</td>\n<td class=\"ltx_td ltx_align_center\">2.8685 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m29\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 1.7456</td>\n<td class=\"ltx_td ltx_align_center\">0.5482 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m30\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.4978</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Model (English)</span></td>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n<td class=\"ltx_td ltx_border_t\"/>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">3.4638 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m31\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.9671</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">3.4873 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m32\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.7857</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.9620 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m33\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.6885</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.9662 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m34\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.1808</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Doubao</span></td>\n<td class=\"ltx_td ltx_align_center\">2.3798 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m35\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 1.0573</td>\n<td class=\"ltx_td ltx_align_center\">2.2355 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m36\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.8918</td>\n<td class=\"ltx_td ltx_align_center\">2.9553 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m37\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 1.1589</td>\n<td class=\"ltx_td ltx_align_center\">0.7995 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m38\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.4005</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span></td>\n<td class=\"ltx_td ltx_align_center\">2.2736 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m39\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.9606</td>\n<td class=\"ltx_td ltx_align_center\">2.0906 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m40\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.8124</td>\n<td class=\"ltx_td ltx_align_center\">2.5127 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m41\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.9702</td>\n<td class=\"ltx_td ltx_align_center\">0.8521 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m42\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.3552</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span></td>\n<td class=\"ltx_td ltx_align_center\">2.2355 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m43\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 1.0354</td>\n<td class=\"ltx_td ltx_align_center\">2.0779 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m44\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.9095</td>\n<td class=\"ltx_td ltx_align_center\">2.5694 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m45\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 1.1366</td>\n<td class=\"ltx_td ltx_align_center\">0.7482 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m46\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.4342</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">G-CosyVoice2</span></td>\n<td class=\"ltx_td ltx_align_center\">2.2168 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m47\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.9916</td>\n<td class=\"ltx_td ltx_align_center\">2.0109 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m48\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.8585</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">3.2530 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m49\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.9637</span></td>\n<td class=\"ltx_td ltx_align_center\">0.9293 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m50\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.2563</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Q-CosyVoice2</span></td>\n<td class=\"ltx_td ltx_align_center\">2.1673 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m51\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.9642</td>\n<td class=\"ltx_td ltx_align_center\">1.8068 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m52\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.8011</td>\n<td class=\"ltx_td ltx_align_center\">3.2476 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m53\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.9113</td>\n<td class=\"ltx_td ltx_align_center\">0.9614 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m54\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.1928</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span></td>\n<td class=\"ltx_td ltx_align_center\">2.0079 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m55\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 1.0420</td>\n<td class=\"ltx_td ltx_align_center\">1.9408 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m56\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.9489</td>\n<td class=\"ltx_td ltx_align_center\">2.4161 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m57\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 1.2583</td>\n<td class=\"ltx_td ltx_align_center\">0.6159 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m58\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.4865</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1.5531 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m59\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.8716</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1.7579 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m60\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.9386</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">2.0097 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m61\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 1.1788</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.4861 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T10.m62\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.5000</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "column",
            "content",
            "annotation",
            "overall",
            "evaluation",
            "glm4voice",
            "gpt4orealtime",
            "chatgpt4o",
            "doubao",
            "audio",
            "archetype",
            "pass",
            "english",
            "bold",
            "mean",
            "gcosyvoice2",
            "results",
            "kimiaudio",
            "qwen25omni",
            "appropriateness",
            "qcosyvoice2",
            "model",
            "mandarin",
            "dimensions",
            "pm",
            "best",
            "human",
            "std",
            "likeness",
            "quality"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Within Mandarin from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, the relative rankings of models are clear. <span class=\"ltx_text ltx_font_typewriter\">Doubao</span> stands out as the strongest performer across all four dimensions, reaching near-ceiling levels in both <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> (4.37) and <span class=\"ltx_text ltx_font_italic\">Content Pass</span> (0.98). It also leads in role-related dimensions such as <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, indicating balanced competence across linguistic accuracy, delivery, and expressiveness. <span class=\"ltx_text ltx_font_typewriter\">G-CosyVoice2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-CosyVoice2</span> follow closely, especially in audio quality, though they lag behind in role appropriateness. <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> achieve mid-range results, reflecting higher variance across samples, while <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> shows competitive content understanding but lower naturalness and appropriateness. <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> consistently ranks the lowest, with large variance, underscoring instability and weaker alignment with archetypal role expectations.</p>\n\n",
            "<p class=\"ltx_p\">The picture shifts in English, elaborated in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Here, <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> emerges as the most effective model by a wide margin, achieving top scores in both <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> (3.46) and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> (3.49), as well as high content pass rates (0.97). This reflects its strong grounding in English discourse norms and role adherence. In contrast, <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> variants perform well in <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> but are weaker in role alignment, highlighting a trade-off between acoustic fidelity and role-play embodiment. Other models such as <span class=\"ltx_text ltx_font_typewriter\">Doubao</span>, <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span>, and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> achieve moderate results, while <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> trail far behind, showing limited robustness in English scenarios. Taken together, the English results reveal both a sharper separation across models and a wider variance in appropriateness scores compared to Mandarin.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Role-play has become a key testbed for generative models, expanding from text-only dialogue to multimodal interaction. Extending role-play to speech captures prosody, emotion, and delivery, but also poses new evaluation challenges. Current pipelines often use audio large language models (ALLMs) as zero-shot judges, which miss paralinguistic cues, collapse multiple aspects into coarse scores, and rely on synthetic speech references that fail to reflect real-world roles. We present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework that contributes at three levels: (i) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-EvalBench</em>, an evaluation benchmark with bilingual human-annotated data and protocols for training and testing speech evaluation models&#160;(SEMs), (ii) <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, a fine-tuned evaluation model, which substantially outperforms zero-shot and few-shot ALLMs, and (iii) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-RoleBench</em>, a speech role-play benchmark that leverages <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> as an automatic judge to compare speech foundation models (SFMs). Speech-DRAME distinguishes between two complementary evaluation strategies: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>, a top-down approach measuring adherence to broad role archetypes, and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em>, a bottom-up approach grounded in real human speech that emphasizes nuanced role quality. Compared to zero-shot ALLM judges, <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves stronger agreement with human ratings (Pearson correlation from 0.480 to 0.629 in archetypes, and 0.390 to 0.625 in realism). By integrating transparent benchmark resources, modeling approaches, and system-level evaluation, Speech-DRAME provides the first comprehensive, reproducible foundation for assessing spoken role-play.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "evaluation",
                    "human",
                    "archetype",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">it collapses diverse aspects of performance into limited coarse dimensions, obscuring critical dimensions of role-play quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite>, and</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome these limitations, we present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span> (Speech Detailed Role-play Assessment with Modeling and Evaluation), a unified framework that moves beyond synthetic or judge-only pipelines. Speech-DRAME is distinguished by three aspects: [i] a dual evaluation paradigm that balances archetype-driven generality with human-grounded realism (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS2\" title=\"3.2 Role-play Evaluation with Two Strategies &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), [ii] human annotations that provide perceptual grounding absent in prior resources (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4\" title=\"4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), and [iii] evaluation-model alignment that enables not only benchmarking of speech foundation models (SFMs) but also systematic analysis of the evaluation models (SEMs) themselves (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5\" title=\"5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). These aspects establish Speech-DRAME as a comprehensive and trustworthy framework for measuring and improving speech role-play.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, Speech-DRAME contributes at three levels: (i) <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>, an evaluation benchmark consisting of human-annotated datasets for archetype and realism role-play; (ii) <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span>, a fine-tuned SEM trained on EvalBench that surpasses zero-shot and few-shot ALLMs; and (iii) <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span>, a speech role-play benchmark that leverages DRAME-Eval as an automatic judge to systematically compare SFMs. By combining dual evaluation strategies, human-anchored supervision, and evaluation-model alignment, Speech-DRAME delivers systematic insights into both role-play systems and evaluation models themselves.\nOur contributions are four-fold:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a <span class=\"ltx_text ltx_font_bold\">formal task definition</span> of speech role-play and its evaluation via speech foundation models (SFMs) for generation and speech evaluation models (SEMs) for assessment, unifying linguistic and paralinguistic dimensions.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>, the first evaluation benchmark with human annotations under both archetype- and realism-based strategies, supporting the training and testing of SEMs.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span>, a fine-tuned SEM trained on EvalBench that substantially outperforms zero-shot and few-shot ALLMs, demonstrating the importance of human supervision for evaluation models.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-native benchmarks.</span>\nMore recently, a number of resources have been introduced to evaluate spoken role-play agents. OmniCharacter-10K <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>)</cite> emphasizes character voice traits, while others&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib39\" title=\"\">2025</a>; Hou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib27\" title=\"\">2025</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib15\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib7\" title=\"\">2024b</a>)</cite> extend evaluation toward vocal communication, assistant capability, and multi-turn realism. Omni models such as StepAudio and KimiAudio <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite> also include role-play scenarios in their evaluation. SpeechRole and VStyle <cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>; Zhan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib76\" title=\"\">2025</a>)</cite> are closer to our work, offering datasets that test interaction ability, expressiveness, and fidelity. However, these resources are primarily built from LLM-generated dialogues and TTS outputs, with limited human validation and a heavy reliance on proprietary ALLM-as-judge pipelines. As a result, they lack grounding in diverse human speech and typically apply only coarse rubrics.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "kimiaudio",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Automatic judging with ALLMs.</span>\nIn parallel, advances in automatic judging highlight both opportunities and limitations. Audio-capable LLMs such as AudioJudge and Audio-Aware LLMs as Judges <cite class=\"ltx_cite ltx_citemacro_citep\">(Manakul et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib44\" title=\"\">2025</a>; Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>)</cite> can assess style, emotion, and delivery, while systems like Auto-ATT <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib65\" title=\"\">2025c</a>)</cite> scale this to Audio Turing Tests. Yet studies consistently reveal high prompt sensitivity and shallow capture of paralinguistic cues, suggesting that zero-shot judging remains insufficient for nuanced role-play evaluation.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Expressive TTS evaluation.</span>\nRecent work has explored evaluation of instruction-following and style-controlled TTS. InstructTTSEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite> explicitly includes role-play scenarios, while EmergentTTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">(Manku et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib45\" title=\"\">2025</a>)</cite> and discrete speech LM benchmarks <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang &amp; Sz&#233;kely, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib63\" title=\"\">2024</a>)</cite> emphasize prosody, spontaneity, and speaker consistency. These efforts push beyond intelligibility and naturalness, but still center on synthetic outputs rather than real human role-play.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our distinction.</span>\nIn contrast, Speech-DRAME unifies these considerations through three elements: (i) a dual evaluation paradigm that combines archetype-driven generality with realism grounded in authentic human speech, (ii) human annotations that provide a reliable perceptual gold standard, and (iii) alignment of SEMs to human ratings, enabling systematic evaluation of both SFMs and the evaluation models themselves. This combination moves beyond synthetic benchmarks or zero-shot judging to provide a comprehensive and trustworthy framework for speech role-play assessment.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation.</span>\nIn principle, a <em class=\"ltx_emph ltx_font_italic\">speech evaluation model</em> (SEM) can assess a generated response using the same conditioning signals:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech role-play evaluation can be framed in multiple ways depending on the available data and the desired granularity of analysis.\nWithin Speech-DRAME, we formalize two distinct strategies inspired by sociology theories<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Please refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A2\" title=\"Appendix B Theoretical Motivation for Dual Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> for detailed discussion of the dual evaluation design.</span></span></span>, including a top-down, archetype-oriented option using synthetic speech and a bottom-up, realism-oriented option from real human speech.\nRather than merging these into a single rubric, we treat them as separate perspectives, each with unique advantages and limitations.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarizes the key contrasts, while further implementation details are provided in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>-&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>). The first strategy follows a <em class=\"ltx_emph ltx_font_italic\">top-down</em> design, inspired by prior text-based role-play benchmarks.\nHere, role-play is judged with respect to <em class=\"ltx_emph ltx_font_italic\">stereotypical archetypes</em> (e.g., &#8220;firefighter&#8221; or &#8220;ER doctor&#8221;), using scene contexts (e.g., &#8220;comfort a child trapped in fire&#8221; or &#8220;ask for help in a serious operation&#8221;).\nThis approach provides accessible, general-purpose scoring that is scalable to a wide range of scenarios, making it suitable for large-scale benchmarking.\nHowever, its reliance on broad stereotypes and global impressions limits its ability to capture fine-grained aspects of delivery (e.g., subtle prosodic shifts, nuanced emotional control)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Holt &amp; Lotto, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib26\" title=\"\">2010</a>)</cite>.\nIn practice, this strategy mirrors simplified annotation settings and is particularly useful when annotation resources are limited&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib21\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS5\" title=\"F.5 Examples of Realism Role-play &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.5</span></a>). The second strategy adopts a <em class=\"ltx_emph ltx_font_italic\">bottom-up</em> design, motivated by real-world performance and real human speech.\nInstead of relying solely on stereotypes, this approach grounds evaluation in recordings from professional and non-professional speakers, drawn from realistic dialogue and media sources.\nJudgments in this setting are more fine-grained and multi-dimensional, addressing aspects such as prosodic dynamics, emotional fidelity, character consistency, and contextual fit.\nWhile this strategy requires more human annotation and careful curation, it provides benchmarks that reflect how role-play is perceived in practice.\nIt thus offers a richer framework for analyzing model outputs in settings where realism and subtlety are essential.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "model",
                    "human",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section details the <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> data curation process. Our data curation methodology follows a two&#8211;stage design: (i) <em class=\"ltx_emph ltx_font_italic\">task preparation</em>, which specifies the roles, scenes, and evaluation settings used to elicit speech; and (ii) <em class=\"ltx_emph ltx_font_italic\">human annotation</em>, which provides reliable ground-truth judgments on generated or real human speech.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human Annotation.</span>\nAll collected audio undergoes a standardized annotation protocol: (1) an initial semantically-based screening to exclude invalid clips; (2) rubric-based scoring across multiple perceptual dimensions; and (3) meta-annotations such as confidence or prompt-quality judgments. Full rubrics are provided in the appendices&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A5\" title=\"Appendix E Annotation Guidelines for Archetype Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">E</span></a> &amp; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>All data collection follows in high standard ethical consideration as outlined in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A9\" title=\"Appendix I Annotation Ethics &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "audio",
                    "dimensions",
                    "human",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Archetype Data Collection &#8227; 4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents a summary of the curated <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> dataset, where we further elaborate the details of archetype and realism evaluation data collection in following subsections, respectively. For benchmark purpose, we provide an official train-test split for the human annotated dataset for <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> so that following researchers can use the dataset to train and evaluate their SEMs for speech role-play.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Archetype evaluation adopts a top&#8211;down perspective from sociology and role theory&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goffman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib23\" title=\"\">1949</a>; Campbell, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib4\" title=\"\">2008</a>)</cite>, abstracting role-play into broad and recognizable categories.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Role and Scene Dimensions.</span>\nRoles are operationalized through seven categories: (1) functional occupations in daily life, (2) functional occupations in fictional or fantasy settings, (3) social identities, (4) named or specific characters, (5) personality traits, (6) relative relationship, (7) basic events. Scenes are anchored by everyday events or state changes that evoke stereotypical reactions. Details of template design and alternative prompting strategies are given in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>We note that some archetypes (e.g., firefighter) inevitably reflect cultural stereotypes. These are used solely for evaluation scalability and do not reflect normative judgments.</span></span></span></p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Sources.</span> The data is collected from a range of speech foundation models, including both cascaded and end-to-end models. We limit that only audio prompts are used as the input but adopt different prompt strategies for best quality with regards to different models. Details about the selected models and prompting details are discussed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS4\" title=\"D.4 Candidate Models for Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "best",
                    "audio",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotation follows two stages: an initial semantic check (i.e., <span class=\"ltx_text ltx_font_italic\">Content Pass</span>) to filter out instruction non-compliance or audio defects, followed by rubric-based scoring on three dimensions: <em class=\"ltx_emph ltx_font_italic\">Audio Quality</em>, <em class=\"ltx_emph ltx_font_italic\">Human-likeness</em>, and <em class=\"ltx_emph ltx_font_italic\">Appropriateness</em>. Confidence judgments are also recorded. Detailed rubric scales are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A5\" title=\"Appendix E Annotation Guidelines for Archetype Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</p>\n\n",
                "matched_terms": [
                    "appropriateness",
                    "pass",
                    "content",
                    "audio",
                    "annotation",
                    "dimensions",
                    "evaluation",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype Evaluation Data Summary.</span>\nThe resulting archetype evaluation dataset comprises 552 distinct scenarios for both Mandarin and English, evaluated across eight models,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>The model list is discussed in Appdenix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS4\" title=\"D.4 Candidate Models for Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.4</span></a>. Note we only include seven models for English due to the deactivation of ChatGPT-4O advanced voice model.</span></span></span> yielding 8,280 samples. In total, the dataset are split into 6,780 for training and 1,500 for testing. For benchmarking, an additional 1,250 scenarios spanning seven role/scene categories are prepared to facilitate archetype role-play benchmarking. More detailed statistics and analyses are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10\" title=\"Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">J</span></a>.</p>\n\n",
                "matched_terms": [
                    "english",
                    "model",
                    "mandarin",
                    "evaluation",
                    "archetype",
                    "chatgpt4o"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotators judge whether performances align with the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em>, using rubrics for <em class=\"ltx_emph ltx_font_italic\">Prosodic Dynamics (i.e., Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis)</em>, <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness (with progressive gating over three sub-dimensions: Emotion Accuracy, Emotion Intensity, Emotion Transition)</em>, <em class=\"ltx_emph ltx_font_italic\">Character Consistency (i.e., Voice Identity Matching and Traits Embodiment)</em>, <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance (i.e., Local Scene Fit and Global Story Coherence)</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>. A prompt quality check and confidence ratings are also collected. Full scales and gating details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "evaluation",
                    "quality",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation Data Summary.</span>\nThe resulting realism evaluation dataset is constructed from a combination of curated media sources, including internal collections, public datasets (e.g., NCSSD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib40\" title=\"\">2024b</a>)</cite>), and original recordings collected for this project.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>All data were obtained under appropriate licenses and are legally available for research release.</span></span></span> We compile 15k annotated role-play samples: 9k in Mandarin and 5k in English. The dataset is divided into 12k samples for training, 2k mixed samples for the base test set, and 1k real-recording samples for the test set. The first test set supports general evaluation using our proposed pipeline with negative samples, while the second focuses exclusively on human speech performed by both professional and amateur voice actors. For benchmarking, 2,000 scenarios from the realism dataset are designated for realism role-play benchmarking. Detailed statistics and analyses are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11\" title=\"Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">K</span></a>.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "evaluation",
                    "human",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the two proposed datasets, we conduct experiments under three settings: zero-shot, few-shot, and finetuning, using pre-trained ALLMs. We formulate the task as a multi-metric estimation problem&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tjandra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib58\" title=\"\">2025</a>; Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib74\" title=\"\">2025</a>; Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib55\" title=\"\">2025b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib54\" title=\"\">a</a>)</cite>, where a single model is trained or prompted to predict multiple evaluation dimensions.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "evaluation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-shot and Few-shot ALLMs.</span>\nWe first evaluate the zero-shot capabilities of both proprietary and public models, including <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>, <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>, <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio</span>, and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>. All evaluations follow a unified prompting template that provides audio input alongside scene and character context.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "kimiaudio",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Finetuning Model: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>.</span>\nIn addition to zero-shot and few-shot evaluations, we fine-tune a dedicated evaluation model, denoted as <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> for archetype and realism role-play evaluation, respectively.\nWe initialize from <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio-7B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib11\" title=\"\">2024</a>)</cite>, which provides a strong foundation for multi-modal audio-text reasoning.\nTo better leverage the multiple human annotations available in our datasets, we adopt a sampling-based training strategy: at each step, one annotator label is randomly selected, following practices in label distribution learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Geng, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib22\" title=\"\">2016</a>; Wen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib67\" title=\"\">2023</a>)</cite>.\nThis stochastic supervision implicitly exposes the model to the empirical annotation distribution, while during inference we output the full distribution over discrete labels (e.g., <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score, consistent with recent decoding-based regression methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Song &amp; Bahri, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>; Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib70\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib64\" title=\"\">2025b</a>; Lukasik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>. We apply LoRA fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib28\" title=\"\">2022</a>)</cite>. All implementation details, including optimizer settings, training schedule, and sampling policy, are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS3\" title=\"L.3 Fine-tuning Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "annotation",
                    "evaluation",
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further conduct ablation studies in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS4\" title=\"L.4 Fine-tuning Ablation Experiments &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.4</span></a>, examining: the choice of different base audio LLMs and full-parameter fine-tuning versus LoRA adaptation.\nThese analyses highlight the robustness of our sampling&#8211;expectation training strategy and clarify the trade-offs in model design.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics for Role-play Evaluation Model.</span> For most dimensions with 1-5 scale, we report Pearson correlation. For <span class=\"ltx_text ltx_font_italic\">Content Pass</span> estimation, we report accuracy.</p>\n\n",
                "matched_terms": [
                    "pass",
                    "content",
                    "model",
                    "dimensions",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype Evaluation.</span>\nThe results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T2\" title=\"Table 2 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reveal clear distinctions across evaluation settings. In the zero-shot condition, proprietary ALLMs such as <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> substantially outperform public models, achieving strong <span class=\"ltx_text ltx_font_italic\">content pass</span> accuracy (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>90%) and moderate correlations with other dimensions(<math alttext=\"0.43\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mn>0.43</mn><annotation encoding=\"application/x-tex\">0.43</annotation></semantics></math>&#8211;<math alttext=\"0.48\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mn>0.48</mn><annotation encoding=\"application/x-tex\">0.48</annotation></semantics></math>). By contrast, public models exhibit limited predictive ability, with correlations often below 0.2. Few-shot prompting provides modest gains for proprietary models, improving both accuracy and correlation (e.g., <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> rises from 0.480 to 0.493 average correlation). The most significant improvement arises from finetuning: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves an average correlation of 0.629, marking a large margin over zero-shot/few-shot models and demonstrating the effectiveness of task-specific supervision.</p>\n\n",
                "matched_terms": [
                    "pass",
                    "content",
                    "evaluation",
                    "results",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T3\" title=\"Table 3 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> extends the analysis to fine-grained realism evaluation. Similar to archetype results, proprietary ALLMs lead the zero-shot setting, with <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> obtaining an average correlation of 0.390, compared to 0.284 for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>. Public models remain weak baselines. Few-shot prompting consistently improves performance: <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> gains +0.042 and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> gains +0.083 over their zero-shot averages, showing the benefit of in-context supervision. Once again, finetuning is most effective: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves 0.625 average correlation, higher than the best few-shot results. Importantly, the gains are consistent across all 10 realism dimensions, confirming that the sampling&#8211;expectation training strategy generalizes beyond simple classification to nuanced paralinguistic cues.</p>\n\n",
                "matched_terms": [
                    "best",
                    "dimensions",
                    "evaluation",
                    "results",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real Recording Test Set.</span>\nThe real-recording evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T4\" title=\"Table 4 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> is notably more challenging, with all models suffering substantial performance drops. Proprietary models in the zero-shot setting achieve only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.04&#8211;0.08 average correlation, and public models often fall below zero. Few-shot prompting yields small but noticeable improvements, especially for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> (0.226 avg.), suggesting that exemplars partially mitigate domain mismatch between synthetic and real human speech. Interestingly, while finetuning achieves the highest overall average (0.247), its relative advantage over few-shot prompting is narrower than in the synthetic setting, and some dimensions (e.g., <span class=\"ltx_text ltx_font_italic\">Character Identity</span>) remain weak. We emphasize that this observed gap between synthetic and real recordings is not merely an artifact but an intended feature of Speech-DRAME, designed to expose the mismatch and drive future research into bridging synthetic-to-real generalization.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "dimensions",
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall.</span> Across both archetype and realism evaluations, we observe three consistent trends: (i) zero-shot performance is limited, particularly for open-source models, (ii) few-shot prompting offers moderate but consistent gains, especially for proprietary ALLMs, and (iii) finetuning with task-specific supervision (<span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>) delivers the largest improvements, achieving state-of-the-art correlations across nearly all dimensions. Nevertheless, results on the real-recording test set underscore the gap between synthetic and real human role-play scenarios, motivating future work in data diversification and domain adaptation.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "dimensions",
                    "human",
                    "results",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on fine-tuned <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, we establish the DRAME-RoleBench to evaluate SFMs&#8217; role play ability. For archetype evaluation, 1,250 scenarios are constructed following the same data construction pipeline as the <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>. For realism evaluation, we reuse 2,000 scenarios in the <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> test set. We follow the same prompt strategy as outlined in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, we consider a comprehensive set of candidate models, including six end-to-end models: <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib47\" title=\"\">2025b</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib75\" title=\"\">2024</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib71\" title=\"\">2025a</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Qwen3Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib72\" title=\"\">2025b</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">StepAudio2Mini</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib68\" title=\"\">2025</a>)</cite>; and ten cascaded models by combining two widely-used textual LLMs (i.e., <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib12\" title=\"\">2025</a>)</cite> and <span class=\"ltx_text ltx_font_typewriter\">Qwen3-30B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib73\" title=\"\">2025</a>)</cite>) and five recent TTS models (i.e., <span class=\"ltx_text ltx_font_typewriter\">F5TTS</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib8\" title=\"\">2024c</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">IndexTTS2</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib82\" title=\"\">2025b</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">MaskGCT</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib66\" title=\"\">2025d</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Vevo1.5</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib79\" title=\"\">2025b</a>)</cite>, and <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib17\" title=\"\">2024b</a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "glm4voice",
                    "gpt4orealtime",
                    "kimiaudio",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype evaluation.</span>\nOn archetype role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T5\" title=\"Table 5 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), cascaded pipelines consistently outperform end-to-end systems. The strongest cascaded systems, particularly <span class=\"ltx_text ltx_font_typewriter\">G-IndexTTS2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-IndexTTS2</span>, achieve both high content pass rates (above 94%) and balanced scores across audio quality, human likeness, and appropriateness. By contrast, end-to-end models such as <span class=\"ltx_text ltx_font_typewriter\">StepAudio2Mini</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> perform competitively in terms of content coverage (above 88%), but their expressive scores remain slightly lower than their cascaded counterparts. This gap suggests that separating reasoning (LLM) from speech rendering (TTS) remains advantageous when archetype fidelity is emphasized. Nevertheless, the fact that top end-to-end systems already surpass 3.0 in average rating indicates rapid closing of the performance gap.</p>\n\n",
                "matched_terms": [
                    "appropriateness",
                    "pass",
                    "content",
                    "audio",
                    "evaluation",
                    "glm4voice",
                    "human",
                    "archetype",
                    "likeness",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism evaluation.</span>\nRealism role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T6\" title=\"Table 6 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) presents a different landscape. Here, performance differences between end-to-end and cascaded systems are less pronounced. Several end-to-end models, such as <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>, reach average scores of 3.24, matching or even exceeding the best cascaded pipelines. Importantly, realism scores are more evenly distributed across prosodic, emotional, character, and contextual dimensions, with most models clustering between 2.9&#8211;3.2. This convergence reflects the difficulty of realism-style evaluation: since the data preparation emphasized contrast with authentic human speech, models are judged under similar conditions, leading to smaller observed gaps.</p>\n\n",
                "matched_terms": [
                    "best",
                    "dimensions",
                    "evaluation",
                    "glm4voice",
                    "human",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human alignment.</span>\nWe further conducted small-scale human studies with 10% of the benchmark data to verify the reliability of the benchmark. Archetype evaluation achieves a Spearman correlation of 0.706 between human judgments and <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, validating that the automatic evaluator closely tracks human preferences in this setting. Realism evaluation, however, achieves a more modest Spearman correlation of 0.375. This lower alignment is expected, since realism data construction did not prioritize fine-grained model separation but instead emphasized distinguishing human versus synthetic speech.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>We view this as evidence that realism presents a harder, less-saturated frontier. Rather than a limitation of DRAME-Eval, this highlights the challenge of designing evaluators that capture nuanced human perception, which motivates future work.</span></span></span> As a result, all models occupy a relatively narrow performance band, reducing sensitivity to model-level differences. We view this as evidence that realism evaluation highlights a harder, less saturated frontier where both SFMs and SEMs require further advances.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "model",
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, cascaded pipelines retain an edge in archetype scenarios, especially in content robustness and expressive fidelity, but end-to-end models are catching up quickly. In realism scenarios, the field remains unsolved: existing systems exhibit similar performance bands and evaluators show weaker alignment with human judgments, underscoring the need for improved realism-grounded data and evaluation. Together, these findings establish <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> as both a reliable testbed for archetype fidelity and a challenging benchmark for realism fidelity.</p>\n\n",
                "matched_terms": [
                    "content",
                    "overall",
                    "evaluation",
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework for assessing spoken role-play through two complementary lenses: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em> (top-down) and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> (bottom-up, human-grounded). Our human-annotated <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> enables training and testing of SEMs, and our fine-tuned <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span> substantially outperforms zero-/few-shot ALLM judges. Building on this, <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> provides system-level comparison across various SFMs, revealing a consistent edge for cascaded pipelines on archetypes and a tighter spread under realism, where real human performance evaluation remain challenging. Together, these resources establish a reproducible, analyzable foundation for spoken role-play evaluation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>Limitations and future works are discussed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A13\" title=\"Appendix M Limitations and Future Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">M</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All data collection and annotation in this work followed strict ethical standards. Our bilingual datasets were constructed exclusively from legally accessible sources, including licensed media, open-source corpora, and original recordings conducted with informed consent. Annotators were recruited with appropriate compensation and provided with clear instructions, including the right to withdraw at any point. To minimize risks, no personally identifiable information was included in the released data, and all recordings were anonymized before annotation. We ensured cultural and linguistic diversity in annotator pools to mitigate potential bias across Mandarin and English role-play scenarios. While this study did not require formal IRB approval, we adhered to best practices in human-subject research, including transparency in task design and careful consideration of annotator well-being. Full details of our annotation protocol and ethical safeguards are presented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A9\" title=\"Appendix I Annotation Ethics &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "best",
                    "english",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data generation:</span> LLMs were employed to expand prompts (e.g., character profiles, scene descriptions) and to produce negative samples in realism evaluation. All outputs were screened by human annotators to ensure validity and reduce bias.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline evaluators:</span> State-of-the-art audio LLMs (e.g., <span class=\"ltx_text ltx_font_typewriter\">GPT4o</span>, <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>) were included as zero-shot judges in our evaluation benchmark, enabling direct comparison with human-aligned SEMs.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "evaluation",
                    "qwen25omni"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation design and task construction:</span> <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> was used to assist with building instructions and summarizing story contexts. Drafted materials were subsequently refined by the authors and validated by human annotators.</p>\n\n",
                "matched_terms": [
                    "human",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our distinction between <span class=\"ltx_text ltx_font_italic\">Archetype Evaluation</span> and <span class=\"ltx_text ltx_font_italic\">Realism Evaluation</span> is not merely pragmatic, but is grounded in established theories of social identity, communication, and performance. The two perspectives capture complementary dimensions of how humans perceive and evaluate roles in interaction.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Accordingly, Archetype Evaluation and Realism Evaluation are best viewed as complementary:</p>\n\n",
                "matched_terms": [
                    "best",
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Archetype Evaluation provides macro-level judgments grounded in socially shared schemas, enabling efficient and broad comparisons.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our main formulation (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS1\" title=\"3.1 Problem Formulation &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) focuses on the <em class=\"ltx_emph ltx_font_italic\">single-turn</em> setting, where each response <math alttext=\"{\\bm{S}}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119930;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{S}}_{r}</annotation></semantics></math> is generated and evaluated independently.\nThis simplification enables clearer analysis of speech role-play quality and aligns with our bilingual annotation protocol.\nNevertheless, the same formulation extends naturally to multi-turn dialogues.</p>\n\n",
                "matched_terms": [
                    "quality",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p4.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> may be a simple mean, a recency-weighted average, or a learned temporal model that captures consistency across turns.\nThe choice of <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p4.m3\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> reflects the intended evaluation focus, e.g., stability of persona traits, adaptation to scene progression, or coherence across multiple speakers.</p>\n\n",
                "matched_terms": [
                    "mean",
                    "evaluation",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This appendix first discusses the summary of task preparation steps for archetype evaluation. Then we provide concrete examples and extended discussion of the archetype evaluation annotation pipeline that are omitted in the main text for clarity.</p>\n\n",
                "matched_terms": [
                    "archetype",
                    "evaluation",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Archetype Evaluation follows a top&#8211;down perspective rooted in sociology and role theory, where social behavior is interpreted through shared stereotypes and generalized expectations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goffman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib23\" title=\"\">1949</a>; Campbell, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib4\" title=\"\">2008</a>)</cite>.\nThis strategy abstracts role-play into broad and easily recognizable archetypes, enabling scalable evaluation of whether a generated response aligns with salient role features.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Exclusion of fine-grained controls.</span>\nWe deliberately excluded explicit emotional states, prosodic instructions, or linguistic constraints.\nThese dimensions are better suited to realism-oriented or style-controlled evaluation, whereas archetype evaluation benefits from concrete, stereotype-level scenarios that sharpen judgments and improve human agreement.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "evaluation",
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Pipeline overview.</span>\nAt a high level, archetype evaluation data are generated through a three-stage pipeline:\n(1) selecting keywords that specify role and scene;\n(2) expanding them into full prompts via structured templates; and\n(3) rendering prompts into speech with contemporary TTS systems.\nThis modular pipeline ensures both scalability and reproducibility, while keeping prompts vivid enough for reliable judgments. (4) Based on the generated data (paired scene and synthesized speech), expert human annotators are recruited for detailed annotation on semantic check (i.e., <span class=\"ltx_text ltx_font_italic\">Content Pass</span>) and three different spoken role-play dimensions (i.e., <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human-likeness</span>, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>).\nImplementation details, including template design, alternative prompting strategies, and annotation guidelines, are provided in the following subsections of this Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "appropriateness",
                    "pass",
                    "content",
                    "audio",
                    "annotation",
                    "dimensions",
                    "evaluation",
                    "human",
                    "archetype",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, archetype evaluation emphasizes breadth and recognizability over subtlety.\nBy grounding role-play in socially recognizable categories and stereotypical situations, it provides a practical balance between scalability and interpretability, complementing the more fine-grained realism evaluation described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS2\" title=\"3.2 Role-play Evaluation with Two Strategies &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We constructed archetype evaluation prompts by combining role categories with contextual scenes.\nThe following illustrates the core categories with representative examples.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Excluded Dimensions.</span> We initially considered more fine-grained control dimensions, but excluded them to preserve the archetype-level focus. Examples of excluded dimensions include:\n<em class=\"ltx_emph ltx_font_italic\">Emotional State &amp; Expression, Single Emotion Prompts, Explicit User Input / Assistant Output structures, Emotional Transition / Fluctuation, Voice Style &amp; Prosody (pacing, intonation, vocal timbre, expressive style, accent/dialect), and Content / Interaction Patterns (task/topic controls, specific linguistic structures, or speech production challenges).</em></p>\n\n",
                "matched_terms": [
                    "content",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These dimensions are more closely aligned with explicit instruction-following or style transfer rather than broad stereotype judgment, and they reduced clarity in human evaluations.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">The Choice of Prompts.</span> We also experimented with broader, less specific templates that allowed more freedom for generative models but produced less discriminative evaluation results:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech generation.</span>\nPrompts were rendered into audio with CosyVoice2-0.5B&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Du et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib17\" title=\"\">2024b</a>)</cite>. The prompts are selected from human-verified high-quality samples, where the selection is conducted from open-source datasets, such as LibriSpeech/Gigaspeech for English, and WenetSpeech for Chinese&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib49\" title=\"\">2015</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib5\" title=\"\">2021</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib77\" title=\"\">2022</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate a diverse set of speech foundation models spanning both English and Mandarin. In total, we include 8 English and 7 Mandarin models, covering open-source and proprietary systems, and balancing end-to-end and cascaded architectures. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.T7\" title=\"Table 7 &#8227; D.4 Candidate Models for Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> provides the full list of models.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Collection for Proprietary Models.</span> For proprietary systems, we adopt two collection strategies. For GPT-4o-realtime, responses are obtained directly via the <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-realtime</span> API. For Doubao and GPT-4o (Advanced Voice Mode), we collect outputs by recording from their official web interfaces. All recordings are captured through the interaction device itself, ensuring that no additional processing or distortion is introduced into the final audio.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "doubao",
                    "gpt4orealtime"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">End-to-End Model Generation.</span>\nFor end-to-end systems, the audio prompt is directly used as input. The system prompt is set to default values to ensure consistent speech generation quality.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Step 1: We first build a speech database (LibriSpeech and GigaSpeech for English, WenetSpeech for Mandarin&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib49\" title=\"\">2015</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib5\" title=\"\">2021</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib77\" title=\"\">2022</a>)</cite>). Each sample is annotated with Gemini-based speech captions describing salient acoustic attributes.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2\" title=\"Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the internal annotation interface used for <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>.\nThe platform provides an integrated environment for both playback and evaluation of model-generated role-play speech samples.\nAnnotators first interact with the audio player interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf1\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>), which visualizes waveforms and spectrograms for multiple versions of the same prompt, enabling side-by-side comparison under a shared role instruction (e.g., firefighter, teacher, or actor).\nThen, they proceed to the annotation interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf2\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a>), where they assign scores for multiple dimensions such as <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, along with reasoning and confidence inputs.\nThis structured interface supports high-consistency, fine-grained evaluation of role-play behaviors in speech models.</p>\n\n",
                "matched_terms": [
                    "appropriateness",
                    "audio",
                    "annotation",
                    "dimensions",
                    "evaluation",
                    "human",
                    "archetype",
                    "likeness",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This appendix contains the full human annotation instructions used for archetype evaluation.</p>\n\n",
                "matched_terms": [
                    "archetype",
                    "evaluation",
                    "human",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Quality (No Context):</span> independence from content, focus on distortions or synthesis artifacts.</p>\n\n",
                "matched_terms": [
                    "content",
                    "audio",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human Likeness (No Context):</span> perceived naturalness and plausibility of human delivery.</p>\n\n",
                "matched_terms": [
                    "likeness",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comments for Audio Quality and Human Likeness are only required for scores <math alttext=\"\\leq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I3.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8804;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\leq 3</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "likeness",
                    "human",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Foreign language content:</span> If <em class=\"ltx_emph ltx_font_italic\">any</em> part before 30s contains a foreign language (even a single word/short phrase) in otherwise English audio, reject the entire clip. \n<br class=\"ltx_break\"/><em class=\"ltx_emph ltx_font_italic\">Exception:</em> Extremely common/recognizable borrowed words (e.g., &#8220;croissant&#8221;) may pass. Niche items (e.g., &#8220;coq au vin&#8221;) should be rejected.</p>\n\n",
                "matched_terms": [
                    "content",
                    "audio",
                    "pass",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><em class=\"ltx_emph ltx_font_italic\">Example (pass):</em> Same prompt; audio says: &#8220;It looks like your stylist put you through a wood chipper.&#8221; <math alttext=\"\\Rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I5.i1.I0.i2.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8658;</mo><annotation encoding=\"application/x-tex\">\\Rightarrow</annotation></semantics></math> content pass = true.</p>\n\n",
                "matched_terms": [
                    "content",
                    "audio",
                    "pass"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><em class=\"ltx_emph ltx_font_italic\">Example (pass):</em> &#8220;Oh no. Are you hurt? Don&#8217;t worry. We&#8217;ll get you taken care of, little one.&#8221; <math alttext=\"\\Rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I5.i2.I0.i2.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8658;</mo><annotation encoding=\"application/x-tex\">\\Rightarrow</annotation></semantics></math> content pass = true.</p>\n\n",
                "matched_terms": [
                    "content",
                    "pass"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Important:</span> Once an audio receives content pass = false, <em class=\"ltx_emph ltx_font_italic\">do not</em> rate rubrics; proceed to the next clip.</p>\n\n",
                "matched_terms": [
                    "content",
                    "audio",
                    "pass"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rubric Rating Instructions (for content pass = true).</span>\nWe shift focus from <em class=\"ltx_emph ltx_font_italic\">what</em> was said to <em class=\"ltx_emph ltx_font_italic\">how</em> it was said (vocal performance). Use gut intuition guided by rubric definitions. Score each audio <em class=\"ltx_emph ltx_font_italic\">in isolation</em> (no ranking across clips).</p>\n\n",
                "matched_terms": [
                    "content",
                    "audio",
                    "pass"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">Audio Quality</span> &amp; <span class=\"ltx_text ltx_font_bold\">Human Likeness</span>: comment only if score <math alttext=\"\\leq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I6.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8804;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\leq 3</annotation></semantics></math>; if 4 or 5, write &#8220;N/A&#8221;.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "likeness",
                    "human",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Pronunciation errors: penalize only on <span class=\"ltx_text ltx_font_bold\">Human Likeness</span> if they significantly affect comprehension; do not affect <span class=\"ltx_text ltx_font_bold\">Appropriateness</span>.</p>\n\n",
                "matched_terms": [
                    "appropriateness",
                    "likeness",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Independently</span> rate technical clarity (ignore content and fit). After scoring Audio Quality, <em class=\"ltx_emph ltx_font_italic\">disregard</em> its issues for the other two dimensions.</p>\n\n",
                "matched_terms": [
                    "content",
                    "audio",
                    "dimensions",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Rate how likely an average listener would believe the clip is human-delivered (ignore Audio Quality issues and contextual fit).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Rate <em class=\"ltx_emph ltx_font_italic\">only</em> how well the <em class=\"ltx_emph ltx_font_italic\">delivery</em> fits the role/scene described in the prompt (tonal fit). Ignore Audio Quality and Human Likeness issues while scoring this dimension.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "likeness",
                    "human",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realism Evaluation adopts a bottom&#8211;up perspective grounded in real human speech data, with the goal of capturing the nuanced delivery, prosody, and contextual fit that synthetic archetype prompts cannot fully represent.\nRather than relying on stereotypes, this strategy leverages real-world recordings and carefully constructed contrasts, enabling fine-grained assessment of model performance under realistic conditions.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "model",
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data from real human speech sources.</span>\nWe construct the realism evaluation set primarily from natural speech material:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, real human ground-truth segments and systematically constructed negatives form a balanced dataset that supports realism-oriented evaluation.\nThe positives provide natural anchors of role-play performance, while the negatives introduce controlled mismatches that test whether evaluation models can distinguish real human speech from synthetic approximations.\nThis design ensures that realism evaluation emphasizes subtle paralinguistic and contextual cues, moving beyond intelligibility or content fidelity.</p>\n\n",
                "matched_terms": [
                    "content",
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During annotation, raters are presented with either: (i) ground-truth stories, (ii) stories with negative local scenes, or (iii) stories with negative character profile. Annotators assign realism scores without being exposed to the underlying mismatch reasons. Instead, these reasons are provided only to the quality-assurance team, who use them as reference for internal validation and consistency checks. While the mismatch reasons may not be perfect, they help streamline quality control procedures and maintain reliability across the dataset.</p>\n\n",
                "matched_terms": [
                    "quality",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Yunjin case study illustrates a fundamental difference between realism evaluation and archetype evaluation. Archetype prompts, as described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>, are constructed by pairing abstract role categories with synthetic contextual scenes. These cover a broad space of possible situations, ranging from modern occupations (e.g., firefighters) to fictional personas (e.g., cult leaders in martial arts fantasy) or social identities (e.g., overprotective parents, laid-back friends). While such prompts provide controlled and diverse coverage, they are ultimately template-based and rely on stereotypes to elicit role-play behaviors.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This distinction highlights the complementary strengths of the two evaluation paradigms. Archetype evaluation systematically explores a wide combinatorial space of roles and situations, but may lack the paralinguistic and contextual richness of real speech. Realism evaluation, on the other hand, anchors assessment in authentic delivery and contextual fit, providing a finer-grained lens on coherence and naturalness. Together, they ensure that role-play evaluation spans both broad stereotypical coverage and nuanced real-world plausibility.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For models using voice prompts, we use an inhouse enhancement model to first process the prompt audio so as to minimize the effect of audio events and background noise.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS1</span> (Phone&#8211;Cosy2 Pipeline).\nThe input is a phone sequence produced by a graphme-to-phoneme&#160;(G2P) front-end. A 0.4B GPT-2 model&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib50\" title=\"\">2019</a>)</cite> predicts <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> S3-tokens, which are then passed through a flow-matching model to generate mel-spectrograms. A neural vocoder from <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> converts the mel features into audio. Both the GPT-2 and flow-matching modules are augmented with speaker embeddings for speaker control.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS3</span> (Embedding&#8211;AudioCode Pipeline).\nHere, the input is Qwen2.5 BPE embeddings, which a 0.4B GPT-2 model maps directly to 4,096-dimensional audio codes. These codes are decoded via a cascaded architecture: <span class=\"ltx_text ltx_font_typewriter\">RepCodec</span> decoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib32\" title=\"\">2024</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> content representation (with speaker embedding) <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DDIM</span> (denoising\ndiffusion implicit model) module&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jeong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib34\" title=\"\">2021</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DAC</span> latent representation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib37\" title=\"\">2023</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DAC</span> decoder. Conditioning is provided through prefix text embeddings (e.g., instructions expressing emotional states such as &#8220;angry, hateful: you betrayed me&#8230;&#8221;), along with speaker ID. The input format is <span class=\"ltx_text ltx_font_typewriter\">[spk_id, BPE embedding, audio codes]</span>.</p>\n\n",
                "matched_terms": [
                    "content",
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS4</span> (Instruction-Conditioned AudioCode Pipeline).\nThis model also employs a 0.4B GPT-2 to predict 4,096-dimensional audio codes, but conditions generation on instruction embeddings. A <span class=\"ltx_text ltx_font_typewriter\">CLIPTextEncoder</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib51\" title=\"\">2021</a>)</cite> converts instruction text into embeddings, which serve as a prefix, combined with speaker ID and phone sequence. The input format is <span class=\"ltx_text ltx_font_typewriter\">[spk_id, instruction embedding sequence, phone sequence, audio codes]</span>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The annotation interface for <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> focuses on assessing contextual and narrative coherence in model-generated speech.\nUnlike the archetype-oriented evaluation, which emphasizes stylistic and emotional fidelity, this platform examines how well the voice performance aligns with a character&#8217;s personality, dialogue situation, and scene progression.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Once familiar with the scene, annotators proceed to the detailed evaluation form shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.F4\" title=\"Figure 4 &#8227; F.7 Annotation Platform for Realism Evaluation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nThis interface is divided into two major sections: prosodic/emotional quality, higher-level coherence and semantic alignment.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This appendix contains the full human annotation instructions used for realism evaluation.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the prompt information is <span class=\"ltx_text ltx_font_bold\">contradictory</span> or <span class=\"ltx_text ltx_font_bold\">insufficient</span> to evaluate (e.g., audio too short, traits mutually incompatible), mark the prompt quality accordingly and <em class=\"ltx_emph ltx_font_italic\">do not</em> force granular ratings that cannot be judged.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt five dimensions, each rated from 1 (worst) to 5 (best). Emotional Expressiveness uses <em class=\"ltx_emph ltx_font_italic\">progressive gating</em>.</p>\n\n",
                "matched_terms": [
                    "best",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Use both sources:</span> Play the audio and skim the transcript together to assess delivery quality, expression, and alignment with character intent (do not penalize transcript mismatches).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Focus only on human speech:</span> Do not factor in background music/soundtracks, environmental or ambient noise, or non-speech audio events; these may reflect the scene but should not influence scoring of the spoken performance.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt is insufficient to evaluate the voice sample.</span> Audio too short to gauge the dimensions; or the prompt lacks specific characteristics/is too vague or multi-directional to know which version of the character is present. (It is OK if audio does not match transcript exactly; if too much is missing to annotate, mark as insufficient.)</p>\n\n",
                "matched_terms": [
                    "audio",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human-grounded, bilingual labels.</span>\nWe annotate in English <em class=\"ltx_emph ltx_font_italic\">and</em> Mandarin with trained raters, capturing cross-lingual prosody and style, while recording rater confidence to qualify downstream usage.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transparent evaluation substrate.</span> Ours produces per-dimension human rationales and confidence; judge-only pipelines (ALLM/Gemini) provide scores with limited per-sample interpretability&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Better for SEM training.</span> Rubric-aligned labels let us <em class=\"ltx_emph ltx_font_italic\">train</em> and <em class=\"ltx_emph ltx_font_italic\">audit</em> SEMs, measure human alignment, and analyze error modes; judge outputs are harder to trust long-term as they change with model versions and prompts.</p>\n\n",
                "matched_terms": [
                    "model",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All human annotation in this work was conducted in accordance with standard ethical practices for research involving human annotators. Annotators were recruited via public hiring for Mandarin and internal lab hiring for English. Prior to participation, all annotators were provided with a clear description of the task, the type of data they would encounter, and their rights as participants, including the ability to opt out at any time without penalty.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "human",
                    "english",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To ensure annotation reliability, annotators were provided with written guidelines, examples, and trial tasks before beginning. Quality control procedures included inter-annotator agreement checks, and comprehensive screening tests. Each speech sample is evaluated by at least 3 annotators. This study did not involve vulnerable populations and posed minimal risk to participants.</p>\n\n",
                "matched_terms": [
                    "quality",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The archetype evaluation provides a detailed view of how speech foundation models (SFMs) perform when tasked with role-play scenarios grounded in stereotypical characters and situations. Human annotations cover four dimensions, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, and <span class=\"ltx_text ltx_font_italic\">Content Pass</span>, and the results reveal important trends, as well as challenges in interpreting scores across languages.</p>\n\n",
                "matched_terms": [
                    "appropriateness",
                    "pass",
                    "content",
                    "audio",
                    "dimensions",
                    "evaluation",
                    "human",
                    "results",
                    "archetype",
                    "likeness",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A critical observation is that annotations in Mandarin and English cannot be directly compared. The annotation groups for the two languages differed in their linguistic backgrounds and cultural interpretations, which leads to systematic differences in scoring. Our bilingual pilot further confirmed that annotators often interpret role-play appropriateness and expressiveness differently depending on their cultural frame of reference. For example, Mandarin raters tend to reward delivery styles that align with culturally expected prosody and politeness strategies, while English raters prioritize spontaneity and naturalness. These discrepancies parallel the findings of the singing voice conversion challenge (SVCC2023) from <cite class=\"ltx_cite ltx_citemacro_citet\">Huang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib31\" title=\"\">2023</a>)</cite>, where native and non-native listeners provided significantly different ratings for the same samples, particularly for naturalness and speaker similarity. Consequently, while absolute scores differ substantially across Mandarin and English, only within-language comparisons are meaningful. Cross-lingual ranking is not valid given the different annotation bases.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "appropriateness",
                    "english",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T11\" title=\"Table 11 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> and Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.F5\" title=\"Figure 5 &#8227; English Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.F6\" title=\"Figure 6 &#8227; English Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> further highlight differences across domains. In Mandarin, performance is relatively consistent across categories, with means clustering around 3.1&#8211;3.3. The highest scores appear in <span class=\"ltx_text ltx_font_typewriter\">SocialIdentity</span> and <span class=\"ltx_text ltx_font_typewriter\">DailyOccupation</span>, suggesting that models handle culturally grounded archetypes more reliably. In English, however, domain variation is much greater. Categories such as <span class=\"ltx_text ltx_font_typewriter\">FantasyOccupation</span> and <span class=\"ltx_text ltx_font_typewriter\">NamedCharacters</span> receive notably lower scores (means below 2.0), reflecting the difficulty of producing expressive, role-appropriate speech in imaginative or culturally specific contexts. Occupational roles remain relatively easier for models in both languages, likely because these scenarios are closer to training data distributions.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although cross-lingual comparisons are invalid, this does not diminish the utility of archetype evaluation for role-play benchmarking. In fact, the observed inconsistencies reinforce a central point: successful role-play requires models to adapt to the cultural and linguistic norms of the target language community. A system that performs well in Mandarin but poorly in English cannot be considered universally capable, since end users will evaluate appropriateness and naturalness according to their native expectations. Thus, archetype evaluation highlights not only model-specific strengths and weaknesses but also the need for culturally grounded evaluation pipelines.</p>\n\n",
                "matched_terms": [
                    "appropriateness",
                    "english",
                    "mandarin",
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another notable pattern is the trade-off between acoustic quality and role embodiment. CosyVoice2 variants achieve strong audio quality but weaker human likeness and appropriateness, whereas <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> prioritizes role coherence at the expense of audio fidelity. This suggests that future systems must better integrate high-quality synthesis with nuanced role adherence. Similarly, the consistently lower scores for imaginative and identity-driven roles underscore the limitations of current SFMs in capturing subtle sociocultural expectations and narrative context.</p>\n\n",
                "matched_terms": [
                    "appropriateness",
                    "audio",
                    "human",
                    "chatgpt4o",
                    "likeness",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In sum, archetype evaluation reveals three major findings:\n(1) Mandarin and English annotations cannot be directly compared, due to linguistic and cultural annotation inconsistencies, consistent with prior evidence from SVCC 2023.\n(2) Within each language, clear model rankings emerge, <span class=\"ltx_text ltx_font_typewriter\">Doubao</span> dominates in Mandarin, while <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> excels in English.\n(3) Domain sensitivity remains a challenge, particularly for imaginative or culturally rich roles, where scores are substantially lower and inter-model variance is higher.</p>\n\n",
                "matched_terms": [
                    "doubao",
                    "english",
                    "model",
                    "annotation",
                    "mandarin",
                    "evaluation",
                    "archetype",
                    "chatgpt4o"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although realism evaluation targets nuanced paralinguistic behaviors, human annotators achieve <em class=\"ltx_emph ltx_font_italic\">consistently high agreement</em> once the rubric disentangles the aspects to be judged.\nAs shown in the agreement summary in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F7\" title=\"Figure 7 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, most dimensions fall in the <math alttext=\"0.86\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>0.86</mn><annotation encoding=\"application/x-tex\">0.86</annotation></semantics></math>&#8211;<math alttext=\"0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mn>0.88</mn><annotation encoding=\"application/x-tex\">0.88</annotation></semantics></math> range, with the lowest agreement on <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> (<math alttext=\"\\approx 0.83\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.83</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.83</annotation></semantics></math>) and the highest on <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em> / <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> (<math alttext=\"\\approx 0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.88</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.88</annotation></semantics></math>).\nEmotion-related, progressive metrics (Intensity and Transition) are slightly lower (<math alttext=\"\\approx 0.85\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.85</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.85</annotation></semantics></math>), reflecting the added difficulty of judging graded expressivity.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then report the overall agreement <math alttext=\"A=1-\\bar{\\sigma}/R\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px2.p1.m8\" intent=\":literal\"><semantics><mrow><mi>A</mi><mo>=</mo><mrow><mn>1</mn><mo>&#8722;</mo><mrow><mover accent=\"true\"><mi>&#963;</mi><mo>&#175;</mo></mover><mo>/</mo><mi>R</mi></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">A=1-\\bar{\\sigma}/R</annotation></semantics></math>, where <math alttext=\"\\bar{\\sigma}\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px2.p1.m9\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>&#963;</mi><mo>&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\bar{\\sigma}</annotation></semantics></math> is the mean of <math alttext=\"\\{\\sigma_{i}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px2.p1.m10\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>&#963;</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\sigma_{i}\\}</annotation></semantics></math> over samples with at least two annotations.\nWe also clamp <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px2.p1.m11\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> to <math alttext=\"[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px2.p1.m12\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0,1]</annotation></semantics></math> and ignore single-annotator cases.\nA minimal, reproducible implementation is:</p>\n\n",
                "matched_terms": [
                    "overall",
                    "mean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F8\" title=\"Figure 8 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F9\" title=\"Figure 9 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> visualize the score distributions per dimension.\nWe highlight three patterns:\n(i) <span class=\"ltx_text ltx_font_bold\">Progressive/gated emotion metrics.</span> For <em class=\"ltx_emph ltx_font_italic\">Emotion Intensity</em> and <em class=\"ltx_emph ltx_font_italic\">Emotion Transition</em>, we assign <span class=\"ltx_text ltx_font_typewriter\">1</span> when the scene is marked <em class=\"ltx_emph ltx_font_italic\">N/A</em> (i.e., emotion accuracy is insufficient to meaningfully judge intensity/transition). This produces a visible mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, a sparse bin at <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>, and slightly lower overall agreement; downstream analyses conditioning on adequate <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> (e.g., mean&#160;<math alttext=\"\\geq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 3</annotation></semantics></math>) recover more bell-shaped distributions.\n(ii) <span class=\"ltx_text ltx_font_bold\">Traits Embodiment.</span> Scores tend to avoid the middle (<math alttext=\"\\sim\\!3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!3</annotation></semantics></math>), concentrating on low values when traits clearly mismatch and high values when traits are strongly realized. This yields the lowest inter-rater agreement among dimensions, reflecting the inherently compositional and role-specific nature of trait judgments.\n(iii) <span class=\"ltx_text ltx_font_bold\">Other dimensions.</span> <em class=\"ltx_emph ltx_font_italic\">Pitch Variation</em>, <em class=\"ltx_emph ltx_font_italic\">Rhythmic Naturalness</em>, <em class=\"ltx_emph ltx_font_italic\">Stress &amp; Emphasis</em>, <em class=\"ltx_emph ltx_font_italic\">Voice Identity Matching</em>, <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em>, <em class=\"ltx_emph ltx_font_italic\">Global Story Coherence</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> are approximately unimodal with modes at <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>&#8211;<math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math>, consistent with moderate&#8211;strong realization in the dataset and with the high agreement reported.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "mean",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, the consistently high agreement confirms that the rubric isolates perceptually coherent factors, so learned evaluators can target them reliably.\nSecond, the gated behavior of <em class=\"ltx_emph ltx_font_italic\">Intensity/Transition</em> motivates a <em class=\"ltx_emph ltx_font_italic\">hierarchical</em> evaluator: verify <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> before scoring graded expressivity, or model the joint structure with conditional heads.\nThird, <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> remains challenging; models may benefit from explicit character-trait priors, long-term context aggregation, or contrastive supervision between matched and mismatched traits.\nFinally, prosody-focused objectives (e.g., pitch/rhythm/stress control) are likely to yield the largest quality gains for amateur-style speech.</p>\n\n",
                "matched_terms": [
                    "model",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For both zeroshot and fewshot experiments, we adopt a streamlined version of the annotation guidelines to ensure consistency with the human annotation process. The exact rubric prompts for archetype and realism evaluation used are provided below:</p>\n\n",
                "matched_terms": [
                    "archetype",
                    "evaluation",
                    "human",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and fewshot settings, we condition the model with the rubric prompt and then append few-shot demonstration triplets (input, audio, gold JSON) to mirror the human annotation protocol. As shown in Listing&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#LST2\" title=\"Listing 2 &#8227; L.1 Zeroshot/Fewshot Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, We use <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> for these few-shot exemplars and require a minified JSON output consistent with the rubric schema.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "model",
                    "human",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and few-shot experiments on commercial models such as <span class=\"ltx_text ltx_font_typewriter\">Gemini</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o</span>, we report results as the average of five independent runs to reduce randomness and obtain more stable predictions. In contrast, for open-source models, we observed that their instruction-following ability is relatively weak. To mitigate this, we prompt them to predict one dimension at a time rather than all dimensions jointly. Specifically, for each metric, after showing the rubric prompt, we request completion in a fixed template and extract the probability distribution over the discrete tokens <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>. The final score is then computed from these probabilities, which we found makes open-source models usable to some extent under this setting.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For few-shot experiments, we randomly sample three exemplars from the training set, with each exemplar chosen such that its human scores are close to the rounded average across all annotators. These exemplars are appended as demonstrations in the prompt. As with the zeroshot case, we run five trials for commercial models and report the averaged results for evaluation.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Effect of repeated runs.\nWhen comparing single-run decoding (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot-1run</span>) against averaging over five runs (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot</span>), we see consistent gains across nearly all dimensions (average score improves from 0.353 to 0.390). This suggests that sampling variance plays a substantial role in the reliability of large language model (LLM) evaluators, and averaging multiple outputs reduces this variance, producing more stable correlation with human annotations.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "model",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In general, the results demonstrate two complementary strategies for stabilizing LLM-based evaluators: (i) averaging multiple runs to reduce randomness, and (ii) enriching prompts with more few-shot exemplars to improve adherence to rubric-based evaluation. Combining both leads to the strongest overall performance, with the nine-shot condition achieving the highest average correlation&#160;(0.469).</p>\n\n",
                "matched_terms": [
                    "overall",
                    "evaluation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the archetype test set, all fine-tuned variants achieve strong <span class=\"ltx_text ltx_font_italic\">Content Pass</span> accuracy above 92%. Differences appear mainly in correlation-based metrics. Here, <span class=\"ltx_text ltx_font_typewriter\">Base-FT</span> yields the highest score on <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> delivers the strongest performance in <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> and the best overall average (0.629). This suggests that instruction-tuned checkpoints paired with parameter-efficient adaptation can better capture higher-level perceptual traits.</p>\n\n",
                "matched_terms": [
                    "appropriateness",
                    "pass",
                    "content",
                    "audio",
                    "overall",
                    "best",
                    "human",
                    "archetype",
                    "likeness",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For realism evaluation on the basic test set, <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> consistently achieves the strongest correlations across most dimensions, especially for <span class=\"ltx_text ltx_font_italic\">Character Identity</span> and <span class=\"ltx_text ltx_font_italic\">Traits</span>, leading to the best average correlation (0.668). <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> also performs competitively (0.630), while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> lags slightly behind (0.625). These results highlight that full fine-tuning may still offer an advantage when the evaluation set is relatively homogeneous and closer to the training distribution.</p>\n\n",
                "matched_terms": [
                    "best",
                    "dimensions",
                    "evaluation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A different pattern emerges on the real recording test set. Both <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> and <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> struggle, yielding negative or near-zero correlations across most dimensions. In contrast, <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> shows clear improvements, achieving positive correlations in all prosodic and emotional dimensions and producing the highest overall average (0.247). This indicates that LoRA adaptation on top of an instruction-tuned backbone provides better generalization to out-of-distribution, real-world data.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> is competitive on in-domain evaluation, it suffers a substantial drop when applied to real recordings. By contrast, <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> offers a more robust balance: it maintains reasonable performance on archetype and basic sets, while significantly outperforming other settings on real-world data. Based on this robustness, we adopt <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> as our final configuration for subsequent experiments.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the comprehensive design of Speech-DRAME, several limitations remain.\nFirst, while our dual evaluation paradigm combines top-down archetype assessment with bottom-up realism evaluation, both strategies are constrained by the quality and scope of available data. Archetype evaluation inevitably relies on simplified stereotypes, which can overlook subtler paralinguistic and contextual cues. Realism evaluation, though richer, still struggles with domain mismatch between synthetic data and real human recordings, as evidenced by the performance drop observed in the real-recording test set. This highlights the persistent gap between benchmark-driven evaluation and real-world variability in speech role-play.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "quality",
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, the present framework emphasizes bilingual evaluation (Mandarin and English), but linguistic and cultural diversity is far broader. Cross-lingual generalization and culturally sensitive evaluation are still open challenges, particularly since we observed annotation discrepancies across language groups. This limits the extent to which Speech-DRAME can claim universal coverage.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "evaluation",
                    "english",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, although we validated DRAME-Eval against human annotations, alignment under realism settings remains modest. This suggests that even human-annotated benchmarks are insufficiently fine-grained to capture the full complexity of human perception in speech role-play. Richer annotation protocols, more diverse speaker populations, and integration of perceptual psychology insights could further improve evaluation fidelity.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Looking forward, we see several promising research directions. We intentionally restrict evaluation to single-turn responses for clarity and tractability. However, multi-turn extensions that capture narrative progression, long-term consistency, and dynamic emotion shifts remain a natural next step. Incorporating multilingual and cross-cultural datasets will enhance generality and fairness. On the modeling side, exploring hybrid evaluation strategies that combine discrete rubrics with continuous perceptual signals (e.g., prosody embeddings or affective measures) may better bridge the gap between automatic and human judgments. Finally, integrating Speech-DRAME into reinforcement learning pipelines as a reward model could help drive the next generation of expressive, human-aligned speech role-play systems.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "model",
                    "human"
                ]
            }
        ]
    },
    "A10.T11": {
        "caption": "Table 11: Domain-level human annotation results for Appropriateness dimension of Mandarin and English archetype evaluation (mean \\pm std).\nWe report results of two languages with inter-model variance separately.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Domain</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Mandarin</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">English</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Score (mean <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T11.m3\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> std)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">Inter-model Var.</td>\n<td class=\"ltx_td ltx_align_center\">Score (mean <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T11.m4\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> std)</td>\n<td class=\"ltx_td ltx_align_center\">Inter-model Var.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">NamedCharacters</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.0343 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T11.m5\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 1.1087</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.1087</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.9896 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T11.m6\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 1.0409</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.0409</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">FantasyOccupation</td>\n<td class=\"ltx_td ltx_align_center\">3.1629 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T11.m7\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.9661</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.9661</td>\n<td class=\"ltx_td ltx_align_center\">1.9444 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T11.m8\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.9809</td>\n<td class=\"ltx_td ltx_align_center\">0.9809</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Traits</td>\n<td class=\"ltx_td ltx_align_center\">3.1670 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T11.m9\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 1.2109</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">1.2109</td>\n<td class=\"ltx_td ltx_align_center\">2.1518 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T11.m10\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 1.0572</td>\n<td class=\"ltx_td ltx_align_center\">1.0572</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">Events</td>\n<td class=\"ltx_td ltx_align_center\">3.2535 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T11.m11\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 1.1523</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">1.1523</td>\n<td class=\"ltx_td ltx_align_center\">2.2608 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T11.m12\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 1.1280</td>\n<td class=\"ltx_td ltx_align_center\">1.1280</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">DailyOccupation</td>\n<td class=\"ltx_td ltx_align_center\">3.2760 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T11.m13\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.8652</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.8652</td>\n<td class=\"ltx_td ltx_align_center\">2.5129 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T11.m14\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 1.1781</td>\n<td class=\"ltx_td ltx_align_center\">1.1781</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\">RelativeRoles</td>\n<td class=\"ltx_td ltx_align_center\">3.2762 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T11.m15\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 1.0255</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">1.0255</td>\n<td class=\"ltx_td ltx_align_center\">2.3594 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T11.m16\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 1.1298</td>\n<td class=\"ltx_td ltx_align_center\">1.1298</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">SocialIdentity</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">3.3263 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T11.m17\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 0.9802</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.9802</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">2.3256 <math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"A10.T11.m18\" intent=\":literal\"><semantics><mo>&#177;</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math> 1.0912</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1.0912</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "dailyoccupation",
            "events",
            "annotation",
            "evaluation",
            "separately",
            "namedcharacters",
            "two",
            "socialidentity",
            "var",
            "domainlevel",
            "fantasyoccupation",
            "archetype",
            "english",
            "report",
            "relativeroles",
            "mean",
            "results",
            "variance",
            "languages",
            "appropriateness",
            "intermodel",
            "score",
            "mandarin",
            "pm",
            "human",
            "domain",
            "dimension",
            "traits",
            "std"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T11\" title=\"Table 11 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">11</span></a> and Figures&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.F5\" title=\"Figure 5 &#8227; English Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.F6\" title=\"Figure 6 &#8227; English Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> further highlight differences across domains. In Mandarin, performance is relatively consistent across categories, with means clustering around 3.1&#8211;3.3. The highest scores appear in <span class=\"ltx_text ltx_font_typewriter\">SocialIdentity</span> and <span class=\"ltx_text ltx_font_typewriter\">DailyOccupation</span>, suggesting that models handle culturally grounded archetypes more reliably. In English, however, domain variation is much greater. Categories such as <span class=\"ltx_text ltx_font_typewriter\">FantasyOccupation</span> and <span class=\"ltx_text ltx_font_typewriter\">NamedCharacters</span> receive notably lower scores (means below 2.0), reflecting the difficulty of producing expressive, role-appropriate speech in imaginative or culturally specific contexts. Occupational roles remain relatively easier for models in both languages, likely because these scenarios are closer to training data distributions.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Role-play has become a key testbed for generative models, expanding from text-only dialogue to multimodal interaction. Extending role-play to speech captures prosody, emotion, and delivery, but also poses new evaluation challenges. Current pipelines often use audio large language models (ALLMs) as zero-shot judges, which miss paralinguistic cues, collapse multiple aspects into coarse scores, and rely on synthetic speech references that fail to reflect real-world roles. We present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework that contributes at three levels: (i) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-EvalBench</em>, an evaluation benchmark with bilingual human-annotated data and protocols for training and testing speech evaluation models&#160;(SEMs), (ii) <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, a fine-tuned evaluation model, which substantially outperforms zero-shot and few-shot ALLMs, and (iii) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-RoleBench</em>, a speech role-play benchmark that leverages <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> as an automatic judge to compare speech foundation models (SFMs). Speech-DRAME distinguishes between two complementary evaluation strategies: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>, a top-down approach measuring adherence to broad role archetypes, and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em>, a bottom-up approach grounded in real human speech that emphasizes nuanced role quality. Compared to zero-shot ALLM judges, <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves stronger agreement with human ratings (Pearson correlation from 0.480 to 0.629 in archetypes, and 0.390 to 0.625 in realism). By integrating transparent benchmark resources, modeling approaches, and system-level evaluation, Speech-DRAME provides the first comprehensive, reproducible foundation for assessing spoken role-play.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "two",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome these limitations, we present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span> (Speech Detailed Role-play Assessment with Modeling and Evaluation), a unified framework that moves beyond synthetic or judge-only pipelines. Speech-DRAME is distinguished by three aspects: [i] a dual evaluation paradigm that balances archetype-driven generality with human-grounded realism (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS2\" title=\"3.2 Role-play Evaluation with Two Strategies &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), [ii] human annotations that provide perceptual grounding absent in prior resources (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4\" title=\"4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), and [iii] evaluation-model alignment that enables not only benchmarking of speech foundation models (SFMs) but also systematic analysis of the evaluation models (SEMs) themselves (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5\" title=\"5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). These aspects establish Speech-DRAME as a comprehensive and trustworthy framework for measuring and improving speech role-play.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, Speech-DRAME contributes at three levels: (i) <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>, an evaluation benchmark consisting of human-annotated datasets for archetype and realism role-play; (ii) <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span>, a fine-tuned SEM trained on EvalBench that surpasses zero-shot and few-shot ALLMs; and (iii) <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span>, a speech role-play benchmark that leverages DRAME-Eval as an automatic judge to systematically compare SFMs. By combining dual evaluation strategies, human-anchored supervision, and evaluation-model alignment, Speech-DRAME delivers systematic insights into both role-play systems and evaluation models themselves.\nOur contributions are four-fold:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>, the first evaluation benchmark with human annotations under both archetype- and realism-based strategies, supporting the training and testing of SEMs.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span>, a fine-tuned SEM trained on EvalBench that substantially outperforms zero-shot and few-shot ALLMs, demonstrating the importance of human supervision for evaluation models.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-native benchmarks.</span>\nMore recently, a number of resources have been introduced to evaluate spoken role-play agents. OmniCharacter-10K <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>)</cite> emphasizes character voice traits, while others&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib39\" title=\"\">2025</a>; Hou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib27\" title=\"\">2025</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib15\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib7\" title=\"\">2024b</a>)</cite> extend evaluation toward vocal communication, assistant capability, and multi-turn realism. Omni models such as StepAudio and KimiAudio <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite> also include role-play scenarios in their evaluation. SpeechRole and VStyle <cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>; Zhan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib76\" title=\"\">2025</a>)</cite> are closer to our work, offering datasets that test interaction ability, expressiveness, and fidelity. However, these resources are primarily built from LLM-generated dialogues and TTS outputs, with limited human validation and a heavy reliance on proprietary ALLM-as-judge pipelines. As a result, they lack grounding in diverse human speech and typically apply only coarse rubrics.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Expressive TTS evaluation.</span>\nRecent work has explored evaluation of instruction-following and style-controlled TTS. InstructTTSEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite> explicitly includes role-play scenarios, while EmergentTTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">(Manku et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib45\" title=\"\">2025</a>)</cite> and discrete speech LM benchmarks <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang &amp; Sz&#233;kely, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib63\" title=\"\">2024</a>)</cite> emphasize prosody, spontaneity, and speaker consistency. These efforts push beyond intelligibility and naturalness, but still center on synthetic outputs rather than real human role-play.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our distinction.</span>\nIn contrast, Speech-DRAME unifies these considerations through three elements: (i) a dual evaluation paradigm that combines archetype-driven generality with realism grounded in authentic human speech, (ii) human annotations that provide a reliable perceptual gold standard, and (iii) alignment of SEMs to human ratings, enabling systematic evaluation of both SFMs and the evaluation models themselves. This combination moves beyond synthetic benchmarks or zero-shot judging to provide a comprehensive and trustworthy framework for speech role-play assessment.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech role-play evaluation can be framed in multiple ways depending on the available data and the desired granularity of analysis.\nWithin Speech-DRAME, we formalize two distinct strategies inspired by sociology theories<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Please refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A2\" title=\"Appendix B Theoretical Motivation for Dual Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> for detailed discussion of the dual evaluation design.</span></span></span>, including a top-down, archetype-oriented option using synthetic speech and a bottom-up, realism-oriented option from real human speech.\nRather than merging these into a single rubric, we treat them as separate perspectives, each with unique advantages and limitations.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarizes the key contrasts, while further implementation details are provided in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>-&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>). The first strategy follows a <em class=\"ltx_emph ltx_font_italic\">top-down</em> design, inspired by prior text-based role-play benchmarks.\nHere, role-play is judged with respect to <em class=\"ltx_emph ltx_font_italic\">stereotypical archetypes</em> (e.g., &#8220;firefighter&#8221; or &#8220;ER doctor&#8221;), using scene contexts (e.g., &#8220;comfort a child trapped in fire&#8221; or &#8220;ask for help in a serious operation&#8221;).\nThis approach provides accessible, general-purpose scoring that is scalable to a wide range of scenarios, making it suitable for large-scale benchmarking.\nHowever, its reliance on broad stereotypes and global impressions limits its ability to capture fine-grained aspects of delivery (e.g., subtle prosodic shifts, nuanced emotional control)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Holt &amp; Lotto, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib26\" title=\"\">2010</a>)</cite>.\nIn practice, this strategy mirrors simplified annotation settings and is particularly useful when annotation resources are limited&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib21\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS5\" title=\"F.5 Examples of Realism Role-play &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.5</span></a>). The second strategy adopts a <em class=\"ltx_emph ltx_font_italic\">bottom-up</em> design, motivated by real-world performance and real human speech.\nInstead of relying solely on stereotypes, this approach grounds evaluation in recordings from professional and non-professional speakers, drawn from realistic dialogue and media sources.\nJudgments in this setting are more fine-grained and multi-dimensional, addressing aspects such as prosodic dynamics, emotional fidelity, character consistency, and contextual fit.\nWhile this strategy requires more human annotation and careful curation, it provides benchmarks that reflect how role-play is perceived in practice.\nIt thus offers a richer framework for analyzing model outputs in settings where realism and subtlety are essential.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This section details the <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> data curation process. Our data curation methodology follows a two&#8211;stage design: (i) <em class=\"ltx_emph ltx_font_italic\">task preparation</em>, which specifies the roles, scenes, and evaluation settings used to elicit speech; and (ii) <em class=\"ltx_emph ltx_font_italic\">human annotation</em>, which provides reliable ground-truth judgments on generated or real human speech.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human Annotation.</span>\nAll collected audio undergoes a standardized annotation protocol: (1) an initial semantically-based screening to exclude invalid clips; (2) rubric-based scoring across multiple perceptual dimensions; and (3) meta-annotations such as confidence or prompt-quality judgments. Full rubrics are provided in the appendices&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A5\" title=\"Appendix E Annotation Guidelines for Archetype Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">E</span></a> &amp; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>All data collection follows in high standard ethical consideration as outlined in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A9\" title=\"Appendix I Annotation Ethics &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "human",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Archetype Data Collection &#8227; 4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents a summary of the curated <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> dataset, where we further elaborate the details of archetype and realism evaluation data collection in following subsections, respectively. For benchmark purpose, we provide an official train-test split for the human annotated dataset for <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> so that following researchers can use the dataset to train and evaluate their SEMs for speech role-play.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Archetype evaluation adopts a top&#8211;down perspective from sociology and role theory&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goffman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib23\" title=\"\">1949</a>; Campbell, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib4\" title=\"\">2008</a>)</cite>, abstracting role-play into broad and recognizable categories.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Role and Scene Dimensions.</span>\nRoles are operationalized through seven categories: (1) functional occupations in daily life, (2) functional occupations in fictional or fantasy settings, (3) social identities, (4) named or specific characters, (5) personality traits, (6) relative relationship, (7) basic events. Scenes are anchored by everyday events or state changes that evoke stereotypical reactions. Details of template design and alternative prompting strategies are given in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>We note that some archetypes (e.g., firefighter) inevitably reflect cultural stereotypes. These are used solely for evaluation scalability and do not reflect normative judgments.</span></span></span></p>\n\n",
                "matched_terms": [
                    "traits",
                    "evaluation",
                    "events"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotation follows two stages: an initial semantic check (i.e., <span class=\"ltx_text ltx_font_italic\">Content Pass</span>) to filter out instruction non-compliance or audio defects, followed by rubric-based scoring on three dimensions: <em class=\"ltx_emph ltx_font_italic\">Audio Quality</em>, <em class=\"ltx_emph ltx_font_italic\">Human-likeness</em>, and <em class=\"ltx_emph ltx_font_italic\">Appropriateness</em>. Confidence judgments are also recorded. Detailed rubric scales are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A5\" title=\"Appendix E Annotation Guidelines for Archetype Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "appropriateness",
                    "two",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype Evaluation Data Summary.</span>\nThe resulting archetype evaluation dataset comprises 552 distinct scenarios for both Mandarin and English, evaluated across eight models,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>The model list is discussed in Appdenix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS4\" title=\"D.4 Candidate Models for Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.4</span></a>. Note we only include seven models for English due to the deactivation of ChatGPT-4O advanced voice model.</span></span></span> yielding 8,280 samples. In total, the dataset are split into 6,780 for training and 1,500 for testing. For benchmarking, an additional 1,250 scenarios spanning seven role/scene categories are prepared to facilitate archetype role-play benchmarking. More detailed statistics and analyses are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10\" title=\"Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">J</span></a>.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "evaluation",
                    "english",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotators judge whether performances align with the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em>, using rubrics for <em class=\"ltx_emph ltx_font_italic\">Prosodic Dynamics (i.e., Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis)</em>, <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness (with progressive gating over three sub-dimensions: Emotion Accuracy, Emotion Intensity, Emotion Transition)</em>, <em class=\"ltx_emph ltx_font_italic\">Character Consistency (i.e., Voice Identity Matching and Traits Embodiment)</em>, <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance (i.e., Local Scene Fit and Global Story Coherence)</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>. A prompt quality check and confidence ratings are also collected. Full scales and gating details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "evaluation",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation Data Summary.</span>\nThe resulting realism evaluation dataset is constructed from a combination of curated media sources, including internal collections, public datasets (e.g., NCSSD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib40\" title=\"\">2024b</a>)</cite>), and original recordings collected for this project.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>All data were obtained under appropriate licenses and are legally available for research release.</span></span></span> We compile 15k annotated role-play samples: 9k in Mandarin and 5k in English. The dataset is divided into 12k samples for training, 2k mixed samples for the base test set, and 1k real-recording samples for the test set. The first test set supports general evaluation using our proposed pipeline with negative samples, while the second focuses exclusively on human speech performed by both professional and amateur voice actors. For benchmarking, 2,000 scenarios from the realism dataset are designated for realism role-play benchmarking. Detailed statistics and analyses are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11\" title=\"Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">K</span></a>.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "evaluation",
                    "human",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the two proposed datasets, we conduct experiments under three settings: zero-shot, few-shot, and finetuning, using pre-trained ALLMs. We formulate the task as a multi-metric estimation problem&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tjandra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib58\" title=\"\">2025</a>; Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib74\" title=\"\">2025</a>; Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib55\" title=\"\">2025b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib54\" title=\"\">a</a>)</cite>, where a single model is trained or prompted to predict multiple evaluation dimensions.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For proprietary models, we report the average of five independent runs to reduce variance and obtain more stable predictions. For open-source models, we observed weaker instruction-following ability and limited adaptation to unfamiliar tasks. To mitigate this, we did not request simultaneous prediction of multiple metrics. Instead, we queried one metric at a time. Following <cite class=\"ltx_cite ltx_citemacro_citet\">Song &amp; Bahri (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>); Lukasik et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>, we interpret model outputs through token distribution over discrete labels (<span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score.</p>\n\n",
                "matched_terms": [
                    "report",
                    "variance",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Finetuning Model: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>.</span>\nIn addition to zero-shot and few-shot evaluations, we fine-tune a dedicated evaluation model, denoted as <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> for archetype and realism role-play evaluation, respectively.\nWe initialize from <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio-7B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib11\" title=\"\">2024</a>)</cite>, which provides a strong foundation for multi-modal audio-text reasoning.\nTo better leverage the multiple human annotations available in our datasets, we adopt a sampling-based training strategy: at each step, one annotator label is randomly selected, following practices in label distribution learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Geng, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib22\" title=\"\">2016</a>; Wen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib67\" title=\"\">2023</a>)</cite>.\nThis stochastic supervision implicitly exposes the model to the empirical annotation distribution, while during inference we output the full distribution over discrete labels (e.g., <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score, consistent with recent decoding-based regression methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Song &amp; Bahri, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>; Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib70\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib64\" title=\"\">2025b</a>; Lukasik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>. We apply LoRA fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib28\" title=\"\">2022</a>)</cite>. All implementation details, including optimizer settings, training schedule, and sampling policy, are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS3\" title=\"L.3 Fine-tuning Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "score",
                    "annotation",
                    "evaluation",
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics for Role-play Evaluation Model.</span> For most dimensions with 1-5 scale, we report Pearson correlation. For <span class=\"ltx_text ltx_font_italic\">Content Pass</span> estimation, we report accuracy.</p>\n\n",
                "matched_terms": [
                    "report",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype Evaluation.</span>\nThe results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T2\" title=\"Table 2 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reveal clear distinctions across evaluation settings. In the zero-shot condition, proprietary ALLMs such as <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> substantially outperform public models, achieving strong <span class=\"ltx_text ltx_font_italic\">content pass</span> accuracy (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>90%) and moderate correlations with other dimensions(<math alttext=\"0.43\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mn>0.43</mn><annotation encoding=\"application/x-tex\">0.43</annotation></semantics></math>&#8211;<math alttext=\"0.48\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mn>0.48</mn><annotation encoding=\"application/x-tex\">0.48</annotation></semantics></math>). By contrast, public models exhibit limited predictive ability, with correlations often below 0.2. Few-shot prompting provides modest gains for proprietary models, improving both accuracy and correlation (e.g., <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> rises from 0.480 to 0.493 average correlation). The most significant improvement arises from finetuning: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves an average correlation of 0.629, marking a large margin over zero-shot/few-shot models and demonstrating the effectiveness of task-specific supervision.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T3\" title=\"Table 3 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> extends the analysis to fine-grained realism evaluation. Similar to archetype results, proprietary ALLMs lead the zero-shot setting, with <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> obtaining an average correlation of 0.390, compared to 0.284 for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>. Public models remain weak baselines. Few-shot prompting consistently improves performance: <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> gains +0.042 and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> gains +0.083 over their zero-shot averages, showing the benefit of in-context supervision. Once again, finetuning is most effective: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves 0.625 average correlation, higher than the best few-shot results. Importantly, the gains are consistent across all 10 realism dimensions, confirming that the sampling&#8211;expectation training strategy generalizes beyond simple classification to nuanced paralinguistic cues.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real Recording Test Set.</span>\nThe real-recording evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T4\" title=\"Table 4 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> is notably more challenging, with all models suffering substantial performance drops. Proprietary models in the zero-shot setting achieve only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.04&#8211;0.08 average correlation, and public models often fall below zero. Few-shot prompting yields small but noticeable improvements, especially for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> (0.226 avg.), suggesting that exemplars partially mitigate domain mismatch between synthetic and real human speech. Interestingly, while finetuning achieves the highest overall average (0.247), its relative advantage over few-shot prompting is narrower than in the synthetic setting, and some dimensions (e.g., <span class=\"ltx_text ltx_font_italic\">Character Identity</span>) remain weak. We emphasize that this observed gap between synthetic and real recordings is not merely an artifact but an intended feature of Speech-DRAME, designed to expose the mismatch and drive future research into bridging synthetic-to-real generalization.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "domain"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall.</span> Across both archetype and realism evaluations, we observe three consistent trends: (i) zero-shot performance is limited, particularly for open-source models, (ii) few-shot prompting offers moderate but consistent gains, especially for proprietary ALLMs, and (iii) finetuning with task-specific supervision (<span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>) delivers the largest improvements, achieving state-of-the-art correlations across nearly all dimensions. Nevertheless, results on the real-recording test set underscore the gap between synthetic and real human role-play scenarios, motivating future work in data diversification and domain adaptation.</p>\n\n",
                "matched_terms": [
                    "domain",
                    "human",
                    "results",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on fine-tuned <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, we establish the DRAME-RoleBench to evaluate SFMs&#8217; role play ability. For archetype evaluation, 1,250 scenarios are constructed following the same data construction pipeline as the <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>. For realism evaluation, we reuse 2,000 scenarios in the <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> test set. We follow the same prompt strategy as outlined in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For evaluation, we consider a comprehensive set of candidate models, including six end-to-end models: <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(OpenAI, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib47\" title=\"\">2025b</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zeng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib75\" title=\"\">2024</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib71\" title=\"\">2025a</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Qwen3Omni</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Xu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib72\" title=\"\">2025b</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">StepAudio2Mini</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib68\" title=\"\">2025</a>)</cite>; and ten cascaded models by combining two widely-used textual LLMs (i.e., <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib12\" title=\"\">2025</a>)</cite> and <span class=\"ltx_text ltx_font_typewriter\">Qwen3-30B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib73\" title=\"\">2025</a>)</cite>) and five recent TTS models (i.e., <span class=\"ltx_text ltx_font_typewriter\">F5TTS</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib8\" title=\"\">2024c</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">IndexTTS2</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib82\" title=\"\">2025b</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">MaskGCT</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib66\" title=\"\">2025d</a>)</cite>, <span class=\"ltx_text ltx_font_typewriter\">Vevo1.5</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib79\" title=\"\">2025b</a>)</cite>, and <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib17\" title=\"\">2024b</a>)</cite>).</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype evaluation.</span>\nOn archetype role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T5\" title=\"Table 5 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), cascaded pipelines consistently outperform end-to-end systems. The strongest cascaded systems, particularly <span class=\"ltx_text ltx_font_typewriter\">G-IndexTTS2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-IndexTTS2</span>, achieve both high content pass rates (above 94%) and balanced scores across audio quality, human likeness, and appropriateness. By contrast, end-to-end models such as <span class=\"ltx_text ltx_font_typewriter\">StepAudio2Mini</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> perform competitively in terms of content coverage (above 88%), but their expressive scores remain slightly lower than their cascaded counterparts. This gap suggests that separating reasoning (LLM) from speech rendering (TTS) remains advantageous when archetype fidelity is emphasized. Nevertheless, the fact that top end-to-end systems already surpass 3.0 in average rating indicates rapid closing of the performance gap.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "appropriateness",
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism evaluation.</span>\nRealism role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T6\" title=\"Table 6 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) presents a different landscape. Here, performance differences between end-to-end and cascaded systems are less pronounced. Several end-to-end models, such as <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>, reach average scores of 3.24, matching or even exceeding the best cascaded pipelines. Importantly, realism scores are more evenly distributed across prosodic, emotional, character, and contextual dimensions, with most models clustering between 2.9&#8211;3.2. This convergence reflects the difficulty of realism-style evaluation: since the data preparation emphasized contrast with authentic human speech, models are judged under similar conditions, leading to smaller observed gaps.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human alignment.</span>\nWe further conducted small-scale human studies with 10% of the benchmark data to verify the reliability of the benchmark. Archetype evaluation achieves a Spearman correlation of 0.706 between human judgments and <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, validating that the automatic evaluator closely tracks human preferences in this setting. Realism evaluation, however, achieves a more modest Spearman correlation of 0.375. This lower alignment is expected, since realism data construction did not prioritize fine-grained model separation but instead emphasized distinguishing human versus synthetic speech.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>We view this as evidence that realism presents a harder, less-saturated frontier. Rather than a limitation of DRAME-Eval, this highlights the challenge of designing evaluators that capture nuanced human perception, which motivates future work.</span></span></span> As a result, all models occupy a relatively narrow performance band, reducing sensitivity to model-level differences. We view this as evidence that realism evaluation highlights a harder, less saturated frontier where both SFMs and SEMs require further advances.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, cascaded pipelines retain an edge in archetype scenarios, especially in content robustness and expressive fidelity, but end-to-end models are catching up quickly. In realism scenarios, the field remains unsolved: existing systems exhibit similar performance bands and evaluators show weaker alignment with human judgments, underscoring the need for improved realism-grounded data and evaluation. Together, these findings establish <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> as both a reliable testbed for archetype fidelity and a challenging benchmark for realism fidelity.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework for assessing spoken role-play through two complementary lenses: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em> (top-down) and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> (bottom-up, human-grounded). Our human-annotated <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> enables training and testing of SEMs, and our fine-tuned <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span> substantially outperforms zero-/few-shot ALLM judges. Building on this, <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> provides system-level comparison across various SFMs, revealing a consistent edge for cascaded pipelines on archetypes and a tighter spread under realism, where real human performance evaluation remain challenging. Together, these resources establish a reproducible, analyzable foundation for spoken role-play evaluation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>Limitations and future works are discussed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A13\" title=\"Appendix M Limitations and Future Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">M</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "two",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All data collection and annotation in this work followed strict ethical standards. Our bilingual datasets were constructed exclusively from legally accessible sources, including licensed media, open-source corpora, and original recordings conducted with informed consent. Annotators were recruited with appropriate compensation and provided with clear instructions, including the right to withdraw at any point. To minimize risks, no personally identifiable information was included in the released data, and all recordings were anonymized before annotation. We ensured cultural and linguistic diversity in annotator pools to mitigate potential bias across Mandarin and English role-play scenarios. While this study did not require formal IRB approval, we adhered to best practices in human-subject research, including transparency in task design and careful consideration of annotator well-being. Full details of our annotation protocol and ethical safeguards are presented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A9\" title=\"Appendix I Annotation Ethics &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "english",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data generation:</span> LLMs were employed to expand prompts (e.g., character profiles, scene descriptions) and to produce negative samples in realism evaluation. All outputs were screened by human annotators to ensure validity and reduce bias.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation design and task construction:</span> <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> was used to assist with building instructions and summarizing story contexts. Drafted materials were subsequently refined by the authors and validated by human annotators.</p>\n\n",
                "matched_terms": [
                    "human",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our distinction between <span class=\"ltx_text ltx_font_italic\">Archetype Evaluation</span> and <span class=\"ltx_text ltx_font_italic\">Realism Evaluation</span> is not merely pragmatic, but is grounded in established theories of social identity, communication, and performance. The two perspectives capture complementary dimensions of how humans perceive and evaluate roles in interaction.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "two",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Accordingly, Archetype Evaluation and Realism Evaluation are best viewed as complementary:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Archetype Evaluation provides macro-level judgments grounded in socially shared schemas, enabling efficient and broad comparisons.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p4.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> may be a simple mean, a recency-weighted average, or a learned temporal model that captures consistency across turns.\nThe choice of <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p4.m3\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> reflects the intended evaluation focus, e.g., stability of persona traits, adaptation to scene progression, or coherence across multiple speakers.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "mean",
                    "evaluation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This appendix first discusses the summary of task preparation steps for archetype evaluation. Then we provide concrete examples and extended discussion of the archetype evaluation annotation pipeline that are omitted in the main text for clarity.</p>\n\n",
                "matched_terms": [
                    "archetype",
                    "evaluation",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Archetype Evaluation follows a top&#8211;down perspective rooted in sociology and role theory, where social behavior is interpreted through shared stereotypes and generalized expectations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goffman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib23\" title=\"\">1949</a>; Campbell, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib4\" title=\"\">2008</a>)</cite>.\nThis strategy abstracts role-play into broad and easily recognizable archetypes, enabling scalable evaluation of whether a generated response aligns with salient role features.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Role dimensions.</span>\nWe operationalize archetypes through five principal categories, each representing a stereotypical dimension of role identity or behavior, including (1) <span class=\"ltx_text ltx_font_bold\">Occupation / Functional Role (Modern Daily Life)</span> (e.g., a firefighter rescuing a trapped civilian). (2) <span class=\"ltx_text ltx_font_bold\">Occupation / Functional Role (Fictional or Fantasy)</span> (e.g., a martial arts cult leader confronting orthodox rivals). (3)&#160;<span class=\"ltx_text ltx_font_bold\">Social Identity</span> (e.g., a doting parent worrying about a child, or a &#8220;Buddhist youth&#8221; eschewing competition). (4) <span class=\"ltx_text ltx_font_bold\">Named / Specific Characters</span> (e.g., Sun Wukong mocking celestial authorities in <em class=\"ltx_emph ltx_font_italic\">Journey to the West</em>). (5) <span class=\"ltx_text ltx_font_bold\">Personality Traits</span> (e.g., &#8220;hot-blooded&#8221; in a competitive setting, or &#8220;complaining&#8221; in response to poor service). (6) <span class=\"ltx_text ltx_font_bold\">Relative Roles</span> (e.g., parent or stranger).</p>\n\n",
                "matched_terms": [
                    "traits",
                    "dimension"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Exclusion of fine-grained controls.</span>\nWe deliberately excluded explicit emotional states, prosodic instructions, or linguistic constraints.\nThese dimensions are better suited to realism-oriented or style-controlled evaluation, whereas archetype evaluation benefits from concrete, stereotype-level scenarios that sharpen judgments and improve human agreement.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Pipeline overview.</span>\nAt a high level, archetype evaluation data are generated through a three-stage pipeline:\n(1) selecting keywords that specify role and scene;\n(2) expanding them into full prompts via structured templates; and\n(3) rendering prompts into speech with contemporary TTS systems.\nThis modular pipeline ensures both scalability and reproducibility, while keeping prompts vivid enough for reliable judgments. (4) Based on the generated data (paired scene and synthesized speech), expert human annotators are recruited for detailed annotation on semantic check (i.e., <span class=\"ltx_text ltx_font_italic\">Content Pass</span>) and three different spoken role-play dimensions (i.e., <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human-likeness</span>, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>).\nImplementation details, including template design, alternative prompting strategies, and annotation guidelines, are provided in the following subsections of this Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "appropriateness",
                    "annotation",
                    "evaluation",
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, archetype evaluation emphasizes breadth and recognizability over subtlety.\nBy grounding role-play in socially recognizable categories and stereotypical situations, it provides a practical balance between scalability and interpretability, complementing the more fine-grained realism evaluation described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS2\" title=\"3.2 Role-play Evaluation with Two Strategies &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We constructed archetype evaluation prompts by combining role categories with contextual scenes.\nThe following illustrates the core categories with representative examples.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">The Choice of Prompts.</span> We also experimented with broader, less specific templates that allowed more freedom for generative models but produced less discriminative evaluation results:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We evaluate a diverse set of speech foundation models spanning both English and Mandarin. In total, we include 8 English and 7 Mandarin models, covering open-source and proprietary systems, and balancing end-to-end and cascaded architectures. Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.T7\" title=\"Table 7 &#8227; D.4 Candidate Models for Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">7</span></a> provides the full list of models.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Step 1: We first build a speech database (LibriSpeech and GigaSpeech for English, WenetSpeech for Mandarin&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Panayotov et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib49\" title=\"\">2015</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib5\" title=\"\">2021</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib77\" title=\"\">2022</a>)</cite>). Each sample is annotated with Gemini-based speech captions describing salient acoustic attributes.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2\" title=\"Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the internal annotation interface used for <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>.\nThe platform provides an integrated environment for both playback and evaluation of model-generated role-play speech samples.\nAnnotators first interact with the audio player interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf1\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>), which visualizes waveforms and spectrograms for multiple versions of the same prompt, enabling side-by-side comparison under a shared role instruction (e.g., firefighter, teacher, or actor).\nThen, they proceed to the annotation interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf2\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a>), where they assign scores for multiple dimensions such as <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, along with reasoning and confidence inputs.\nThis structured interface supports high-consistency, fine-grained evaluation of role-play behaviors in speech models.</p>\n\n",
                "matched_terms": [
                    "appropriateness",
                    "annotation",
                    "evaluation",
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This appendix contains the full human annotation instructions used for archetype evaluation.</p>\n\n",
                "matched_terms": [
                    "archetype",
                    "evaluation",
                    "human",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">Audio Quality</span> &amp; <span class=\"ltx_text ltx_font_bold\">Human Likeness</span>: comment only if score <math alttext=\"\\leq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I6.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8804;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\leq 3</annotation></semantics></math>; if 4 or 5, write &#8220;N/A&#8221;.</p>\n\n",
                "matched_terms": [
                    "score",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">Appropriateness</span>: comment for <em class=\"ltx_emph ltx_font_italic\">all</em> scores 1&#8211;5. Explain <em class=\"ltx_emph ltx_font_italic\">why</em> the <em class=\"ltx_emph ltx_font_italic\">delivery</em> was/was not a good match to the in-context traits (role &amp; scene).</p>\n\n",
                "matched_terms": [
                    "traits",
                    "appropriateness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Pronunciation errors: penalize only on <span class=\"ltx_text ltx_font_bold\">Human Likeness</span> if they significantly affect comprehension; do not affect <span class=\"ltx_text ltx_font_bold\">Appropriateness</span>.</p>\n\n",
                "matched_terms": [
                    "appropriateness",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Rate <em class=\"ltx_emph ltx_font_italic\">only</em> how well the <em class=\"ltx_emph ltx_font_italic\">delivery</em> fits the role/scene described in the prompt (tonal fit). Ignore Audio Quality and Human Likeness issues while scoring this dimension.</p>\n\n",
                "matched_terms": [
                    "human",
                    "dimension"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realism Evaluation adopts a bottom&#8211;up perspective grounded in real human speech data, with the goal of capturing the nuanced delivery, prosody, and contextual fit that synthetic archetype prompts cannot fully represent.\nRather than relying on stereotypes, this strategy leverages real-world recordings and carefully constructed contrasts, enabling fine-grained assessment of model performance under realistic conditions.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data from real human speech sources.</span>\nWe construct the realism evaluation set primarily from natural speech material:</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, real human ground-truth segments and systematically constructed negatives form a balanced dataset that supports realism-oriented evaluation.\nThe positives provide natural anchors of role-play performance, while the negatives introduce controlled mismatches that test whether evaluation models can distinguish real human speech from synthetic approximations.\nThis design ensures that realism evaluation emphasizes subtle paralinguistic and contextual cues, moving beyond intelligibility or content fidelity.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Yunjin case study illustrates a fundamental difference between realism evaluation and archetype evaluation. Archetype prompts, as described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>, are constructed by pairing abstract role categories with synthetic contextual scenes. These cover a broad space of possible situations, ranging from modern occupations (e.g., firefighters) to fictional personas (e.g., cult leaders in martial arts fantasy) or social identities (e.g., overprotective parents, laid-back friends). While such prompts provide controlled and diverse coverage, they are ultimately template-based and rely on stereotypes to elicit role-play behaviors.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This distinction highlights the complementary strengths of the two evaluation paradigms. Archetype evaluation systematically explores a wide combinatorial space of roles and situations, but may lack the paralinguistic and contextual richness of real speech. Realism evaluation, on the other hand, anchors assessment in authentic delivery and contextual fit, providing a finer-grained lens on coherence and naturalness. Together, they ensure that role-play evaluation spans both broad stereotypical coverage and nuanced real-world plausibility.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "two",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The annotation interface for <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> focuses on assessing contextual and narrative coherence in model-generated speech.\nUnlike the archetype-oriented evaluation, which emphasizes stylistic and emotional fidelity, this platform examines how well the voice performance aligns with a character&#8217;s personality, dialogue situation, and scene progression.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Once familiar with the scene, annotators proceed to the detailed evaluation form shown in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.F4\" title=\"Figure 4 &#8227; F.7 Annotation Platform for Realism Evaluation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nThis interface is divided into two major sections: prosodic/emotional quality, higher-level coherence and semantic alignment.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This appendix contains the full human annotation instructions used for realism evaluation.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Focus evaluation on the <em class=\"ltx_emph ltx_font_italic\">spoken performance of the target speaker only</em>; ignore background music, ambient noise, and non-speech events.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "events"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Focus only on human speech:</span> Do not factor in background music/soundtracks, environmental or ambient noise, or non-speech audio events; these may reflect the scene but should not influence scoring of the spoken performance.</p>\n\n",
                "matched_terms": [
                    "human",
                    "events"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identity mismatch:</span> If the clip&#8217;s speaker is clearly not the intended target speaker, assign a score of <span class=\"ltx_text ltx_font_bold\">1</span> for every rating dimension that relies on the Character Profile (e.g., Character Style Traits, Trait Embodiment, Character Consistency).</p>\n\n",
                "matched_terms": [
                    "traits",
                    "score",
                    "dimension"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt contains contradictory information.</span> Conflicts make accurate annotation difficult or impossible. Examples: incompatible accent specifications; Local Scene drastically at odds with intended emotion (see <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>); Local and global scenes inconsistent; traits described as &#8220;evolving over time&#8221; without a timeline.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human-grounded, bilingual labels.</span>\nWe annotate in English <em class=\"ltx_emph ltx_font_italic\">and</em> Mandarin with trained raters, capturing cross-lingual prosody and style, while recording rater confidence to qualify downstream usage.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "english"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Transparent evaluation substrate.</span> Ours produces per-dimension human rationales and confidence; judge-only pipelines (ALLM/Gemini) provide scores with limited per-sample interpretability&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All human annotation in this work was conducted in accordance with standard ethical practices for research involving human annotators. Annotators were recruited via public hiring for Mandarin and internal lab hiring for English. Prior to participation, all annotators were provided with a clear description of the task, the type of data they would encounter, and their rights as participants, including the ability to opt out at any time without penalty.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "human",
                    "english",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The archetype evaluation provides a detailed view of how speech foundation models (SFMs) perform when tasked with role-play scenarios grounded in stereotypical characters and situations. Human annotations cover four dimensions, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, and <span class=\"ltx_text ltx_font_italic\">Content Pass</span>, and the results reveal important trends, as well as challenges in interpreting scores across languages.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "appropriateness",
                    "evaluation",
                    "human",
                    "results",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A critical observation is that annotations in Mandarin and English cannot be directly compared. The annotation groups for the two languages differed in their linguistic backgrounds and cultural interpretations, which leads to systematic differences in scoring. Our bilingual pilot further confirmed that annotators often interpret role-play appropriateness and expressiveness differently depending on their cultural frame of reference. For example, Mandarin raters tend to reward delivery styles that align with culturally expected prosody and politeness strategies, while English raters prioritize spontaneity and naturalness. These discrepancies parallel the findings of the singing voice conversion challenge (SVCC2023) from <cite class=\"ltx_cite ltx_citemacro_citet\">Huang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib31\" title=\"\">2023</a>)</cite>, where native and non-native listeners provided significantly different ratings for the same samples, particularly for naturalness and speaker similarity. Consequently, while absolute scores differ substantially across Mandarin and English, only within-language comparisons are meaningful. Cross-lingual ranking is not valid given the different annotation bases.</p>\n\n",
                "matched_terms": [
                    "languages",
                    "appropriateness",
                    "english",
                    "annotation",
                    "mandarin",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Within Mandarin from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, the relative rankings of models are clear. <span class=\"ltx_text ltx_font_typewriter\">Doubao</span> stands out as the strongest performer across all four dimensions, reaching near-ceiling levels in both <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> (4.37) and <span class=\"ltx_text ltx_font_italic\">Content Pass</span> (0.98). It also leads in role-related dimensions such as <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, indicating balanced competence across linguistic accuracy, delivery, and expressiveness. <span class=\"ltx_text ltx_font_typewriter\">G-CosyVoice2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-CosyVoice2</span> follow closely, especially in audio quality, though they lag behind in role appropriateness. <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> achieve mid-range results, reflecting higher variance across samples, while <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> shows competitive content understanding but lower naturalness and appropriateness. <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> consistently ranks the lowest, with large variance, underscoring instability and weaker alignment with archetypal role expectations.</p>\n\n",
                "matched_terms": [
                    "appropriateness",
                    "mandarin",
                    "human",
                    "results",
                    "variance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The picture shifts in English, elaborated in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Here, <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> emerges as the most effective model by a wide margin, achieving top scores in both <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> (3.46) and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> (3.49), as well as high content pass rates (0.97). This reflects its strong grounding in English discourse norms and role adherence. In contrast, <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> variants perform well in <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> but are weaker in role alignment, highlighting a trade-off between acoustic fidelity and role-play embodiment. Other models such as <span class=\"ltx_text ltx_font_typewriter\">Doubao</span>, <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span>, and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> achieve moderate results, while <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> trail far behind, showing limited robustness in English scenarios. Taken together, the English results reveal both a sharper separation across models and a wider variance in appropriateness scores compared to Mandarin.</p>\n\n",
                "matched_terms": [
                    "appropriateness",
                    "english",
                    "mandarin",
                    "human",
                    "results",
                    "variance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although cross-lingual comparisons are invalid, this does not diminish the utility of archetype evaluation for role-play benchmarking. In fact, the observed inconsistencies reinforce a central point: successful role-play requires models to adapt to the cultural and linguistic norms of the target language community. A system that performs well in Mandarin but poorly in English cannot be considered universally capable, since end users will evaluate appropriateness and naturalness according to their native expectations. Thus, archetype evaluation highlights not only model-specific strengths and weaknesses but also the need for culturally grounded evaluation pipelines.</p>\n\n",
                "matched_terms": [
                    "appropriateness",
                    "english",
                    "mandarin",
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another notable pattern is the trade-off between acoustic quality and role embodiment. CosyVoice2 variants achieve strong audio quality but weaker human likeness and appropriateness, whereas <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> prioritizes role coherence at the expense of audio fidelity. This suggests that future systems must better integrate high-quality synthesis with nuanced role adherence. Similarly, the consistently lower scores for imaginative and identity-driven roles underscore the limitations of current SFMs in capturing subtle sociocultural expectations and narrative context.</p>\n\n",
                "matched_terms": [
                    "appropriateness",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In sum, archetype evaluation reveals three major findings:\n(1) Mandarin and English annotations cannot be directly compared, due to linguistic and cultural annotation inconsistencies, consistent with prior evidence from SVCC 2023.\n(2) Within each language, clear model rankings emerge, <span class=\"ltx_text ltx_font_typewriter\">Doubao</span> dominates in Mandarin, while <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> excels in English.\n(3) Domain sensitivity remains a challenge, particularly for imaginative or culturally rich roles, where scores are substantially lower and inter-model variance is higher.</p>\n\n",
                "matched_terms": [
                    "intermodel",
                    "english",
                    "annotation",
                    "mandarin",
                    "evaluation",
                    "domain",
                    "archetype",
                    "variance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although realism evaluation targets nuanced paralinguistic behaviors, human annotators achieve <em class=\"ltx_emph ltx_font_italic\">consistently high agreement</em> once the rubric disentangles the aspects to be judged.\nAs shown in the agreement summary in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F7\" title=\"Figure 7 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, most dimensions fall in the <math alttext=\"0.86\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>0.86</mn><annotation encoding=\"application/x-tex\">0.86</annotation></semantics></math>&#8211;<math alttext=\"0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mn>0.88</mn><annotation encoding=\"application/x-tex\">0.88</annotation></semantics></math> range, with the lowest agreement on <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> (<math alttext=\"\\approx 0.83\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.83</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.83</annotation></semantics></math>) and the highest on <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em> / <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> (<math alttext=\"\\approx 0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.88</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.88</annotation></semantics></math>).\nEmotion-related, progressive metrics (Intensity and Transition) are slightly lower (<math alttext=\"\\approx 0.85\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.85</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.85</annotation></semantics></math>), reflecting the added difficulty of judging graded expressivity.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "evaluation",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We then report the overall agreement <math alttext=\"A=1-\\bar{\\sigma}/R\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px2.p1.m8\" intent=\":literal\"><semantics><mrow><mi>A</mi><mo>=</mo><mrow><mn>1</mn><mo>&#8722;</mo><mrow><mover accent=\"true\"><mi>&#963;</mi><mo>&#175;</mo></mover><mo>/</mo><mi>R</mi></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">A=1-\\bar{\\sigma}/R</annotation></semantics></math>, where <math alttext=\"\\bar{\\sigma}\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px2.p1.m9\" intent=\":literal\"><semantics><mover accent=\"true\"><mi>&#963;</mi><mo>&#175;</mo></mover><annotation encoding=\"application/x-tex\">\\bar{\\sigma}</annotation></semantics></math> is the mean of <math alttext=\"\\{\\sigma_{i}\\}\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px2.p1.m10\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">{</mo><msub><mi>&#963;</mi><mi>i</mi></msub><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">\\{\\sigma_{i}\\}</annotation></semantics></math> over samples with at least two annotations.\nWe also clamp <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px2.p1.m11\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> to <math alttext=\"[0,1]\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px2.p1.m12\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[0,1]</annotation></semantics></math> and ignore single-annotator cases.\nA minimal, reproducible implementation is:</p>\n\n",
                "matched_terms": [
                    "mean",
                    "report",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F8\" title=\"Figure 8 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F9\" title=\"Figure 9 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> visualize the score distributions per dimension.\nWe highlight three patterns:\n(i) <span class=\"ltx_text ltx_font_bold\">Progressive/gated emotion metrics.</span> For <em class=\"ltx_emph ltx_font_italic\">Emotion Intensity</em> and <em class=\"ltx_emph ltx_font_italic\">Emotion Transition</em>, we assign <span class=\"ltx_text ltx_font_typewriter\">1</span> when the scene is marked <em class=\"ltx_emph ltx_font_italic\">N/A</em> (i.e., emotion accuracy is insufficient to meaningfully judge intensity/transition). This produces a visible mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, a sparse bin at <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>, and slightly lower overall agreement; downstream analyses conditioning on adequate <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> (e.g., mean&#160;<math alttext=\"\\geq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 3</annotation></semantics></math>) recover more bell-shaped distributions.\n(ii) <span class=\"ltx_text ltx_font_bold\">Traits Embodiment.</span> Scores tend to avoid the middle (<math alttext=\"\\sim\\!3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!3</annotation></semantics></math>), concentrating on low values when traits clearly mismatch and high values when traits are strongly realized. This yields the lowest inter-rater agreement among dimensions, reflecting the inherently compositional and role-specific nature of trait judgments.\n(iii) <span class=\"ltx_text ltx_font_bold\">Other dimensions.</span> <em class=\"ltx_emph ltx_font_italic\">Pitch Variation</em>, <em class=\"ltx_emph ltx_font_italic\">Rhythmic Naturalness</em>, <em class=\"ltx_emph ltx_font_italic\">Stress &amp; Emphasis</em>, <em class=\"ltx_emph ltx_font_italic\">Voice Identity Matching</em>, <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em>, <em class=\"ltx_emph ltx_font_italic\">Global Story Coherence</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> are approximately unimodal with modes at <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>&#8211;<math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math>, consistent with moderate&#8211;strong realization in the dataset and with the high agreement reported.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "mean",
                    "score",
                    "dimension"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For both zeroshot and fewshot experiments, we adopt a streamlined version of the annotation guidelines to ensure consistency with the human annotation process. The exact rubric prompts for archetype and realism evaluation used are provided below:</p>\n\n",
                "matched_terms": [
                    "archetype",
                    "evaluation",
                    "human",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and fewshot settings, we condition the model with the rubric prompt and then append few-shot demonstration triplets (input, audio, gold JSON) to mirror the human annotation protocol. As shown in Listing&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#LST2\" title=\"Listing 2 &#8227; L.1 Zeroshot/Fewshot Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, We use <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> for these few-shot exemplars and require a minified JSON output consistent with the rubric schema.</p>\n\n",
                "matched_terms": [
                    "human",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and few-shot experiments on commercial models such as <span class=\"ltx_text ltx_font_typewriter\">Gemini</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o</span>, we report results as the average of five independent runs to reduce randomness and obtain more stable predictions. In contrast, for open-source models, we observed that their instruction-following ability is relatively weak. To mitigate this, we prompt them to predict one dimension at a time rather than all dimensions jointly. Specifically, for each metric, after showing the rubric prompt, we request completion in a fixed template and extract the probability distribution over the discrete tokens <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>. The final score is then computed from these probabilities, which we found makes open-source models usable to some extent under this setting.</p>\n\n",
                "matched_terms": [
                    "report",
                    "score",
                    "results",
                    "dimension"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For few-shot experiments, we randomly sample three exemplars from the training set, with each exemplar chosen such that its human scores are close to the rounded average across all annotators. These exemplars are appended as demonstrations in the prompt. As with the zeroshot case, we run five trials for commercial models and report the averaged results for evaluation.</p>\n\n",
                "matched_terms": [
                    "report",
                    "evaluation",
                    "human",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Effect of repeated runs.\nWhen comparing single-run decoding (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot-1run</span>) against averaging over five runs (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot</span>), we see consistent gains across nearly all dimensions (average score improves from 0.353 to 0.390). This suggests that sampling variance plays a substantial role in the reliability of large language model (LLM) evaluators, and averaging multiple outputs reduces this variance, producing more stable correlation with human annotations.</p>\n\n",
                "matched_terms": [
                    "human",
                    "score",
                    "variance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In general, the results demonstrate two complementary strategies for stabilizing LLM-based evaluators: (i) averaging multiple runs to reduce randomness, and (ii) enriching prompts with more few-shot exemplars to improve adherence to rubric-based evaluation. Combining both leads to the strongest overall performance, with the nine-shot condition achieving the highest average correlation&#160;(0.469).</p>\n\n",
                "matched_terms": [
                    "results",
                    "evaluation",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the archetype test set, all fine-tuned variants achieve strong <span class=\"ltx_text ltx_font_italic\">Content Pass</span> accuracy above 92%. Differences appear mainly in correlation-based metrics. Here, <span class=\"ltx_text ltx_font_typewriter\">Base-FT</span> yields the highest score on <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> delivers the strongest performance in <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> and the best overall average (0.629). This suggests that instruction-tuned checkpoints paired with parameter-efficient adaptation can better capture higher-level perceptual traits.</p>\n\n",
                "matched_terms": [
                    "appropriateness",
                    "score",
                    "human",
                    "archetype",
                    "traits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For realism evaluation on the basic test set, <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> consistently achieves the strongest correlations across most dimensions, especially for <span class=\"ltx_text ltx_font_italic\">Character Identity</span> and <span class=\"ltx_text ltx_font_italic\">Traits</span>, leading to the best average correlation (0.668). <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> also performs competitively (0.630), while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> lags slightly behind (0.625). These results highlight that full fine-tuning may still offer an advantage when the evaluation set is relatively homogeneous and closer to the training distribution.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "evaluation",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> is competitive on in-domain evaluation, it suffers a substantial drop when applied to real recordings. By contrast, <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> offers a more robust balance: it maintains reasonable performance on archetype and basic sets, while significantly outperforming other settings on real-world data. Based on this robustness, we adopt <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> as our final configuration for subsequent experiments.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the comprehensive design of Speech-DRAME, several limitations remain.\nFirst, while our dual evaluation paradigm combines top-down archetype assessment with bottom-up realism evaluation, both strategies are constrained by the quality and scope of available data. Archetype evaluation inevitably relies on simplified stereotypes, which can overlook subtler paralinguistic and contextual cues. Realism evaluation, though richer, still struggles with domain mismatch between synthetic data and real human recordings, as evidenced by the performance drop observed in the real-recording test set. This highlights the persistent gap between benchmark-driven evaluation and real-world variability in speech role-play.</p>\n\n",
                "matched_terms": [
                    "domain",
                    "evaluation",
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Second, the present framework emphasizes bilingual evaluation (Mandarin and English), but linguistic and cultural diversity is far broader. Cross-lingual generalization and culturally sensitive evaluation are still open challenges, particularly since we observed annotation discrepancies across language groups. This limits the extent to which Speech-DRAME can claim universal coverage.</p>\n\n",
                "matched_terms": [
                    "mandarin",
                    "evaluation",
                    "english",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, although we validated DRAME-Eval against human annotations, alignment under realism settings remains modest. This suggests that even human-annotated benchmarks are insufficiently fine-grained to capture the full complexity of human perception in speech role-play. Richer annotation protocols, more diverse speaker populations, and integration of perceptual psychology insights could further improve evaluation fidelity.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human",
                    "annotation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Looking forward, we see several promising research directions. We intentionally restrict evaluation to single-turn responses for clarity and tractability. However, multi-turn extensions that capture narrative progression, long-term consistency, and dynamic emotion shifts remain a natural next step. Incorporating multilingual and cross-cultural datasets will enhance generality and fairness. On the modeling side, exploring hybrid evaluation strategies that combine discrete rubrics with continuous perceptual signals (e.g., prosody embeddings or affective measures) may better bridge the gap between automatic and human judgments. Finally, integrating Speech-DRAME into reinforcement learning pipelines as a reward model could help drive the next generation of expressive, human-aligned speech role-play systems.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "human"
                ]
            }
        ]
    },
    "A12.T12": {
        "caption": "Table 12: Realism role-play evaluator performance with Gemini2.5Pro in different fewshot conditions. Here, 1run stands for only conduct generation for once.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" rowspan=\"2\">Config.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" colspan=\"3\">Prosodic Dynamics</td>\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" colspan=\"3\">Emotional Expressiveness</td>\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" colspan=\"2\">Character Consistency</td>\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" colspan=\"2\">Contextual Relevance</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\">Avg.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Pitch</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Rynthm</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Stress</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Accuracy</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Intensity</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Transition</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Identity</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Traits</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Local</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Global</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">Zeroshot-1run</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.277</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.325</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.303</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.355</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.286</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.295</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.480</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.475</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.308</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.424</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.353</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Zeroshot</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.319</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.352</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.336</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.391</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.316</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.336</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.529</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.509</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.349</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.465</td>\n<td class=\"ltx_td ltx_align_center\">0.390</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">3-Shot-1run</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.316</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.335</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.323</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.358</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.279</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.294</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.530</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.476</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.335</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.428</td>\n<td class=\"ltx_td ltx_align_center\">0.366</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">3-Shot</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.400</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.450</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.432</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.414</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.344</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.353</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.584</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.505</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.374</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.477</td>\n<td class=\"ltx_td ltx_align_center\">0.433</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">6-Shot</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.419</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.431</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.427</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.455</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.393</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.391</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.606</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.540</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.404</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.519</td>\n<td class=\"ltx_td ltx_align_center\">0.459</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">9-Shot</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.419</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.453</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.434</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.464</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.404</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.416</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.613</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.551</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.407</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.524</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.469</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "roleplay",
            "pitch",
            "identity",
            "consistency",
            "once",
            "prosodic",
            "fewshot",
            "dynamics",
            "config",
            "realism",
            "zeroshot",
            "conduct",
            "zeroshot1run",
            "evaluator",
            "emotional",
            "gemini25pro",
            "character",
            "generation",
            "stands",
            "rynthm",
            "stress",
            "conditions",
            "accuracy",
            "performance",
            "avg",
            "3shot",
            "relevance",
            "intensity",
            "transition",
            "3shot1run",
            "only",
            "traits",
            "here",
            "6shot",
            "global",
            "local",
            "contextual",
            "1run",
            "9shot",
            "expressiveness",
            "different"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.T12\" title=\"Table 12 &#8227; L.2 Zeroshot/Fewshot Ablation Experiments &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> shows how different prompting strategies affect realism evaluation performance with <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>. Several observations can be drawn:</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Role-play has become a key testbed for generative models, expanding from text-only dialogue to multimodal interaction. Extending role-play to speech captures prosody, emotion, and delivery, but also poses new evaluation challenges. Current pipelines often use audio large language models (ALLMs) as zero-shot judges, which miss paralinguistic cues, collapse multiple aspects into coarse scores, and rely on synthetic speech references that fail to reflect real-world roles. We present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework that contributes at three levels: (i) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-EvalBench</em>, an evaluation benchmark with bilingual human-annotated data and protocols for training and testing speech evaluation models&#160;(SEMs), (ii) <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, a fine-tuned evaluation model, which substantially outperforms zero-shot and few-shot ALLMs, and (iii) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-RoleBench</em>, a speech role-play benchmark that leverages <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> as an automatic judge to compare speech foundation models (SFMs). Speech-DRAME distinguishes between two complementary evaluation strategies: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>, a top-down approach measuring adherence to broad role archetypes, and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em>, a bottom-up approach grounded in real human speech that emphasizes nuanced role quality. Compared to zero-shot ALLM judges, <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves stronger agreement with human ratings (Pearson correlation from 0.480 to 0.629 in archetypes, and 0.390 to 0.625 in realism). By integrating transparent benchmark resources, modeling approaches, and system-level evaluation, Speech-DRAME provides the first comprehensive, reproducible foundation for assessing spoken role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "zeroshot",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Role-play has been recently recognized a cornerstone of interactive systems, from initial text-based focuses to the growing ecosystem of multimodal dialogue agents&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib6\" title=\"\">2024a</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib59\" title=\"\">2024</a>; Dai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib13\" title=\"\">2025</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib43\" title=\"\">2024</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>)</cite>. With the rapid advancement of large language models (LLMs), role-play has evolved beyond scripted interactions into dynamic character-driven conversations, supporting applications in education, entertainment, and human&#8211;AI collaboration&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib69\" title=\"\">2024</a>; Agatsuma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib1\" title=\"\">2024</a>; Fung &amp; Laing, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib20\" title=\"\">2024</a>)</cite>. Extending role-play to speech-based interaction offers an even richer communication channel, capturing prosody, emotion, and character consistency in ways that text alone cannot&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "consistency",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Evaluating speech role-play, however, remains a major challenge. Current pipelines often rely on ALLMs as judges, typically in a zero-shot setting&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>; Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>; Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>)</cite>. While this strategy can provide rough ratings, it falls short for speech role-play:</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">it lacks sensitivity to paralinguistic cues such as intonation, pacing, and emotional dynamics that define role delivery&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>; Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib81\" title=\"\">2025a</a>)</cite>,</p>\n\n",
                "matched_terms": [
                    "dynamics",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">it collapses diverse aspects of performance into limited coarse dimensions, obscuring critical dimensions of role-play quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite>, and</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome these limitations, we present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span> (Speech Detailed Role-play Assessment with Modeling and Evaluation), a unified framework that moves beyond synthetic or judge-only pipelines. Speech-DRAME is distinguished by three aspects: [i] a dual evaluation paradigm that balances archetype-driven generality with human-grounded realism (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS2\" title=\"3.2 Role-play Evaluation with Two Strategies &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), [ii] human annotations that provide perceptual grounding absent in prior resources (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4\" title=\"4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), and [iii] evaluation-model alignment that enables not only benchmarking of speech foundation models (SFMs) but also systematic analysis of the evaluation models (SEMs) themselves (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5\" title=\"5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). These aspects establish Speech-DRAME as a comprehensive and trustworthy framework for measuring and improving speech role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "only",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, Speech-DRAME contributes at three levels: (i) <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>, an evaluation benchmark consisting of human-annotated datasets for archetype and realism role-play; (ii) <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span>, a fine-tuned SEM trained on EvalBench that surpasses zero-shot and few-shot ALLMs; and (iii) <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span>, a speech role-play benchmark that leverages DRAME-Eval as an automatic judge to systematically compare SFMs. By combining dual evaluation strategies, human-anchored supervision, and evaluation-model alignment, Speech-DRAME delivers systematic insights into both role-play systems and evaluation models themselves.\nOur contributions are four-fold:</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "zeroshot",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a <span class=\"ltx_text ltx_font_bold\">formal task definition</span> of speech role-play and its evaluation via speech foundation models (SFMs) for generation and speech evaluation models (SEMs) for assessment, unifying linguistic and paralinguistic dimensions.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span>, a fine-tuned SEM trained on EvalBench that substantially outperforms zero-shot and few-shot ALLMs, demonstrating the importance of human supervision for evaluation models.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-based role-play evaluation.</span>\nEarly research focused on text-only dialogue benchmarks. Datasets such as RoleLLM, CharacterBox, and RoleMRC <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib62\" title=\"\">2024</a>; Tu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib60\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib61\" title=\"\">2025a</a>; Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib53\" title=\"\">2023</a>; Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib41\" title=\"\">2025</a>)</cite> test whether models can sustain persona consistency and character fidelity, using LLM-as-judge pipelines for scalability. However, studies like PersonaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib81\" title=\"\">2025a</a>)</cite> have shown that LLM-as-judge approaches are far from perfect, struggling with tasks as basic as tracking speaker identity. This highlights broader concerns about reliability even in text settings, which become only more severe when extended to speech, where prosody, emotion, and vocal style play a central role.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "identity",
                    "consistency",
                    "character",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-native benchmarks.</span>\nMore recently, a number of resources have been introduced to evaluate spoken role-play agents. OmniCharacter-10K <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>)</cite> emphasizes character voice traits, while others&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib39\" title=\"\">2025</a>; Hou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib27\" title=\"\">2025</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib15\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib7\" title=\"\">2024b</a>)</cite> extend evaluation toward vocal communication, assistant capability, and multi-turn realism. Omni models such as StepAudio and KimiAudio <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite> also include role-play scenarios in their evaluation. SpeechRole and VStyle <cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>; Zhan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib76\" title=\"\">2025</a>)</cite> are closer to our work, offering datasets that test interaction ability, expressiveness, and fidelity. However, these resources are primarily built from LLM-generated dialogues and TTS outputs, with limited human validation and a heavy reliance on proprietary ALLM-as-judge pipelines. As a result, they lack grounding in diverse human speech and typically apply only coarse rubrics.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "character",
                    "expressiveness",
                    "traits",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Automatic judging with ALLMs.</span>\nIn parallel, advances in automatic judging highlight both opportunities and limitations. Audio-capable LLMs such as AudioJudge and Audio-Aware LLMs as Judges <cite class=\"ltx_cite ltx_citemacro_citep\">(Manakul et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib44\" title=\"\">2025</a>; Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>)</cite> can assess style, emotion, and delivery, while systems like Auto-ATT <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib65\" title=\"\">2025c</a>)</cite> scale this to Audio Turing Tests. Yet studies consistently reveal high prompt sensitivity and shallow capture of paralinguistic cues, suggesting that zero-shot judging remains insufficient for nuanced role-play evaluation.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Expressive TTS evaluation.</span>\nRecent work has explored evaluation of instruction-following and style-controlled TTS. InstructTTSEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite> explicitly includes role-play scenarios, while EmergentTTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">(Manku et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib45\" title=\"\">2025</a>)</cite> and discrete speech LM benchmarks <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang &amp; Sz&#233;kely, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib63\" title=\"\">2024</a>)</cite> emphasize prosody, spontaneity, and speaker consistency. These efforts push beyond intelligibility and naturalness, but still center on synthetic outputs rather than real human role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "consistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our distinction.</span>\nIn contrast, Speech-DRAME unifies these considerations through three elements: (i) a dual evaluation paradigm that combines archetype-driven generality with realism grounded in authentic human speech, (ii) human annotations that provide a reliable perceptual gold standard, and (iii) alignment of SEMs to human ratings, enabling systematic evaluation of both SFMs and the evaluation models themselves. This combination moves beyond synthetic benchmarks or zero-shot judging to provide a comprehensive and trustworthy framework for speech role-play assessment.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech role-play.</span>\nWe formalize <em class=\"ltx_emph ltx_font_italic\">speech role-play</em> as a conditional speech generation problem grounded in a character profile and scene description.\nGiven a dialogue context <math alttext=\"{\\bm{C}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{speech}}</annotation></semantics></math>, a character profile <math alttext=\"{\\bm{C}}_{\\text{profile}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>profile</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{profile}}</annotation></semantics></math>, and a scene specification <math alttext=\"{\\bm{C}}_{\\text{scene}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>scene</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{scene}}</annotation></semantics></math>, a <em class=\"ltx_emph ltx_font_italic\">speech foundation model</em> (SFM) generates a role-played speech response <math alttext=\"{\\bm{S}}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119930;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{S}}_{r}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "generation",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>). The first strategy follows a <em class=\"ltx_emph ltx_font_italic\">top-down</em> design, inspired by prior text-based role-play benchmarks.\nHere, role-play is judged with respect to <em class=\"ltx_emph ltx_font_italic\">stereotypical archetypes</em> (e.g., &#8220;firefighter&#8221; or &#8220;ER doctor&#8221;), using scene contexts (e.g., &#8220;comfort a child trapped in fire&#8221; or &#8220;ask for help in a serious operation&#8221;).\nThis approach provides accessible, general-purpose scoring that is scalable to a wide range of scenarios, making it suitable for large-scale benchmarking.\nHowever, its reliance on broad stereotypes and global impressions limits its ability to capture fine-grained aspects of delivery (e.g., subtle prosodic shifts, nuanced emotional control)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Holt &amp; Lotto, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib26\" title=\"\">2010</a>)</cite>.\nIn practice, this strategy mirrors simplified annotation settings and is particularly useful when annotation resources are limited&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib21\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "roleplay",
                    "emotional",
                    "prosodic",
                    "here"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS5\" title=\"F.5 Examples of Realism Role-play &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.5</span></a>). The second strategy adopts a <em class=\"ltx_emph ltx_font_italic\">bottom-up</em> design, motivated by real-world performance and real human speech.\nInstead of relying solely on stereotypes, this approach grounds evaluation in recordings from professional and non-professional speakers, drawn from realistic dialogue and media sources.\nJudgments in this setting are more fine-grained and multi-dimensional, addressing aspects such as prosodic dynamics, emotional fidelity, character consistency, and contextual fit.\nWhile this strategy requires more human annotation and careful curation, it provides benchmarks that reflect how role-play is perceived in practice.\nIt thus offers a richer framework for analyzing model outputs in settings where realism and subtlety are essential.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "contextual",
                    "consistency",
                    "emotional",
                    "character",
                    "prosodic",
                    "dynamics",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Archetype Data Collection &#8227; 4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents a summary of the curated <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> dataset, where we further elaborate the details of archetype and realism evaluation data collection in following subsections, respectively. For benchmark purpose, we provide an official train-test split for the human annotated dataset for <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> so that following researchers can use the dataset to train and evaluate their SEMs for speech role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Sources.</span> The data is collected from a range of speech foundation models, including both cascaded and end-to-end models. We limit that only audio prompts are used as the input but adopt different prompt strategies for best quality with regards to different models. Details about the selected models and prompting details are discussed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS4\" title=\"D.4 Candidate Models for Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "only",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype Evaluation Data Summary.</span>\nThe resulting archetype evaluation dataset comprises 552 distinct scenarios for both Mandarin and English, evaluated across eight models,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>The model list is discussed in Appdenix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS4\" title=\"D.4 Candidate Models for Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.4</span></a>. Note we only include seven models for English due to the deactivation of ChatGPT-4O advanced voice model.</span></span></span> yielding 8,280 samples. In total, the dataset are split into 6,780 for training and 1,500 for testing. For benchmarking, an additional 1,250 scenarios spanning seven role/scene categories are prepared to facilitate archetype role-play benchmarking. More detailed statistics and analyses are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10\" title=\"Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">J</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realism evaluation adopts a bottom&#8211;up perspective, emphasizing authentic delivery, prosody, and contextual fit.</p>\n\n",
                "matched_terms": [
                    "contextual",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Sources and Negative Samples.</span>\nWe curate speech from (i) ground-truth media segments, (ii) retrieved character profiles, and (iii) generated local scene descriptions. To stress-test evaluation models, we construct contrastive mismatches: (i) negative local scenes with varying degrees of mismatch, and (ii) negative character profiles created by re-synthesizing transcripts with TTS systems. Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6\" title=\"Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F</span></a> details the prompt design and TTS generation details.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "local",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotators judge whether performances align with the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em>, using rubrics for <em class=\"ltx_emph ltx_font_italic\">Prosodic Dynamics (i.e., Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis)</em>, <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness (with progressive gating over three sub-dimensions: Emotion Accuracy, Emotion Intensity, Emotion Transition)</em>, <em class=\"ltx_emph ltx_font_italic\">Character Consistency (i.e., Voice Identity Matching and Traits Embodiment)</em>, <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance (i.e., Local Scene Fit and Global Story Coherence)</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>. A prompt quality check and confidence ratings are also collected. Full scales and gating details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "pitch",
                    "identity",
                    "relevance",
                    "local",
                    "consistency",
                    "emotional",
                    "contextual",
                    "character",
                    "accuracy",
                    "prosodic",
                    "intensity",
                    "transition",
                    "stress",
                    "expressiveness",
                    "traits",
                    "dynamics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real-world Recoding.</span>\nAs noted in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S2\" title=\"2 Related Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, most prior efforts have relied heavily on synthetic data, mostly from TTS systems based on curated professional role-play voices. While such data can achieve high fidelity, it often lacks the diversity and spontaneity of real-world delivery, limiting the generalizability of evaluation models. To address this gap, we expand realism evaluation beyond synthetic and curated voices by incorporating newly recorded speech from both professional and amateur voice actors. This design introduces a broader spectrum of delivery styles, from polished professional acting to more natural and less controlled amateur performance.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation Data Summary.</span>\nThe resulting realism evaluation dataset is constructed from a combination of curated media sources, including internal collections, public datasets (e.g., NCSSD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib40\" title=\"\">2024b</a>)</cite>), and original recordings collected for this project.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>All data were obtained under appropriate licenses and are legally available for research release.</span></span></span> We compile 15k annotated role-play samples: 9k in Mandarin and 5k in English. The dataset is divided into 12k samples for training, 2k mixed samples for the base test set, and 1k real-recording samples for the test set. The first test set supports general evaluation using our proposed pipeline with negative samples, while the second focuses exclusively on human speech performed by both professional and amateur voice actors. For benchmarking, 2,000 scenarios from the realism dataset are designated for realism role-play benchmarking. Detailed statistics and analyses are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11\" title=\"Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">K</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Building on the two proposed datasets, we conduct experiments under three settings: zero-shot, few-shot, and finetuning, using pre-trained ALLMs. We formulate the task as a multi-metric estimation problem&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Tjandra et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib58\" title=\"\">2025</a>; Yao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib74\" title=\"\">2025</a>; Shi et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib55\" title=\"\">2025b</a>; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib54\" title=\"\">a</a>)</cite>, where a single model is trained or prompted to predict multiple evaluation dimensions.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "conduct",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-shot and Few-shot ALLMs.</span>\nWe first evaluate the zero-shot capabilities of both proprietary and public models, including <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>, <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>, <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio</span>, and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>. All evaluations follow a unified prompting template that provides audio input alongside scene and character context.</p>\n\n",
                "matched_terms": [
                    "character",
                    "zeroshot",
                    "gemini25pro",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Finetuning Model: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>.</span>\nIn addition to zero-shot and few-shot evaluations, we fine-tune a dedicated evaluation model, denoted as <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> for archetype and realism role-play evaluation, respectively.\nWe initialize from <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio-7B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib11\" title=\"\">2024</a>)</cite>, which provides a strong foundation for multi-modal audio-text reasoning.\nTo better leverage the multiple human annotations available in our datasets, we adopt a sampling-based training strategy: at each step, one annotator label is randomly selected, following practices in label distribution learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Geng, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib22\" title=\"\">2016</a>; Wen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib67\" title=\"\">2023</a>)</cite>.\nThis stochastic supervision implicitly exposes the model to the empirical annotation distribution, while during inference we output the full distribution over discrete labels (e.g., <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score, consistent with recent decoding-based regression methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Song &amp; Bahri, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>; Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib70\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib64\" title=\"\">2025b</a>; Lukasik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>. We apply LoRA fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib28\" title=\"\">2022</a>)</cite>. All implementation details, including optimizer settings, training schedule, and sampling policy, are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS3\" title=\"L.3 Fine-tuning Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "zeroshot",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further conduct ablation studies in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS4\" title=\"L.4 Fine-tuning Ablation Experiments &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.4</span></a>, examining: the choice of different base audio LLMs and full-parameter fine-tuning versus LoRA adaptation.\nThese analyses highlight the robustness of our sampling&#8211;expectation training strategy and clarify the trade-offs in model design.</p>\n\n",
                "matched_terms": [
                    "different",
                    "conduct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics for Role-play Evaluation Model.</span> For most dimensions with 1-5 scale, we report Pearson correlation. For <span class=\"ltx_text ltx_font_italic\">Content Pass</span> estimation, we report accuracy.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype Evaluation.</span>\nThe results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T2\" title=\"Table 2 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reveal clear distinctions across evaluation settings. In the zero-shot condition, proprietary ALLMs such as <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> substantially outperform public models, achieving strong <span class=\"ltx_text ltx_font_italic\">content pass</span> accuracy (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>90%) and moderate correlations with other dimensions(<math alttext=\"0.43\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mn>0.43</mn><annotation encoding=\"application/x-tex\">0.43</annotation></semantics></math>&#8211;<math alttext=\"0.48\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mn>0.48</mn><annotation encoding=\"application/x-tex\">0.48</annotation></semantics></math>). By contrast, public models exhibit limited predictive ability, with correlations often below 0.2. Few-shot prompting provides modest gains for proprietary models, improving both accuracy and correlation (e.g., <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> rises from 0.480 to 0.493 average correlation). The most significant improvement arises from finetuning: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves an average correlation of 0.629, marking a large margin over zero-shot/few-shot models and demonstrating the effectiveness of task-specific supervision.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "zeroshot",
                    "gemini25pro",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T3\" title=\"Table 3 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> extends the analysis to fine-grained realism evaluation. Similar to archetype results, proprietary ALLMs lead the zero-shot setting, with <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> obtaining an average correlation of 0.390, compared to 0.284 for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>. Public models remain weak baselines. Few-shot prompting consistently improves performance: <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> gains +0.042 and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> gains +0.083 over their zero-shot averages, showing the benefit of in-context supervision. Once again, finetuning is most effective: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves 0.625 average correlation, higher than the best few-shot results. Importantly, the gains are consistent across all 10 realism dimensions, confirming that the sampling&#8211;expectation training strategy generalizes beyond simple classification to nuanced paralinguistic cues.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "zeroshot",
                    "once",
                    "gemini25pro",
                    "fewshot",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real Recording Test Set.</span>\nThe real-recording evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T4\" title=\"Table 4 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> is notably more challenging, with all models suffering substantial performance drops. Proprietary models in the zero-shot setting achieve only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.04&#8211;0.08 average correlation, and public models often fall below zero. Few-shot prompting yields small but noticeable improvements, especially for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> (0.226 avg.), suggesting that exemplars partially mitigate domain mismatch between synthetic and real human speech. Interestingly, while finetuning achieves the highest overall average (0.247), its relative advantage over few-shot prompting is narrower than in the synthetic setting, and some dimensions (e.g., <span class=\"ltx_text ltx_font_italic\">Character Identity</span>) remain weak. We emphasize that this observed gap between synthetic and real recordings is not merely an artifact but an intended feature of Speech-DRAME, designed to expose the mismatch and drive future research into bridging synthetic-to-real generalization.</p>\n\n",
                "matched_terms": [
                    "avg",
                    "zeroshot",
                    "identity",
                    "character",
                    "fewshot",
                    "only",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall.</span> Across both archetype and realism evaluations, we observe three consistent trends: (i) zero-shot performance is limited, particularly for open-source models, (ii) few-shot prompting offers moderate but consistent gains, especially for proprietary ALLMs, and (iii) finetuning with task-specific supervision (<span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>) delivers the largest improvements, achieving state-of-the-art correlations across nearly all dimensions. Nevertheless, results on the real-recording test set underscore the gap between synthetic and real human role-play scenarios, motivating future work in data diversification and domain adaptation.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "zeroshot",
                    "fewshot",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype evaluation.</span>\nOn archetype role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T5\" title=\"Table 5 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), cascaded pipelines consistently outperform end-to-end systems. The strongest cascaded systems, particularly <span class=\"ltx_text ltx_font_typewriter\">G-IndexTTS2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-IndexTTS2</span>, achieve both high content pass rates (above 94%) and balanced scores across audio quality, human likeness, and appropriateness. By contrast, end-to-end models such as <span class=\"ltx_text ltx_font_typewriter\">StepAudio2Mini</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> perform competitively in terms of content coverage (above 88%), but their expressive scores remain slightly lower than their cascaded counterparts. This gap suggests that separating reasoning (LLM) from speech rendering (TTS) remains advantageous when archetype fidelity is emphasized. Nevertheless, the fact that top end-to-end systems already surpass 3.0 in average rating indicates rapid closing of the performance gap.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism evaluation.</span>\nRealism role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T6\" title=\"Table 6 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) presents a different landscape. Here, performance differences between end-to-end and cascaded systems are less pronounced. Several end-to-end models, such as <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>, reach average scores of 3.24, matching or even exceeding the best cascaded pipelines. Importantly, realism scores are more evenly distributed across prosodic, emotional, character, and contextual dimensions, with most models clustering between 2.9&#8211;3.2. This convergence reflects the difficulty of realism-style evaluation: since the data preparation emphasized contrast with authentic human speech, models are judged under similar conditions, leading to smaller observed gaps.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "contextual",
                    "emotional",
                    "character",
                    "prosodic",
                    "conditions",
                    "different",
                    "here",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human alignment.</span>\nWe further conducted small-scale human studies with 10% of the benchmark data to verify the reliability of the benchmark. Archetype evaluation achieves a Spearman correlation of 0.706 between human judgments and <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, validating that the automatic evaluator closely tracks human preferences in this setting. Realism evaluation, however, achieves a more modest Spearman correlation of 0.375. This lower alignment is expected, since realism data construction did not prioritize fine-grained model separation but instead emphasized distinguishing human versus synthetic speech.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>We view this as evidence that realism presents a harder, less-saturated frontier. Rather than a limitation of DRAME-Eval, this highlights the challenge of designing evaluators that capture nuanced human perception, which motivates future work.</span></span></span> As a result, all models occupy a relatively narrow performance band, reducing sensitivity to model-level differences. We view this as evidence that realism evaluation highlights a harder, less saturated frontier where both SFMs and SEMs require further advances.</p>\n\n",
                "matched_terms": [
                    "evaluator",
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, cascaded pipelines retain an edge in archetype scenarios, especially in content robustness and expressive fidelity, but end-to-end models are catching up quickly. In realism scenarios, the field remains unsolved: existing systems exhibit similar performance bands and evaluators show weaker alignment with human judgments, underscoring the need for improved realism-grounded data and evaluation. Together, these findings establish <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> as both a reliable testbed for archetype fidelity and a challenging benchmark for realism fidelity.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework for assessing spoken role-play through two complementary lenses: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em> (top-down) and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> (bottom-up, human-grounded). Our human-annotated <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> enables training and testing of SEMs, and our fine-tuned <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span> substantially outperforms zero-/few-shot ALLM judges. Building on this, <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> provides system-level comparison across various SFMs, revealing a consistent edge for cascaded pipelines on archetypes and a tighter spread under realism, where real human performance evaluation remain challenging. Together, these resources establish a reproducible, analyzable foundation for spoken role-play evaluation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>Limitations and future works are discussed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A13\" title=\"Appendix M Limitations and Future Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">M</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data generation:</span> LLMs were employed to expand prompts (e.g., character profiles, scene descriptions) and to produce negative samples in realism evaluation. All outputs were screened by human annotators to ensure validity and reduce bias.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "realism",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline evaluators:</span> State-of-the-art audio LLMs (e.g., <span class=\"ltx_text ltx_font_typewriter\">GPT4o</span>, <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>) were included as zero-shot judges in our evaluation benchmark, enabling direct comparison with human-aligned SEMs.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our distinction between <span class=\"ltx_text ltx_font_italic\">Archetype Evaluation</span> and <span class=\"ltx_text ltx_font_italic\">Realism Evaluation</span> is not merely pragmatic, but is grounded in established theories of social identity, communication, and performance. The two perspectives capture complementary dimensions of how humans perceive and evaluate roles in interaction.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetypes as Collective Schemas</span>\nThe notion of archetypes originates from Jungian psychology, where they represent recurrent, culturally embedded character prototypes in the collective unconscious&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jung, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib36\" title=\"\">2014</a>)</cite>. In media and performance studies, archetypes have been shown to guide audience expectations, providing top-down interpretive frames for evaluating whether a character &#8220;fits&#8221; within familiar categories such as hero, mentor, or trickster&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Campbell, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib4\" title=\"\">2008</a>)</cite>. Archetype-based reasoning allows for scalable, general-purpose judgments because it leverages culturally shared schemas rather than idiosyncratic details.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism as Situated Performance</span>\nIn contrast, realism highlights the fine-grained enactment of roles in situated interaction. Goffman&#8217;s dramaturgical sociology emphasizes that everyday social life is itself a performance, where believability depends on subtle paralinguistic and pragmatic cues such as prosody, hesitation, and emotional shading&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goffman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib23\" title=\"\">1949</a>)</cite>. Similarly, interactional sociolinguistics has demonstrated how micro-level features like intonation, pacing, and turn-taking patterns shape the perception of authenticity in communication&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gumperz, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib25\" title=\"\">1982</a>)</cite>. Realism Evaluation thus anchors role-play assessment in bottom-up sensitivity to authentic delivery, beyond schematic expectations.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "emotional",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Complementarity of the Two Perspectives</span>\nPerformance theory provides a unifying lens for these dual dimensions. Schechner&#8217;s view of performance as both &#8220;restored behavior&#8221; (scripted, repeatable structures) and &#8220;living process&#8221; (situated enactment)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Schechner, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib52\" title=\"\">2017</a>)</cite> underscores the need to consider both archetypal scripts and enacted realism. Likewise, sociolinguistic theories of identity recognize both <span class=\"ltx_text ltx_font_italic\">ascribed categories</span> (archetypes imposed by social convention) and <span class=\"ltx_text ltx_font_italic\">performed identities</span> (emergent in interaction)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bucholtz &amp; Hall, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib2\" title=\"\">2005</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p4.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> may be a simple mean, a recency-weighted average, or a learned temporal model that captures consistency across turns.\nThe choice of <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p4.m3\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> reflects the intended evaluation focus, e.g., stability of persona traits, adaptation to scene progression, or coherence across multiple speakers.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "consistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Discussion.</span>\nWhile our released dataset and experiments concentrate on single-turn evaluation, the modular nature of the formulation ensures that extending to multi-turn is straightforward.\nFuture work can leverage <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p5.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> to study richer aspects of speech role-play, such as persona persistence, interaction dynamics, and role consistency across extended dialogues.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dynamics",
                    "consistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Role dimensions.</span>\nWe operationalize archetypes through five principal categories, each representing a stereotypical dimension of role identity or behavior, including (1) <span class=\"ltx_text ltx_font_bold\">Occupation / Functional Role (Modern Daily Life)</span> (e.g., a firefighter rescuing a trapped civilian). (2) <span class=\"ltx_text ltx_font_bold\">Occupation / Functional Role (Fictional or Fantasy)</span> (e.g., a martial arts cult leader confronting orthodox rivals). (3)&#160;<span class=\"ltx_text ltx_font_bold\">Social Identity</span> (e.g., a doting parent worrying about a child, or a &#8220;Buddhist youth&#8221; eschewing competition). (4) <span class=\"ltx_text ltx_font_bold\">Named / Specific Characters</span> (e.g., Sun Wukong mocking celestial authorities in <em class=\"ltx_emph ltx_font_italic\">Journey to the West</em>). (5) <span class=\"ltx_text ltx_font_bold\">Personality Traits</span> (e.g., &#8220;hot-blooded&#8221; in a competitive setting, or &#8220;complaining&#8221; in response to poor service). (6) <span class=\"ltx_text ltx_font_bold\">Relative Roles</span> (e.g., parent or stranger).</p>\n\n",
                "matched_terms": [
                    "identity",
                    "traits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scene dimensions.</span>\nIn addition to roles, we incorporate contextual triggers that anchor role-play in stereotypical situations.\nA central focus is on <span class=\"ltx_text ltx_font_bold\">everyday external events or state changes</span> (e.g., receiving a promotion, being caught in gossip), which provide concrete scenarios for testing archetypical responses.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "contextual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Exclusion of fine-grained controls.</span>\nWe deliberately excluded explicit emotional states, prosodic instructions, or linguistic constraints.\nThese dimensions are better suited to realism-oriented or style-controlled evaluation, whereas archetype evaluation benefits from concrete, stereotype-level scenarios that sharpen judgments and improve human agreement.</p>\n\n",
                "matched_terms": [
                    "prosodic",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Pipeline overview.</span>\nAt a high level, archetype evaluation data are generated through a three-stage pipeline:\n(1) selecting keywords that specify role and scene;\n(2) expanding them into full prompts via structured templates; and\n(3) rendering prompts into speech with contemporary TTS systems.\nThis modular pipeline ensures both scalability and reproducibility, while keeping prompts vivid enough for reliable judgments. (4) Based on the generated data (paired scene and synthesized speech), expert human annotators are recruited for detailed annotation on semantic check (i.e., <span class=\"ltx_text ltx_font_italic\">Content Pass</span>) and three different spoken role-play dimensions (i.e., <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human-likeness</span>, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>).\nImplementation details, including template design, alternative prompting strategies, and annotation guidelines, are provided in the following subsections of this Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, archetype evaluation emphasizes breadth and recognizability over subtlety.\nBy grounding role-play in socially recognizable categories and stereotypical situations, it provides a practical balance between scalability and interpretability, complementing the more fine-grained realism evaluation described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS2\" title=\"3.2 Role-play Evaluation with Two Strategies &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Excluded Dimensions.</span> We initially considered more fine-grained control dimensions, but excluded them to preserve the archetype-level focus. Examples of excluded dimensions include:\n<em class=\"ltx_emph ltx_font_italic\">Emotional State &amp; Expression, Single Emotion Prompts, Explicit User Input / Assistant Output structures, Emotional Transition / Fluctuation, Voice Style &amp; Prosody (pacing, intonation, vocal timbre, expressive style, accent/dialect), and Content / Interaction Patterns (task/topic controls, specific linguistic structures, or speech production challenges).</em></p>\n\n",
                "matched_terms": [
                    "transition",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Cascaded Model Generation.</span>\nFor cascaded systems, we only use Cosyvoice2 as the TTS model in our collection of data. We select <span class=\"ltx_text ltx_font_typewriter\">Qwen3-30B-Instruct</span> and <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> as the text LLM candidates&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Yang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib73\" title=\"\">2025</a>; Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib12\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "only",
                    "gemini25pro"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the generation, we adopt a retrieval-augmented generation&#160;(RAG) strategy that combines text LLMs with zero-shot TTS models:</p>\n\n",
                "matched_terms": [
                    "generation",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">30-second focus:</span> Assessment should focus on the first 30 seconds of the generation ONLY. Do not consider any content past 30s; disregard it entirely from your assessment and rubric ratings. (You may stop listening beyond 30s.)</p>\n\n",
                "matched_terms": [
                    "generation",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Do <em class=\"ltx_emph ltx_font_italic\">not</em> critique word choice or logic; past the initial rejection step, only the <em class=\"ltx_emph ltx_font_italic\">performance</em> matters.</p>\n\n",
                "matched_terms": [
                    "only",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realism Evaluation adopts a bottom&#8211;up perspective grounded in real human speech data, with the goal of capturing the nuanced delivery, prosody, and contextual fit that synthetic archetype prompts cannot fully represent.\nRather than relying on stereotypes, this strategy leverages real-world recordings and carefully constructed contrasts, enabling fine-grained assessment of model performance under realistic conditions.</p>\n\n",
                "matched_terms": [
                    "conditions",
                    "contextual",
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generated local scenes.</span>\nWe supplement retrieved character information with automatically generated local scene descriptions summarizing the surrounding plot or setting.\nAppendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS3\" title=\"F.3 Prompts for Local Scene Generation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.3</span></a> includes representative prompts and generation details.</p>\n\n",
                "matched_terms": [
                    "generation",
                    "local",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative local scenes.</span>\nFor each original segment, we generate additional scene descriptions at different levels of mismatch, ranging from subtle alterations to fully unrelated scenarios.\nAppendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS4\" title=\"F.4 Prompts for Negative Scenes &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.4</span></a> outlines the prompting strategies and example outputs.</p>\n\n",
                "matched_terms": [
                    "different",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, real human ground-truth segments and systematically constructed negatives form a balanced dataset that supports realism-oriented evaluation.\nThe positives provide natural anchors of role-play performance, while the negatives introduce controlled mismatches that test whether evaluation models can distinguish real human speech from synthetic approximations.\nThis design ensures that realism evaluation emphasizes subtle paralinguistic and contextual cues, moving beyond intelligibility or content fidelity.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "contextual",
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During annotation, raters are presented with either: (i) ground-truth stories, (ii) stories with negative local scenes, or (iii) stories with negative character profile. Annotators assign realism scores without being exposed to the underlying mismatch reasons. Instead, these reasons are provided only to the quality-assurance team, who use them as reference for internal validation and consistency checks. While the mismatch reasons may not be perfect, they help streamline quality control procedures and maintain reliability across the dataset.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "local",
                    "consistency",
                    "character",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To make the construction of the realism evaluation task concrete, we present a worked example that begins with a curated local scene and character profile, followed by the same transcript adapted into mismatched contexts. These mismatches appear either at the <em class=\"ltx_emph ltx_font_italic\">local-scene</em> level or at both the <em class=\"ltx_emph ltx_font_italic\">character profile and scene</em> level, thereby providing controlled degradations of realism.</p>\n\n",
                "matched_terms": [
                    "local",
                    "realism",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the character Yunjin from <span class=\"ltx_text ltx_font_italic\">Genshin Impact</span> as a running case study, which is also included in our released dataset.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>As acknowledged, the use of real-world recordings is conducted under legal and ethical review. All curated data will be released as part of our open-source package.</span></span></span> This example illustrates how alignment between a speaker&#8217;s persona, local context, and delivery underpins realism, and how systematically breaking that alignment produces contrastive test cases.</p>\n\n",
                "matched_terms": [
                    "local",
                    "realism",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Curated Local Scene.</span>\nYunjin is narrating a fierce battle through operatic singing. Her verses depict a hero charging into enemy lines, slaying the enemy commander, and securing peace. The atmosphere is filled with tension and intensity, highlighting the hero&#8217;s extraordinary bravery and determination.</p>\n\n",
                "matched_terms": [
                    "local",
                    "intensity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative Local Scenes.</span>\nTo stress realism evaluation, we insert the same transcript into alternative backgrounds. These reveal how context-shifted delivery degrades narrative coherence:</p>\n\n",
                "matched_terms": [
                    "realism",
                    "stress",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative Character Profiles.</span>\nWe replace Yunjin with new character profiles and scenes. Although the transcript remains the same, realism degrades because the delivery no longer matches the speaker&#8217;s persona or situation.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Yunjin case study illustrates a fundamental difference between realism evaluation and archetype evaluation. Archetype prompts, as described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>, are constructed by pairing abstract role categories with synthetic contextual scenes. These cover a broad space of possible situations, ranging from modern occupations (e.g., firefighters) to fictional personas (e.g., cult leaders in martial arts fantasy) or social identities (e.g., overprotective parents, laid-back friends). While such prompts provide controlled and diverse coverage, they are ultimately template-based and rely on stereotypes to elicit role-play behaviors.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "contextual",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the Yunjin example is drawn from authentic performance, where the character&#8217;s persona (an opera singer) and local scene (a battlefield narration) are naturally aligned through artistic delivery. The realism task does not merely test whether a model can follow a prompt, but whether it can preserve coherence between a speaker&#8217;s global identity, immediate context, and expressive prosody. When Yunjin&#8217;s transcript is transplanted into mismatched settings or paired with incongruent speaker profiles, the breakdown in realism becomes apparent: the same words shift from inspiring to awkward, jarring, or even absurd.</p>\n\n",
                "matched_terms": [
                    "global",
                    "realism",
                    "local",
                    "identity",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This distinction highlights the complementary strengths of the two evaluation paradigms. Archetype evaluation systematically explores a wide combinatorial space of roles and situations, but may lack the paralinguistic and contextual richness of real speech. Realism evaluation, on the other hand, anchors assessment in authentic delivery and contextual fit, providing a finer-grained lens on coherence and naturalness. Together, they ensure that role-play evaluation spans both broad stereotypical coverage and nuanced real-world plausibility.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "contextual",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed a diverse set of TTS systems to generate negative character profiles and resynthesized speech for Realism Evaluation.\nThese included both commercial and non-commercial models, with variation in style control, conditioning prompts, and reference usage.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.T8\" title=\"Table 8 &#8227; Same-Speaker Voice Clone. &#8227; F.6 TTS Systems and Settings &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> summarizes the systems and settings.\nBelow, we detail the major strategies used across systems:</p>\n\n",
                "matched_terms": [
                    "realism",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A character profile was first converted into a descriptive voice specification by prompting <span class=\"ltx_text ltx_font_typewriter\">deepseek-v3</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib38\" title=\"\">2024a</a>)</cite>.\nThis description was submitted to the TTS service to generate candidate voices.\nWe then previewed samples via Gemini2.5Pro, selecting the voice that best matched the speaker style of the original recording before proceeding to generate speech.</p>\n\n",
                "matched_terms": [
                    "gemini25pro",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS3</span> (Embedding&#8211;AudioCode Pipeline).\nHere, the input is Qwen2.5 BPE embeddings, which a 0.4B GPT-2 model maps directly to 4,096-dimensional audio codes. These codes are decoded via a cascaded architecture: <span class=\"ltx_text ltx_font_typewriter\">RepCodec</span> decoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib32\" title=\"\">2024</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> content representation (with speaker embedding) <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DDIM</span> (denoising\ndiffusion implicit model) module&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jeong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib34\" title=\"\">2021</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DAC</span> latent representation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib37\" title=\"\">2023</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DAC</span> decoder. Conditioning is provided through prefix text embeddings (e.g., instructions expressing emotional states such as &#8220;angry, hateful: you betrayed me&#8230;&#8221;), along with speaker ID. The input format is <span class=\"ltx_text ltx_font_typewriter\">[spk_id, BPE embedding, audio codes]</span>.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "here"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS4</span> (Instruction-Conditioned AudioCode Pipeline).\nThis model also employs a 0.4B GPT-2 to predict 4,096-dimensional audio codes, but conditions generation on instruction embeddings. A <span class=\"ltx_text ltx_font_typewriter\">CLIPTextEncoder</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Radford et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib51\" title=\"\">2021</a>)</cite> converts instruction text into embeddings, which serve as a prefix, combined with speaker ID and phone sequence. The input format is <span class=\"ltx_text ltx_font_typewriter\">[spk_id, instruction embedding sequence, phone sequence, audio codes]</span>.</p>\n\n",
                "matched_terms": [
                    "conditions",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The annotation interface for <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> focuses on assessing contextual and narrative coherence in model-generated speech.\nUnlike the archetype-oriented evaluation, which emphasizes stylistic and emotional fidelity, this platform examines how well the voice performance aligns with a character&#8217;s personality, dialogue situation, and scene progression.</p>\n\n",
                "matched_terms": [
                    "contextual",
                    "realism",
                    "emotional",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.F3\" title=\"Figure 3 &#8227; F.7 Annotation Platform for Realism Evaluation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the core playback interface.\nAnnotators can preview each audio sample with synchronized spectrogram and waveform displays, while reviewing contextual metadata including the <span class=\"ltx_text ltx_font_italic\">Local Scene</span>, <span class=\"ltx_text ltx_font_italic\">Character Style</span>, and <span class=\"ltx_text ltx_font_italic\">Character Profile</span>.\nThese descriptions help situate the listening task, providing the background necessary for judging realism in the character&#8217;s vocal and behavioral expression.</p>\n\n",
                "matched_terms": [
                    "local",
                    "realism",
                    "contextual",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Read the <span class=\"ltx_text ltx_font_bold\">Character Profile</span> (long-term identity, style) and <span class=\"ltx_text ltx_font_bold\">Local Scene</span> (immediate context, goal/emotion) before listening.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Focus evaluation on the <em class=\"ltx_emph ltx_font_italic\">spoken performance of the target speaker only</em>; ignore background music, ambient noise, and non-speech events.</p>\n\n",
                "matched_terms": [
                    "only",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt five dimensions, each rated from 1 (worst) to 5 (best). Emotional Expressiveness uses <em class=\"ltx_emph ltx_font_italic\">progressive gating</em>.</p>\n\n",
                "matched_terms": [
                    "expressiveness",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prosodic Dynamics</span> (Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis): naturalness of patterns and rhythms.</p>\n\n",
                "matched_terms": [
                    "prosodic",
                    "dynamics",
                    "stress",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotional Expressiveness</span> (Progressive gating):</p>\n\n",
                "matched_terms": [
                    "expressiveness",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Emotion Intensity Control (rate only if 2.1 <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"A7.I2.i2.I0.i2.p1.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math> 3),</p>\n\n",
                "matched_terms": [
                    "only",
                    "intensity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Character Consistency</span> (Voice Identity Matching, Trait Embodiment): alignment with Character Profile.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "consistency",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contextual Relevance</span> (Local Scene Fit, Global Story Coherence): fit to immediate scene and long-term identity.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "identity",
                    "relevance",
                    "contextual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Read first:</span> Carefully read the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em> to understand the long-term identity and immediate goal/emotion.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Focus only on human speech:</span> Do not factor in background music/soundtracks, environmental or ambient noise, or non-speech audio events; these may reflect the scene but should not influence scoring of the spoken performance.</p>\n\n",
                "matched_terms": [
                    "only",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identity mismatch:</span> If the clip&#8217;s speaker is clearly not the intended target speaker, assign a score of <span class=\"ltx_text ltx_font_bold\">1</span> for every rating dimension that relies on the Character Profile (e.g., Character Style Traits, Trait Embodiment, Character Consistency).</p>\n\n",
                "matched_terms": [
                    "identity",
                    "consistency",
                    "traits",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt contains contradictory information.</span> Conflicts make accurate annotation difficult or impossible. Examples: incompatible accent specifications; Local Scene drastically at odds with intended emotion (see <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>); Local and global scenes inconsistent; traits described as &#8220;evolving over time&#8221; without a timeline.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "global",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Question 2: Observed Character Style Traits.</span> Based on the Character Profile, are the defined Style Traits clearly present in the delivery?</p>\n\n",
                "matched_terms": [
                    "traits",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rubric Rating Instructions.</span>\nScore each subcriterion on a 1&#8211;5 scale. Use the detailed rubrics below and the examples to maintain calibration across characters, tones, and emotional styles. Prioritize in-character believability; do not reward theatricality, emotional intensity, or technical polish unless they naturally fit the character <em class=\"ltx_emph ltx_font_italic\">and</em> scene.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "intensity",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Focus: pitch, rhythm, and emphasis in speech.\n<span class=\"ltx_text ltx_font_bold\">Guideline 1.1 Pitch Variation (check Local Scene, Character Profile, and audio).</span></p>\n\n",
                "matched_terms": [
                    "character",
                    "local",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 1.2 Rhythmic Naturalness (check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "character",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 1.3 Stress and Emphasis (check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "character",
                    "stress",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Focus: alignment of expressed emotion with character style and situational demand. \n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Gating:</span> Only rate 2.2 if 2.1 <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"A7.SS4.SSSx5.p1.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math> 3; only rate 2.3 if 2.2 <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"A7.SS4.SSSx5.p1.m2\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math> 3.</p>\n\n",
                "matched_terms": [
                    "only",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 2.1 Emotion Accuracy (Required; check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "character",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 2.2 Emotion Intensity Control (Only if 2.1 <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"A7.SS4.SSSx5.p3.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math> 3; check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "character",
                    "only",
                    "intensity",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 2.3 Dynamic Range (Only if 2.2 <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"A7.SS4.SSSx5.p4.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math> 3; check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "character",
                    "only",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 3.1 Voice Identity Matching (Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 4.2 Global Story (Character Profile) Coherence (check Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "global",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Progressive gating for emotion.</span>\nOur <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness</em> uses gated sub-scores (accuracy <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A8.SS2.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> intensity control <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A8.SS2.p3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> dynamic range), preventing spurious high scores when base emotion is misidentified.\nThis reduces label noise and improves SEM alignment stability.</p>\n\n",
                "matched_terms": [
                    "expressiveness",
                    "accuracy",
                    "emotional",
                    "intensity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contrastive negatives and semantic controls.</span>\nRealism includes profile/scene mismatches and re-synthesized counterfactuals to force evaluators (human/SEM) to use paralinguistic cues, not only transcript semantics.</p>\n\n",
                "matched_terms": [
                    "only",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A critical observation is that annotations in Mandarin and English cannot be directly compared. The annotation groups for the two languages differed in their linguistic backgrounds and cultural interpretations, which leads to systematic differences in scoring. Our bilingual pilot further confirmed that annotators often interpret role-play appropriateness and expressiveness differently depending on their cultural frame of reference. For example, Mandarin raters tend to reward delivery styles that align with culturally expected prosody and politeness strategies, while English raters prioritize spontaneity and naturalness. These discrepancies parallel the findings of the singing voice conversion challenge (SVCC2023) from <cite class=\"ltx_cite ltx_citemacro_citet\">Huang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib31\" title=\"\">2023</a>)</cite>, where native and non-native listeners provided significantly different ratings for the same samples, particularly for naturalness and speaker similarity. Consequently, while absolute scores differ substantially across Mandarin and English, only within-language comparisons are meaningful. Cross-lingual ranking is not valid given the different annotation bases.</p>\n\n",
                "matched_terms": [
                    "expressiveness",
                    "roleplay",
                    "only",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Within Mandarin from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, the relative rankings of models are clear. <span class=\"ltx_text ltx_font_typewriter\">Doubao</span> stands out as the strongest performer across all four dimensions, reaching near-ceiling levels in both <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> (4.37) and <span class=\"ltx_text ltx_font_italic\">Content Pass</span> (0.98). It also leads in role-related dimensions such as <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, indicating balanced competence across linguistic accuracy, delivery, and expressiveness. <span class=\"ltx_text ltx_font_typewriter\">G-CosyVoice2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-CosyVoice2</span> follow closely, especially in audio quality, though they lag behind in role appropriateness. <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> achieve mid-range results, reflecting higher variance across samples, while <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> shows competitive content understanding but lower naturalness and appropriateness. <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> consistently ranks the lowest, with large variance, underscoring instability and weaker alignment with archetypal role expectations.</p>\n\n",
                "matched_terms": [
                    "expressiveness",
                    "accuracy",
                    "stands"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The picture shifts in English, elaborated in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Here, <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> emerges as the most effective model by a wide margin, achieving top scores in both <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> (3.46) and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> (3.49), as well as high content pass rates (0.97). This reflects its strong grounding in English discourse norms and role adherence. In contrast, <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> variants perform well in <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> but are weaker in role alignment, highlighting a trade-off between acoustic fidelity and role-play embodiment. Other models such as <span class=\"ltx_text ltx_font_typewriter\">Doubao</span>, <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span>, and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> achieve moderate results, while <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> trail far behind, showing limited robustness in English scenarios. Taken together, the English results reveal both a sharper separation across models and a wider variance in appropriateness scores compared to Mandarin.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "here"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although cross-lingual comparisons are invalid, this does not diminish the utility of archetype evaluation for role-play benchmarking. In fact, the observed inconsistencies reinforce a central point: successful role-play requires models to adapt to the cultural and linguistic norms of the target language community. A system that performs well in Mandarin but poorly in English cannot be considered universally capable, since end users will evaluate appropriateness and naturalness according to their native expectations. Thus, archetype evaluation highlights not only model-specific strengths and weaknesses but also the need for culturally grounded evaluation pipelines.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "only"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although realism evaluation targets nuanced paralinguistic behaviors, human annotators achieve <em class=\"ltx_emph ltx_font_italic\">consistently high agreement</em> once the rubric disentangles the aspects to be judged.\nAs shown in the agreement summary in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F7\" title=\"Figure 7 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, most dimensions fall in the <math alttext=\"0.86\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>0.86</mn><annotation encoding=\"application/x-tex\">0.86</annotation></semantics></math>&#8211;<math alttext=\"0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mn>0.88</mn><annotation encoding=\"application/x-tex\">0.88</annotation></semantics></math> range, with the lowest agreement on <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> (<math alttext=\"\\approx 0.83\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.83</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.83</annotation></semantics></math>) and the highest on <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em> / <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> (<math alttext=\"\\approx 0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.88</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.88</annotation></semantics></math>).\nEmotion-related, progressive metrics (Intensity and Transition) are slightly lower (<math alttext=\"\\approx 0.85\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.85</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.85</annotation></semantics></math>), reflecting the added difficulty of judging graded expressivity.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "local",
                    "once",
                    "intensity",
                    "transition",
                    "traits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F8\" title=\"Figure 8 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F9\" title=\"Figure 9 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> visualize the score distributions per dimension.\nWe highlight three patterns:\n(i) <span class=\"ltx_text ltx_font_bold\">Progressive/gated emotion metrics.</span> For <em class=\"ltx_emph ltx_font_italic\">Emotion Intensity</em> and <em class=\"ltx_emph ltx_font_italic\">Emotion Transition</em>, we assign <span class=\"ltx_text ltx_font_typewriter\">1</span> when the scene is marked <em class=\"ltx_emph ltx_font_italic\">N/A</em> (i.e., emotion accuracy is insufficient to meaningfully judge intensity/transition). This produces a visible mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, a sparse bin at <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>, and slightly lower overall agreement; downstream analyses conditioning on adequate <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> (e.g., mean&#160;<math alttext=\"\\geq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 3</annotation></semantics></math>) recover more bell-shaped distributions.\n(ii) <span class=\"ltx_text ltx_font_bold\">Traits Embodiment.</span> Scores tend to avoid the middle (<math alttext=\"\\sim\\!3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!3</annotation></semantics></math>), concentrating on low values when traits clearly mismatch and high values when traits are strongly realized. This yields the lowest inter-rater agreement among dimensions, reflecting the inherently compositional and role-specific nature of trait judgments.\n(iii) <span class=\"ltx_text ltx_font_bold\">Other dimensions.</span> <em class=\"ltx_emph ltx_font_italic\">Pitch Variation</em>, <em class=\"ltx_emph ltx_font_italic\">Rhythmic Naturalness</em>, <em class=\"ltx_emph ltx_font_italic\">Stress &amp; Emphasis</em>, <em class=\"ltx_emph ltx_font_italic\">Voice Identity Matching</em>, <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em>, <em class=\"ltx_emph ltx_font_italic\">Global Story Coherence</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> are approximately unimodal with modes at <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>&#8211;<math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math>, consistent with moderate&#8211;strong realization in the dataset and with the high agreement reported.</p>\n\n",
                "matched_terms": [
                    "global",
                    "pitch",
                    "identity",
                    "local",
                    "intensity",
                    "transition",
                    "stress",
                    "traits",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prosody control (Pitch, Rhythm, Stress).</span> Professional speech is shifted toward higher bins with tighter spread, indicating steadier control and fewer low-end tails.</p>\n\n",
                "matched_terms": [
                    "stress",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotion rendering (Accuracy, Intensity, Transition).</span> Professionals show higher centers and reduced mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.I1.i2.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> (fewer gated <span class=\"ltx_text ltx_font_typewriter\">N/A</span>s), suggesting more consistent emotional grounding and smoother dynamics; amateurs exhibit wider variance and more mass at the extremes.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "intensity",
                    "accuracy",
                    "transition",
                    "dynamics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identity and traits (Voice Identity, Traits Embodiment).</span> Professionals present clearer peaks in upper bins, while amateurs are broader or bimodal, reflecting inconsistent character anchoring.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "traits",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scene and semantics (Local Scene Fit, Global Coherence, Semantic Match).</span> Differences are present but smaller; both groups cluster in the mid&#8211;high range, with professionals exhibiting slightly narrower distributions.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, the consistently high agreement confirms that the rubric isolates perceptually coherent factors, so learned evaluators can target them reliably.\nSecond, the gated behavior of <em class=\"ltx_emph ltx_font_italic\">Intensity/Transition</em> motivates a <em class=\"ltx_emph ltx_font_italic\">hierarchical</em> evaluator: verify <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> before scoring graded expressivity, or model the joint structure with conditional heads.\nThird, <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> remains challenging; models may benefit from explicit character-trait priors, long-term context aggregation, or contrastive supervision between matched and mismatched traits.\nFinally, prosody-focused objectives (e.g., pitch/rhythm/stress control) are likely to yield the largest quality gains for amateur-style speech.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "evaluator",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For both zeroshot and fewshot experiments, we adopt a streamlined version of the annotation guidelines to ensure consistency with the human annotation process. The exact rubric prompts for archetype and realism evaluation used are provided below:</p>\n\n",
                "matched_terms": [
                    "realism",
                    "consistency",
                    "zeroshot",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and fewshot settings, we condition the model with the rubric prompt and then append few-shot demonstration triplets (input, audio, gold JSON) to mirror the human annotation protocol. As shown in Listing&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#LST2\" title=\"Listing 2 &#8227; L.1 Zeroshot/Fewshot Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, We use <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> for these few-shot exemplars and require a minified JSON output consistent with the rubric schema.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "gemini25pro",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and few-shot experiments on commercial models such as <span class=\"ltx_text ltx_font_typewriter\">Gemini</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o</span>, we report results as the average of five independent runs to reduce randomness and obtain more stable predictions. In contrast, for open-source models, we observed that their instruction-following ability is relatively weak. To mitigate this, we prompt them to predict one dimension at a time rather than all dimensions jointly. Specifically, for each metric, after showing the rubric prompt, we request completion in a fixed template and extract the probability distribution over the discrete tokens <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>. The final score is then computed from these probabilities, which we found makes open-source models usable to some extent under this setting.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For few-shot experiments, we randomly sample three exemplars from the training set, with each exemplar chosen such that its human scores are close to the rounded average across all annotators. These exemplars are appended as demonstrations in the prompt. As with the zeroshot case, we run five trials for commercial models and report the averaged results for evaluation.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Effect of repeated runs.\nWhen comparing single-run decoding (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot-1run</span>) against averaging over five runs (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot</span>), we see consistent gains across nearly all dimensions (average score improves from 0.353 to 0.390). This suggests that sampling variance plays a substantial role in the reliability of large language model (LLM) evaluators, and averaging multiple outputs reduces this variance, producing more stable correlation with human annotations.</p>\n\n",
                "matched_terms": [
                    "zeroshot1run",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Impact of few-shot conditioning.\nIntroducing even three examples (<span class=\"ltx_text ltx_font_typewriter\">3-Shot</span>) yields a clear improvement over zeroshot (0.433 vs. 0.390). This indicates that providing explicit annotated demonstrations helps the model better align with the rubric. Interestingly, when using only a single run with few-shot demonstrations (<span class=\"ltx_text ltx_font_typewriter\">3-Shot-1run</span>), the performance is only marginally better than pure zeroshot, highlighting that both averaging and demonstrations are complementary: demonstrations improve grounding, while multiple runs mitigate stochasticity.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "3shot",
                    "fewshot",
                    "3shot1run",
                    "only",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Scaling the number of demonstrations.\nIncreasing the few-shot set size from three to six and nine further boosts performance (0.459 and 0.469, respectively). The improvements are most pronounced in prosodic and emotional categories (e.g., <em class=\"ltx_emph ltx_font_italic\">Pitch</em>, <em class=\"ltx_emph ltx_font_italic\">Accuracy</em>, <em class=\"ltx_emph ltx_font_italic\">Intensity</em>), suggesting that more examples provide stronger guidance in modeling subtle paralinguistic cues. Gains in <em class=\"ltx_emph ltx_font_italic\">Character Consistency</em> and <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance</em> are also noticeable, although smaller in magnitude, which may indicate that these higher-level judgments are less sensitive to prompt scaling once a few demonstrations are provided.</p>\n\n",
                "matched_terms": [
                    "pitch",
                    "relevance",
                    "contextual",
                    "consistency",
                    "emotional",
                    "once",
                    "character",
                    "intensity",
                    "prosodic",
                    "fewshot",
                    "accuracy",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In general, the results demonstrate two complementary strategies for stabilizing LLM-based evaluators: (i) averaging multiple runs to reduce randomness, and (ii) enriching prompts with more few-shot exemplars to improve adherence to rubric-based evaluation. Combining both leads to the strongest overall performance, with the nine-shot condition achieving the highest average correlation&#160;(0.469).</p>\n\n",
                "matched_terms": [
                    "performance",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the archetype test set, all fine-tuned variants achieve strong <span class=\"ltx_text ltx_font_italic\">Content Pass</span> accuracy above 92%. Differences appear mainly in correlation-based metrics. Here, <span class=\"ltx_text ltx_font_typewriter\">Base-FT</span> yields the highest score on <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> delivers the strongest performance in <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> and the best overall average (0.629). This suggests that instruction-tuned checkpoints paired with parameter-efficient adaptation can better capture higher-level perceptual traits.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "accuracy",
                    "here",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For realism evaluation on the basic test set, <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> consistently achieves the strongest correlations across most dimensions, especially for <span class=\"ltx_text ltx_font_italic\">Character Identity</span> and <span class=\"ltx_text ltx_font_italic\">Traits</span>, leading to the best average correlation (0.668). <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> also performs competitively (0.630), while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> lags slightly behind (0.625). These results highlight that full fine-tuning may still offer an advantage when the evaluation set is relatively homogeneous and closer to the training distribution.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "realism",
                    "traits",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A different pattern emerges on the real recording test set. Both <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> and <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> struggle, yielding negative or near-zero correlations across most dimensions. In contrast, <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> shows clear improvements, achieving positive correlations in all prosodic and emotional dimensions and producing the highest overall average (0.247). This indicates that LoRA adaptation on top of an instruction-tuned backbone provides better generalization to out-of-distribution, real-world data.</p>\n\n",
                "matched_terms": [
                    "different",
                    "prosodic",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the comprehensive design of Speech-DRAME, several limitations remain.\nFirst, while our dual evaluation paradigm combines top-down archetype assessment with bottom-up realism evaluation, both strategies are constrained by the quality and scope of available data. Archetype evaluation inevitably relies on simplified stereotypes, which can overlook subtler paralinguistic and contextual cues. Realism evaluation, though richer, still struggles with domain mismatch between synthetic data and real human recordings, as evidenced by the performance drop observed in the real-recording test set. This highlights the persistent gap between benchmark-driven evaluation and real-world variability in speech role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "contextual",
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Third, while DRAME-Eval substantially outperforms zero-shot and few-shot ALLMs, its reliance on fine-tuning from a strong base model raises questions of accessibility and scalability. Future work should explore lightweight adaptation techniques, semi-supervised training with partially annotated data, and leveraging emergent capabilities of open-source multimodal LLMs.</p>\n\n",
                "matched_terms": [
                    "zeroshot",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, although we validated DRAME-Eval against human annotations, alignment under realism settings remains modest. This suggests that even human-annotated benchmarks are insufficiently fine-grained to capture the full complexity of human perception in speech role-play. Richer annotation protocols, more diverse speaker populations, and integration of perceptual psychology insights could further improve evaluation fidelity.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Looking forward, we see several promising research directions. We intentionally restrict evaluation to single-turn responses for clarity and tractability. However, multi-turn extensions that capture narrative progression, long-term consistency, and dynamic emotion shifts remain a natural next step. Incorporating multilingual and cross-cultural datasets will enhance generality and fairness. On the modeling side, exploring hybrid evaluation strategies that combine discrete rubrics with continuous perceptual signals (e.g., prosody embeddings or affective measures) may better bridge the gap between automatic and human judgments. Finally, integrating Speech-DRAME into reinforcement learning pipelines as a reward model could help drive the next generation of expressive, human-aligned speech role-play systems.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "generation",
                    "consistency"
                ]
            }
        ]
    },
    "A12.T13": {
        "caption": "Table 13: Archetype role-play evaluator performance. We report classification accuracy for Content Pass and Pearson correlation coefficients for other dimensions. The Avg. column is the average of all correlation coefficients.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\">Config.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">Content Pass Acc.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">Audio Quality</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">Human Likeness</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">Appropriateness</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Avg.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">Base-FT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">92.1%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.688</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.648</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.551</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.616</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">92.6%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.668</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.648</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.545</td>\n<td class=\"ltx_td ltx_align_center\">0.620</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">92.6%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.650</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.684</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.492</td>\n<td class=\"ltx_td ltx_align_center\">0.609</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">93.6%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.682</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.680</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.525</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.629</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "roleplay",
            "column",
            "content",
            "pearson",
            "instructft",
            "config",
            "coefficients",
            "evaluator",
            "audio",
            "average",
            "baseft",
            "archetype",
            "accuracy",
            "performance",
            "avg",
            "pass",
            "report",
            "classification",
            "correlation",
            "appropriateness",
            "baselora",
            "dimensions",
            "all",
            "other",
            "acc",
            "human",
            "instructlora",
            "likeness",
            "quality"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.T13\" title=\"Table 13 &#8227; Overall summary. &#8227; L.4 Fine-tuning Ablation Experiments &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.T14\" title=\"Table 14 &#8227; Overall summary. &#8227; L.4 Fine-tuning Ablation Experiments &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.T15\" title=\"Table 15 &#8227; Overall summary. &#8227; L.4 Fine-tuning Ablation Experiments &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">15</span></a> present ablation studies across different fine-tuning strategies, including the use of <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio-7B-Base</span> or <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio-7B-Instruct</span>; and the use of fully fine-tuning or LoRA fine-tuning. We use <span class=\"ltx_text ltx_font_typewriter\">Base-FT</span>, <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span>, <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span>, and <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> to represents different configuration settings in following experiments.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Role-play has become a key testbed for generative models, expanding from text-only dialogue to multimodal interaction. Extending role-play to speech captures prosody, emotion, and delivery, but also poses new evaluation challenges. Current pipelines often use audio large language models (ALLMs) as zero-shot judges, which miss paralinguistic cues, collapse multiple aspects into coarse scores, and rely on synthetic speech references that fail to reflect real-world roles. We present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework that contributes at three levels: (i) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-EvalBench</em>, an evaluation benchmark with bilingual human-annotated data and protocols for training and testing speech evaluation models&#160;(SEMs), (ii) <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, a fine-tuned evaluation model, which substantially outperforms zero-shot and few-shot ALLMs, and (iii) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-RoleBench</em>, a speech role-play benchmark that leverages <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> as an automatic judge to compare speech foundation models (SFMs). Speech-DRAME distinguishes between two complementary evaluation strategies: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>, a top-down approach measuring adherence to broad role archetypes, and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em>, a bottom-up approach grounded in real human speech that emphasizes nuanced role quality. Compared to zero-shot ALLM judges, <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves stronger agreement with human ratings (Pearson correlation from 0.480 to 0.629 in archetypes, and 0.390 to 0.625 in realism). By integrating transparent benchmark resources, modeling approaches, and system-level evaluation, Speech-DRAME provides the first comprehensive, reproducible foundation for assessing spoken role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "correlation",
                    "audio",
                    "pearson",
                    "human",
                    "archetype",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">it collapses diverse aspects of performance into limited coarse dimensions, obscuring critical dimensions of role-play quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite>, and</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dimensions",
                    "performance",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">it depends on synthetic speech references rather than real human speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>)</cite>, limiting its ability to reflect real-world diversity in speech role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome these limitations, we present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span> (Speech Detailed Role-play Assessment with Modeling and Evaluation), a unified framework that moves beyond synthetic or judge-only pipelines. Speech-DRAME is distinguished by three aspects: [i] a dual evaluation paradigm that balances archetype-driven generality with human-grounded realism (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS2\" title=\"3.2 Role-play Evaluation with Two Strategies &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), [ii] human annotations that provide perceptual grounding absent in prior resources (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4\" title=\"4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), and [iii] evaluation-model alignment that enables not only benchmarking of speech foundation models (SFMs) but also systematic analysis of the evaluation models (SEMs) themselves (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5\" title=\"5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). These aspects establish Speech-DRAME as a comprehensive and trustworthy framework for measuring and improving speech role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, Speech-DRAME contributes at three levels: (i) <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>, an evaluation benchmark consisting of human-annotated datasets for archetype and realism role-play; (ii) <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span>, a fine-tuned SEM trained on EvalBench that surpasses zero-shot and few-shot ALLMs; and (iii) <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span>, a speech role-play benchmark that leverages DRAME-Eval as an automatic judge to systematically compare SFMs. By combining dual evaluation strategies, human-anchored supervision, and evaluation-model alignment, Speech-DRAME delivers systematic insights into both role-play systems and evaluation models themselves.\nOur contributions are four-fold:</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a <span class=\"ltx_text ltx_font_bold\">formal task definition</span> of speech role-play and its evaluation via speech foundation models (SFMs) for generation and speech evaluation models (SEMs) for assessment, unifying linguistic and paralinguistic dimensions.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>, the first evaluation benchmark with human annotations under both archetype- and realism-based strategies, supporting the training and testing of SEMs.</p>\n\n",
                "matched_terms": [
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We establish <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span>, a speech role-play benchmark that leverages DRAME-Eval as an automatic judge to systematically compare both end-to-end and cascaded SFMs across multiple role quality dimensions.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Relevant codes, pre-trained models, and datasets will be open-sourced at \n<br class=\"ltx_break\"/><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/Anuttacon/speech_drame\" title=\"\">https://github.com/Anuttacon/speech_drame</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-native benchmarks.</span>\nMore recently, a number of resources have been introduced to evaluate spoken role-play agents. OmniCharacter-10K <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>)</cite> emphasizes character voice traits, while others&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib39\" title=\"\">2025</a>; Hou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib27\" title=\"\">2025</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib15\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib7\" title=\"\">2024b</a>)</cite> extend evaluation toward vocal communication, assistant capability, and multi-turn realism. Omni models such as StepAudio and KimiAudio <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite> also include role-play scenarios in their evaluation. SpeechRole and VStyle <cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>; Zhan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib76\" title=\"\">2025</a>)</cite> are closer to our work, offering datasets that test interaction ability, expressiveness, and fidelity. However, these resources are primarily built from LLM-generated dialogues and TTS outputs, with limited human validation and a heavy reliance on proprietary ALLM-as-judge pipelines. As a result, they lack grounding in diverse human speech and typically apply only coarse rubrics.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Automatic judging with ALLMs.</span>\nIn parallel, advances in automatic judging highlight both opportunities and limitations. Audio-capable LLMs such as AudioJudge and Audio-Aware LLMs as Judges <cite class=\"ltx_cite ltx_citemacro_citep\">(Manakul et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib44\" title=\"\">2025</a>; Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>)</cite> can assess style, emotion, and delivery, while systems like Auto-ATT <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib65\" title=\"\">2025c</a>)</cite> scale this to Audio Turing Tests. Yet studies consistently reveal high prompt sensitivity and shallow capture of paralinguistic cues, suggesting that zero-shot judging remains insufficient for nuanced role-play evaluation.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Expressive TTS evaluation.</span>\nRecent work has explored evaluation of instruction-following and style-controlled TTS. InstructTTSEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite> explicitly includes role-play scenarios, while EmergentTTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">(Manku et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib45\" title=\"\">2025</a>)</cite> and discrete speech LM benchmarks <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang &amp; Sz&#233;kely, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib63\" title=\"\">2024</a>)</cite> emphasize prosody, spontaneity, and speaker consistency. These efforts push beyond intelligibility and naturalness, but still center on synthetic outputs rather than real human role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our distinction.</span>\nIn contrast, Speech-DRAME unifies these considerations through three elements: (i) a dual evaluation paradigm that combines archetype-driven generality with realism grounded in authentic human speech, (ii) human annotations that provide a reliable perceptual gold standard, and (iii) alignment of SEMs to human ratings, enabling systematic evaluation of both SFMs and the evaluation models themselves. This combination moves beyond synthetic benchmarks or zero-shot judging to provide a comprehensive and trustworthy framework for speech role-play assessment.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech role-play evaluation can be framed in multiple ways depending on the available data and the desired granularity of analysis.\nWithin Speech-DRAME, we formalize two distinct strategies inspired by sociology theories<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Please refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A2\" title=\"Appendix B Theoretical Motivation for Dual Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> for detailed discussion of the dual evaluation design.</span></span></span>, including a top-down, archetype-oriented option using synthetic speech and a bottom-up, realism-oriented option from real human speech.\nRather than merging these into a single rubric, we treat them as separate perspectives, each with unique advantages and limitations.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarizes the key contrasts, while further implementation details are provided in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>-&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS5\" title=\"F.5 Examples of Realism Role-play &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.5</span></a>). The second strategy adopts a <em class=\"ltx_emph ltx_font_italic\">bottom-up</em> design, motivated by real-world performance and real human speech.\nInstead of relying solely on stereotypes, this approach grounds evaluation in recordings from professional and non-professional speakers, drawn from realistic dialogue and media sources.\nJudgments in this setting are more fine-grained and multi-dimensional, addressing aspects such as prosodic dynamics, emotional fidelity, character consistency, and contextual fit.\nWhile this strategy requires more human annotation and careful curation, it provides benchmarks that reflect how role-play is perceived in practice.\nIt thus offers a richer framework for analyzing model outputs in settings where realism and subtlety are essential.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "human",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human Annotation.</span>\nAll collected audio undergoes a standardized annotation protocol: (1) an initial semantically-based screening to exclude invalid clips; (2) rubric-based scoring across multiple perceptual dimensions; and (3) meta-annotations such as confidence or prompt-quality judgments. Full rubrics are provided in the appendices&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A5\" title=\"Appendix E Annotation Guidelines for Archetype Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">E</span></a> &amp; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>All data collection follows in high standard ethical consideration as outlined in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A9\" title=\"Appendix I Annotation Ethics &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "audio",
                    "all",
                    "dimensions",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Archetype Data Collection &#8227; 4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents a summary of the curated <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> dataset, where we further elaborate the details of archetype and realism evaluation data collection in following subsections, respectively. For benchmark purpose, we provide an official train-test split for the human annotated dataset for <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> so that following researchers can use the dataset to train and evaluate their SEMs for speech role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Archetype evaluation adopts a top&#8211;down perspective from sociology and role theory&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goffman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib23\" title=\"\">1949</a>; Campbell, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib4\" title=\"\">2008</a>)</cite>, abstracting role-play into broad and recognizable categories.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Sources.</span> The data is collected from a range of speech foundation models, including both cascaded and end-to-end models. We limit that only audio prompts are used as the input but adopt different prompt strategies for best quality with regards to different models. Details about the selected models and prompting details are discussed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS4\" title=\"D.4 Candidate Models for Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.4</span></a>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotation follows two stages: an initial semantic check (i.e., <span class=\"ltx_text ltx_font_italic\">Content Pass</span>) to filter out instruction non-compliance or audio defects, followed by rubric-based scoring on three dimensions: <em class=\"ltx_emph ltx_font_italic\">Audio Quality</em>, <em class=\"ltx_emph ltx_font_italic\">Human-likeness</em>, and <em class=\"ltx_emph ltx_font_italic\">Appropriateness</em>. Confidence judgments are also recorded. Detailed rubric scales are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A5\" title=\"Appendix E Annotation Guidelines for Archetype Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">E</span></a>.</p>\n\n",
                "matched_terms": [
                    "appropriateness",
                    "pass",
                    "content",
                    "audio",
                    "dimensions",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype Evaluation Data Summary.</span>\nThe resulting archetype evaluation dataset comprises 552 distinct scenarios for both Mandarin and English, evaluated across eight models,<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>The model list is discussed in Appdenix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS4\" title=\"D.4 Candidate Models for Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.4</span></a>. Note we only include seven models for English due to the deactivation of ChatGPT-4O advanced voice model.</span></span></span> yielding 8,280 samples. In total, the dataset are split into 6,780 for training and 1,500 for testing. For benchmarking, an additional 1,250 scenarios spanning seven role/scene categories are prepared to facilitate archetype role-play benchmarking. More detailed statistics and analyses are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10\" title=\"Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">J</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotators judge whether performances align with the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em>, using rubrics for <em class=\"ltx_emph ltx_font_italic\">Prosodic Dynamics (i.e., Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis)</em>, <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness (with progressive gating over three sub-dimensions: Emotion Accuracy, Emotion Intensity, Emotion Transition)</em>, <em class=\"ltx_emph ltx_font_italic\">Character Consistency (i.e., Voice Identity Matching and Traits Embodiment)</em>, <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance (i.e., Local Scene Fit and Global Story Coherence)</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>. A prompt quality check and confidence ratings are also collected. Full scales and gating details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "dimensions",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real-world Recoding.</span>\nAs noted in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S2\" title=\"2 Related Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, most prior efforts have relied heavily on synthetic data, mostly from TTS systems based on curated professional role-play voices. While such data can achieve high fidelity, it often lacks the diversity and spontaneity of real-world delivery, limiting the generalizability of evaluation models. To address this gap, we expand realism evaluation beyond synthetic and curated voices by incorporating newly recorded speech from both professional and amateur voice actors. This design introduces a broader spectrum of delivery styles, from polished professional acting to more natural and less controlled amateur performance.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation Data Summary.</span>\nThe resulting realism evaluation dataset is constructed from a combination of curated media sources, including internal collections, public datasets (e.g., NCSSD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib40\" title=\"\">2024b</a>)</cite>), and original recordings collected for this project.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>All data were obtained under appropriate licenses and are legally available for research release.</span></span></span> We compile 15k annotated role-play samples: 9k in Mandarin and 5k in English. The dataset is divided into 12k samples for training, 2k mixed samples for the base test set, and 1k real-recording samples for the test set. The first test set supports general evaluation using our proposed pipeline with negative samples, while the second focuses exclusively on human speech performed by both professional and amateur voice actors. For benchmarking, 2,000 scenarios from the realism dataset are designated for realism role-play benchmarking. Detailed statistics and analyses are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11\" title=\"Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">K</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-shot and Few-shot ALLMs.</span>\nWe first evaluate the zero-shot capabilities of both proprietary and public models, including <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>, <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>, <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio</span>, and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>. All evaluations follow a unified prompting template that provides audio input alongside scene and character context.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For proprietary models, we report the average of five independent runs to reduce variance and obtain more stable predictions. For open-source models, we observed weaker instruction-following ability and limited adaptation to unfamiliar tasks. To mitigate this, we did not request simultaneous prediction of multiple metrics. Instead, we queried one metric at a time. Following <cite class=\"ltx_cite ltx_citemacro_citet\">Song &amp; Bahri (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>); Lukasik et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>, we interpret model outputs through token distribution over discrete labels (<span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score.</p>\n\n",
                "matched_terms": [
                    "report",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Finetuning Model: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>.</span>\nIn addition to zero-shot and few-shot evaluations, we fine-tune a dedicated evaluation model, denoted as <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> for archetype and realism role-play evaluation, respectively.\nWe initialize from <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio-7B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib11\" title=\"\">2024</a>)</cite>, which provides a strong foundation for multi-modal audio-text reasoning.\nTo better leverage the multiple human annotations available in our datasets, we adopt a sampling-based training strategy: at each step, one annotator label is randomly selected, following practices in label distribution learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Geng, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib22\" title=\"\">2016</a>; Wen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib67\" title=\"\">2023</a>)</cite>.\nThis stochastic supervision implicitly exposes the model to the empirical annotation distribution, while during inference we output the full distribution over discrete labels (e.g., <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score, consistent with recent decoding-based regression methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Song &amp; Bahri, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>; Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib70\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib64\" title=\"\">2025b</a>; Lukasik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>. We apply LoRA fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib28\" title=\"\">2022</a>)</cite>. All implementation details, including optimizer settings, training schedule, and sampling policy, are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS3\" title=\"L.3 Fine-tuning Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "all",
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics for Role-play Evaluation Model.</span> For most dimensions with 1-5 scale, we report Pearson correlation. For <span class=\"ltx_text ltx_font_italic\">Content Pass</span> estimation, we report accuracy.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "correlation",
                    "pass",
                    "content",
                    "report",
                    "pearson",
                    "dimensions",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype Evaluation.</span>\nThe results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T2\" title=\"Table 2 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reveal clear distinctions across evaluation settings. In the zero-shot condition, proprietary ALLMs such as <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> substantially outperform public models, achieving strong <span class=\"ltx_text ltx_font_italic\">content pass</span> accuracy (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>90%) and moderate correlations with other dimensions(<math alttext=\"0.43\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mn>0.43</mn><annotation encoding=\"application/x-tex\">0.43</annotation></semantics></math>&#8211;<math alttext=\"0.48\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mn>0.48</mn><annotation encoding=\"application/x-tex\">0.48</annotation></semantics></math>). By contrast, public models exhibit limited predictive ability, with correlations often below 0.2. Few-shot prompting provides modest gains for proprietary models, improving both accuracy and correlation (e.g., <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> rises from 0.480 to 0.493 average correlation). The most significant improvement arises from finetuning: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves an average correlation of 0.629, marking a large margin over zero-shot/few-shot models and demonstrating the effectiveness of task-specific supervision.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "pass",
                    "content",
                    "average",
                    "other",
                    "archetype",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T3\" title=\"Table 3 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> extends the analysis to fine-grained realism evaluation. Similar to archetype results, proprietary ALLMs lead the zero-shot setting, with <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> obtaining an average correlation of 0.390, compared to 0.284 for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>. Public models remain weak baselines. Few-shot prompting consistently improves performance: <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> gains +0.042 and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> gains +0.083 over their zero-shot averages, showing the benefit of in-context supervision. Once again, finetuning is most effective: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves 0.625 average correlation, higher than the best few-shot results. Importantly, the gains are consistent across all 10 realism dimensions, confirming that the sampling&#8211;expectation training strategy generalizes beyond simple classification to nuanced paralinguistic cues.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "average",
                    "dimensions",
                    "classification",
                    "all",
                    "archetype",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real Recording Test Set.</span>\nThe real-recording evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T4\" title=\"Table 4 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> is notably more challenging, with all models suffering substantial performance drops. Proprietary models in the zero-shot setting achieve only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.04&#8211;0.08 average correlation, and public models often fall below zero. Few-shot prompting yields small but noticeable improvements, especially for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> (0.226 avg.), suggesting that exemplars partially mitigate domain mismatch between synthetic and real human speech. Interestingly, while finetuning achieves the highest overall average (0.247), its relative advantage over few-shot prompting is narrower than in the synthetic setting, and some dimensions (e.g., <span class=\"ltx_text ltx_font_italic\">Character Identity</span>) remain weak. We emphasize that this observed gap between synthetic and real recordings is not merely an artifact but an intended feature of Speech-DRAME, designed to expose the mismatch and drive future research into bridging synthetic-to-real generalization.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "avg",
                    "average",
                    "dimensions",
                    "all",
                    "human",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall.</span> Across both archetype and realism evaluations, we observe three consistent trends: (i) zero-shot performance is limited, particularly for open-source models, (ii) few-shot prompting offers moderate but consistent gains, especially for proprietary ALLMs, and (iii) finetuning with task-specific supervision (<span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>) delivers the largest improvements, achieving state-of-the-art correlations across nearly all dimensions. Nevertheless, results on the real-recording test set underscore the gap between synthetic and real human role-play scenarios, motivating future work in data diversification and domain adaptation.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dimensions",
                    "all",
                    "human",
                    "archetype",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype evaluation.</span>\nOn archetype role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T5\" title=\"Table 5 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), cascaded pipelines consistently outperform end-to-end systems. The strongest cascaded systems, particularly <span class=\"ltx_text ltx_font_typewriter\">G-IndexTTS2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-IndexTTS2</span>, achieve both high content pass rates (above 94%) and balanced scores across audio quality, human likeness, and appropriateness. By contrast, end-to-end models such as <span class=\"ltx_text ltx_font_typewriter\">StepAudio2Mini</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> perform competitively in terms of content coverage (above 88%), but their expressive scores remain slightly lower than their cascaded counterparts. This gap suggests that separating reasoning (LLM) from speech rendering (TTS) remains advantageous when archetype fidelity is emphasized. Nevertheless, the fact that top end-to-end systems already surpass 3.0 in average rating indicates rapid closing of the performance gap.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "appropriateness",
                    "quality",
                    "pass",
                    "content",
                    "audio",
                    "average",
                    "human",
                    "archetype",
                    "likeness",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism evaluation.</span>\nRealism role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T6\" title=\"Table 6 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) presents a different landscape. Here, performance differences between end-to-end and cascaded systems are less pronounced. Several end-to-end models, such as <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>, reach average scores of 3.24, matching or even exceeding the best cascaded pipelines. Importantly, realism scores are more evenly distributed across prosodic, emotional, character, and contextual dimensions, with most models clustering between 2.9&#8211;3.2. This convergence reflects the difficulty of realism-style evaluation: since the data preparation emphasized contrast with authentic human speech, models are judged under similar conditions, leading to smaller observed gaps.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "average",
                    "dimensions",
                    "human",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human alignment.</span>\nWe further conducted small-scale human studies with 10% of the benchmark data to verify the reliability of the benchmark. Archetype evaluation achieves a Spearman correlation of 0.706 between human judgments and <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, validating that the automatic evaluator closely tracks human preferences in this setting. Realism evaluation, however, achieves a more modest Spearman correlation of 0.375. This lower alignment is expected, since realism data construction did not prioritize fine-grained model separation but instead emphasized distinguishing human versus synthetic speech.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>We view this as evidence that realism presents a harder, less-saturated frontier. Rather than a limitation of DRAME-Eval, this highlights the challenge of designing evaluators that capture nuanced human perception, which motivates future work.</span></span></span> As a result, all models occupy a relatively narrow performance band, reducing sensitivity to model-level differences. We view this as evidence that realism evaluation highlights a harder, less saturated frontier where both SFMs and SEMs require further advances.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "evaluator",
                    "all",
                    "human",
                    "archetype",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, cascaded pipelines retain an edge in archetype scenarios, especially in content robustness and expressive fidelity, but end-to-end models are catching up quickly. In realism scenarios, the field remains unsolved: existing systems exhibit similar performance bands and evaluators show weaker alignment with human judgments, underscoring the need for improved realism-grounded data and evaluation. Together, these findings establish <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> as both a reliable testbed for archetype fidelity and a challenging benchmark for realism fidelity.</p>\n\n",
                "matched_terms": [
                    "content",
                    "human",
                    "performance",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework for assessing spoken role-play through two complementary lenses: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em> (top-down) and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> (bottom-up, human-grounded). Our human-annotated <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> enables training and testing of SEMs, and our fine-tuned <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span> substantially outperforms zero-/few-shot ALLM judges. Building on this, <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> provides system-level comparison across various SFMs, revealing a consistent edge for cascaded pipelines on archetypes and a tighter spread under realism, where real human performance evaluation remain challenging. Together, these resources establish a reproducible, analyzable foundation for spoken role-play evaluation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>Limitations and future works are discussed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A13\" title=\"Appendix M Limitations and Future Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">M</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "performance",
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All data collection and annotation in this work followed strict ethical standards. Our bilingual datasets were constructed exclusively from legally accessible sources, including licensed media, open-source corpora, and original recordings conducted with informed consent. Annotators were recruited with appropriate compensation and provided with clear instructions, including the right to withdraw at any point. To minimize risks, no personally identifiable information was included in the released data, and all recordings were anonymized before annotation. We ensured cultural and linguistic diversity in annotator pools to mitigate potential bias across Mandarin and English role-play scenarios. While this study did not require formal IRB approval, we adhered to best practices in human-subject research, including transparency in task design and careful consideration of annotator well-being. Full details of our annotation protocol and ethical safeguards are presented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A9\" title=\"Appendix I Annotation Ethics &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data generation:</span> LLMs were employed to expand prompts (e.g., character profiles, scene descriptions) and to produce negative samples in realism evaluation. All outputs were screened by human annotators to ensure validity and reduce bias.</p>\n\n",
                "matched_terms": [
                    "all",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our distinction between <span class=\"ltx_text ltx_font_italic\">Archetype Evaluation</span> and <span class=\"ltx_text ltx_font_italic\">Realism Evaluation</span> is not merely pragmatic, but is grounded in established theories of social identity, communication, and performance. The two perspectives capture complementary dimensions of how humans perceive and evaluate roles in interaction.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "performance",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism as Situated Performance</span>\nIn contrast, realism highlights the fine-grained enactment of roles in situated interaction. Goffman&#8217;s dramaturgical sociology emphasizes that everyday social life is itself a performance, where believability depends on subtle paralinguistic and pragmatic cues such as prosody, hesitation, and emotional shading&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goffman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib23\" title=\"\">1949</a>)</cite>. Similarly, interactional sociolinguistics has demonstrated how micro-level features like intonation, pacing, and turn-taking patterns shape the perception of authenticity in communication&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gumperz, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib25\" title=\"\">1982</a>)</cite>. Realism Evaluation thus anchors role-play assessment in bottom-up sensitivity to authentic delivery, beyond schematic expectations.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Complementarity of the Two Perspectives</span>\nPerformance theory provides a unifying lens for these dual dimensions. Schechner&#8217;s view of performance as both &#8220;restored behavior&#8221; (scripted, repeatable structures) and &#8220;living process&#8221; (situated enactment)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Schechner, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib52\" title=\"\">2017</a>)</cite> underscores the need to consider both archetypal scripts and enacted realism. Likewise, sociolinguistic theories of identity recognize both <span class=\"ltx_text ltx_font_italic\">ascribed categories</span> (archetypes imposed by social convention) and <span class=\"ltx_text ltx_font_italic\">performed identities</span> (emergent in interaction)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bucholtz &amp; Hall, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib2\" title=\"\">2005</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, they mirror the dual nature of human role assessment, balancing scalability and comparability with depth and nuance. This theoretical grounding reinforces the design of Speech-DRAME as a framework that does not privilege one perspective over the other, but instead integrates both to reflect the full spectrum of how roles are evaluated in speech-based interaction.</p>\n\n",
                "matched_terms": [
                    "other",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our main formulation (Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS1\" title=\"3.1 Problem Formulation &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>) focuses on the <em class=\"ltx_emph ltx_font_italic\">single-turn</em> setting, where each response <math alttext=\"{\\bm{S}}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119930;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{S}}_{r}</annotation></semantics></math> is generated and evaluated independently.\nThis simplification enables clearer analysis of speech role-play quality and aligns with our bilingual annotation protocol.\nNevertheless, the same formulation extends naturally to multi-turn dialogues.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Archetype Evaluation follows a top&#8211;down perspective rooted in sociology and role theory, where social behavior is interpreted through shared stereotypes and generalized expectations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goffman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib23\" title=\"\">1949</a>; Campbell, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib4\" title=\"\">2008</a>)</cite>.\nThis strategy abstracts role-play into broad and easily recognizable archetypes, enabling scalable evaluation of whether a generated response aligns with salient role features.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scene dimensions.</span>\nIn addition to roles, we incorporate contextual triggers that anchor role-play in stereotypical situations.\nA central focus is on <span class=\"ltx_text ltx_font_bold\">everyday external events or state changes</span> (e.g., receiving a promotion, being caught in gossip), which provide concrete scenarios for testing archetypical responses.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Exclusion of fine-grained controls.</span>\nWe deliberately excluded explicit emotional states, prosodic instructions, or linguistic constraints.\nThese dimensions are better suited to realism-oriented or style-controlled evaluation, whereas archetype evaluation benefits from concrete, stereotype-level scenarios that sharpen judgments and improve human agreement.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Pipeline overview.</span>\nAt a high level, archetype evaluation data are generated through a three-stage pipeline:\n(1) selecting keywords that specify role and scene;\n(2) expanding them into full prompts via structured templates; and\n(3) rendering prompts into speech with contemporary TTS systems.\nThis modular pipeline ensures both scalability and reproducibility, while keeping prompts vivid enough for reliable judgments. (4) Based on the generated data (paired scene and synthesized speech), expert human annotators are recruited for detailed annotation on semantic check (i.e., <span class=\"ltx_text ltx_font_italic\">Content Pass</span>) and three different spoken role-play dimensions (i.e., <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human-likeness</span>, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>).\nImplementation details, including template design, alternative prompting strategies, and annotation guidelines, are provided in the following subsections of this Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "appropriateness",
                    "pass",
                    "content",
                    "audio",
                    "dimensions",
                    "human",
                    "archetype",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, archetype evaluation emphasizes breadth and recognizability over subtlety.\nBy grounding role-play in socially recognizable categories and stereotypical situations, it provides a practical balance between scalability and interpretability, complementing the more fine-grained realism evaluation described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS2\" title=\"3.2 Role-play Evaluation with Two Strategies &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Excluded Dimensions.</span> We initially considered more fine-grained control dimensions, but excluded them to preserve the archetype-level focus. Examples of excluded dimensions include:\n<em class=\"ltx_emph ltx_font_italic\">Emotional State &amp; Expression, Single Emotion Prompts, Explicit User Input / Assistant Output structures, Emotional Transition / Fluctuation, Voice Style &amp; Prosody (pacing, intonation, vocal timbre, expressive style, accent/dialect), and Content / Interaction Patterns (task/topic controls, specific linguistic structures, or speech production challenges).</em></p>\n\n",
                "matched_terms": [
                    "content",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">These dimensions are more closely aligned with explicit instruction-following or style transfer rather than broad stereotype judgment, and they reduced clarity in human evaluations.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All steps are automatically processed by <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Comanici et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib12\" title=\"\">2025</a>)</cite>, but each output is manually verified to ensure high quality for downstream usage. We demonstrate an example for Roles: Occupation / Functional Role in Fictional or Fantasy as follows:</p>\n\n",
                "matched_terms": [
                    "all",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Collection for Proprietary Models.</span> For proprietary systems, we adopt two collection strategies. For GPT-4o-realtime, responses are obtained directly via the <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-realtime</span> API. For Doubao and GPT-4o (Advanced Voice Mode), we collect outputs by recording from their official web interfaces. All recordings are captured through the interaction device itself, ensuring that no additional processing or distortion is introduced into the final audio.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">End-to-End Model Generation.</span>\nFor end-to-end systems, the audio prompt is directly used as input. The system prompt is set to default values to ensure consistent speech generation quality.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2\" title=\"Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the internal annotation interface used for <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>.\nThe platform provides an integrated environment for both playback and evaluation of model-generated role-play speech samples.\nAnnotators first interact with the audio player interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf1\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>), which visualizes waveforms and spectrograms for multiple versions of the same prompt, enabling side-by-side comparison under a shared role instruction (e.g., firefighter, teacher, or actor).\nThen, they proceed to the annotation interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf2\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a>), where they assign scores for multiple dimensions such as <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, along with reasoning and confidence inputs.\nThis structured interface supports high-consistency, fine-grained evaluation of role-play behaviors in speech models.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "appropriateness",
                    "audio",
                    "dimensions",
                    "human",
                    "archetype",
                    "likeness",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This appendix contains the full human annotation instructions used for archetype evaluation.</p>\n\n",
                "matched_terms": [
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Audio Quality (No Context):</span> independence from content, focus on distortions or synthesis artifacts.</p>\n\n",
                "matched_terms": [
                    "content",
                    "audio",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human Likeness (No Context):</span> perceived naturalness and plausibility of human delivery.</p>\n\n",
                "matched_terms": [
                    "likeness",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Appropriateness (Context Considered):</span> tonal fit of the vocal performance to the described role and scene.</p>\n\n",
                "matched_terms": [
                    "appropriateness",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comments are mandatory for Appropriateness (all scores).</p>\n\n",
                "matched_terms": [
                    "all",
                    "appropriateness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Comments for Audio Quality and Human Likeness are only required for scores <math alttext=\"\\leq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I3.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8804;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\leq 3</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "likeness",
                    "human",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Foreign language content:</span> If <em class=\"ltx_emph ltx_font_italic\">any</em> part before 30s contains a foreign language (even a single word/short phrase) in otherwise English audio, reject the entire clip. \n<br class=\"ltx_break\"/><em class=\"ltx_emph ltx_font_italic\">Exception:</em> Extremely common/recognizable borrowed words (e.g., &#8220;croissant&#8221;) may pass. Niche items (e.g., &#8220;coq au vin&#8221;) should be rejected.</p>\n\n",
                "matched_terms": [
                    "content",
                    "audio",
                    "pass"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><em class=\"ltx_emph ltx_font_italic\">Example (pass):</em> Same prompt; audio says: &#8220;It looks like your stylist put you through a wood chipper.&#8221; <math alttext=\"\\Rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I5.i1.I0.i2.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8658;</mo><annotation encoding=\"application/x-tex\">\\Rightarrow</annotation></semantics></math> content pass = true.</p>\n\n",
                "matched_terms": [
                    "content",
                    "audio",
                    "pass"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><em class=\"ltx_emph ltx_font_italic\">Example (pass):</em> &#8220;Oh no. Are you hurt? Don&#8217;t worry. We&#8217;ll get you taken care of, little one.&#8221; <math alttext=\"\\Rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I5.i2.I0.i2.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8658;</mo><annotation encoding=\"application/x-tex\">\\Rightarrow</annotation></semantics></math> content pass = true.</p>\n\n",
                "matched_terms": [
                    "content",
                    "pass"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Important:</span> Once an audio receives content pass = false, <em class=\"ltx_emph ltx_font_italic\">do not</em> rate rubrics; proceed to the next clip.</p>\n\n",
                "matched_terms": [
                    "content",
                    "audio",
                    "pass"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rubric Rating Instructions (for content pass = true).</span>\nWe shift focus from <em class=\"ltx_emph ltx_font_italic\">what</em> was said to <em class=\"ltx_emph ltx_font_italic\">how</em> it was said (vocal performance). Use gut intuition guided by rubric definitions. Score each audio <em class=\"ltx_emph ltx_font_italic\">in isolation</em> (no ranking across clips).</p>\n\n",
                "matched_terms": [
                    "content",
                    "audio",
                    "pass",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">Audio Quality</span> &amp; <span class=\"ltx_text ltx_font_bold\">Human Likeness</span>: comment only if score <math alttext=\"\\leq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A5.I6.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8804;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\leq 3</annotation></semantics></math>; if 4 or 5, write &#8220;N/A&#8221;.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "likeness",
                    "human",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">Appropriateness</span>: comment for <em class=\"ltx_emph ltx_font_italic\">all</em> scores 1&#8211;5. Explain <em class=\"ltx_emph ltx_font_italic\">why</em> the <em class=\"ltx_emph ltx_font_italic\">delivery</em> was/was not a good match to the in-context traits (role &amp; scene).</p>\n\n",
                "matched_terms": [
                    "all",
                    "appropriateness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Evaluate the <em class=\"ltx_emph ltx_font_italic\">performance</em>, not the script content.</p>\n\n",
                "matched_terms": [
                    "content",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Pronunciation errors: penalize only on <span class=\"ltx_text ltx_font_bold\">Human Likeness</span> if they significantly affect comprehension; do not affect <span class=\"ltx_text ltx_font_bold\">Appropriateness</span>.</p>\n\n",
                "matched_terms": [
                    "appropriateness",
                    "likeness",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Independently</span> rate technical clarity (ignore content and fit). After scoring Audio Quality, <em class=\"ltx_emph ltx_font_italic\">disregard</em> its issues for the other two dimensions.</p>\n\n",
                "matched_terms": [
                    "content",
                    "audio",
                    "dimensions",
                    "other",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Rate how likely an average listener would believe the clip is human-delivered (ignore Audio Quality issues and contextual fit).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "average",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Rate <em class=\"ltx_emph ltx_font_italic\">only</em> how well the <em class=\"ltx_emph ltx_font_italic\">delivery</em> fits the role/scene described in the prompt (tonal fit). Ignore Audio Quality and Human Likeness issues while scoring this dimension.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "likeness",
                    "human",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realism Evaluation adopts a bottom&#8211;up perspective grounded in real human speech data, with the goal of capturing the nuanced delivery, prosody, and contextual fit that synthetic archetype prompts cannot fully represent.\nRather than relying on stereotypes, this strategy leverages real-world recordings and carefully constructed contrasts, enabling fine-grained assessment of model performance under realistic conditions.</p>\n\n",
                "matched_terms": [
                    "human",
                    "performance",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, real human ground-truth segments and systematically constructed negatives form a balanced dataset that supports realism-oriented evaluation.\nThe positives provide natural anchors of role-play performance, while the negatives introduce controlled mismatches that test whether evaluation models can distinguish real human speech from synthetic approximations.\nThis design ensures that realism evaluation emphasizes subtle paralinguistic and contextual cues, moving beyond intelligibility or content fidelity.</p>\n\n",
                "matched_terms": [
                    "content",
                    "roleplay",
                    "human",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Yunjin case study illustrates a fundamental difference between realism evaluation and archetype evaluation. Archetype prompts, as described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>, are constructed by pairing abstract role categories with synthetic contextual scenes. These cover a broad space of possible situations, ranging from modern occupations (e.g., firefighters) to fictional personas (e.g., cult leaders in martial arts fantasy) or social identities (e.g., overprotective parents, laid-back friends). While such prompts provide controlled and diverse coverage, they are ultimately template-based and rely on stereotypes to elicit role-play behaviors.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This distinction highlights the complementary strengths of the two evaluation paradigms. Archetype evaluation systematically explores a wide combinatorial space of roles and situations, but may lack the paralinguistic and contextual richness of real speech. Realism evaluation, on the other hand, anchors assessment in authentic delivery and contextual fit, providing a finer-grained lens on coherence and naturalness. Together, they ensure that role-play evaluation spans both broad stereotypical coverage and nuanced real-world plausibility.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "other",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">InhouseTTS3</span> (Embedding&#8211;AudioCode Pipeline).\nHere, the input is Qwen2.5 BPE embeddings, which a 0.4B GPT-2 model maps directly to 4,096-dimensional audio codes. These codes are decoded via a cascaded architecture: <span class=\"ltx_text ltx_font_typewriter\">RepCodec</span> decoder&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib32\" title=\"\">2024</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> content representation (with speaker embedding) <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DDIM</span> (denoising\ndiffusion implicit model) module&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jeong et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib34\" title=\"\">2021</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DAC</span> latent representation&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Kumar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib37\" title=\"\">2023</a>)</cite> <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A6.I5.i3.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> <span class=\"ltx_text ltx_font_typewriter\">DAC</span> decoder. Conditioning is provided through prefix text embeddings (e.g., instructions expressing emotional states such as &#8220;angry, hateful: you betrayed me&#8230;&#8221;), along with speaker ID. The input format is <span class=\"ltx_text ltx_font_typewriter\">[spk_id, BPE embedding, audio codes]</span>.</p>\n\n",
                "matched_terms": [
                    "content",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the audio&#8217;s speaker is <em class=\"ltx_emph ltx_font_italic\">clearly not</em> the intended target identity, assign a score of <span class=\"ltx_text ltx_font_bold\">1</span> to all Profile-dependent dimensions (e.g., Voice Identity Matching, Trait Embodiment) and proceed with other dimensions as applicable.</p>\n\n",
                "matched_terms": [
                    "all",
                    "other",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the prompt information is <span class=\"ltx_text ltx_font_bold\">contradictory</span> or <span class=\"ltx_text ltx_font_bold\">insufficient</span> to evaluate (e.g., audio too short, traits mutually incompatible), mark the prompt quality accordingly and <em class=\"ltx_emph ltx_font_italic\">do not</em> force granular ratings that cannot be judged.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Use both sources:</span> Play the audio and skim the transcript together to assess delivery quality, expression, and alignment with character intent (do not penalize transcript mismatches).</p>\n\n",
                "matched_terms": [
                    "audio",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Focus only on human speech:</span> Do not factor in background music/soundtracks, environmental or ambient noise, or non-speech audio events; these may reflect the scene but should not influence scoring of the spoken performance.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "human",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt is insufficient to evaluate the voice sample.</span> Audio too short to gauge the dimensions; or the prompt lacks specific characteristics/is too vague or multi-directional to know which version of the character is present. (It is OK if audio does not match transcript exactly; if too much is missing to annotate, mark as insufficient.)</p>\n\n",
                "matched_terms": [
                    "audio",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 2.1 Emotion Accuracy (Required; check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "audio",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All human annotation in this work was conducted in accordance with standard ethical practices for research involving human annotators. Annotators were recruited via public hiring for Mandarin and internal lab hiring for English. Prior to participation, all annotators were provided with a clear description of the task, the type of data they would encounter, and their rights as participants, including the ability to opt out at any time without penalty.</p>\n\n",
                "matched_terms": [
                    "all",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The archetype evaluation provides a detailed view of how speech foundation models (SFMs) perform when tasked with role-play scenarios grounded in stereotypical characters and situations. Human annotations cover four dimensions, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, and <span class=\"ltx_text ltx_font_italic\">Content Pass</span>, and the results reveal important trends, as well as challenges in interpreting scores across languages.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "appropriateness",
                    "pass",
                    "content",
                    "audio",
                    "dimensions",
                    "human",
                    "archetype",
                    "likeness",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A critical observation is that annotations in Mandarin and English cannot be directly compared. The annotation groups for the two languages differed in their linguistic backgrounds and cultural interpretations, which leads to systematic differences in scoring. Our bilingual pilot further confirmed that annotators often interpret role-play appropriateness and expressiveness differently depending on their cultural frame of reference. For example, Mandarin raters tend to reward delivery styles that align with culturally expected prosody and politeness strategies, while English raters prioritize spontaneity and naturalness. These discrepancies parallel the findings of the singing voice conversion challenge (SVCC2023) from <cite class=\"ltx_cite ltx_citemacro_citet\">Huang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib31\" title=\"\">2023</a>)</cite>, where native and non-native listeners provided significantly different ratings for the same samples, particularly for naturalness and speaker similarity. Consequently, while absolute scores differ substantially across Mandarin and English, only within-language comparisons are meaningful. Cross-lingual ranking is not valid given the different annotation bases.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "appropriateness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Within Mandarin from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, the relative rankings of models are clear. <span class=\"ltx_text ltx_font_typewriter\">Doubao</span> stands out as the strongest performer across all four dimensions, reaching near-ceiling levels in both <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> (4.37) and <span class=\"ltx_text ltx_font_italic\">Content Pass</span> (0.98). It also leads in role-related dimensions such as <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, indicating balanced competence across linguistic accuracy, delivery, and expressiveness. <span class=\"ltx_text ltx_font_typewriter\">G-CosyVoice2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-CosyVoice2</span> follow closely, especially in audio quality, though they lag behind in role appropriateness. <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> achieve mid-range results, reflecting higher variance across samples, while <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> shows competitive content understanding but lower naturalness and appropriateness. <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> consistently ranks the lowest, with large variance, underscoring instability and weaker alignment with archetypal role expectations.</p>\n\n",
                "matched_terms": [
                    "appropriateness",
                    "pass",
                    "content",
                    "audio",
                    "dimensions",
                    "all",
                    "human",
                    "accuracy",
                    "likeness",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The picture shifts in English, elaborated in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Here, <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> emerges as the most effective model by a wide margin, achieving top scores in both <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> (3.46) and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> (3.49), as well as high content pass rates (0.97). This reflects its strong grounding in English discourse norms and role adherence. In contrast, <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> variants perform well in <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> but are weaker in role alignment, highlighting a trade-off between acoustic fidelity and role-play embodiment. Other models such as <span class=\"ltx_text ltx_font_typewriter\">Doubao</span>, <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span>, and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> achieve moderate results, while <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> trail far behind, showing limited robustness in English scenarios. Taken together, the English results reveal both a sharper separation across models and a wider variance in appropriateness scores compared to Mandarin.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "appropriateness",
                    "pass",
                    "content",
                    "audio",
                    "other",
                    "human",
                    "likeness",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although cross-lingual comparisons are invalid, this does not diminish the utility of archetype evaluation for role-play benchmarking. In fact, the observed inconsistencies reinforce a central point: successful role-play requires models to adapt to the cultural and linguistic norms of the target language community. A system that performs well in Mandarin but poorly in English cannot be considered universally capable, since end users will evaluate appropriateness and naturalness according to their native expectations. Thus, archetype evaluation highlights not only model-specific strengths and weaknesses but also the need for culturally grounded evaluation pipelines.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "appropriateness",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another notable pattern is the trade-off between acoustic quality and role embodiment. CosyVoice2 variants achieve strong audio quality but weaker human likeness and appropriateness, whereas <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> prioritizes role coherence at the expense of audio fidelity. This suggests that future systems must better integrate high-quality synthesis with nuanced role adherence. Similarly, the consistently lower scores for imaginative and identity-driven roles underscore the limitations of current SFMs in capturing subtle sociocultural expectations and narrative context.</p>\n\n",
                "matched_terms": [
                    "appropriateness",
                    "audio",
                    "human",
                    "likeness",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although realism evaluation targets nuanced paralinguistic behaviors, human annotators achieve <em class=\"ltx_emph ltx_font_italic\">consistently high agreement</em> once the rubric disentangles the aspects to be judged.\nAs shown in the agreement summary in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F7\" title=\"Figure 7 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, most dimensions fall in the <math alttext=\"0.86\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>0.86</mn><annotation encoding=\"application/x-tex\">0.86</annotation></semantics></math>&#8211;<math alttext=\"0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mn>0.88</mn><annotation encoding=\"application/x-tex\">0.88</annotation></semantics></math> range, with the lowest agreement on <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> (<math alttext=\"\\approx 0.83\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.83</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.83</annotation></semantics></math>) and the highest on <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em> / <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> (<math alttext=\"\\approx 0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.88</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.88</annotation></semantics></math>).\nEmotion-related, progressive metrics (Intensity and Transition) are slightly lower (<math alttext=\"\\approx 0.85\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.85</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.85</annotation></semantics></math>), reflecting the added difficulty of judging graded expressivity.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F8\" title=\"Figure 8 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F9\" title=\"Figure 9 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> visualize the score distributions per dimension.\nWe highlight three patterns:\n(i) <span class=\"ltx_text ltx_font_bold\">Progressive/gated emotion metrics.</span> For <em class=\"ltx_emph ltx_font_italic\">Emotion Intensity</em> and <em class=\"ltx_emph ltx_font_italic\">Emotion Transition</em>, we assign <span class=\"ltx_text ltx_font_typewriter\">1</span> when the scene is marked <em class=\"ltx_emph ltx_font_italic\">N/A</em> (i.e., emotion accuracy is insufficient to meaningfully judge intensity/transition). This produces a visible mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, a sparse bin at <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>, and slightly lower overall agreement; downstream analyses conditioning on adequate <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> (e.g., mean&#160;<math alttext=\"\\geq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 3</annotation></semantics></math>) recover more bell-shaped distributions.\n(ii) <span class=\"ltx_text ltx_font_bold\">Traits Embodiment.</span> Scores tend to avoid the middle (<math alttext=\"\\sim\\!3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!3</annotation></semantics></math>), concentrating on low values when traits clearly mismatch and high values when traits are strongly realized. This yields the lowest inter-rater agreement among dimensions, reflecting the inherently compositional and role-specific nature of trait judgments.\n(iii) <span class=\"ltx_text ltx_font_bold\">Other dimensions.</span> <em class=\"ltx_emph ltx_font_italic\">Pitch Variation</em>, <em class=\"ltx_emph ltx_font_italic\">Rhythmic Naturalness</em>, <em class=\"ltx_emph ltx_font_italic\">Stress &amp; Emphasis</em>, <em class=\"ltx_emph ltx_font_italic\">Voice Identity Matching</em>, <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em>, <em class=\"ltx_emph ltx_font_italic\">Global Story Coherence</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> are approximately unimodal with modes at <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>&#8211;<math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math>, consistent with moderate&#8211;strong realization in the dataset and with the high agreement reported.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "other",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, the consistently high agreement confirms that the rubric isolates perceptually coherent factors, so learned evaluators can target them reliably.\nSecond, the gated behavior of <em class=\"ltx_emph ltx_font_italic\">Intensity/Transition</em> motivates a <em class=\"ltx_emph ltx_font_italic\">hierarchical</em> evaluator: verify <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> before scoring graded expressivity, or model the joint structure with conditional heads.\nThird, <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> remains challenging; models may benefit from explicit character-trait priors, long-term context aggregation, or contrastive supervision between matched and mismatched traits.\nFinally, prosody-focused objectives (e.g., pitch/rhythm/stress control) are likely to yield the largest quality gains for amateur-style speech.</p>\n\n",
                "matched_terms": [
                    "evaluator",
                    "quality",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For both zeroshot and fewshot experiments, we adopt a streamlined version of the annotation guidelines to ensure consistency with the human annotation process. The exact rubric prompts for archetype and realism evaluation used are provided below:</p>\n\n",
                "matched_terms": [
                    "human",
                    "archetype"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and fewshot settings, we condition the model with the rubric prompt and then append few-shot demonstration triplets (input, audio, gold JSON) to mirror the human annotation protocol. As shown in Listing&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#LST2\" title=\"Listing 2 &#8227; L.1 Zeroshot/Fewshot Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, We use <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> for these few-shot exemplars and require a minified JSON output consistent with the rubric schema.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and few-shot experiments on commercial models such as <span class=\"ltx_text ltx_font_typewriter\">Gemini</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o</span>, we report results as the average of five independent runs to reduce randomness and obtain more stable predictions. In contrast, for open-source models, we observed that their instruction-following ability is relatively weak. To mitigate this, we prompt them to predict one dimension at a time rather than all dimensions jointly. Specifically, for each metric, after showing the rubric prompt, we request completion in a fixed template and extract the probability distribution over the discrete tokens <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>. The final score is then computed from these probabilities, which we found makes open-source models usable to some extent under this setting.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "report",
                    "average",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For few-shot experiments, we randomly sample three exemplars from the training set, with each exemplar chosen such that its human scores are close to the rounded average across all annotators. These exemplars are appended as demonstrations in the prompt. As with the zeroshot case, we run five trials for commercial models and report the averaged results for evaluation.</p>\n\n",
                "matched_terms": [
                    "all",
                    "report",
                    "average",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Effect of repeated runs.\nWhen comparing single-run decoding (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot-1run</span>) against averaging over five runs (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot</span>), we see consistent gains across nearly all dimensions (average score improves from 0.353 to 0.390). This suggests that sampling variance plays a substantial role in the reliability of large language model (LLM) evaluators, and averaging multiple outputs reduces this variance, producing more stable correlation with human annotations.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "average",
                    "dimensions",
                    "all",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Scaling the number of demonstrations.\nIncreasing the few-shot set size from three to six and nine further boosts performance (0.459 and 0.469, respectively). The improvements are most pronounced in prosodic and emotional categories (e.g., <em class=\"ltx_emph ltx_font_italic\">Pitch</em>, <em class=\"ltx_emph ltx_font_italic\">Accuracy</em>, <em class=\"ltx_emph ltx_font_italic\">Intensity</em>), suggesting that more examples provide stronger guidance in modeling subtle paralinguistic cues. Gains in <em class=\"ltx_emph ltx_font_italic\">Character Consistency</em> and <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance</em> are also noticeable, although smaller in magnitude, which may indicate that these higher-level judgments are less sensitive to prompt scaling once a few demonstrations are provided.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In general, the results demonstrate two complementary strategies for stabilizing LLM-based evaluators: (i) averaging multiple runs to reduce randomness, and (ii) enriching prompts with more few-shot exemplars to improve adherence to rubric-based evaluation. Combining both leads to the strongest overall performance, with the nine-shot condition achieving the highest average correlation&#160;(0.469).</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "average",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the archetype test set, all fine-tuned variants achieve strong <span class=\"ltx_text ltx_font_italic\">Content Pass</span> accuracy above 92%. Differences appear mainly in correlation-based metrics. Here, <span class=\"ltx_text ltx_font_typewriter\">Base-FT</span> yields the highest score on <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> delivers the strongest performance in <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> and the best overall average (0.629). This suggests that instruction-tuned checkpoints paired with parameter-efficient adaptation can better capture higher-level perceptual traits.</p>\n\n",
                "matched_terms": [
                    "appropriateness",
                    "quality",
                    "pass",
                    "content",
                    "audio",
                    "average",
                    "baseft",
                    "all",
                    "instructlora",
                    "human",
                    "archetype",
                    "accuracy",
                    "likeness",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For realism evaluation on the basic test set, <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> consistently achieves the strongest correlations across most dimensions, especially for <span class=\"ltx_text ltx_font_italic\">Character Identity</span> and <span class=\"ltx_text ltx_font_italic\">Traits</span>, leading to the best average correlation (0.668). <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> also performs competitively (0.630), while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> lags slightly behind (0.625). These results highlight that full fine-tuning may still offer an advantage when the evaluation set is relatively homogeneous and closer to the training distribution.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "baselora",
                    "average",
                    "dimensions",
                    "instructft",
                    "instructlora"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A different pattern emerges on the real recording test set. Both <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> and <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> struggle, yielding negative or near-zero correlations across most dimensions. In contrast, <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> shows clear improvements, achieving positive correlations in all prosodic and emotional dimensions and producing the highest overall average (0.247). This indicates that LoRA adaptation on top of an instruction-tuned backbone provides better generalization to out-of-distribution, real-world data.</p>\n\n",
                "matched_terms": [
                    "baselora",
                    "average",
                    "dimensions",
                    "instructft",
                    "all",
                    "instructlora"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> is competitive on in-domain evaluation, it suffers a substantial drop when applied to real recordings. By contrast, <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> offers a more robust balance: it maintains reasonable performance on archetype and basic sets, while significantly outperforming other settings on real-world data. Based on this robustness, we adopt <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> as our final configuration for subsequent experiments.</p>\n\n",
                "matched_terms": [
                    "instructft",
                    "instructlora",
                    "other",
                    "archetype",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the comprehensive design of Speech-DRAME, several limitations remain.\nFirst, while our dual evaluation paradigm combines top-down archetype assessment with bottom-up realism evaluation, both strategies are constrained by the quality and scope of available data. Archetype evaluation inevitably relies on simplified stereotypes, which can overlook subtler paralinguistic and contextual cues. Realism evaluation, though richer, still struggles with domain mismatch between synthetic data and real human recordings, as evidenced by the performance drop observed in the real-recording test set. This highlights the persistent gap between benchmark-driven evaluation and real-world variability in speech role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "performance",
                    "human",
                    "archetype",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, although we validated DRAME-Eval against human annotations, alignment under realism settings remains modest. This suggests that even human-annotated benchmarks are insufficiently fine-grained to capture the full complexity of human perception in speech role-play. Richer annotation protocols, more diverse speaker populations, and integration of perceptual psychology insights could further improve evaluation fidelity.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "human"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Looking forward, we see several promising research directions. We intentionally restrict evaluation to single-turn responses for clarity and tractability. However, multi-turn extensions that capture narrative progression, long-term consistency, and dynamic emotion shifts remain a natural next step. Incorporating multilingual and cross-cultural datasets will enhance generality and fairness. On the modeling side, exploring hybrid evaluation strategies that combine discrete rubrics with continuous perceptual signals (e.g., prosody embeddings or affective measures) may better bridge the gap between automatic and human judgments. Finally, integrating Speech-DRAME into reinforcement learning pipelines as a reward model could help drive the next generation of expressive, human-aligned speech role-play systems.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "human"
                ]
            }
        ]
    },
    "A12.T14": {
        "caption": "Table 14: Realism role-play evaluator performance (basic test set). We report Pearson correlation coefficients for other dimensions. The Avg. column is the average of all coefficients.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" rowspan=\"2\">Config.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" colspan=\"3\">Prosodic Dynamics</td>\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" colspan=\"3\">Emotional Expressiveness</td>\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" colspan=\"2\">Character Consistency</td>\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" colspan=\"2\">Contextual Relevance</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\">Avg.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Pitch</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Rynthm</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Stress</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Accuracy</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Intensity</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Transition</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Identity</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Traits</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Local</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Global</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">Base-FT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.643</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.633</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.655</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.657</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.600</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.599</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.689</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.668</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.611</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.707</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.646</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.628</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.633</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.643</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.637</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.586</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.590</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.668</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.651</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.578</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.684</td>\n<td class=\"ltx_td ltx_align_center\">0.630</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.657</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.655</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.675</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.674</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.627</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.626</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.752</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.685</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.615</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.714</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.668</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.621</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.627</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.631</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.632</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.596</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.601</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.660</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.655</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.563</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.668</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.625</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "roleplay",
            "column",
            "pitch",
            "identity",
            "consistency",
            "pearson",
            "basic",
            "prosodic",
            "instructft",
            "dynamics",
            "config",
            "realism",
            "coefficients",
            "evaluator",
            "average",
            "emotional",
            "baseft",
            "character",
            "rynthm",
            "stress",
            "test",
            "accuracy",
            "performance",
            "avg",
            "relevance",
            "report",
            "intensity",
            "transition",
            "traits",
            "set",
            "global",
            "correlation",
            "baselora",
            "local",
            "contextual",
            "dimensions",
            "all",
            "other",
            "instructlora",
            "expressiveness"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.T13\" title=\"Table 13 &#8227; Overall summary. &#8227; L.4 Fine-tuning Ablation Experiments &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.T14\" title=\"Table 14 &#8227; Overall summary. &#8227; L.4 Fine-tuning Ablation Experiments &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.T15\" title=\"Table 15 &#8227; Overall summary. &#8227; L.4 Fine-tuning Ablation Experiments &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">15</span></a> present ablation studies across different fine-tuning strategies, including the use of <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio-7B-Base</span> or <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio-7B-Instruct</span>; and the use of fully fine-tuning or LoRA fine-tuning. We use <span class=\"ltx_text ltx_font_typewriter\">Base-FT</span>, <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span>, <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span>, and <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> to represents different configuration settings in following experiments.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Role-play has become a key testbed for generative models, expanding from text-only dialogue to multimodal interaction. Extending role-play to speech captures prosody, emotion, and delivery, but also poses new evaluation challenges. Current pipelines often use audio large language models (ALLMs) as zero-shot judges, which miss paralinguistic cues, collapse multiple aspects into coarse scores, and rely on synthetic speech references that fail to reflect real-world roles. We present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework that contributes at three levels: (i) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-EvalBench</em>, an evaluation benchmark with bilingual human-annotated data and protocols for training and testing speech evaluation models&#160;(SEMs), (ii) <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, a fine-tuned evaluation model, which substantially outperforms zero-shot and few-shot ALLMs, and (iii) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-RoleBench</em>, a speech role-play benchmark that leverages <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> as an automatic judge to compare speech foundation models (SFMs). Speech-DRAME distinguishes between two complementary evaluation strategies: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>, a top-down approach measuring adherence to broad role archetypes, and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em>, a bottom-up approach grounded in real human speech that emphasizes nuanced role quality. Compared to zero-shot ALLM judges, <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves stronger agreement with human ratings (Pearson correlation from 0.480 to 0.629 in archetypes, and 0.390 to 0.625 in realism). By integrating transparent benchmark resources, modeling approaches, and system-level evaluation, Speech-DRAME provides the first comprehensive, reproducible foundation for assessing spoken role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "correlation",
                    "realism",
                    "pearson"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Role-play has been recently recognized a cornerstone of interactive systems, from initial text-based focuses to the growing ecosystem of multimodal dialogue agents&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib6\" title=\"\">2024a</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib59\" title=\"\">2024</a>; Dai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib13\" title=\"\">2025</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib43\" title=\"\">2024</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>)</cite>. With the rapid advancement of large language models (LLMs), role-play has evolved beyond scripted interactions into dynamic character-driven conversations, supporting applications in education, entertainment, and human&#8211;AI collaboration&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib69\" title=\"\">2024</a>; Agatsuma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib1\" title=\"\">2024</a>; Fung &amp; Laing, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib20\" title=\"\">2024</a>)</cite>. Extending role-play to speech-based interaction offers an even richer communication channel, capturing prosody, emotion, and character consistency in ways that text alone cannot&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "consistency",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">it lacks sensitivity to paralinguistic cues such as intonation, pacing, and emotional dynamics that define role delivery&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>; Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib81\" title=\"\">2025a</a>)</cite>,</p>\n\n",
                "matched_terms": [
                    "dynamics",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">it collapses diverse aspects of performance into limited coarse dimensions, obscuring critical dimensions of role-play quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite>, and</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dimensions",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome these limitations, we present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span> (Speech Detailed Role-play Assessment with Modeling and Evaluation), a unified framework that moves beyond synthetic or judge-only pipelines. Speech-DRAME is distinguished by three aspects: [i] a dual evaluation paradigm that balances archetype-driven generality with human-grounded realism (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS2\" title=\"3.2 Role-play Evaluation with Two Strategies &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), [ii] human annotations that provide perceptual grounding absent in prior resources (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4\" title=\"4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), and [iii] evaluation-model alignment that enables not only benchmarking of speech foundation models (SFMs) but also systematic analysis of the evaluation models (SEMs) themselves (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5\" title=\"5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). These aspects establish Speech-DRAME as a comprehensive and trustworthy framework for measuring and improving speech role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, Speech-DRAME contributes at three levels: (i) <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>, an evaluation benchmark consisting of human-annotated datasets for archetype and realism role-play; (ii) <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span>, a fine-tuned SEM trained on EvalBench that surpasses zero-shot and few-shot ALLMs; and (iii) <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span>, a speech role-play benchmark that leverages DRAME-Eval as an automatic judge to systematically compare SFMs. By combining dual evaluation strategies, human-anchored supervision, and evaluation-model alignment, Speech-DRAME delivers systematic insights into both role-play systems and evaluation models themselves.\nOur contributions are four-fold:</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a <span class=\"ltx_text ltx_font_bold\">formal task definition</span> of speech role-play and its evaluation via speech foundation models (SFMs) for generation and speech evaluation models (SEMs) for assessment, unifying linguistic and paralinguistic dimensions.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-based role-play evaluation.</span>\nEarly research focused on text-only dialogue benchmarks. Datasets such as RoleLLM, CharacterBox, and RoleMRC <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib62\" title=\"\">2024</a>; Tu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib60\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib61\" title=\"\">2025a</a>; Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib53\" title=\"\">2023</a>; Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib41\" title=\"\">2025</a>)</cite> test whether models can sustain persona consistency and character fidelity, using LLM-as-judge pipelines for scalability. However, studies like PersonaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib81\" title=\"\">2025a</a>)</cite> have shown that LLM-as-judge approaches are far from perfect, struggling with tasks as basic as tracking speaker identity. This highlights broader concerns about reliability even in text settings, which become only more severe when extended to speech, where prosody, emotion, and vocal style play a central role.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "identity",
                    "consistency",
                    "character",
                    "basic",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-native benchmarks.</span>\nMore recently, a number of resources have been introduced to evaluate spoken role-play agents. OmniCharacter-10K <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>)</cite> emphasizes character voice traits, while others&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib39\" title=\"\">2025</a>; Hou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib27\" title=\"\">2025</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib15\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib7\" title=\"\">2024b</a>)</cite> extend evaluation toward vocal communication, assistant capability, and multi-turn realism. Omni models such as StepAudio and KimiAudio <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite> also include role-play scenarios in their evaluation. SpeechRole and VStyle <cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>; Zhan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib76\" title=\"\">2025</a>)</cite> are closer to our work, offering datasets that test interaction ability, expressiveness, and fidelity. However, these resources are primarily built from LLM-generated dialogues and TTS outputs, with limited human validation and a heavy reliance on proprietary ALLM-as-judge pipelines. As a result, they lack grounding in diverse human speech and typically apply only coarse rubrics.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "character",
                    "expressiveness",
                    "test",
                    "traits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Expressive TTS evaluation.</span>\nRecent work has explored evaluation of instruction-following and style-controlled TTS. InstructTTSEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite> explicitly includes role-play scenarios, while EmergentTTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">(Manku et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib45\" title=\"\">2025</a>)</cite> and discrete speech LM benchmarks <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang &amp; Sz&#233;kely, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib63\" title=\"\">2024</a>)</cite> emphasize prosody, spontaneity, and speaker consistency. These efforts push beyond intelligibility and naturalness, but still center on synthetic outputs rather than real human role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "consistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our distinction.</span>\nIn contrast, Speech-DRAME unifies these considerations through three elements: (i) a dual evaluation paradigm that combines archetype-driven generality with realism grounded in authentic human speech, (ii) human annotations that provide a reliable perceptual gold standard, and (iii) alignment of SEMs to human ratings, enabling systematic evaluation of both SFMs and the evaluation models themselves. This combination moves beyond synthetic benchmarks or zero-shot judging to provide a comprehensive and trustworthy framework for speech role-play assessment.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech role-play.</span>\nWe formalize <em class=\"ltx_emph ltx_font_italic\">speech role-play</em> as a conditional speech generation problem grounded in a character profile and scene description.\nGiven a dialogue context <math alttext=\"{\\bm{C}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{speech}}</annotation></semantics></math>, a character profile <math alttext=\"{\\bm{C}}_{\\text{profile}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>profile</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{profile}}</annotation></semantics></math>, and a scene specification <math alttext=\"{\\bm{C}}_{\\text{scene}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>scene</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{scene}}</annotation></semantics></math>, a <em class=\"ltx_emph ltx_font_italic\">speech foundation model</em> (SFM) generates a role-played speech response <math alttext=\"{\\bm{S}}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119930;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{S}}_{r}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>). The first strategy follows a <em class=\"ltx_emph ltx_font_italic\">top-down</em> design, inspired by prior text-based role-play benchmarks.\nHere, role-play is judged with respect to <em class=\"ltx_emph ltx_font_italic\">stereotypical archetypes</em> (e.g., &#8220;firefighter&#8221; or &#8220;ER doctor&#8221;), using scene contexts (e.g., &#8220;comfort a child trapped in fire&#8221; or &#8220;ask for help in a serious operation&#8221;).\nThis approach provides accessible, general-purpose scoring that is scalable to a wide range of scenarios, making it suitable for large-scale benchmarking.\nHowever, its reliance on broad stereotypes and global impressions limits its ability to capture fine-grained aspects of delivery (e.g., subtle prosodic shifts, nuanced emotional control)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Holt &amp; Lotto, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib26\" title=\"\">2010</a>)</cite>.\nIn practice, this strategy mirrors simplified annotation settings and is particularly useful when annotation resources are limited&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib21\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "roleplay",
                    "emotional",
                    "prosodic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS5\" title=\"F.5 Examples of Realism Role-play &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.5</span></a>). The second strategy adopts a <em class=\"ltx_emph ltx_font_italic\">bottom-up</em> design, motivated by real-world performance and real human speech.\nInstead of relying solely on stereotypes, this approach grounds evaluation in recordings from professional and non-professional speakers, drawn from realistic dialogue and media sources.\nJudgments in this setting are more fine-grained and multi-dimensional, addressing aspects such as prosodic dynamics, emotional fidelity, character consistency, and contextual fit.\nWhile this strategy requires more human annotation and careful curation, it provides benchmarks that reflect how role-play is perceived in practice.\nIt thus offers a richer framework for analyzing model outputs in settings where realism and subtlety are essential.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "contextual",
                    "consistency",
                    "emotional",
                    "character",
                    "prosodic",
                    "dynamics",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human Annotation.</span>\nAll collected audio undergoes a standardized annotation protocol: (1) an initial semantically-based screening to exclude invalid clips; (2) rubric-based scoring across multiple perceptual dimensions; and (3) meta-annotations such as confidence or prompt-quality judgments. Full rubrics are provided in the appendices&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A5\" title=\"Appendix E Annotation Guidelines for Archetype Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">E</span></a> &amp; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>All data collection follows in high standard ethical consideration as outlined in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A9\" title=\"Appendix I Annotation Ethics &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "all",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Archetype Data Collection &#8227; 4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents a summary of the curated <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> dataset, where we further elaborate the details of archetype and realism evaluation data collection in following subsections, respectively. For benchmark purpose, we provide an official train-test split for the human annotated dataset for <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> so that following researchers can use the dataset to train and evaluate their SEMs for speech role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Role and Scene Dimensions.</span>\nRoles are operationalized through seven categories: (1) functional occupations in daily life, (2) functional occupations in fictional or fantasy settings, (3) social identities, (4) named or specific characters, (5) personality traits, (6) relative relationship, (7) basic events. Scenes are anchored by everyday events or state changes that evoke stereotypical reactions. Details of template design and alternative prompting strategies are given in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>We note that some archetypes (e.g., firefighter) inevitably reflect cultural stereotypes. These are used solely for evaluation scalability and do not reflect normative judgments.</span></span></span></p>\n\n",
                "matched_terms": [
                    "basic",
                    "dimensions",
                    "traits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realism evaluation adopts a bottom&#8211;up perspective, emphasizing authentic delivery, prosody, and contextual fit.</p>\n\n",
                "matched_terms": [
                    "contextual",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Sources and Negative Samples.</span>\nWe curate speech from (i) ground-truth media segments, (ii) retrieved character profiles, and (iii) generated local scene descriptions. To stress-test evaluation models, we construct contrastive mismatches: (i) negative local scenes with varying degrees of mismatch, and (ii) negative character profiles created by re-synthesizing transcripts with TTS systems. Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6\" title=\"Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F</span></a> details the prompt design and TTS generation details.</p>\n\n",
                "matched_terms": [
                    "local",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotators judge whether performances align with the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em>, using rubrics for <em class=\"ltx_emph ltx_font_italic\">Prosodic Dynamics (i.e., Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis)</em>, <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness (with progressive gating over three sub-dimensions: Emotion Accuracy, Emotion Intensity, Emotion Transition)</em>, <em class=\"ltx_emph ltx_font_italic\">Character Consistency (i.e., Voice Identity Matching and Traits Embodiment)</em>, <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance (i.e., Local Scene Fit and Global Story Coherence)</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>. A prompt quality check and confidence ratings are also collected. Full scales and gating details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "pitch",
                    "identity",
                    "relevance",
                    "local",
                    "consistency",
                    "emotional",
                    "contextual",
                    "character",
                    "accuracy",
                    "prosodic",
                    "intensity",
                    "transition",
                    "stress",
                    "dimensions",
                    "expressiveness",
                    "traits",
                    "dynamics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real-world Recoding.</span>\nAs noted in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S2\" title=\"2 Related Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, most prior efforts have relied heavily on synthetic data, mostly from TTS systems based on curated professional role-play voices. While such data can achieve high fidelity, it often lacks the diversity and spontaneity of real-world delivery, limiting the generalizability of evaluation models. To address this gap, we expand realism evaluation beyond synthetic and curated voices by incorporating newly recorded speech from both professional and amateur voice actors. This design introduces a broader spectrum of delivery styles, from polished professional acting to more natural and less controlled amateur performance.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation Data Summary.</span>\nThe resulting realism evaluation dataset is constructed from a combination of curated media sources, including internal collections, public datasets (e.g., NCSSD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib40\" title=\"\">2024b</a>)</cite>), and original recordings collected for this project.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>All data were obtained under appropriate licenses and are legally available for research release.</span></span></span> We compile 15k annotated role-play samples: 9k in Mandarin and 5k in English. The dataset is divided into 12k samples for training, 2k mixed samples for the base test set, and 1k real-recording samples for the test set. The first test set supports general evaluation using our proposed pipeline with negative samples, while the second focuses exclusively on human speech performed by both professional and amateur voice actors. For benchmarking, 2,000 scenarios from the realism dataset are designated for realism role-play benchmarking. Detailed statistics and analyses are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11\" title=\"Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">K</span></a>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "roleplay",
                    "realism",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-shot and Few-shot ALLMs.</span>\nWe first evaluate the zero-shot capabilities of both proprietary and public models, including <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>, <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>, <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio</span>, and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>. All evaluations follow a unified prompting template that provides audio input alongside scene and character context.</p>\n\n",
                "matched_terms": [
                    "all",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For proprietary models, we report the average of five independent runs to reduce variance and obtain more stable predictions. For open-source models, we observed weaker instruction-following ability and limited adaptation to unfamiliar tasks. To mitigate this, we did not request simultaneous prediction of multiple metrics. Instead, we queried one metric at a time. Following <cite class=\"ltx_cite ltx_citemacro_citet\">Song &amp; Bahri (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>); Lukasik et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>, we interpret model outputs through token distribution over discrete labels (<span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score.</p>\n\n",
                "matched_terms": [
                    "report",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For few-shot experiments, we focus primarily on proprietary ALLMs, which demonstrate stronger instruction-following capabilities and have been widely used as judges in prior work. For each test query, we provide three additional annotated samples from the training set as in-context exemplars.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>Please find our detailed settings such as prompts, decoding algorithms in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS1\" title=\"L.1 Zeroshot/Fewshot Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.1</span></a> with additional ablation analysis in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS2\" title=\"L.2 Zeroshot/Fewshot Ablation Experiments &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.2</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "set",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Finetuning Model: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>.</span>\nIn addition to zero-shot and few-shot evaluations, we fine-tune a dedicated evaluation model, denoted as <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> for archetype and realism role-play evaluation, respectively.\nWe initialize from <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio-7B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib11\" title=\"\">2024</a>)</cite>, which provides a strong foundation for multi-modal audio-text reasoning.\nTo better leverage the multiple human annotations available in our datasets, we adopt a sampling-based training strategy: at each step, one annotator label is randomly selected, following practices in label distribution learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Geng, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib22\" title=\"\">2016</a>; Wen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib67\" title=\"\">2023</a>)</cite>.\nThis stochastic supervision implicitly exposes the model to the empirical annotation distribution, while during inference we output the full distribution over discrete labels (e.g., <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score, consistent with recent decoding-based regression methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Song &amp; Bahri, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>; Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib70\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib64\" title=\"\">2025b</a>; Lukasik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>. We apply LoRA fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib28\" title=\"\">2022</a>)</cite>. All implementation details, including optimizer settings, training schedule, and sampling policy, are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS3\" title=\"L.3 Fine-tuning Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics for Role-play Evaluation Model.</span> For most dimensions with 1-5 scale, we report Pearson correlation. For <span class=\"ltx_text ltx_font_italic\">Content Pass</span> estimation, we report accuracy.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "correlation",
                    "report",
                    "pearson",
                    "dimensions",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype Evaluation.</span>\nThe results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T2\" title=\"Table 2 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reveal clear distinctions across evaluation settings. In the zero-shot condition, proprietary ALLMs such as <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> substantially outperform public models, achieving strong <span class=\"ltx_text ltx_font_italic\">content pass</span> accuracy (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>90%) and moderate correlations with other dimensions(<math alttext=\"0.43\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mn>0.43</mn><annotation encoding=\"application/x-tex\">0.43</annotation></semantics></math>&#8211;<math alttext=\"0.48\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mn>0.48</mn><annotation encoding=\"application/x-tex\">0.48</annotation></semantics></math>). By contrast, public models exhibit limited predictive ability, with correlations often below 0.2. Few-shot prompting provides modest gains for proprietary models, improving both accuracy and correlation (e.g., <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> rises from 0.480 to 0.493 average correlation). The most significant improvement arises from finetuning: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves an average correlation of 0.629, marking a large margin over zero-shot/few-shot models and demonstrating the effectiveness of task-specific supervision.</p>\n\n",
                "matched_terms": [
                    "other",
                    "accuracy",
                    "correlation",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T3\" title=\"Table 3 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> extends the analysis to fine-grained realism evaluation. Similar to archetype results, proprietary ALLMs lead the zero-shot setting, with <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> obtaining an average correlation of 0.390, compared to 0.284 for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>. Public models remain weak baselines. Few-shot prompting consistently improves performance: <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> gains +0.042 and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> gains +0.083 over their zero-shot averages, showing the benefit of in-context supervision. Once again, finetuning is most effective: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves 0.625 average correlation, higher than the best few-shot results. Importantly, the gains are consistent across all 10 realism dimensions, confirming that the sampling&#8211;expectation training strategy generalizes beyond simple classification to nuanced paralinguistic cues.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "realism",
                    "average",
                    "dimensions",
                    "all",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real Recording Test Set.</span>\nThe real-recording evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T4\" title=\"Table 4 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> is notably more challenging, with all models suffering substantial performance drops. Proprietary models in the zero-shot setting achieve only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.04&#8211;0.08 average correlation, and public models often fall below zero. Few-shot prompting yields small but noticeable improvements, especially for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> (0.226 avg.), suggesting that exemplars partially mitigate domain mismatch between synthetic and real human speech. Interestingly, while finetuning achieves the highest overall average (0.247), its relative advantage over few-shot prompting is narrower than in the synthetic setting, and some dimensions (e.g., <span class=\"ltx_text ltx_font_italic\">Character Identity</span>) remain weak. We emphasize that this observed gap between synthetic and real recordings is not merely an artifact but an intended feature of Speech-DRAME, designed to expose the mismatch and drive future research into bridging synthetic-to-real generalization.</p>\n\n",
                "matched_terms": [
                    "set",
                    "correlation",
                    "avg",
                    "identity",
                    "average",
                    "character",
                    "dimensions",
                    "all",
                    "test",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall.</span> Across both archetype and realism evaluations, we observe three consistent trends: (i) zero-shot performance is limited, particularly for open-source models, (ii) few-shot prompting offers moderate but consistent gains, especially for proprietary ALLMs, and (iii) finetuning with task-specific supervision (<span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>) delivers the largest improvements, achieving state-of-the-art correlations across nearly all dimensions. Nevertheless, results on the real-recording test set underscore the gap between synthetic and real human role-play scenarios, motivating future work in data diversification and domain adaptation.</p>\n\n",
                "matched_terms": [
                    "set",
                    "roleplay",
                    "realism",
                    "dimensions",
                    "all",
                    "test",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on fine-tuned <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, we establish the DRAME-RoleBench to evaluate SFMs&#8217; role play ability. For archetype evaluation, 1,250 scenarios are constructed following the same data construction pipeline as the <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>. For realism evaluation, we reuse 2,000 scenarios in the <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> test set. We follow the same prompt strategy as outlined in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "realism",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype evaluation.</span>\nOn archetype role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T5\" title=\"Table 5 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), cascaded pipelines consistently outperform end-to-end systems. The strongest cascaded systems, particularly <span class=\"ltx_text ltx_font_typewriter\">G-IndexTTS2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-IndexTTS2</span>, achieve both high content pass rates (above 94%) and balanced scores across audio quality, human likeness, and appropriateness. By contrast, end-to-end models such as <span class=\"ltx_text ltx_font_typewriter\">StepAudio2Mini</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> perform competitively in terms of content coverage (above 88%), but their expressive scores remain slightly lower than their cascaded counterparts. This gap suggests that separating reasoning (LLM) from speech rendering (TTS) remains advantageous when archetype fidelity is emphasized. Nevertheless, the fact that top end-to-end systems already surpass 3.0 in average rating indicates rapid closing of the performance gap.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "average",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism evaluation.</span>\nRealism role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T6\" title=\"Table 6 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) presents a different landscape. Here, performance differences between end-to-end and cascaded systems are less pronounced. Several end-to-end models, such as <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>, reach average scores of 3.24, matching or even exceeding the best cascaded pipelines. Importantly, realism scores are more evenly distributed across prosodic, emotional, character, and contextual dimensions, with most models clustering between 2.9&#8211;3.2. This convergence reflects the difficulty of realism-style evaluation: since the data preparation emphasized contrast with authentic human speech, models are judged under similar conditions, leading to smaller observed gaps.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "contextual",
                    "average",
                    "emotional",
                    "character",
                    "prosodic",
                    "dimensions",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human alignment.</span>\nWe further conducted small-scale human studies with 10% of the benchmark data to verify the reliability of the benchmark. Archetype evaluation achieves a Spearman correlation of 0.706 between human judgments and <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, validating that the automatic evaluator closely tracks human preferences in this setting. Realism evaluation, however, achieves a more modest Spearman correlation of 0.375. This lower alignment is expected, since realism data construction did not prioritize fine-grained model separation but instead emphasized distinguishing human versus synthetic speech.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>We view this as evidence that realism presents a harder, less-saturated frontier. Rather than a limitation of DRAME-Eval, this highlights the challenge of designing evaluators that capture nuanced human perception, which motivates future work.</span></span></span> As a result, all models occupy a relatively narrow performance band, reducing sensitivity to model-level differences. We view this as evidence that realism evaluation highlights a harder, less saturated frontier where both SFMs and SEMs require further advances.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "realism",
                    "evaluator",
                    "all",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, cascaded pipelines retain an edge in archetype scenarios, especially in content robustness and expressive fidelity, but end-to-end models are catching up quickly. In realism scenarios, the field remains unsolved: existing systems exhibit similar performance bands and evaluators show weaker alignment with human judgments, underscoring the need for improved realism-grounded data and evaluation. Together, these findings establish <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> as both a reliable testbed for archetype fidelity and a challenging benchmark for realism fidelity.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework for assessing spoken role-play through two complementary lenses: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em> (top-down) and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> (bottom-up, human-grounded). Our human-annotated <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> enables training and testing of SEMs, and our fine-tuned <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span> substantially outperforms zero-/few-shot ALLM judges. Building on this, <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> provides system-level comparison across various SFMs, revealing a consistent edge for cascaded pipelines on archetypes and a tighter spread under realism, where real human performance evaluation remain challenging. Together, these resources establish a reproducible, analyzable foundation for spoken role-play evaluation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>Limitations and future works are discussed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A13\" title=\"Appendix M Limitations and Future Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">M</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All data collection and annotation in this work followed strict ethical standards. Our bilingual datasets were constructed exclusively from legally accessible sources, including licensed media, open-source corpora, and original recordings conducted with informed consent. Annotators were recruited with appropriate compensation and provided with clear instructions, including the right to withdraw at any point. To minimize risks, no personally identifiable information was included in the released data, and all recordings were anonymized before annotation. We ensured cultural and linguistic diversity in annotator pools to mitigate potential bias across Mandarin and English role-play scenarios. While this study did not require formal IRB approval, we adhered to best practices in human-subject research, including transparency in task design and careful consideration of annotator well-being. Full details of our annotation protocol and ethical safeguards are presented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A9\" title=\"Appendix I Annotation Ethics &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments were conducted on widely available GPUs (NVIDIA H100). Random seeds are fixed across all runs to ensure consistency. By releasing datasets, models, and code, we aim to make all major results in this paper fully reproducible and facilitate future extensions of our framework.</p>\n\n",
                "matched_terms": [
                    "all",
                    "consistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data generation:</span> LLMs were employed to expand prompts (e.g., character profiles, scene descriptions) and to produce negative samples in realism evaluation. All outputs were screened by human annotators to ensure validity and reduce bias.</p>\n\n",
                "matched_terms": [
                    "all",
                    "realism",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our distinction between <span class=\"ltx_text ltx_font_italic\">Archetype Evaluation</span> and <span class=\"ltx_text ltx_font_italic\">Realism Evaluation</span> is not merely pragmatic, but is grounded in established theories of social identity, communication, and performance. The two perspectives capture complementary dimensions of how humans perceive and evaluate roles in interaction.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "dimensions",
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetypes as Collective Schemas</span>\nThe notion of archetypes originates from Jungian psychology, where they represent recurrent, culturally embedded character prototypes in the collective unconscious&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jung, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib36\" title=\"\">2014</a>)</cite>. In media and performance studies, archetypes have been shown to guide audience expectations, providing top-down interpretive frames for evaluating whether a character &#8220;fits&#8221; within familiar categories such as hero, mentor, or trickster&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Campbell, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib4\" title=\"\">2008</a>)</cite>. Archetype-based reasoning allows for scalable, general-purpose judgments because it leverages culturally shared schemas rather than idiosyncratic details.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism as Situated Performance</span>\nIn contrast, realism highlights the fine-grained enactment of roles in situated interaction. Goffman&#8217;s dramaturgical sociology emphasizes that everyday social life is itself a performance, where believability depends on subtle paralinguistic and pragmatic cues such as prosody, hesitation, and emotional shading&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goffman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib23\" title=\"\">1949</a>)</cite>. Similarly, interactional sociolinguistics has demonstrated how micro-level features like intonation, pacing, and turn-taking patterns shape the perception of authenticity in communication&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gumperz, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib25\" title=\"\">1982</a>)</cite>. Realism Evaluation thus anchors role-play assessment in bottom-up sensitivity to authentic delivery, beyond schematic expectations.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "emotional",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Complementarity of the Two Perspectives</span>\nPerformance theory provides a unifying lens for these dual dimensions. Schechner&#8217;s view of performance as both &#8220;restored behavior&#8221; (scripted, repeatable structures) and &#8220;living process&#8221; (situated enactment)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Schechner, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib52\" title=\"\">2017</a>)</cite> underscores the need to consider both archetypal scripts and enacted realism. Likewise, sociolinguistic theories of identity recognize both <span class=\"ltx_text ltx_font_italic\">ascribed categories</span> (archetypes imposed by social convention) and <span class=\"ltx_text ltx_font_italic\">performed identities</span> (emergent in interaction)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bucholtz &amp; Hall, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib2\" title=\"\">2005</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "dimensions",
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p4.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> may be a simple mean, a recency-weighted average, or a learned temporal model that captures consistency across turns.\nThe choice of <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p4.m3\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> reflects the intended evaluation focus, e.g., stability of persona traits, adaptation to scene progression, or coherence across multiple speakers.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "average",
                    "consistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Discussion.</span>\nWhile our released dataset and experiments concentrate on single-turn evaluation, the modular nature of the formulation ensures that extending to multi-turn is straightforward.\nFuture work can leverage <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p5.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> to study richer aspects of speech role-play, such as persona persistence, interaction dynamics, and role consistency across extended dialogues.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dynamics",
                    "consistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Role dimensions.</span>\nWe operationalize archetypes through five principal categories, each representing a stereotypical dimension of role identity or behavior, including (1) <span class=\"ltx_text ltx_font_bold\">Occupation / Functional Role (Modern Daily Life)</span> (e.g., a firefighter rescuing a trapped civilian). (2) <span class=\"ltx_text ltx_font_bold\">Occupation / Functional Role (Fictional or Fantasy)</span> (e.g., a martial arts cult leader confronting orthodox rivals). (3)&#160;<span class=\"ltx_text ltx_font_bold\">Social Identity</span> (e.g., a doting parent worrying about a child, or a &#8220;Buddhist youth&#8221; eschewing competition). (4) <span class=\"ltx_text ltx_font_bold\">Named / Specific Characters</span> (e.g., Sun Wukong mocking celestial authorities in <em class=\"ltx_emph ltx_font_italic\">Journey to the West</em>). (5) <span class=\"ltx_text ltx_font_bold\">Personality Traits</span> (e.g., &#8220;hot-blooded&#8221; in a competitive setting, or &#8220;complaining&#8221; in response to poor service). (6) <span class=\"ltx_text ltx_font_bold\">Relative Roles</span> (e.g., parent or stranger).</p>\n\n",
                "matched_terms": [
                    "identity",
                    "dimensions",
                    "traits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scene dimensions.</span>\nIn addition to roles, we incorporate contextual triggers that anchor role-play in stereotypical situations.\nA central focus is on <span class=\"ltx_text ltx_font_bold\">everyday external events or state changes</span> (e.g., receiving a promotion, being caught in gossip), which provide concrete scenarios for testing archetypical responses.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "contextual",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Exclusion of fine-grained controls.</span>\nWe deliberately excluded explicit emotional states, prosodic instructions, or linguistic constraints.\nThese dimensions are better suited to realism-oriented or style-controlled evaluation, whereas archetype evaluation benefits from concrete, stereotype-level scenarios that sharpen judgments and improve human agreement.</p>\n\n",
                "matched_terms": [
                    "prosodic",
                    "emotional",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Pipeline overview.</span>\nAt a high level, archetype evaluation data are generated through a three-stage pipeline:\n(1) selecting keywords that specify role and scene;\n(2) expanding them into full prompts via structured templates; and\n(3) rendering prompts into speech with contemporary TTS systems.\nThis modular pipeline ensures both scalability and reproducibility, while keeping prompts vivid enough for reliable judgments. (4) Based on the generated data (paired scene and synthesized speech), expert human annotators are recruited for detailed annotation on semantic check (i.e., <span class=\"ltx_text ltx_font_italic\">Content Pass</span>) and three different spoken role-play dimensions (i.e., <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human-likeness</span>, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>).\nImplementation details, including template design, alternative prompting strategies, and annotation guidelines, are provided in the following subsections of this Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, archetype evaluation emphasizes breadth and recognizability over subtlety.\nBy grounding role-play in socially recognizable categories and stereotypical situations, it provides a practical balance between scalability and interpretability, complementing the more fine-grained realism evaluation described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS2\" title=\"3.2 Role-play Evaluation with Two Strategies &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Excluded Dimensions.</span> We initially considered more fine-grained control dimensions, but excluded them to preserve the archetype-level focus. Examples of excluded dimensions include:\n<em class=\"ltx_emph ltx_font_italic\">Emotional State &amp; Expression, Single Emotion Prompts, Explicit User Input / Assistant Output structures, Emotional Transition / Fluctuation, Voice Style &amp; Prosody (pacing, intonation, vocal timbre, expressive style, accent/dialect), and Content / Interaction Patterns (task/topic controls, specific linguistic structures, or speech production challenges).</em></p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "transition",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2\" title=\"Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the internal annotation interface used for <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>.\nThe platform provides an integrated environment for both playback and evaluation of model-generated role-play speech samples.\nAnnotators first interact with the audio player interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf1\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>), which visualizes waveforms and spectrograms for multiple versions of the same prompt, enabling side-by-side comparison under a shared role instruction (e.g., firefighter, teacher, or actor).\nThen, they proceed to the annotation interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf2\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a>), where they assign scores for multiple dimensions such as <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, along with reasoning and confidence inputs.\nThis structured interface supports high-consistency, fine-grained evaluation of role-play behaviors in speech models.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">Appropriateness</span>: comment for <em class=\"ltx_emph ltx_font_italic\">all</em> scores 1&#8211;5. Explain <em class=\"ltx_emph ltx_font_italic\">why</em> the <em class=\"ltx_emph ltx_font_italic\">delivery</em> was/was not a good match to the in-context traits (role &amp; scene).</p>\n\n",
                "matched_terms": [
                    "traits",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Independently</span> rate technical clarity (ignore content and fit). After scoring Audio Quality, <em class=\"ltx_emph ltx_font_italic\">disregard</em> its issues for the other two dimensions.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Rate how likely an average listener would believe the clip is human-delivered (ignore Audio Quality issues and contextual fit).</p>\n\n",
                "matched_terms": [
                    "contextual",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realism Evaluation adopts a bottom&#8211;up perspective grounded in real human speech data, with the goal of capturing the nuanced delivery, prosody, and contextual fit that synthetic archetype prompts cannot fully represent.\nRather than relying on stereotypes, this strategy leverages real-world recordings and carefully constructed contrasts, enabling fine-grained assessment of model performance under realistic conditions.</p>\n\n",
                "matched_terms": [
                    "contextual",
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data from real human speech sources.</span>\nWe construct the realism evaluation set primarily from natural speech material:</p>\n\n",
                "matched_terms": [
                    "set",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generated local scenes.</span>\nWe supplement retrieved character information with automatically generated local scene descriptions summarizing the surrounding plot or setting.\nAppendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS3\" title=\"F.3 Prompts for Local Scene Generation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.3</span></a> includes representative prompts and generation details.</p>\n\n",
                "matched_terms": [
                    "local",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, real human ground-truth segments and systematically constructed negatives form a balanced dataset that supports realism-oriented evaluation.\nThe positives provide natural anchors of role-play performance, while the negatives introduce controlled mismatches that test whether evaluation models can distinguish real human speech from synthetic approximations.\nThis design ensures that realism evaluation emphasizes subtle paralinguistic and contextual cues, moving beyond intelligibility or content fidelity.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "contextual",
                    "test",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During annotation, raters are presented with either: (i) ground-truth stories, (ii) stories with negative local scenes, or (iii) stories with negative character profile. Annotators assign realism scores without being exposed to the underlying mismatch reasons. Instead, these reasons are provided only to the quality-assurance team, who use them as reference for internal validation and consistency checks. While the mismatch reasons may not be perfect, they help streamline quality control procedures and maintain reliability across the dataset.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "local",
                    "consistency",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To make the construction of the realism evaluation task concrete, we present a worked example that begins with a curated local scene and character profile, followed by the same transcript adapted into mismatched contexts. These mismatches appear either at the <em class=\"ltx_emph ltx_font_italic\">local-scene</em> level or at both the <em class=\"ltx_emph ltx_font_italic\">character profile and scene</em> level, thereby providing controlled degradations of realism.</p>\n\n",
                "matched_terms": [
                    "local",
                    "realism",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the character Yunjin from <span class=\"ltx_text ltx_font_italic\">Genshin Impact</span> as a running case study, which is also included in our released dataset.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>As acknowledged, the use of real-world recordings is conducted under legal and ethical review. All curated data will be released as part of our open-source package.</span></span></span> This example illustrates how alignment between a speaker&#8217;s persona, local context, and delivery underpins realism, and how systematically breaking that alignment produces contrastive test cases.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "local",
                    "character",
                    "all",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Curated Local Scene.</span>\nYunjin is narrating a fierce battle through operatic singing. Her verses depict a hero charging into enemy lines, slaying the enemy commander, and securing peace. The atmosphere is filled with tension and intensity, highlighting the hero&#8217;s extraordinary bravery and determination.</p>\n\n",
                "matched_terms": [
                    "local",
                    "intensity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative Local Scenes.</span>\nTo stress realism evaluation, we insert the same transcript into alternative backgrounds. These reveal how context-shifted delivery degrades narrative coherence:</p>\n\n",
                "matched_terms": [
                    "realism",
                    "stress",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative Character Profiles.</span>\nWe replace Yunjin with new character profiles and scenes. Although the transcript remains the same, realism degrades because the delivery no longer matches the speaker&#8217;s persona or situation.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Yunjin case study illustrates a fundamental difference between realism evaluation and archetype evaluation. Archetype prompts, as described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>, are constructed by pairing abstract role categories with synthetic contextual scenes. These cover a broad space of possible situations, ranging from modern occupations (e.g., firefighters) to fictional personas (e.g., cult leaders in martial arts fantasy) or social identities (e.g., overprotective parents, laid-back friends). While such prompts provide controlled and diverse coverage, they are ultimately template-based and rely on stereotypes to elicit role-play behaviors.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "contextual",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the Yunjin example is drawn from authentic performance, where the character&#8217;s persona (an opera singer) and local scene (a battlefield narration) are naturally aligned through artistic delivery. The realism task does not merely test whether a model can follow a prompt, but whether it can preserve coherence between a speaker&#8217;s global identity, immediate context, and expressive prosody. When Yunjin&#8217;s transcript is transplanted into mismatched settings or paired with incongruent speaker profiles, the breakdown in realism becomes apparent: the same words shift from inspiring to awkward, jarring, or even absurd.</p>\n\n",
                "matched_terms": [
                    "global",
                    "realism",
                    "local",
                    "identity",
                    "test",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This distinction highlights the complementary strengths of the two evaluation paradigms. Archetype evaluation systematically explores a wide combinatorial space of roles and situations, but may lack the paralinguistic and contextual richness of real speech. Realism evaluation, on the other hand, anchors assessment in authentic delivery and contextual fit, providing a finer-grained lens on coherence and naturalness. Together, they ensure that role-play evaluation spans both broad stereotypical coverage and nuanced real-world plausibility.</p>\n\n",
                "matched_terms": [
                    "other",
                    "roleplay",
                    "contextual",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed a diverse set of TTS systems to generate negative character profiles and resynthesized speech for Realism Evaluation.\nThese included both commercial and non-commercial models, with variation in style control, conditioning prompts, and reference usage.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.T8\" title=\"Table 8 &#8227; Same-Speaker Voice Clone. &#8227; F.6 TTS Systems and Settings &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> summarizes the systems and settings.\nBelow, we detail the major strategies used across systems:</p>\n\n",
                "matched_terms": [
                    "set",
                    "realism",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We leveraged all publicly available speaker style information released by the corresponding websites.\nFor each input sample, we employed <span class=\"ltx_text ltx_font_typewriter\">deepseek-v3</span> to select the best-matching speaker based on the original speech caption.\nIn the case of <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5TTS</span>, the character profile was also appended as an additional instruction to guide synthesis.</p>\n\n",
                "matched_terms": [
                    "all",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The annotation interface for <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> focuses on assessing contextual and narrative coherence in model-generated speech.\nUnlike the archetype-oriented evaluation, which emphasizes stylistic and emotional fidelity, this platform examines how well the voice performance aligns with a character&#8217;s personality, dialogue situation, and scene progression.</p>\n\n",
                "matched_terms": [
                    "contextual",
                    "realism",
                    "emotional",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.F3\" title=\"Figure 3 &#8227; F.7 Annotation Platform for Realism Evaluation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the core playback interface.\nAnnotators can preview each audio sample with synchronized spectrogram and waveform displays, while reviewing contextual metadata including the <span class=\"ltx_text ltx_font_italic\">Local Scene</span>, <span class=\"ltx_text ltx_font_italic\">Character Style</span>, and <span class=\"ltx_text ltx_font_italic\">Character Profile</span>.\nThese descriptions help situate the listening task, providing the background necessary for judging realism in the character&#8217;s vocal and behavioral expression.</p>\n\n",
                "matched_terms": [
                    "local",
                    "realism",
                    "contextual",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Read the <span class=\"ltx_text ltx_font_bold\">Character Profile</span> (long-term identity, style) and <span class=\"ltx_text ltx_font_bold\">Local Scene</span> (immediate context, goal/emotion) before listening.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the audio&#8217;s speaker is <em class=\"ltx_emph ltx_font_italic\">clearly not</em> the intended target identity, assign a score of <span class=\"ltx_text ltx_font_bold\">1</span> to all Profile-dependent dimensions (e.g., Voice Identity Matching, Trait Embodiment) and proceed with other dimensions as applicable.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "all",
                    "other",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Detailed case examples and progressive gating for emotional dimensions are provided in the full instruction document.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt five dimensions, each rated from 1 (worst) to 5 (best). Emotional Expressiveness uses <em class=\"ltx_emph ltx_font_italic\">progressive gating</em>.</p>\n\n",
                "matched_terms": [
                    "expressiveness",
                    "dimensions",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prosodic Dynamics</span> (Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis): naturalness of patterns and rhythms.</p>\n\n",
                "matched_terms": [
                    "prosodic",
                    "dynamics",
                    "stress",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotional Expressiveness</span> (Progressive gating):</p>\n\n",
                "matched_terms": [
                    "expressiveness",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Character Consistency</span> (Voice Identity Matching, Trait Embodiment): alignment with Character Profile.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "consistency",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contextual Relevance</span> (Local Scene Fit, Global Story Coherence): fit to immediate scene and long-term identity.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "identity",
                    "relevance",
                    "contextual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Read first:</span> Carefully read the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em> to understand the long-term identity and immediate goal/emotion.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identity mismatch:</span> If the clip&#8217;s speaker is clearly not the intended target speaker, assign a score of <span class=\"ltx_text ltx_font_bold\">1</span> for every rating dimension that relies on the Character Profile (e.g., Character Style Traits, Trait Embodiment, Character Consistency).</p>\n\n",
                "matched_terms": [
                    "identity",
                    "consistency",
                    "traits",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt contains contradictory information.</span> Conflicts make accurate annotation difficult or impossible. Examples: incompatible accent specifications; Local Scene drastically at odds with intended emotion (see <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>); Local and global scenes inconsistent; traits described as &#8220;evolving over time&#8221; without a timeline.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "global",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt is insufficient to evaluate the voice sample.</span> Audio too short to gauge the dimensions; or the prompt lacks specific characteristics/is too vague or multi-directional to know which version of the character is present. (It is OK if audio does not match transcript exactly; if too much is missing to annotate, mark as insufficient.)</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Checkbox 1: Multi-Speaker Influence.</span> Check <em class=\"ltx_emph ltx_font_italic\">Yes</em> if other speakers were present and made it harder to isolate/judge the target character (check <em class=\"ltx_emph ltx_font_italic\">Yes</em> if uncertain). Otherwise check <em class=\"ltx_emph ltx_font_italic\">No</em>.</p>\n\n",
                "matched_terms": [
                    "other",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Question 2: Observed Character Style Traits.</span> Based on the Character Profile, are the defined Style Traits clearly present in the delivery?</p>\n\n",
                "matched_terms": [
                    "traits",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rubric Rating Instructions.</span>\nScore each subcriterion on a 1&#8211;5 scale. Use the detailed rubrics below and the examples to maintain calibration across characters, tones, and emotional styles. Prioritize in-character believability; do not reward theatricality, emotional intensity, or technical polish unless they naturally fit the character <em class=\"ltx_emph ltx_font_italic\">and</em> scene.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "intensity",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Focus: pitch, rhythm, and emphasis in speech.\n<span class=\"ltx_text ltx_font_bold\">Guideline 1.1 Pitch Variation (check Local Scene, Character Profile, and audio).</span></p>\n\n",
                "matched_terms": [
                    "character",
                    "local",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 1.2 Rhythmic Naturalness (check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "character",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 1.3 Stress and Emphasis (check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "character",
                    "stress",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 2.1 Emotion Accuracy (Required; check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "character",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 2.2 Emotion Intensity Control (Only if 2.1 <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"A7.SS4.SSSx5.p3.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math> 3; check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "character",
                    "intensity",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 2.3 Dynamic Range (Only if 2.2 <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"A7.SS4.SSSx5.p4.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math> 3; check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "character",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 3.1 Voice Identity Matching (Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 4.2 Global Story (Character Profile) Coherence (check Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "global",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Progressive gating for emotion.</span>\nOur <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness</em> uses gated sub-scores (accuracy <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A8.SS2.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> intensity control <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A8.SS2.p3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> dynamic range), preventing spurious high scores when base emotion is misidentified.\nThis reduces label noise and improves SEM alignment stability.</p>\n\n",
                "matched_terms": [
                    "expressiveness",
                    "accuracy",
                    "emotional",
                    "intensity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The archetype evaluation provides a detailed view of how speech foundation models (SFMs) perform when tasked with role-play scenarios grounded in stereotypical characters and situations. Human annotations cover four dimensions, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, and <span class=\"ltx_text ltx_font_italic\">Content Pass</span>, and the results reveal important trends, as well as challenges in interpreting scores across languages.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A critical observation is that annotations in Mandarin and English cannot be directly compared. The annotation groups for the two languages differed in their linguistic backgrounds and cultural interpretations, which leads to systematic differences in scoring. Our bilingual pilot further confirmed that annotators often interpret role-play appropriateness and expressiveness differently depending on their cultural frame of reference. For example, Mandarin raters tend to reward delivery styles that align with culturally expected prosody and politeness strategies, while English raters prioritize spontaneity and naturalness. These discrepancies parallel the findings of the singing voice conversion challenge (SVCC2023) from <cite class=\"ltx_cite ltx_citemacro_citet\">Huang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib31\" title=\"\">2023</a>)</cite>, where native and non-native listeners provided significantly different ratings for the same samples, particularly for naturalness and speaker similarity. Consequently, while absolute scores differ substantially across Mandarin and English, only within-language comparisons are meaningful. Cross-lingual ranking is not valid given the different annotation bases.</p>\n\n",
                "matched_terms": [
                    "expressiveness",
                    "roleplay"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Within Mandarin from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, the relative rankings of models are clear. <span class=\"ltx_text ltx_font_typewriter\">Doubao</span> stands out as the strongest performer across all four dimensions, reaching near-ceiling levels in both <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> (4.37) and <span class=\"ltx_text ltx_font_italic\">Content Pass</span> (0.98). It also leads in role-related dimensions such as <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, indicating balanced competence across linguistic accuracy, delivery, and expressiveness. <span class=\"ltx_text ltx_font_typewriter\">G-CosyVoice2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-CosyVoice2</span> follow closely, especially in audio quality, though they lag behind in role appropriateness. <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> achieve mid-range results, reflecting higher variance across samples, while <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> shows competitive content understanding but lower naturalness and appropriateness. <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> consistently ranks the lowest, with large variance, underscoring instability and weaker alignment with archetypal role expectations.</p>\n\n",
                "matched_terms": [
                    "expressiveness",
                    "accuracy",
                    "all",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The picture shifts in English, elaborated in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Here, <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> emerges as the most effective model by a wide margin, achieving top scores in both <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> (3.46) and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> (3.49), as well as high content pass rates (0.97). This reflects its strong grounding in English discourse norms and role adherence. In contrast, <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> variants perform well in <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> but are weaker in role alignment, highlighting a trade-off between acoustic fidelity and role-play embodiment. Other models such as <span class=\"ltx_text ltx_font_typewriter\">Doubao</span>, <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span>, and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> achieve moderate results, while <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> trail far behind, showing limited robustness in English scenarios. Taken together, the English results reveal both a sharper separation across models and a wider variance in appropriateness scores compared to Mandarin.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although realism evaluation targets nuanced paralinguistic behaviors, human annotators achieve <em class=\"ltx_emph ltx_font_italic\">consistently high agreement</em> once the rubric disentangles the aspects to be judged.\nAs shown in the agreement summary in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F7\" title=\"Figure 7 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, most dimensions fall in the <math alttext=\"0.86\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>0.86</mn><annotation encoding=\"application/x-tex\">0.86</annotation></semantics></math>&#8211;<math alttext=\"0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mn>0.88</mn><annotation encoding=\"application/x-tex\">0.88</annotation></semantics></math> range, with the lowest agreement on <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> (<math alttext=\"\\approx 0.83\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.83</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.83</annotation></semantics></math>) and the highest on <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em> / <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> (<math alttext=\"\\approx 0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.88</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.88</annotation></semantics></math>).\nEmotion-related, progressive metrics (Intensity and Transition) are slightly lower (<math alttext=\"\\approx 0.85\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.85</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.85</annotation></semantics></math>), reflecting the added difficulty of judging graded expressivity.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "local",
                    "intensity",
                    "dimensions",
                    "transition",
                    "traits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F8\" title=\"Figure 8 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F9\" title=\"Figure 9 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> visualize the score distributions per dimension.\nWe highlight three patterns:\n(i) <span class=\"ltx_text ltx_font_bold\">Progressive/gated emotion metrics.</span> For <em class=\"ltx_emph ltx_font_italic\">Emotion Intensity</em> and <em class=\"ltx_emph ltx_font_italic\">Emotion Transition</em>, we assign <span class=\"ltx_text ltx_font_typewriter\">1</span> when the scene is marked <em class=\"ltx_emph ltx_font_italic\">N/A</em> (i.e., emotion accuracy is insufficient to meaningfully judge intensity/transition). This produces a visible mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, a sparse bin at <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>, and slightly lower overall agreement; downstream analyses conditioning on adequate <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> (e.g., mean&#160;<math alttext=\"\\geq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 3</annotation></semantics></math>) recover more bell-shaped distributions.\n(ii) <span class=\"ltx_text ltx_font_bold\">Traits Embodiment.</span> Scores tend to avoid the middle (<math alttext=\"\\sim\\!3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!3</annotation></semantics></math>), concentrating on low values when traits clearly mismatch and high values when traits are strongly realized. This yields the lowest inter-rater agreement among dimensions, reflecting the inherently compositional and role-specific nature of trait judgments.\n(iii) <span class=\"ltx_text ltx_font_bold\">Other dimensions.</span> <em class=\"ltx_emph ltx_font_italic\">Pitch Variation</em>, <em class=\"ltx_emph ltx_font_italic\">Rhythmic Naturalness</em>, <em class=\"ltx_emph ltx_font_italic\">Stress &amp; Emphasis</em>, <em class=\"ltx_emph ltx_font_italic\">Voice Identity Matching</em>, <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em>, <em class=\"ltx_emph ltx_font_italic\">Global Story Coherence</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> are approximately unimodal with modes at <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>&#8211;<math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math>, consistent with moderate&#8211;strong realization in the dataset and with the high agreement reported.</p>\n\n",
                "matched_terms": [
                    "global",
                    "pitch",
                    "identity",
                    "local",
                    "intensity",
                    "dimensions",
                    "transition",
                    "other",
                    "stress",
                    "traits",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prosody control (Pitch, Rhythm, Stress).</span> Professional speech is shifted toward higher bins with tighter spread, indicating steadier control and fewer low-end tails.</p>\n\n",
                "matched_terms": [
                    "stress",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotion rendering (Accuracy, Intensity, Transition).</span> Professionals show higher centers and reduced mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.I1.i2.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> (fewer gated <span class=\"ltx_text ltx_font_typewriter\">N/A</span>s), suggesting more consistent emotional grounding and smoother dynamics; amateurs exhibit wider variance and more mass at the extremes.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "intensity",
                    "accuracy",
                    "transition",
                    "dynamics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identity and traits (Voice Identity, Traits Embodiment).</span> Professionals present clearer peaks in upper bins, while amateurs are broader or bimodal, reflecting inconsistent character anchoring.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "traits",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scene and semantics (Local Scene Fit, Global Coherence, Semantic Match).</span> Differences are present but smaller; both groups cluster in the mid&#8211;high range, with professionals exhibiting slightly narrower distributions.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, the consistently high agreement confirms that the rubric isolates perceptually coherent factors, so learned evaluators can target them reliably.\nSecond, the gated behavior of <em class=\"ltx_emph ltx_font_italic\">Intensity/Transition</em> motivates a <em class=\"ltx_emph ltx_font_italic\">hierarchical</em> evaluator: verify <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> before scoring graded expressivity, or model the joint structure with conditional heads.\nThird, <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> remains challenging; models may benefit from explicit character-trait priors, long-term context aggregation, or contrastive supervision between matched and mismatched traits.\nFinally, prosody-focused objectives (e.g., pitch/rhythm/stress control) are likely to yield the largest quality gains for amateur-style speech.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "evaluator",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For both zeroshot and fewshot experiments, we adopt a streamlined version of the annotation guidelines to ensure consistency with the human annotation process. The exact rubric prompts for archetype and realism evaluation used are provided below:</p>\n\n",
                "matched_terms": [
                    "realism",
                    "consistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and few-shot experiments on commercial models such as <span class=\"ltx_text ltx_font_typewriter\">Gemini</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o</span>, we report results as the average of five independent runs to reduce randomness and obtain more stable predictions. In contrast, for open-source models, we observed that their instruction-following ability is relatively weak. To mitigate this, we prompt them to predict one dimension at a time rather than all dimensions jointly. Specifically, for each metric, after showing the rubric prompt, we request completion in a fixed template and extract the probability distribution over the discrete tokens <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>. The final score is then computed from these probabilities, which we found makes open-source models usable to some extent under this setting.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "report",
                    "average",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For few-shot experiments, we randomly sample three exemplars from the training set, with each exemplar chosen such that its human scores are close to the rounded average across all annotators. These exemplars are appended as demonstrations in the prompt. As with the zeroshot case, we run five trials for commercial models and report the averaged results for evaluation.</p>\n\n",
                "matched_terms": [
                    "set",
                    "all",
                    "report",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.T12\" title=\"Table 12 &#8227; L.2 Zeroshot/Fewshot Ablation Experiments &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> shows how different prompting strategies affect realism evaluation performance with <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>. Several observations can be drawn:</p>\n\n",
                "matched_terms": [
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Effect of repeated runs.\nWhen comparing single-run decoding (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot-1run</span>) against averaging over five runs (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot</span>), we see consistent gains across nearly all dimensions (average score improves from 0.353 to 0.390). This suggests that sampling variance plays a substantial role in the reliability of large language model (LLM) evaluators, and averaging multiple outputs reduces this variance, producing more stable correlation with human annotations.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "correlation",
                    "average",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Scaling the number of demonstrations.\nIncreasing the few-shot set size from three to six and nine further boosts performance (0.459 and 0.469, respectively). The improvements are most pronounced in prosodic and emotional categories (e.g., <em class=\"ltx_emph ltx_font_italic\">Pitch</em>, <em class=\"ltx_emph ltx_font_italic\">Accuracy</em>, <em class=\"ltx_emph ltx_font_italic\">Intensity</em>), suggesting that more examples provide stronger guidance in modeling subtle paralinguistic cues. Gains in <em class=\"ltx_emph ltx_font_italic\">Character Consistency</em> and <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance</em> are also noticeable, although smaller in magnitude, which may indicate that these higher-level judgments are less sensitive to prompt scaling once a few demonstrations are provided.</p>\n\n",
                "matched_terms": [
                    "set",
                    "pitch",
                    "relevance",
                    "contextual",
                    "consistency",
                    "emotional",
                    "character",
                    "intensity",
                    "prosodic",
                    "accuracy",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In general, the results demonstrate two complementary strategies for stabilizing LLM-based evaluators: (i) averaging multiple runs to reduce randomness, and (ii) enriching prompts with more few-shot exemplars to improve adherence to rubric-based evaluation. Combining both leads to the strongest overall performance, with the nine-shot condition achieving the highest average correlation&#160;(0.469).</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "average",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the archetype test set, all fine-tuned variants achieve strong <span class=\"ltx_text ltx_font_italic\">Content Pass</span> accuracy above 92%. Differences appear mainly in correlation-based metrics. Here, <span class=\"ltx_text ltx_font_typewriter\">Base-FT</span> yields the highest score on <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> delivers the strongest performance in <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> and the best overall average (0.629). This suggests that instruction-tuned checkpoints paired with parameter-efficient adaptation can better capture higher-level perceptual traits.</p>\n\n",
                "matched_terms": [
                    "set",
                    "baseft",
                    "average",
                    "all",
                    "instructlora",
                    "test",
                    "traits",
                    "accuracy",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For realism evaluation on the basic test set, <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> consistently achieves the strongest correlations across most dimensions, especially for <span class=\"ltx_text ltx_font_italic\">Character Identity</span> and <span class=\"ltx_text ltx_font_italic\">Traits</span>, leading to the best average correlation (0.668). <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> also performs competitively (0.630), while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> lags slightly behind (0.625). These results highlight that full fine-tuning may still offer an advantage when the evaluation set is relatively homogeneous and closer to the training distribution.</p>\n\n",
                "matched_terms": [
                    "set",
                    "correlation",
                    "realism",
                    "baselora",
                    "identity",
                    "average",
                    "character",
                    "basic",
                    "dimensions",
                    "instructft",
                    "instructlora",
                    "test",
                    "traits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A different pattern emerges on the real recording test set. Both <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> and <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> struggle, yielding negative or near-zero correlations across most dimensions. In contrast, <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> shows clear improvements, achieving positive correlations in all prosodic and emotional dimensions and producing the highest overall average (0.247). This indicates that LoRA adaptation on top of an instruction-tuned backbone provides better generalization to out-of-distribution, real-world data.</p>\n\n",
                "matched_terms": [
                    "set",
                    "baselora",
                    "average",
                    "emotional",
                    "prosodic",
                    "instructft",
                    "dimensions",
                    "all",
                    "instructlora",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> is competitive on in-domain evaluation, it suffers a substantial drop when applied to real recordings. By contrast, <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> offers a more robust balance: it maintains reasonable performance on archetype and basic sets, while significantly outperforming other settings on real-world data. Based on this robustness, we adopt <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> as our final configuration for subsequent experiments.</p>\n\n",
                "matched_terms": [
                    "basic",
                    "instructft",
                    "instructlora",
                    "other",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the comprehensive design of Speech-DRAME, several limitations remain.\nFirst, while our dual evaluation paradigm combines top-down archetype assessment with bottom-up realism evaluation, both strategies are constrained by the quality and scope of available data. Archetype evaluation inevitably relies on simplified stereotypes, which can overlook subtler paralinguistic and contextual cues. Realism evaluation, though richer, still struggles with domain mismatch between synthetic data and real human recordings, as evidenced by the performance drop observed in the real-recording test set. This highlights the persistent gap between benchmark-driven evaluation and real-world variability in speech role-play.</p>\n\n",
                "matched_terms": [
                    "set",
                    "roleplay",
                    "realism",
                    "contextual",
                    "test",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, although we validated DRAME-Eval against human annotations, alignment under realism settings remains modest. This suggests that even human-annotated benchmarks are insufficiently fine-grained to capture the full complexity of human perception in speech role-play. Richer annotation protocols, more diverse speaker populations, and integration of perceptual psychology insights could further improve evaluation fidelity.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Looking forward, we see several promising research directions. We intentionally restrict evaluation to single-turn responses for clarity and tractability. However, multi-turn extensions that capture narrative progression, long-term consistency, and dynamic emotion shifts remain a natural next step. Incorporating multilingual and cross-cultural datasets will enhance generality and fairness. On the modeling side, exploring hybrid evaluation strategies that combine discrete rubrics with continuous perceptual signals (e.g., prosody embeddings or affective measures) may better bridge the gap between automatic and human judgments. Finally, integrating Speech-DRAME into reinforcement learning pipelines as a reward model could help drive the next generation of expressive, human-aligned speech role-play systems.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "consistency"
                ]
            }
        ]
    },
    "A12.T15": {
        "caption": "Table 15: Realism role-play evaluator performance (real recording test set). We report Pearson correlation coefficients for other dimensions. The Avg. column is the average of all coefficients.",
        "body": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" rowspan=\"2\">Config.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" colspan=\"3\">Prosodic Dynamics</td>\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" colspan=\"3\">Emotional Expressiveness</td>\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" colspan=\"2\">Character Consistency</td>\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" colspan=\"2\">Contextual Relevance</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\">Avg.</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Pitch</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Rynthm</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Stress</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Accuracy</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Intensity</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Transition</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Identity</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Traits</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Local</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Global</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_typewriter\">Base-FT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">-0.422</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">-0.119</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">-0.333</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">-0.261</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">-0.330</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">-0.351</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.059</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.538</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">-0.3083</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">-0.153</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">-0.168</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.372</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.016</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.304</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.195</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.273</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.249</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.091</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">0.458</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.291</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.146</td>\n<td class=\"ltx_td ltx_align_center\">-0.148</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.269</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.031</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.202</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.208</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.222</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.242</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.076</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.543</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.219</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">-0.094</td>\n<td class=\"ltx_td ltx_align_center\">-0.087</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.288</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.270</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.362</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.277</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.331</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.397</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">-0.034</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.077</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">0.217</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">0.279</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">0.247</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "roleplay",
            "column",
            "pitch",
            "identity",
            "consistency",
            "pearson",
            "prosodic",
            "instructft",
            "dynamics",
            "config",
            "realism",
            "coefficients",
            "evaluator",
            "average",
            "emotional",
            "baseft",
            "recording",
            "character",
            "rynthm",
            "stress",
            "test",
            "accuracy",
            "performance",
            "real",
            "avg",
            "relevance",
            "report",
            "intensity",
            "transition",
            "traits",
            "set",
            "global",
            "correlation",
            "baselora",
            "local",
            "contextual",
            "dimensions",
            "all",
            "other",
            "instructlora",
            "expressiveness"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Tables&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.T13\" title=\"Table 13 &#8227; Overall summary. &#8227; L.4 Fine-tuning Ablation Experiments &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">13</span></a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.T14\" title=\"Table 14 &#8227; Overall summary. &#8227; L.4 Fine-tuning Ablation Experiments &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">14</span></a>, and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.T15\" title=\"Table 15 &#8227; Overall summary. &#8227; L.4 Fine-tuning Ablation Experiments &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">15</span></a> present ablation studies across different fine-tuning strategies, including the use of <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio-7B-Base</span> or <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio-7B-Instruct</span>; and the use of fully fine-tuning or LoRA fine-tuning. We use <span class=\"ltx_text ltx_font_typewriter\">Base-FT</span>, <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span>, <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span>, and <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> to represents different configuration settings in following experiments.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Role-play has become a key testbed for generative models, expanding from text-only dialogue to multimodal interaction. Extending role-play to speech captures prosody, emotion, and delivery, but also poses new evaluation challenges. Current pipelines often use audio large language models (ALLMs) as zero-shot judges, which miss paralinguistic cues, collapse multiple aspects into coarse scores, and rely on synthetic speech references that fail to reflect real-world roles. We present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework that contributes at three levels: (i) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-EvalBench</em>, an evaluation benchmark with bilingual human-annotated data and protocols for training and testing speech evaluation models&#160;(SEMs), (ii) <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, a fine-tuned evaluation model, which substantially outperforms zero-shot and few-shot ALLMs, and (iii) <em class=\"ltx_emph ltx_font_italic\">Speech-DRAME-RoleBench</em>, a speech role-play benchmark that leverages <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> as an automatic judge to compare speech foundation models (SFMs). Speech-DRAME distinguishes between two complementary evaluation strategies: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>, a top-down approach measuring adherence to broad role archetypes, and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em>, a bottom-up approach grounded in real human speech that emphasizes nuanced role quality. Compared to zero-shot ALLM judges, <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves stronger agreement with human ratings (Pearson correlation from 0.480 to 0.629 in archetypes, and 0.390 to 0.625 in realism). By integrating transparent benchmark resources, modeling approaches, and system-level evaluation, Speech-DRAME provides the first comprehensive, reproducible foundation for assessing spoken role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "correlation",
                    "realism",
                    "pearson",
                    "real"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Role-play has been recently recognized a cornerstone of interactive systems, from initial text-based focuses to the growing ecosystem of multimodal dialogue agents&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib6\" title=\"\">2024a</a>; Tseng et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib59\" title=\"\">2024</a>; Dai et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib13\" title=\"\">2025</a>; Ma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib43\" title=\"\">2024</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>)</cite>. With the rapid advancement of large language models (LLMs), role-play has evolved beyond scripted interactions into dynamic character-driven conversations, supporting applications in education, entertainment, and human&#8211;AI collaboration&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Wu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib69\" title=\"\">2024</a>; Agatsuma et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib1\" title=\"\">2024</a>; Fung &amp; Laing, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib20\" title=\"\">2024</a>)</cite>. Extending role-play to speech-based interaction offers an even richer communication channel, capturing prosody, emotion, and character consistency in ways that text alone cannot&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "consistency",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">it lacks sensitivity to paralinguistic cues such as intonation, pacing, and emotional dynamics that define role delivery&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>; Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib81\" title=\"\">2025a</a>)</cite>,</p>\n\n",
                "matched_terms": [
                    "dynamics",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">it collapses diverse aspects of performance into limited coarse dimensions, obscuring critical dimensions of role-play quality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib9\" title=\"\">2025</a>; Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite>, and</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dimensions",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">it depends on synthetic speech references rather than real human speech&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>; Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>)</cite>, limiting its ability to reflect real-world diversity in speech role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "real"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To overcome these limitations, we present <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span> (Speech Detailed Role-play Assessment with Modeling and Evaluation), a unified framework that moves beyond synthetic or judge-only pipelines. Speech-DRAME is distinguished by three aspects: [i] a dual evaluation paradigm that balances archetype-driven generality with human-grounded realism (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS2\" title=\"3.2 Role-play Evaluation with Two Strategies &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>), [ii] human annotations that provide perceptual grounding absent in prior resources (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4\" title=\"4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>), and [iii] evaluation-model alignment that enables not only benchmarking of speech foundation models (SFMs) but also systematic analysis of the evaluation models (SEMs) themselves (see Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5\" title=\"5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>). These aspects establish Speech-DRAME as a comprehensive and trustworthy framework for measuring and improving speech role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">As illustrated in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, Speech-DRAME contributes at three levels: (i) <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>, an evaluation benchmark consisting of human-annotated datasets for archetype and realism role-play; (ii) <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span>, a fine-tuned SEM trained on EvalBench that surpasses zero-shot and few-shot ALLMs; and (iii) <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span>, a speech role-play benchmark that leverages DRAME-Eval as an automatic judge to systematically compare SFMs. By combining dual evaluation strategies, human-anchored supervision, and evaluation-model alignment, Speech-DRAME delivers systematic insights into both role-play systems and evaluation models themselves.\nOur contributions are four-fold:</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We introduce a <span class=\"ltx_text ltx_font_bold\">formal task definition</span> of speech role-play and its evaluation via speech foundation models (SFMs) for generation and speech evaluation models (SEMs) for assessment, unifying linguistic and paralinguistic dimensions.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Text-based role-play evaluation.</span>\nEarly research focused on text-only dialogue benchmarks. Datasets such as RoleLLM, CharacterBox, and RoleMRC <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib62\" title=\"\">2024</a>; Tu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib60\" title=\"\">2024</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib61\" title=\"\">2025a</a>; Shen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib53\" title=\"\">2023</a>; Lu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib41\" title=\"\">2025</a>)</cite> test whether models can sustain persona consistency and character fidelity, using LLM-as-judge pipelines for scalability. However, studies like PersonaEval <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib81\" title=\"\">2025a</a>)</cite> have shown that LLM-as-judge approaches are far from perfect, struggling with tasks as basic as tracking speaker identity. This highlights broader concerns about reliability even in text settings, which become only more severe when extended to speech, where prosody, emotion, and vocal style play a central role.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "identity",
                    "consistency",
                    "character",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech-native benchmarks.</span>\nMore recently, a number of resources have been introduced to evaluate spoken role-play agents. OmniCharacter-10K <cite class=\"ltx_cite ltx_citemacro_citep\">(Zhang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib78\" title=\"\">2025a</a>)</cite> emphasizes character voice traits, while others&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib39\" title=\"\">2025</a>; Hou et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib27\" title=\"\">2025</a>; Du et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib15\" title=\"\">2025</a>; Chen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib7\" title=\"\">2024b</a>)</cite> extend evaluation toward vocal communication, assistant capability, and multi-turn realism. Omni models such as StepAudio and KimiAudio <cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib29\" title=\"\">2025a</a>; Ding et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib14\" title=\"\">2025</a>)</cite> also include role-play scenarios in their evaluation. SpeechRole and VStyle <cite class=\"ltx_cite ltx_citemacro_citep\">(Jiang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib35\" title=\"\">2025</a>; Zhan et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib76\" title=\"\">2025</a>)</cite> are closer to our work, offering datasets that test interaction ability, expressiveness, and fidelity. However, these resources are primarily built from LLM-generated dialogues and TTS outputs, with limited human validation and a heavy reliance on proprietary ALLM-as-judge pipelines. As a result, they lack grounding in diverse human speech and typically apply only coarse rubrics.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "character",
                    "expressiveness",
                    "test",
                    "traits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Expressive TTS evaluation.</span>\nRecent work has explored evaluation of instruction-following and style-controlled TTS. InstructTTSEval&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Huang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib30\" title=\"\">2025b</a>)</cite> explicitly includes role-play scenarios, while EmergentTTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">(Manku et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib45\" title=\"\">2025</a>)</cite> and discrete speech LM benchmarks <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang &amp; Sz&#233;kely, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib63\" title=\"\">2024</a>)</cite> emphasize prosody, spontaneity, and speaker consistency. These efforts push beyond intelligibility and naturalness, but still center on synthetic outputs rather than real human role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "consistency",
                    "real"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Our distinction.</span>\nIn contrast, Speech-DRAME unifies these considerations through three elements: (i) a dual evaluation paradigm that combines archetype-driven generality with realism grounded in authentic human speech, (ii) human annotations that provide a reliable perceptual gold standard, and (iii) alignment of SEMs to human ratings, enabling systematic evaluation of both SFMs and the evaluation models themselves. This combination moves beyond synthetic benchmarks or zero-shot judging to provide a comprehensive and trustworthy framework for speech role-play assessment.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech role-play.</span>\nWe formalize <em class=\"ltx_emph ltx_font_italic\">speech role-play</em> as a conditional speech generation problem grounded in a character profile and scene description.\nGiven a dialogue context <math alttext=\"{\\bm{C}}_{\\text{speech}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>speech</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{speech}}</annotation></semantics></math>, a character profile <math alttext=\"{\\bm{C}}_{\\text{profile}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>profile</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{profile}}</annotation></semantics></math>, and a scene specification <math alttext=\"{\\bm{C}}_{\\text{scene}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>&#119914;</mi><mtext>scene</mtext></msub><annotation encoding=\"application/x-tex\">{\\bm{C}}_{\\text{scene}}</annotation></semantics></math>, a <em class=\"ltx_emph ltx_font_italic\">speech foundation model</em> (SFM) generates a role-played speech response <math alttext=\"{\\bm{S}}_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>&#119930;</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">{\\bm{S}}_{r}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech role-play evaluation can be framed in multiple ways depending on the available data and the desired granularity of analysis.\nWithin Speech-DRAME, we formalize two distinct strategies inspired by sociology theories<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Please refer to Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A2\" title=\"Appendix B Theoretical Motivation for Dual Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">B</span></a> for detailed discussion of the dual evaluation design.</span></span></span>, including a top-down, archetype-oriented option using synthetic speech and a bottom-up, realism-oriented option from real human speech.\nRather than merging these into a single rubric, we treat them as separate perspectives, each with unique advantages and limitations.\nFigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> summarizes the key contrasts, while further implementation details are provided in the Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>-&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "real"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>). The first strategy follows a <em class=\"ltx_emph ltx_font_italic\">top-down</em> design, inspired by prior text-based role-play benchmarks.\nHere, role-play is judged with respect to <em class=\"ltx_emph ltx_font_italic\">stereotypical archetypes</em> (e.g., &#8220;firefighter&#8221; or &#8220;ER doctor&#8221;), using scene contexts (e.g., &#8220;comfort a child trapped in fire&#8221; or &#8220;ask for help in a serious operation&#8221;).\nThis approach provides accessible, general-purpose scoring that is scalable to a wide range of scenarios, making it suitable for large-scale benchmarking.\nHowever, its reliance on broad stereotypes and global impressions limits its ability to capture fine-grained aspects of delivery (e.g., subtle prosodic shifts, nuanced emotional control)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Holt &amp; Lotto, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib26\" title=\"\">2010</a>)</cite>.\nIn practice, this strategy mirrors simplified annotation settings and is particularly useful when annotation resources are limited&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gemmeke et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib21\" title=\"\">2017</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "roleplay",
                    "emotional",
                    "prosodic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism-based Role-play Evaluation</span> (examples in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS5\" title=\"F.5 Examples of Realism Role-play &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.5</span></a>). The second strategy adopts a <em class=\"ltx_emph ltx_font_italic\">bottom-up</em> design, motivated by real-world performance and real human speech.\nInstead of relying solely on stereotypes, this approach grounds evaluation in recordings from professional and non-professional speakers, drawn from realistic dialogue and media sources.\nJudgments in this setting are more fine-grained and multi-dimensional, addressing aspects such as prosodic dynamics, emotional fidelity, character consistency, and contextual fit.\nWhile this strategy requires more human annotation and careful curation, it provides benchmarks that reflect how role-play is perceived in practice.\nIt thus offers a richer framework for analyzing model outputs in settings where realism and subtlety are essential.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "contextual",
                    "consistency",
                    "emotional",
                    "character",
                    "prosodic",
                    "real",
                    "dynamics",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human Annotation.</span>\nAll collected audio undergoes a standardized annotation protocol: (1) an initial semantically-based screening to exclude invalid clips; (2) rubric-based scoring across multiple perceptual dimensions; and (3) meta-annotations such as confidence or prompt-quality judgments. Full rubrics are provided in the appendices&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A5\" title=\"Appendix E Annotation Guidelines for Archetype Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">E</span></a> &amp; <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>All data collection follows in high standard ethical consideration as outlined in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A9\" title=\"Appendix I Annotation Ethics &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "all",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S4.T1\" title=\"Table 1 &#8227; 4.1 Archetype Data Collection &#8227; 4 DRAME-EvalBench Data Curation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> presents a summary of the curated <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> dataset, where we further elaborate the details of archetype and realism evaluation data collection in following subsections, respectively. For benchmark purpose, we provide an official train-test split for the human annotated dataset for <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> so that following researchers can use the dataset to train and evaluate their SEMs for speech role-play.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Role and Scene Dimensions.</span>\nRoles are operationalized through seven categories: (1) functional occupations in daily life, (2) functional occupations in fictional or fantasy settings, (3) social identities, (4) named or specific characters, (5) personality traits, (6) relative relationship, (7) basic events. Scenes are anchored by everyday events or state changes that evoke stereotypical reactions. Details of template design and alternative prompting strategies are given in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\">4</span>We note that some archetypes (e.g., firefighter) inevitably reflect cultural stereotypes. These are used solely for evaluation scalability and do not reflect normative judgments.</span></span></span></p>\n\n",
                "matched_terms": [
                    "traits",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realism evaluation adopts a bottom&#8211;up perspective, emphasizing authentic delivery, prosody, and contextual fit.</p>\n\n",
                "matched_terms": [
                    "contextual",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data Sources and Negative Samples.</span>\nWe curate speech from (i) ground-truth media segments, (ii) retrieved character profiles, and (iii) generated local scene descriptions. To stress-test evaluation models, we construct contrastive mismatches: (i) negative local scenes with varying degrees of mismatch, and (ii) negative character profiles created by re-synthesizing transcripts with TTS systems. Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6\" title=\"Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F</span></a> details the prompt design and TTS generation details.</p>\n\n",
                "matched_terms": [
                    "local",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Annotation Protocol and Evaluation Dimensions.</span>\nAnnotators judge whether performances align with the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em>, using rubrics for <em class=\"ltx_emph ltx_font_italic\">Prosodic Dynamics (i.e., Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis)</em>, <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness (with progressive gating over three sub-dimensions: Emotion Accuracy, Emotion Intensity, Emotion Transition)</em>, <em class=\"ltx_emph ltx_font_italic\">Character Consistency (i.e., Voice Identity Matching and Traits Embodiment)</em>, <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance (i.e., Local Scene Fit and Global Story Coherence)</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>. A prompt quality check and confidence ratings are also collected. Full scales and gating details are in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A7\" title=\"Appendix G Annotation Guidelines for Realism Evaluation &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">G</span></a>.</p>\n\n",
                "matched_terms": [
                    "global",
                    "pitch",
                    "identity",
                    "relevance",
                    "local",
                    "consistency",
                    "emotional",
                    "contextual",
                    "character",
                    "accuracy",
                    "prosodic",
                    "intensity",
                    "transition",
                    "stress",
                    "dimensions",
                    "expressiveness",
                    "traits",
                    "dynamics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real-world Recoding.</span>\nAs noted in Sec.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S2\" title=\"2 Related Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, most prior efforts have relied heavily on synthetic data, mostly from TTS systems based on curated professional role-play voices. While such data can achieve high fidelity, it often lacks the diversity and spontaneity of real-world delivery, limiting the generalizability of evaluation models. To address this gap, we expand realism evaluation beyond synthetic and curated voices by incorporating newly recorded speech from both professional and amateur voice actors. This design introduces a broader spectrum of delivery styles, from polished professional acting to more natural and less controlled amateur performance.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation Data Summary.</span>\nThe resulting realism evaluation dataset is constructed from a combination of curated media sources, including internal collections, public datasets (e.g., NCSSD&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib40\" title=\"\">2024b</a>)</cite>), and original recordings collected for this project.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span>All data were obtained under appropriate licenses and are legally available for research release.</span></span></span> We compile 15k annotated role-play samples: 9k in Mandarin and 5k in English. The dataset is divided into 12k samples for training, 2k mixed samples for the base test set, and 1k real-recording samples for the test set. The first test set supports general evaluation using our proposed pipeline with negative samples, while the second focuses exclusively on human speech performed by both professional and amateur voice actors. For benchmarking, 2,000 scenarios from the realism dataset are designated for realism role-play benchmarking. Detailed statistics and analyses are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11\" title=\"Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">K</span></a>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "roleplay",
                    "realism",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Zero-shot and Few-shot ALLMs.</span>\nWe first evaluate the zero-shot capabilities of both proprietary and public models, including <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>, <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>, <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span>, <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio</span>, and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>. All evaluations follow a unified prompting template that provides audio input alongside scene and character context.</p>\n\n",
                "matched_terms": [
                    "all",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For proprietary models, we report the average of five independent runs to reduce variance and obtain more stable predictions. For open-source models, we observed weaker instruction-following ability and limited adaptation to unfamiliar tasks. To mitigate this, we did not request simultaneous prediction of multiple metrics. Instead, we queried one metric at a time. Following <cite class=\"ltx_cite ltx_citemacro_citet\">Song &amp; Bahri (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>); Lukasik et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>, we interpret model outputs through token distribution over discrete labels (<span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score.</p>\n\n",
                "matched_terms": [
                    "report",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For few-shot experiments, we focus primarily on proprietary ALLMs, which demonstrate stronger instruction-following capabilities and have been widely used as judges in prior work. For each test query, we provide three additional annotated samples from the training set as in-context exemplars.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote7\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">7</sup><span class=\"ltx_tag ltx_tag_note\">7</span>Please find our detailed settings such as prompts, decoding algorithms in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS1\" title=\"L.1 Zeroshot/Fewshot Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.1</span></a> with additional ablation analysis in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS2\" title=\"L.2 Zeroshot/Fewshot Ablation Experiments &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.2</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "set",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Finetuning Model: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>.</span>\nIn addition to zero-shot and few-shot evaluations, we fine-tune a dedicated evaluation model, denoted as <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> for archetype and realism role-play evaluation, respectively.\nWe initialize from <span class=\"ltx_text ltx_font_typewriter\">Qwen2Audio-7B-Instruct</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Chu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib11\" title=\"\">2024</a>)</cite>, which provides a strong foundation for multi-modal audio-text reasoning.\nTo better leverage the multiple human annotations available in our datasets, we adopt a sampling-based training strategy: at each step, one annotator label is randomly selected, following practices in label distribution learning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Geng, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib22\" title=\"\">2016</a>; Wen et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib67\" title=\"\">2023</a>)</cite>.\nThis stochastic supervision implicitly exposes the model to the empirical annotation distribution, while during inference we output the full distribution over discrete labels (e.g., <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>) and compute the expectation as the final score, consistent with recent decoding-based regression methods&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Song &amp; Bahri, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib56\" title=\"\">2025</a>; Xiao et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib70\" title=\"\">2025</a>; Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib64\" title=\"\">2025b</a>; Lukasik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib42\" title=\"\">2024</a>)</cite>. We apply LoRA fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Hu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib28\" title=\"\">2022</a>)</cite>. All implementation details, including optimizer settings, training schedule, and sampling policy, are provided in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.SS3\" title=\"L.3 Fine-tuning Setup for Evaluation Models &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">L.3</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metrics for Role-play Evaluation Model.</span> For most dimensions with 1-5 scale, we report Pearson correlation. For <span class=\"ltx_text ltx_font_italic\">Content Pass</span> estimation, we report accuracy.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "correlation",
                    "report",
                    "pearson",
                    "dimensions",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype Evaluation.</span>\nThe results in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T2\" title=\"Table 2 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> reveal clear distinctions across evaluation settings. In the zero-shot condition, proprietary ALLMs such as <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> substantially outperform public models, achieving strong <span class=\"ltx_text ltx_font_italic\">content pass</span> accuracy (<math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>90%) and moderate correlations with other dimensions(<math alttext=\"0.43\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mn>0.43</mn><annotation encoding=\"application/x-tex\">0.43</annotation></semantics></math>&#8211;<math alttext=\"0.48\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mn>0.48</mn><annotation encoding=\"application/x-tex\">0.48</annotation></semantics></math>). By contrast, public models exhibit limited predictive ability, with correlations often below 0.2. Few-shot prompting provides modest gains for proprietary models, improving both accuracy and correlation (e.g., <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> rises from 0.480 to 0.493 average correlation). The most significant improvement arises from finetuning: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves an average correlation of 0.629, marking a large margin over zero-shot/few-shot models and demonstrating the effectiveness of task-specific supervision.</p>\n\n",
                "matched_terms": [
                    "other",
                    "accuracy",
                    "correlation",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism Evaluation.</span>\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T3\" title=\"Table 3 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> extends the analysis to fine-grained realism evaluation. Similar to archetype results, proprietary ALLMs lead the zero-shot setting, with <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> obtaining an average correlation of 0.390, compared to 0.284 for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span>. Public models remain weak baselines. Few-shot prompting consistently improves performance: <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span> gains +0.042 and <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> gains +0.083 over their zero-shot averages, showing the benefit of in-context supervision. Once again, finetuning is most effective: <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span> achieves 0.625 average correlation, higher than the best few-shot results. Importantly, the gains are consistent across all 10 realism dimensions, confirming that the sampling&#8211;expectation training strategy generalizes beyond simple classification to nuanced paralinguistic cues.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "realism",
                    "average",
                    "dimensions",
                    "all",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Real Recording Test Set.</span>\nThe real-recording evaluation in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S5.T4\" title=\"Table 4 &#8227; 5.2 DRAME-EvalBench Experimental Analysis &#8227; 5 DRAME-EvalBench (Role-play Evaluation Modeling) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">4</span></a> is notably more challenging, with all models suffering substantial performance drops. Proprietary models in the zero-shot setting achieve only <math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p3.m1\" intent=\":literal\"><semantics><mo>&#8764;</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math>0.04&#8211;0.08 average correlation, and public models often fall below zero. Few-shot prompting yields small but noticeable improvements, especially for <span class=\"ltx_text ltx_font_typewriter\">GPT4o-audio</span> (0.226 avg.), suggesting that exemplars partially mitigate domain mismatch between synthetic and real human speech. Interestingly, while finetuning achieves the highest overall average (0.247), its relative advantage over few-shot prompting is narrower than in the synthetic setting, and some dimensions (e.g., <span class=\"ltx_text ltx_font_italic\">Character Identity</span>) remain weak. We emphasize that this observed gap between synthetic and real recordings is not merely an artifact but an intended feature of Speech-DRAME, designed to expose the mismatch and drive future research into bridging synthetic-to-real generalization.</p>\n\n",
                "matched_terms": [
                    "set",
                    "correlation",
                    "avg",
                    "identity",
                    "average",
                    "character",
                    "recording",
                    "real",
                    "dimensions",
                    "all",
                    "test",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Overall.</span> Across both archetype and realism evaluations, we observe three consistent trends: (i) zero-shot performance is limited, particularly for open-source models, (ii) few-shot prompting offers moderate but consistent gains, especially for proprietary ALLMs, and (iii) finetuning with task-specific supervision (<span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>) delivers the largest improvements, achieving state-of-the-art correlations across nearly all dimensions. Nevertheless, results on the real-recording test set underscore the gap between synthetic and real human role-play scenarios, motivating future work in data diversification and domain adaptation.</p>\n\n",
                "matched_terms": [
                    "set",
                    "roleplay",
                    "realism",
                    "dimensions",
                    "all",
                    "performance",
                    "test",
                    "real"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on fine-tuned <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, we establish the DRAME-RoleBench to evaluate SFMs&#8217; role play ability. For archetype evaluation, 1,250 scenarios are constructed following the same data construction pipeline as the <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span>. For realism evaluation, we reuse 2,000 scenarios in the <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> test set. We follow the same prompt strategy as outlined in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "set",
                    "realism",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetype evaluation.</span>\nOn archetype role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T5\" title=\"Table 5 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>), cascaded pipelines consistently outperform end-to-end systems. The strongest cascaded systems, particularly <span class=\"ltx_text ltx_font_typewriter\">G-IndexTTS2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-IndexTTS2</span>, achieve both high content pass rates (above 94%) and balanced scores across audio quality, human likeness, and appropriateness. By contrast, end-to-end models such as <span class=\"ltx_text ltx_font_typewriter\">StepAudio2Mini</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> perform competitively in terms of content coverage (above 88%), but their expressive scores remain slightly lower than their cascaded counterparts. This gap suggests that separating reasoning (LLM) from speech rendering (TTS) remains advantageous when archetype fidelity is emphasized. Nevertheless, the fact that top end-to-end systems already surpass 3.0 in average rating indicates rapid closing of the performance gap.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "average",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism evaluation.</span>\nRealism role-play (Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S6.T6\" title=\"Table 6 &#8227; 6 DRAME-RoleBench &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>) presents a different landscape. Here, performance differences between end-to-end and cascaded systems are less pronounced. Several end-to-end models, such as <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span>, reach average scores of 3.24, matching or even exceeding the best cascaded pipelines. Importantly, realism scores are more evenly distributed across prosodic, emotional, character, and contextual dimensions, with most models clustering between 2.9&#8211;3.2. This convergence reflects the difficulty of realism-style evaluation: since the data preparation emphasized contrast with authentic human speech, models are judged under similar conditions, leading to smaller observed gaps.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "contextual",
                    "average",
                    "emotional",
                    "character",
                    "prosodic",
                    "dimensions",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human alignment.</span>\nWe further conducted small-scale human studies with 10% of the benchmark data to verify the reliability of the benchmark. Archetype evaluation achieves a Spearman correlation of 0.706 between human judgments and <span class=\"ltx_text ltx_font_typewriter\">DRAME-Eval</span>, validating that the automatic evaluator closely tracks human preferences in this setting. Realism evaluation, however, achieves a more modest Spearman correlation of 0.375. This lower alignment is expected, since realism data construction did not prioritize fine-grained model separation but instead emphasized distinguishing human versus synthetic speech.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote8\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">8</sup><span class=\"ltx_tag ltx_tag_note\">8</span>We view this as evidence that realism presents a harder, less-saturated frontier. Rather than a limitation of DRAME-Eval, this highlights the challenge of designing evaluators that capture nuanced human perception, which motivates future work.</span></span></span> As a result, all models occupy a relatively narrow performance band, reducing sensitivity to model-level differences. We view this as evidence that realism evaluation highlights a harder, less saturated frontier where both SFMs and SEMs require further advances.</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "realism",
                    "evaluator",
                    "all",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, cascaded pipelines retain an edge in archetype scenarios, especially in content robustness and expressive fidelity, but end-to-end models are catching up quickly. In realism scenarios, the field remains unsolved: existing systems exhibit similar performance bands and evaluators show weaker alignment with human judgments, underscoring the need for improved realism-grounded data and evaluation. Together, these findings establish <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> as both a reliable testbed for archetype fidelity and a challenging benchmark for realism fidelity.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented <span class=\"ltx_text ltx_font_bold\">Speech-DRAME</span>, a unified framework for assessing spoken role-play through two complementary lenses: <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em> (top-down) and <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> (bottom-up, human-grounded). Our human-annotated <span class=\"ltx_text ltx_font_bold\">DRAME-EvalBench</span> enables training and testing of SEMs, and our fine-tuned <span class=\"ltx_text ltx_font_bold\">DRAME-Eval</span> substantially outperforms zero-/few-shot ALLM judges. Building on this, <span class=\"ltx_text ltx_font_bold\">DRAME-RoleBench</span> provides system-level comparison across various SFMs, revealing a consistent edge for cascaded pipelines on archetypes and a tighter spread under realism, where real human performance evaluation remain challenging. Together, these resources establish a reproducible, analyzable foundation for spoken role-play evaluation.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\">9</span>Limitations and future works are discussed in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A13\" title=\"Appendix M Limitations and Future Works &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">M</span></a>.</span></span></span></p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "real",
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All data collection and annotation in this work followed strict ethical standards. Our bilingual datasets were constructed exclusively from legally accessible sources, including licensed media, open-source corpora, and original recordings conducted with informed consent. Annotators were recruited with appropriate compensation and provided with clear instructions, including the right to withdraw at any point. To minimize risks, no personally identifiable information was included in the released data, and all recordings were anonymized before annotation. We ensured cultural and linguistic diversity in annotator pools to mitigate potential bias across Mandarin and English role-play scenarios. While this study did not require formal IRB approval, we adhered to best practices in human-subject research, including transparency in task design and careful consideration of annotator well-being. Full details of our annotation protocol and ethical safeguards are presented in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A9\" title=\"Appendix I Annotation Ethics &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">I</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our experiments were conducted on widely available GPUs (NVIDIA H100). Random seeds are fixed across all runs to ensure consistency. By releasing datasets, models, and code, we aim to make all major results in this paper fully reproducible and facilitate future extensions of our framework.</p>\n\n",
                "matched_terms": [
                    "all",
                    "consistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data generation:</span> LLMs were employed to expand prompts (e.g., character profiles, scene descriptions) and to produce negative samples in realism evaluation. All outputs were screened by human annotators to ensure validity and reduce bias.</p>\n\n",
                "matched_terms": [
                    "all",
                    "realism",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our distinction between <span class=\"ltx_text ltx_font_italic\">Archetype Evaluation</span> and <span class=\"ltx_text ltx_font_italic\">Realism Evaluation</span> is not merely pragmatic, but is grounded in established theories of social identity, communication, and performance. The two perspectives capture complementary dimensions of how humans perceive and evaluate roles in interaction.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "dimensions",
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Archetypes as Collective Schemas</span>\nThe notion of archetypes originates from Jungian psychology, where they represent recurrent, culturally embedded character prototypes in the collective unconscious&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Jung, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib36\" title=\"\">2014</a>)</cite>. In media and performance studies, archetypes have been shown to guide audience expectations, providing top-down interpretive frames for evaluating whether a character &#8220;fits&#8221; within familiar categories such as hero, mentor, or trickster&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Campbell, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib4\" title=\"\">2008</a>)</cite>. Archetype-based reasoning allows for scalable, general-purpose judgments because it leverages culturally shared schemas rather than idiosyncratic details.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Realism as Situated Performance</span>\nIn contrast, realism highlights the fine-grained enactment of roles in situated interaction. Goffman&#8217;s dramaturgical sociology emphasizes that everyday social life is itself a performance, where believability depends on subtle paralinguistic and pragmatic cues such as prosody, hesitation, and emotional shading&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Goffman, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib23\" title=\"\">1949</a>)</cite>. Similarly, interactional sociolinguistics has demonstrated how micro-level features like intonation, pacing, and turn-taking patterns shape the perception of authenticity in communication&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Gumperz, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib25\" title=\"\">1982</a>)</cite>. Realism Evaluation thus anchors role-play assessment in bottom-up sensitivity to authentic delivery, beyond schematic expectations.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "emotional",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Complementarity of the Two Perspectives</span>\nPerformance theory provides a unifying lens for these dual dimensions. Schechner&#8217;s view of performance as both &#8220;restored behavior&#8221; (scripted, repeatable structures) and &#8220;living process&#8221; (situated enactment)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Schechner, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib52\" title=\"\">2017</a>)</cite> underscores the need to consider both archetypal scripts and enacted realism. Likewise, sociolinguistic theories of identity recognize both <span class=\"ltx_text ltx_font_italic\">ascribed categories</span> (archetypes imposed by social convention) and <span class=\"ltx_text ltx_font_italic\">performed identities</span> (emergent in interaction)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Bucholtz &amp; Hall, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib2\" title=\"\">2005</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "dimensions",
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p4.m2\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> may be a simple mean, a recency-weighted average, or a learned temporal model that captures consistency across turns.\nThe choice of <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p4.m3\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> reflects the intended evaluation focus, e.g., stability of persona traits, adaptation to scene progression, or coherence across multiple speakers.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "average",
                    "consistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Discussion.</span>\nWhile our released dataset and experiments concentrate on single-turn evaluation, the modular nature of the formulation ensures that extending to multi-turn is straightforward.\nFuture work can leverage <math alttext=\"\\Psi\" class=\"ltx_Math\" display=\"inline\" id=\"A3.p5.m1\" intent=\":literal\"><semantics><mi mathvariant=\"normal\">&#936;</mi><annotation encoding=\"application/x-tex\">\\Psi</annotation></semantics></math> to study richer aspects of speech role-play, such as persona persistence, interaction dynamics, and role consistency across extended dialogues.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dynamics",
                    "consistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Role dimensions.</span>\nWe operationalize archetypes through five principal categories, each representing a stereotypical dimension of role identity or behavior, including (1) <span class=\"ltx_text ltx_font_bold\">Occupation / Functional Role (Modern Daily Life)</span> (e.g., a firefighter rescuing a trapped civilian). (2) <span class=\"ltx_text ltx_font_bold\">Occupation / Functional Role (Fictional or Fantasy)</span> (e.g., a martial arts cult leader confronting orthodox rivals). (3)&#160;<span class=\"ltx_text ltx_font_bold\">Social Identity</span> (e.g., a doting parent worrying about a child, or a &#8220;Buddhist youth&#8221; eschewing competition). (4) <span class=\"ltx_text ltx_font_bold\">Named / Specific Characters</span> (e.g., Sun Wukong mocking celestial authorities in <em class=\"ltx_emph ltx_font_italic\">Journey to the West</em>). (5) <span class=\"ltx_text ltx_font_bold\">Personality Traits</span> (e.g., &#8220;hot-blooded&#8221; in a competitive setting, or &#8220;complaining&#8221; in response to poor service). (6) <span class=\"ltx_text ltx_font_bold\">Relative Roles</span> (e.g., parent or stranger).</p>\n\n",
                "matched_terms": [
                    "identity",
                    "dimensions",
                    "traits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scene dimensions.</span>\nIn addition to roles, we incorporate contextual triggers that anchor role-play in stereotypical situations.\nA central focus is on <span class=\"ltx_text ltx_font_bold\">everyday external events or state changes</span> (e.g., receiving a promotion, being caught in gossip), which provide concrete scenarios for testing archetypical responses.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "contextual",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Exclusion of fine-grained controls.</span>\nWe deliberately excluded explicit emotional states, prosodic instructions, or linguistic constraints.\nThese dimensions are better suited to realism-oriented or style-controlled evaluation, whereas archetype evaluation benefits from concrete, stereotype-level scenarios that sharpen judgments and improve human agreement.</p>\n\n",
                "matched_terms": [
                    "prosodic",
                    "emotional",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Pipeline overview.</span>\nAt a high level, archetype evaluation data are generated through a three-stage pipeline:\n(1) selecting keywords that specify role and scene;\n(2) expanding them into full prompts via structured templates; and\n(3) rendering prompts into speech with contemporary TTS systems.\nThis modular pipeline ensures both scalability and reproducibility, while keeping prompts vivid enough for reliable judgments. (4) Based on the generated data (paired scene and synthesized speech), expert human annotators are recruited for detailed annotation on semantic check (i.e., <span class=\"ltx_text ltx_font_italic\">Content Pass</span>) and three different spoken role-play dimensions (i.e., <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human-likeness</span>, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>).\nImplementation details, including template design, alternative prompting strategies, and annotation guidelines, are provided in the following subsections of this Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4\" title=\"Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Overall, archetype evaluation emphasizes breadth and recognizability over subtlety.\nBy grounding role-play in socially recognizable categories and stereotypical situations, it provides a practical balance between scalability and interpretability, complementing the more fine-grained realism evaluation described in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#S3.SS2\" title=\"3.2 Role-play Evaluation with Two Strategies &#8227; 3 Speech Role-play: Formulation and Evaluation Strategies &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Excluded Dimensions.</span> We initially considered more fine-grained control dimensions, but excluded them to preserve the archetype-level focus. Examples of excluded dimensions include:\n<em class=\"ltx_emph ltx_font_italic\">Emotional State &amp; Expression, Single Emotion Prompts, Explicit User Input / Assistant Output structures, Emotional Transition / Fluctuation, Voice Style &amp; Prosody (pacing, intonation, vocal timbre, expressive style, accent/dialect), and Content / Interaction Patterns (task/topic controls, specific linguistic structures, or speech production challenges).</em></p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "transition",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Collection for Proprietary Models.</span> For proprietary systems, we adopt two collection strategies. For GPT-4o-realtime, responses are obtained directly via the <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-realtime</span> API. For Doubao and GPT-4o (Advanced Voice Mode), we collect outputs by recording from their official web interfaces. All recordings are captured through the interaction device itself, ensuring that no additional processing or distortion is introduced into the final audio.</p>\n\n",
                "matched_terms": [
                    "all",
                    "recording"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2\" title=\"Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> illustrates the internal annotation interface used for <em class=\"ltx_emph ltx_font_italic\">Archetype Evaluation</em>.\nThe platform provides an integrated environment for both playback and evaluation of model-generated role-play speech samples.\nAnnotators first interact with the audio player interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf1\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>), which visualizes waveforms and spectrograms for multiple versions of the same prompt, enabling side-by-side comparison under a shared role instruction (e.g., firefighter, teacher, or actor).\nThen, they proceed to the annotation interface (subfigure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.F2.sf2\" title=\"In Figure 2 &#8227; D.5 Annotation Platform for Archetype Evaluation &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a>), where they assign scores for multiple dimensions such as <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, along with reasoning and confidence inputs.\nThis structured interface supports high-consistency, fine-grained evaluation of role-play behaviors in speech models.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For <span class=\"ltx_text ltx_font_bold\">Appropriateness</span>: comment for <em class=\"ltx_emph ltx_font_italic\">all</em> scores 1&#8211;5. Explain <em class=\"ltx_emph ltx_font_italic\">why</em> the <em class=\"ltx_emph ltx_font_italic\">delivery</em> was/was not a good match to the in-context traits (role &amp; scene).</p>\n\n",
                "matched_terms": [
                    "traits",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_italic\">Independently</span> rate technical clarity (ignore content and fit). After scoring Audio Quality, <em class=\"ltx_emph ltx_font_italic\">disregard</em> its issues for the other two dimensions.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Rate how likely an average listener would believe the clip is human-delivered (ignore Audio Quality issues and contextual fit).</p>\n\n",
                "matched_terms": [
                    "contextual",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Realism Evaluation adopts a bottom&#8211;up perspective grounded in real human speech data, with the goal of capturing the nuanced delivery, prosody, and contextual fit that synthetic archetype prompts cannot fully represent.\nRather than relying on stereotypes, this strategy leverages real-world recordings and carefully constructed contrasts, enabling fine-grained assessment of model performance under realistic conditions.</p>\n\n",
                "matched_terms": [
                    "real",
                    "contextual",
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Data from real human speech sources.</span>\nWe construct the realism evaluation set primarily from natural speech material:</p>\n\n",
                "matched_terms": [
                    "set",
                    "realism",
                    "real"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Generated local scenes.</span>\nWe supplement retrieved character information with automatically generated local scene descriptions summarizing the surrounding plot or setting.\nAppendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.SS3\" title=\"F.3 Prompts for Local Scene Generation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">F.3</span></a> includes representative prompts and generation details.</p>\n\n",
                "matched_terms": [
                    "local",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Together, real human ground-truth segments and systematically constructed negatives form a balanced dataset that supports realism-oriented evaluation.\nThe positives provide natural anchors of role-play performance, while the negatives introduce controlled mismatches that test whether evaluation models can distinguish real human speech from synthetic approximations.\nThis design ensures that realism evaluation emphasizes subtle paralinguistic and contextual cues, moving beyond intelligibility or content fidelity.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "contextual",
                    "performance",
                    "test",
                    "real"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During annotation, raters are presented with either: (i) ground-truth stories, (ii) stories with negative local scenes, or (iii) stories with negative character profile. Annotators assign realism scores without being exposed to the underlying mismatch reasons. Instead, these reasons are provided only to the quality-assurance team, who use them as reference for internal validation and consistency checks. While the mismatch reasons may not be perfect, they help streamline quality control procedures and maintain reliability across the dataset.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "local",
                    "consistency",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To make the construction of the realism evaluation task concrete, we present a worked example that begins with a curated local scene and character profile, followed by the same transcript adapted into mismatched contexts. These mismatches appear either at the <em class=\"ltx_emph ltx_font_italic\">local-scene</em> level or at both the <em class=\"ltx_emph ltx_font_italic\">character profile and scene</em> level, thereby providing controlled degradations of realism.</p>\n\n",
                "matched_terms": [
                    "local",
                    "realism",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use the character Yunjin from <span class=\"ltx_text ltx_font_italic\">Genshin Impact</span> as a running case study, which is also included in our released dataset.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span>As acknowledged, the use of real-world recordings is conducted under legal and ethical review. All curated data will be released as part of our open-source package.</span></span></span> This example illustrates how alignment between a speaker&#8217;s persona, local context, and delivery underpins realism, and how systematically breaking that alignment produces contrastive test cases.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "local",
                    "character",
                    "all",
                    "test"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Curated Local Scene.</span>\nYunjin is narrating a fierce battle through operatic singing. Her verses depict a hero charging into enemy lines, slaying the enemy commander, and securing peace. The atmosphere is filled with tension and intensity, highlighting the hero&#8217;s extraordinary bravery and determination.</p>\n\n",
                "matched_terms": [
                    "local",
                    "intensity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative Local Scenes.</span>\nTo stress realism evaluation, we insert the same transcript into alternative backgrounds. These reveal how context-shifted delivery degrades narrative coherence:</p>\n\n",
                "matched_terms": [
                    "realism",
                    "stress",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Negative Character Profiles.</span>\nWe replace Yunjin with new character profiles and scenes. Although the transcript remains the same, realism degrades because the delivery no longer matches the speaker&#8217;s persona or situation.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Yunjin case study illustrates a fundamental difference between realism evaluation and archetype evaluation. Archetype prompts, as described in Appendix&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A4.SS2\" title=\"D.2 Role and Scene Categories &#8227; Appendix D Implementation Details for Archetype Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">D.2</span></a>, are constructed by pairing abstract role categories with synthetic contextual scenes. These cover a broad space of possible situations, ranging from modern occupations (e.g., firefighters) to fictional personas (e.g., cult leaders in martial arts fantasy) or social identities (e.g., overprotective parents, laid-back friends). While such prompts provide controlled and diverse coverage, they are ultimately template-based and rely on stereotypes to elicit role-play behaviors.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "contextual",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In contrast, the Yunjin example is drawn from authentic performance, where the character&#8217;s persona (an opera singer) and local scene (a battlefield narration) are naturally aligned through artistic delivery. The realism task does not merely test whether a model can follow a prompt, but whether it can preserve coherence between a speaker&#8217;s global identity, immediate context, and expressive prosody. When Yunjin&#8217;s transcript is transplanted into mismatched settings or paired with incongruent speaker profiles, the breakdown in realism becomes apparent: the same words shift from inspiring to awkward, jarring, or even absurd.</p>\n\n",
                "matched_terms": [
                    "global",
                    "realism",
                    "local",
                    "identity",
                    "test",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This distinction highlights the complementary strengths of the two evaluation paradigms. Archetype evaluation systematically explores a wide combinatorial space of roles and situations, but may lack the paralinguistic and contextual richness of real speech. Realism evaluation, on the other hand, anchors assessment in authentic delivery and contextual fit, providing a finer-grained lens on coherence and naturalness. Together, they ensure that role-play evaluation spans both broad stereotypical coverage and nuanced real-world plausibility.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism",
                    "contextual",
                    "other",
                    "real"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employed a diverse set of TTS systems to generate negative character profiles and resynthesized speech for Realism Evaluation.\nThese included both commercial and non-commercial models, with variation in style control, conditioning prompts, and reference usage.\nTable&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.T8\" title=\"Table 8 &#8227; Same-Speaker Voice Clone. &#8227; F.6 TTS Systems and Settings &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> summarizes the systems and settings.\nBelow, we detail the major strategies used across systems:</p>\n\n",
                "matched_terms": [
                    "set",
                    "realism",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We leveraged all publicly available speaker style information released by the corresponding websites.\nFor each input sample, we employed <span class=\"ltx_text ltx_font_typewriter\">deepseek-v3</span> to select the best-matching speaker based on the original speech caption.\nIn the case of <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5TTS</span>, the character profile was also appended as an additional instruction to guide synthesis.</p>\n\n",
                "matched_terms": [
                    "all",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A character profile was first converted into a descriptive voice specification by prompting <span class=\"ltx_text ltx_font_typewriter\">deepseek-v3</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib38\" title=\"\">2024a</a>)</cite>.\nThis description was submitted to the TTS service to generate candidate voices.\nWe then previewed samples via Gemini2.5Pro, selecting the voice that best matched the speaker style of the original recording before proceeding to generate speech.</p>\n\n",
                "matched_terms": [
                    "recording",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The annotation interface for <em class=\"ltx_emph ltx_font_italic\">Realism Evaluation</em> focuses on assessing contextual and narrative coherence in model-generated speech.\nUnlike the archetype-oriented evaluation, which emphasizes stylistic and emotional fidelity, this platform examines how well the voice performance aligns with a character&#8217;s personality, dialogue situation, and scene progression.</p>\n\n",
                "matched_terms": [
                    "contextual",
                    "realism",
                    "emotional",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A6.F3\" title=\"Figure 3 &#8227; F.7 Annotation Platform for Realism Evaluation &#8227; Appendix F Implementation Details for Realism Evaluation Tasks &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> shows the core playback interface.\nAnnotators can preview each audio sample with synchronized spectrogram and waveform displays, while reviewing contextual metadata including the <span class=\"ltx_text ltx_font_italic\">Local Scene</span>, <span class=\"ltx_text ltx_font_italic\">Character Style</span>, and <span class=\"ltx_text ltx_font_italic\">Character Profile</span>.\nThese descriptions help situate the listening task, providing the background necessary for judging realism in the character&#8217;s vocal and behavioral expression.</p>\n\n",
                "matched_terms": [
                    "local",
                    "realism",
                    "contextual",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Read the <span class=\"ltx_text ltx_font_bold\">Character Profile</span> (long-term identity, style) and <span class=\"ltx_text ltx_font_bold\">Local Scene</span> (immediate context, goal/emotion) before listening.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">If the audio&#8217;s speaker is <em class=\"ltx_emph ltx_font_italic\">clearly not</em> the intended target identity, assign a score of <span class=\"ltx_text ltx_font_bold\">1</span> to all Profile-dependent dimensions (e.g., Voice Identity Matching, Trait Embodiment) and proceed with other dimensions as applicable.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "all",
                    "other",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Detailed case examples and progressive gating for emotional dimensions are provided in the full instruction document.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We adopt five dimensions, each rated from 1 (worst) to 5 (best). Emotional Expressiveness uses <em class=\"ltx_emph ltx_font_italic\">progressive gating</em>.</p>\n\n",
                "matched_terms": [
                    "expressiveness",
                    "dimensions",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prosodic Dynamics</span> (Pitch Variation, Rhythmic Naturalness, Stress &amp; Emphasis): naturalness of patterns and rhythms.</p>\n\n",
                "matched_terms": [
                    "prosodic",
                    "dynamics",
                    "stress",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotional Expressiveness</span> (Progressive gating):</p>\n\n",
                "matched_terms": [
                    "expressiveness",
                    "emotional"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Character Consistency</span> (Voice Identity Matching, Trait Embodiment): alignment with Character Profile.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "consistency",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Contextual Relevance</span> (Local Scene Fit, Global Story Coherence): fit to immediate scene and long-term identity.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local",
                    "identity",
                    "relevance",
                    "contextual"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Read first:</span> Carefully read the <em class=\"ltx_emph ltx_font_italic\">Character Profile</em> and <em class=\"ltx_emph ltx_font_italic\">Local Scene</em> to understand the long-term identity and immediate goal/emotion.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identity mismatch:</span> If the clip&#8217;s speaker is clearly not the intended target speaker, assign a score of <span class=\"ltx_text ltx_font_bold\">1</span> for every rating dimension that relies on the Character Profile (e.g., Character Style Traits, Trait Embodiment, Character Consistency).</p>\n\n",
                "matched_terms": [
                    "identity",
                    "consistency",
                    "traits",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt contains contradictory information.</span> Conflicts make accurate annotation difficult or impossible. Examples: incompatible accent specifications; Local Scene drastically at odds with intended emotion (see <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em>); Local and global scenes inconsistent; traits described as &#8220;evolving over time&#8221; without a timeline.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "global",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt is insufficient to evaluate the voice sample.</span> Audio too short to gauge the dimensions; or the prompt lacks specific characteristics/is too vague or multi-directional to know which version of the character is present. (It is OK if audio does not match transcript exactly; if too much is missing to annotate, mark as insufficient.)</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Checkbox 1: Multi-Speaker Influence.</span> Check <em class=\"ltx_emph ltx_font_italic\">Yes</em> if other speakers were present and made it harder to isolate/judge the target character (check <em class=\"ltx_emph ltx_font_italic\">Yes</em> if uncertain). Otherwise check <em class=\"ltx_emph ltx_font_italic\">No</em>.</p>\n\n",
                "matched_terms": [
                    "other",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Question 2: Observed Character Style Traits.</span> Based on the Character Profile, are the defined Style Traits clearly present in the delivery?</p>\n\n",
                "matched_terms": [
                    "traits",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Rubric Rating Instructions.</span>\nScore each subcriterion on a 1&#8211;5 scale. Use the detailed rubrics below and the examples to maintain calibration across characters, tones, and emotional styles. Prioritize in-character believability; do not reward theatricality, emotional intensity, or technical polish unless they naturally fit the character <em class=\"ltx_emph ltx_font_italic\">and</em> scene.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "intensity",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Focus: pitch, rhythm, and emphasis in speech.\n<span class=\"ltx_text ltx_font_bold\">Guideline 1.1 Pitch Variation (check Local Scene, Character Profile, and audio).</span></p>\n\n",
                "matched_terms": [
                    "character",
                    "local",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 1.2 Rhythmic Naturalness (check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "character",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 1.3 Stress and Emphasis (check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "character",
                    "stress",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 2.1 Emotion Accuracy (Required; check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "character",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 2.2 Emotion Intensity Control (Only if 2.1 <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"A7.SS4.SSSx5.p3.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math> 3; check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "character",
                    "intensity",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 2.3 Dynamic Range (Only if 2.2 <math alttext=\"\\geq\" class=\"ltx_Math\" display=\"inline\" id=\"A7.SS4.SSSx5.p4.m1\" intent=\":literal\"><semantics><mo>&#8805;</mo><annotation encoding=\"application/x-tex\">\\geq</annotation></semantics></math> 3; check Local Scene, Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "character",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 3.1 Voice Identity Matching (Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "identity",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text ltx_font_bold\">Guideline 4.2 Global Story (Character Profile) Coherence (check Character Profile, and audio).</span>\n</p>\n\n",
                "matched_terms": [
                    "global",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Progressive gating for emotion.</span>\nOur <em class=\"ltx_emph ltx_font_italic\">Emotional Expressiveness</em> uses gated sub-scores (accuracy <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A8.SS2.p3.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> intensity control <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"A8.SS2.p3.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> dynamic range), preventing spurious high scores when base emotion is misidentified.\nThis reduces label noise and improves SEM alignment stability.</p>\n\n",
                "matched_terms": [
                    "expressiveness",
                    "accuracy",
                    "emotional",
                    "intensity"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The archetype evaluation provides a detailed view of how speech foundation models (SFMs) perform when tasked with role-play scenarios grounded in stereotypical characters and situations. Human annotations cover four dimensions, <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, <span class=\"ltx_text ltx_font_italic\">Audio Quality</span>, and <span class=\"ltx_text ltx_font_italic\">Content Pass</span>, and the results reveal important trends, as well as challenges in interpreting scores across languages.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A critical observation is that annotations in Mandarin and English cannot be directly compared. The annotation groups for the two languages differed in their linguistic backgrounds and cultural interpretations, which leads to systematic differences in scoring. Our bilingual pilot further confirmed that annotators often interpret role-play appropriateness and expressiveness differently depending on their cultural frame of reference. For example, Mandarin raters tend to reward delivery styles that align with culturally expected prosody and politeness strategies, while English raters prioritize spontaneity and naturalness. These discrepancies parallel the findings of the singing voice conversion challenge (SVCC2023) from <cite class=\"ltx_cite ltx_citemacro_citet\">Huang et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#bib.bib31\" title=\"\">2023</a>)</cite>, where native and non-native listeners provided significantly different ratings for the same samples, particularly for naturalness and speaker similarity. Consequently, while absolute scores differ substantially across Mandarin and English, only within-language comparisons are meaningful. Cross-lingual ranking is not valid given the different annotation bases.</p>\n\n",
                "matched_terms": [
                    "expressiveness",
                    "roleplay"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Within Mandarin from Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>, the relative rankings of models are clear. <span class=\"ltx_text ltx_font_typewriter\">Doubao</span> stands out as the strongest performer across all four dimensions, reaching near-ceiling levels in both <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> (4.37) and <span class=\"ltx_text ltx_font_italic\">Content Pass</span> (0.98). It also leads in role-related dimensions such as <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span>, indicating balanced competence across linguistic accuracy, delivery, and expressiveness. <span class=\"ltx_text ltx_font_typewriter\">G-CosyVoice2</span> and <span class=\"ltx_text ltx_font_typewriter\">Q-CosyVoice2</span> follow closely, especially in audio quality, though they lag behind in role appropriateness. <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span> achieve mid-range results, reflecting higher variance across samples, while <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> shows competitive content understanding but lower naturalness and appropriateness. <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> consistently ranks the lowest, with large variance, underscoring instability and weaker alignment with archetypal role expectations.</p>\n\n",
                "matched_terms": [
                    "expressiveness",
                    "accuracy",
                    "all",
                    "dimensions"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The picture shifts in English, elaborated in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A10.T10\" title=\"Table 10 &#8227; Mandarin Results. &#8227; Appendix J Detailed Data Analysis (Archetype Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>. Here, <span class=\"ltx_text ltx_font_typewriter\">ChatGPT-4o</span> emerges as the most effective model by a wide margin, achieving top scores in both <span class=\"ltx_text ltx_font_italic\">Appropriateness</span> (3.46) and <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> (3.49), as well as high content pass rates (0.97). This reflects its strong grounding in English discourse norms and role adherence. In contrast, <span class=\"ltx_text ltx_font_typewriter\">CosyVoice2</span> variants perform well in <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> but are weaker in role alignment, highlighting a trade-off between acoustic fidelity and role-play embodiment. Other models such as <span class=\"ltx_text ltx_font_typewriter\">Doubao</span>, <span class=\"ltx_text ltx_font_typewriter\">GLM4-Voice</span>, and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o-realtime</span> achieve moderate results, while <span class=\"ltx_text ltx_font_typewriter\">KimiAudio</span> and <span class=\"ltx_text ltx_font_typewriter\">Qwen2.5Omni</span> trail far behind, showing limited robustness in English scenarios. Taken together, the English results reveal both a sharper separation across models and a wider variance in appropriateness scores compared to Mandarin.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "other"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Although realism evaluation targets nuanced paralinguistic behaviors, human annotators achieve <em class=\"ltx_emph ltx_font_italic\">consistently high agreement</em> once the rubric disentangles the aspects to be judged.\nAs shown in the agreement summary in Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F7\" title=\"Figure 7 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>, most dimensions fall in the <math alttext=\"0.86\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m1\" intent=\":literal\"><semantics><mn>0.86</mn><annotation encoding=\"application/x-tex\">0.86</annotation></semantics></math>&#8211;<math alttext=\"0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m2\" intent=\":literal\"><semantics><mn>0.88</mn><annotation encoding=\"application/x-tex\">0.88</annotation></semantics></math> range, with the lowest agreement on <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> (<math alttext=\"\\approx 0.83\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.83</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.83</annotation></semantics></math>) and the highest on <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em> / <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> (<math alttext=\"\\approx 0.88\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.88</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.88</annotation></semantics></math>).\nEmotion-related, progressive metrics (Intensity and Transition) are slightly lower (<math alttext=\"\\approx 0.85\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px1.p1.m5\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mn>0.85</mn></mrow><annotation encoding=\"application/x-tex\">\\approx 0.85</annotation></semantics></math>), reflecting the added difficulty of judging graded expressivity.</p>\n\n",
                "matched_terms": [
                    "realism",
                    "local",
                    "intensity",
                    "dimensions",
                    "transition",
                    "traits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fig.&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F8\" title=\"Figure 8 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>&#8211;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A11.F9\" title=\"Figure 9 &#8227; Agreement definition. &#8227; Appendix K Detailed Data Analysis (Realism Evaluation) &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">9</span></a> visualize the score distributions per dimension.\nWe highlight three patterns:\n(i) <span class=\"ltx_text ltx_font_bold\">Progressive/gated emotion metrics.</span> For <em class=\"ltx_emph ltx_font_italic\">Emotion Intensity</em> and <em class=\"ltx_emph ltx_font_italic\">Emotion Transition</em>, we assign <span class=\"ltx_text ltx_font_typewriter\">1</span> when the scene is marked <em class=\"ltx_emph ltx_font_italic\">N/A</em> (i.e., emotion accuracy is insufficient to meaningfully judge intensity/transition). This produces a visible mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math>, a sparse bin at <math alttext=\"2\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m2\" intent=\":literal\"><semantics><mn>2</mn><annotation encoding=\"application/x-tex\">2</annotation></semantics></math>, and slightly lower overall agreement; downstream analyses conditioning on adequate <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> (e.g., mean&#160;<math alttext=\"\\geq 3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8805;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\geq 3</annotation></semantics></math>) recover more bell-shaped distributions.\n(ii) <span class=\"ltx_text ltx_font_bold\">Traits Embodiment.</span> Scores tend to avoid the middle (<math alttext=\"\\sim\\!3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m4\" intent=\":literal\"><semantics><mrow><mi/><mo rspace=\"0.108em\">&#8764;</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">\\sim\\!3</annotation></semantics></math>), concentrating on low values when traits clearly mismatch and high values when traits are strongly realized. This yields the lowest inter-rater agreement among dimensions, reflecting the inherently compositional and role-specific nature of trait judgments.\n(iii) <span class=\"ltx_text ltx_font_bold\">Other dimensions.</span> <em class=\"ltx_emph ltx_font_italic\">Pitch Variation</em>, <em class=\"ltx_emph ltx_font_italic\">Rhythmic Naturalness</em>, <em class=\"ltx_emph ltx_font_italic\">Stress &amp; Emphasis</em>, <em class=\"ltx_emph ltx_font_italic\">Voice Identity Matching</em>, <em class=\"ltx_emph ltx_font_italic\">Local Scene Fit</em>, <em class=\"ltx_emph ltx_font_italic\">Global Story Coherence</em>, and <em class=\"ltx_emph ltx_font_italic\">Semantic Match</em> are approximately unimodal with modes at <math alttext=\"3\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m5\" intent=\":literal\"><semantics><mn>3</mn><annotation encoding=\"application/x-tex\">3</annotation></semantics></math>&#8211;<math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"A11.SS0.SSS0.Px3.p1.m6\" intent=\":literal\"><semantics><mn>4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math>, consistent with moderate&#8211;strong realization in the dataset and with the high agreement reported.</p>\n\n",
                "matched_terms": [
                    "global",
                    "pitch",
                    "identity",
                    "local",
                    "intensity",
                    "dimensions",
                    "transition",
                    "other",
                    "stress",
                    "traits",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prosody control (Pitch, Rhythm, Stress).</span> Professional speech is shifted toward higher bins with tighter spread, indicating steadier control and fewer low-end tails.</p>\n\n",
                "matched_terms": [
                    "stress",
                    "pitch"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Emotion rendering (Accuracy, Intensity, Transition).</span> Professionals show higher centers and reduced mass at <math alttext=\"1\" class=\"ltx_Math\" display=\"inline\" id=\"A11.I1.i2.p1.m1\" intent=\":literal\"><semantics><mn>1</mn><annotation encoding=\"application/x-tex\">1</annotation></semantics></math> (fewer gated <span class=\"ltx_text ltx_font_typewriter\">N/A</span>s), suggesting more consistent emotional grounding and smoother dynamics; amateurs exhibit wider variance and more mass at the extremes.</p>\n\n",
                "matched_terms": [
                    "emotional",
                    "intensity",
                    "accuracy",
                    "transition",
                    "dynamics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Identity and traits (Voice Identity, Traits Embodiment).</span> Professionals present clearer peaks in upper bins, while amateurs are broader or bimodal, reflecting inconsistent character anchoring.</p>\n\n",
                "matched_terms": [
                    "identity",
                    "traits",
                    "character"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Scene and semantics (Local Scene Fit, Global Coherence, Semantic Match).</span> Differences are present but smaller; both groups cluster in the mid&#8211;high range, with professionals exhibiting slightly narrower distributions.</p>\n\n",
                "matched_terms": [
                    "global",
                    "local"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">First, the consistently high agreement confirms that the rubric isolates perceptually coherent factors, so learned evaluators can target them reliably.\nSecond, the gated behavior of <em class=\"ltx_emph ltx_font_italic\">Intensity/Transition</em> motivates a <em class=\"ltx_emph ltx_font_italic\">hierarchical</em> evaluator: verify <em class=\"ltx_emph ltx_font_italic\">Emotion Accuracy</em> before scoring graded expressivity, or model the joint structure with conditional heads.\nThird, <em class=\"ltx_emph ltx_font_italic\">Traits Embodiment</em> remains challenging; models may benefit from explicit character-trait priors, long-term context aggregation, or contrastive supervision between matched and mismatched traits.\nFinally, prosody-focused objectives (e.g., pitch/rhythm/stress control) are likely to yield the largest quality gains for amateur-style speech.</p>\n\n",
                "matched_terms": [
                    "traits",
                    "evaluator",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For both zeroshot and fewshot experiments, we adopt a streamlined version of the annotation guidelines to ensure consistency with the human annotation process. The exact rubric prompts for archetype and realism evaluation used are provided below:</p>\n\n",
                "matched_terms": [
                    "realism",
                    "consistency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For zeroshot and few-shot experiments on commercial models such as <span class=\"ltx_text ltx_font_typewriter\">Gemini</span> and <span class=\"ltx_text ltx_font_typewriter\">GPT-4o</span>, we report results as the average of five independent runs to reduce randomness and obtain more stable predictions. In contrast, for open-source models, we observed that their instruction-following ability is relatively weak. To mitigate this, we prompt them to predict one dimension at a time rather than all dimensions jointly. Specifically, for each metric, after showing the rubric prompt, we request completion in a fixed template and extract the probability distribution over the discrete tokens <span class=\"ltx_text ltx_font_typewriter\">[1]</span>&#8211;<span class=\"ltx_text ltx_font_typewriter\">[5]</span>. The final score is then computed from these probabilities, which we found makes open-source models usable to some extent under this setting.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "report",
                    "average",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For few-shot experiments, we randomly sample three exemplars from the training set, with each exemplar chosen such that its human scores are close to the rounded average across all annotators. These exemplars are appended as demonstrations in the prompt. As with the zeroshot case, we run five trials for commercial models and report the averaged results for evaluation.</p>\n\n",
                "matched_terms": [
                    "set",
                    "all",
                    "report",
                    "average"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.01261v1#A12.T12\" title=\"Table 12 &#8227; L.2 Zeroshot/Fewshot Ablation Experiments &#8227; Appendix L Evaluation Model Experiments &#8227; Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> shows how different prompting strategies affect realism evaluation performance with <span class=\"ltx_text ltx_font_typewriter\">Gemini2.5Pro</span>. Several observations can be drawn:</p>\n\n",
                "matched_terms": [
                    "realism",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Effect of repeated runs.\nWhen comparing single-run decoding (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot-1run</span>) against averaging over five runs (<span class=\"ltx_text ltx_font_typewriter\">Zeroshot</span>), we see consistent gains across nearly all dimensions (average score improves from 0.353 to 0.390). This suggests that sampling variance plays a substantial role in the reliability of large language model (LLM) evaluators, and averaging multiple outputs reduces this variance, producing more stable correlation with human annotations.</p>\n\n",
                "matched_terms": [
                    "dimensions",
                    "correlation",
                    "average",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Scaling the number of demonstrations.\nIncreasing the few-shot set size from three to six and nine further boosts performance (0.459 and 0.469, respectively). The improvements are most pronounced in prosodic and emotional categories (e.g., <em class=\"ltx_emph ltx_font_italic\">Pitch</em>, <em class=\"ltx_emph ltx_font_italic\">Accuracy</em>, <em class=\"ltx_emph ltx_font_italic\">Intensity</em>), suggesting that more examples provide stronger guidance in modeling subtle paralinguistic cues. Gains in <em class=\"ltx_emph ltx_font_italic\">Character Consistency</em> and <em class=\"ltx_emph ltx_font_italic\">Contextual Relevance</em> are also noticeable, although smaller in magnitude, which may indicate that these higher-level judgments are less sensitive to prompt scaling once a few demonstrations are provided.</p>\n\n",
                "matched_terms": [
                    "set",
                    "pitch",
                    "relevance",
                    "contextual",
                    "consistency",
                    "emotional",
                    "character",
                    "intensity",
                    "prosodic",
                    "accuracy",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In general, the results demonstrate two complementary strategies for stabilizing LLM-based evaluators: (i) averaging multiple runs to reduce randomness, and (ii) enriching prompts with more few-shot exemplars to improve adherence to rubric-based evaluation. Combining both leads to the strongest overall performance, with the nine-shot condition achieving the highest average correlation&#160;(0.469).</p>\n\n",
                "matched_terms": [
                    "correlation",
                    "average",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">On the archetype test set, all fine-tuned variants achieve strong <span class=\"ltx_text ltx_font_italic\">Content Pass</span> accuracy above 92%. Differences appear mainly in correlation-based metrics. Here, <span class=\"ltx_text ltx_font_typewriter\">Base-FT</span> yields the highest score on <span class=\"ltx_text ltx_font_italic\">Audio Quality</span> and <span class=\"ltx_text ltx_font_italic\">Appropriateness</span>, while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> delivers the strongest performance in <span class=\"ltx_text ltx_font_italic\">Human Likeness</span> and the best overall average (0.629). This suggests that instruction-tuned checkpoints paired with parameter-efficient adaptation can better capture higher-level perceptual traits.</p>\n\n",
                "matched_terms": [
                    "set",
                    "baseft",
                    "average",
                    "all",
                    "instructlora",
                    "test",
                    "traits",
                    "accuracy",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For realism evaluation on the basic test set, <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> consistently achieves the strongest correlations across most dimensions, especially for <span class=\"ltx_text ltx_font_italic\">Character Identity</span> and <span class=\"ltx_text ltx_font_italic\">Traits</span>, leading to the best average correlation (0.668). <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> also performs competitively (0.630), while <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> lags slightly behind (0.625). These results highlight that full fine-tuning may still offer an advantage when the evaluation set is relatively homogeneous and closer to the training distribution.</p>\n\n",
                "matched_terms": [
                    "set",
                    "correlation",
                    "realism",
                    "baselora",
                    "identity",
                    "average",
                    "character",
                    "dimensions",
                    "instructft",
                    "instructlora",
                    "test",
                    "traits"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A different pattern emerges on the real recording test set. Both <span class=\"ltx_text ltx_font_typewriter\">Base-LoRA</span> and <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> struggle, yielding negative or near-zero correlations across most dimensions. In contrast, <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> shows clear improvements, achieving positive correlations in all prosodic and emotional dimensions and producing the highest overall average (0.247). This indicates that LoRA adaptation on top of an instruction-tuned backbone provides better generalization to out-of-distribution, real-world data.</p>\n\n",
                "matched_terms": [
                    "set",
                    "baselora",
                    "average",
                    "emotional",
                    "recording",
                    "prosodic",
                    "instructft",
                    "dimensions",
                    "all",
                    "instructlora",
                    "test",
                    "real"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While <span class=\"ltx_text ltx_font_typewriter\">Instruct-FT</span> is competitive on in-domain evaluation, it suffers a substantial drop when applied to real recordings. By contrast, <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> offers a more robust balance: it maintains reasonable performance on archetype and basic sets, while significantly outperforming other settings on real-world data. Based on this robustness, we adopt <span class=\"ltx_text ltx_font_typewriter\">Instruct-LoRA</span> as our final configuration for subsequent experiments.</p>\n\n",
                "matched_terms": [
                    "real",
                    "instructft",
                    "other",
                    "instructlora",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Despite the comprehensive design of Speech-DRAME, several limitations remain.\nFirst, while our dual evaluation paradigm combines top-down archetype assessment with bottom-up realism evaluation, both strategies are constrained by the quality and scope of available data. Archetype evaluation inevitably relies on simplified stereotypes, which can overlook subtler paralinguistic and contextual cues. Realism evaluation, though richer, still struggles with domain mismatch between synthetic data and real human recordings, as evidenced by the performance drop observed in the real-recording test set. This highlights the persistent gap between benchmark-driven evaluation and real-world variability in speech role-play.</p>\n\n",
                "matched_terms": [
                    "set",
                    "roleplay",
                    "realism",
                    "contextual",
                    "performance",
                    "test",
                    "real"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, although we validated DRAME-Eval against human annotations, alignment under realism settings remains modest. This suggests that even human-annotated benchmarks are insufficiently fine-grained to capture the full complexity of human perception in speech role-play. Richer annotation protocols, more diverse speaker populations, and integration of perceptual psychology insights could further improve evaluation fidelity.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "realism"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Looking forward, we see several promising research directions. We intentionally restrict evaluation to single-turn responses for clarity and tractability. However, multi-turn extensions that capture narrative progression, long-term consistency, and dynamic emotion shifts remain a natural next step. Incorporating multilingual and cross-cultural datasets will enhance generality and fairness. On the modeling side, exploring hybrid evaluation strategies that combine discrete rubrics with continuous perceptual signals (e.g., prosody embeddings or affective measures) may better bridge the gap between automatic and human judgments. Finally, integrating Speech-DRAME into reinforcement learning pipelines as a reward model could help drive the next generation of expressive, human-aligned speech role-play systems.</p>\n\n",
                "matched_terms": [
                    "roleplay",
                    "consistency"
                ]
            }
        ]
    }
}