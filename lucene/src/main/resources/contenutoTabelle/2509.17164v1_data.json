{
    "S4.T1": {
        "source_file": "STAR: Speech-to-Audio Generation via Representation Learning",
        "caption": "Table 1: Linear probing classification results on AudioCaps.\nThe caption representation performance serves as the topline.",
        "body": "Input\nCaption\nSpeech\n\n\nEncoder\nCLAP\nFlan-T5\nHuBERT\nWavLM\nDAC\n\n\n\n\nMLP\n0.500\n0.419\n0.457\n0.451\n0.114\n\n\nQ-Former\n0.472\n0.486\n0.457\n0.453\n0.108",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Input</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"2\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Caption</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Speech</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Encoder</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">CLAP</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Flan-T5</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">HuBERT</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">WavLM</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">DAC</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">MLP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.500</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.419</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.457</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.451</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.114</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">Q-Former</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.472</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.486</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">0.457</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.453</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.45pt;padding-bottom:0.45pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">0.108</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "representation",
            "caption",
            "mlp",
            "topline",
            "flant5",
            "hubert",
            "input",
            "audiocaps",
            "encoder",
            "speech",
            "classification",
            "serves",
            "clap",
            "wavlm",
            "probing",
            "results",
            "dac",
            "performance",
            "qformer",
            "linear"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The spoken sound event semantic representation capabilities are shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#S4.T1\" title=\"In 4 Experimental Setup &#8227; STAR: Speech-to-Audio Generation via Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nWe also incorporate text encoders with the caption input as a topline since the input is the original caption.\nTwo text encoders are evaluated: audio-oriented CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib18\" title=\"\">18</a>]</cite>, and general-purpose Flan-T5&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib19\" title=\"\">19</a>]</cite>.\nBoth have been shown effective in TTA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib16\" title=\"\">16</a>]</cite>.\nResults show that semantic representations are comparable to the caption representation topline, indicating <span class=\"ltx_text ltx_font_bold\">the spoken sound event semantics can be effectively\nand directly extracted from speech signals</span>.\nSpecifically, HuBERT performs slightly better than WavLM.\nBoth outperforms DAC, as they focus on semantic information, whereas DAC emphasizes acoustic details.\nThe Q-Former exhibits superior performance across different representations compared to MLP.\nTherefore, we adopt HuBERT and Q-Former in the following experiments by default.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This work presents STAR, the first end-to-end speech-to-audio generation framework, designed to enhance efficiency and address error propagation inherent in cascaded systems.\nUnlike prior approaches relying on text or vision, STAR leverages speech as it constitutes a natural modality for interaction.\nAs an initial step to validate the feasibility of the system, we demonstrate through representation learning experiments that spoken sound event semantics can be effectively extracted from raw speech, capturing both auditory events and scene cues.\nLeveraging the semantic representations, STAR incorporates a bridge network for representation mapping and a two-stage training strategy to achieve end-to-end synthesis.\nWith a <math alttext=\"76.9\\%\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mrow><mn>76.9</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">76.9\\%</annotation></semantics></math> reduction in speech processing latency, STAR demonstrates superior generation performance over the cascaded systems.\nOverall, STAR establishes speech as a direct interaction signal for audio generation, thereby bridging representation learning and multimodal synthesis.\nGenerated samples are available at <a class=\"ltx_ref ltx_href ltx_font_italic\" href=\"https://zeyuxie29.github.io/STAR/\" style=\"--ltx-fg-color:#00FFFF;\" title=\"\">https://zeyuxie29.github.io/STAR</a>.</p>\n\n",
                "matched_terms": [
                    "representation",
                    "performance",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While significant progress in audio generation, speech &#8212; a natural and fundamental form of human communication and an interaction-oriented input modality &#8212; has received little attention. For instance, given a spoken query such as &#8220;the sound of birds&#8221;, no current system can directly produce the corresponding audio.\nTo address this gap, we propose the speech-to-audio (STA) task, which aims to understand event information within speech and generate corresponding audio.\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Naturally, one would argue that such task can be achieved by a cascaded system consisting of automatic speech recognition (ASR) and text-to-audio (TTA) models.\nHowever, the cascaded system introduces unnecessary latency, which can significantly hinder the human-computer interaction experience.\nOur analysis reveals that at least <math alttext=\"156ms\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m1\" intent=\":literal\"><semantics><mrow><mn>156</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">156ms</annotation></semantics></math> of delay is required to process a speech input in a cascaded system, while our end-to-end (E2E) approach reduces the latency to <math alttext=\"36ms\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m2\" intent=\":literal\"><semantics><mrow><mn>36</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">36ms</annotation></semantics></math> (<math alttext=\"\\approx 76.9\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mrow><mn>76.9</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\approx 76.9\\%</annotation></semantics></math> reduction).\nFurthermore, E2E models offer the key advantage of unified training with speech language models, enabling seamless integration of audio generation into human-computer interaction.\nThis unlocks new capabilities for systems like Speech SpeechGPT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib1\" title=\"\">1</a>]</cite> and Qwen-Omni&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib2\" title=\"\">2</a>]</cite>, overcoming the fundamental limitations of cascaded systems.\nBy design, E2E systems deliver low latency, native integration, and reduced cascading errors, establishing a new track for audio language models.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another pertinent issue concerns the feasibility of the E2E STA system &#8212; namely, whether sufficient information can be <span class=\"ltx_text ltx_font_bold\">directly extracted</span> from speech signals to drive audio generation.\nSpecifically, can we extract speech representations that contain semantic information of sound events like &#8220;dog barking&#8221; or &#8220;bell ringing&#8221;?\nSuch representations are referred to as <span class=\"ltx_text ltx_framed ltx_framed_underline\">spoken sound event semantics</span>.\nTo this end, we conducted a series of representation learning experiments to evaluate the cross-modal information extraction capability of different encoders.\nThe results suggest that self-supervised pre-training speech encoders, such as HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib3\" title=\"\">3</a>]</cite> and WavLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib4\" title=\"\">4</a>]</cite>, <span class=\"ltx_text ltx_font_bold\">are capable of extracting spoken sound event semantics from speech signals</span>.\nOn the strength of this evidence, we developed STAR (<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">S</span>peech-<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">T</span>o-<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">A</span>udio generation via <span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">R</span>epresentation learning).</p>\n\n",
                "matched_terms": [
                    "representation",
                    "speech",
                    "hubert",
                    "wavlm",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better bridge the speech and audio modalities, we draw on insights from large-model fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib5\" title=\"\">5</a>]</cite> to introduce a bridge network and a two-stage training strategy.\nSpecifically, our framework consists of a speech encoder for extracting speech embeddings, a bridge network that maps them to spoken sound event semantics, and a flow matching model for generation.\nThe two-stage training comprises (1) representation learning for speech-audio semantic alignment, and (2) generative model training.\nExperimental results demonstrate that our end-to-end system significantly reduces speech processing latency while surpassing the generation performance of cascaded systems.\nThis also validates the effectiveness of leveraging speech as a direct input modality to guide audio generation, making it a pioneering exploration for integration with audio LLMs.\nOur contributions are as follows:\n</p>\n\n",
                "matched_terms": [
                    "representation",
                    "speech",
                    "input",
                    "performance",
                    "encoder",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Significantly reduces speech processing latency from <math alttext=\"156ms\" class=\"ltx_Math\" display=\"inline\" id=\"S1.I1.i4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>156</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">156ms</annotation></semantics></math> to <math alttext=\"36ms\" class=\"ltx_Math\" display=\"inline\" id=\"S1.I1.i4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>36</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">36ms</annotation></semantics></math> (<math alttext=\"\\approx 76.9\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S1.I1.i4.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mrow><mn>76.9</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\approx 76.9\\%</annotation></semantics></math> reduction), while surpassing the generation performance of cascaded systems.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Representation learning was conducted to validate the feasibility of end-to-end STA, as it is essential for extracting sufficient semantic information from speech &#8212; particularly information pertaining to sound event types.\nHence we utilize a combination of speech encoder and bridge network to (1) extract high-level compact features from raw speech signals and (2) map features to spoken sound event semantic representation that\ncontain sound event information, respectively.</p>\n\n",
                "matched_terms": [
                    "representation",
                    "speech",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate the narrative, we first provide the following definitions:\nA <span class=\"ltx_text ltx_framed ltx_framed_underline\">caption</span> refers to textual content that describes sound events.\nIn STA, the input is the speech waveform that reads the caption out, hereafter referred to as <span class=\"ltx_text ltx_framed ltx_framed_underline\">speech</span>.\nThe generated content is <span class=\"ltx_text ltx_framed ltx_framed_underline\">audio</span>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "caption",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A speech encoder is necessary to extract compact embedding from redundant raw speech signals.\nWe investigate the efficacy of different types of encoders: one type includes HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib3\" title=\"\">3</a>]</cite> and WavLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib4\" title=\"\">4</a>]</cite>, which are pre-trained with masked language modeling objectives; the other type consists of codec models, such as the descript-audio-codec (DAC)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib6\" title=\"\">6</a>]</cite>, which is trained using an autoencoding approach.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "dac",
                    "hubert",
                    "wavlm",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The extracted speech embedding is denoted as <math alttext=\"\\mathcal{\\hat{S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\mathcal{\\hat{S}}</annotation></semantics></math>.\nTo align <math alttext=\"\\mathcal{\\hat{S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\mathcal{\\hat{S}}</annotation></semantics></math> with sound event semantic representations <math alttext=\"\\mathcal{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><annotation encoding=\"application/x-tex\">\\mathcal{S}</annotation></semantics></math> for guiding audio generation, we employ a bridge network for mapping.\nTwo architectures are investigated:\n(1) MLP directly projects <math alttext=\"\\mathcal{\\hat{S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\"><semantics><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\mathcal{\\hat{S}}</annotation></semantics></math> by fully-connected layers and an average pooling operation;\n(2) Q-Former&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib7\" title=\"\">7</a>]</cite> encodes <math alttext=\"\\mathcal{\\hat{S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\"><semantics><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\mathcal{\\hat{S}}</annotation></semantics></math> through cross-attention with a fixed number of learnable query embeddings.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "qformer",
                    "mlp"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our representation learning experiments, the downstream task of sound event classification is utilized to evaluate the quality of the mapped representation, since accurate sound event identification is paramount for audio generation.</p>\n\n",
                "matched_terms": [
                    "representation",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The representation learning results (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#S5.SS1\" title=\"5.1 Representation Learning Evaluation &#8227; 5 Results and Analysis &#8227; STAR: Speech-to-Audio Generation via Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>) demonstrate that spoken sound event semantics can be effectively and directly extracted from speech signals, thus enabling the development of an E2E STA system.\nAs shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#S1.F2\" title=\"In 1 Introduction &#8227; STAR: Speech-to-Audio Generation via Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, STAR combines the speech encoder and bridge network from representation experiments to extract semantic representations, and further incorporates a generation module composed of flow matching and a variational autoencoder (VAE).</p>\n\n",
                "matched_terms": [
                    "representation",
                    "speech",
                    "encoder",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The VAE is employed to extract representations from audio to reduce the computation burden.\nThe VAE encoder compresses the audio <math alttext=\"\\mathcal{A}\\in\\mathbb{R}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{A}\\in\\mathbb{R}^{T}</annotation></semantics></math> into the latent representation <math alttext=\"\\mathbf{z}\\in\\mathbb{R}^{\\frac{T}{R}\\times D}=[\\mathbf{z}_{\\mu},\\mathbf{z}_{\\sigma}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#119859;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mfrac><mi>T</mi><mi>R</mi></mfrac><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119859;</mi><mi>&#956;</mi></msub><mo>,</mo><msub><mi>&#119859;</mi><mi>&#963;</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{z}\\in\\mathbb{R}^{\\frac{T}{R}\\times D}=[\\mathbf{z}_{\\mu},\\mathbf{z}_{\\sigma}]</annotation></semantics></math> with a compression ratio <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math>.\nThe VAE decoder reconstructs the audio <math alttext=\"\\mathcal{\\tilde{A}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\mathcal{\\tilde{A}}</annotation></semantics></math> based on samples drawn from the distribution <math alttext=\"\\mathcal{N}(\\mathbf{z}_{\\mu},\\mathbf{z}_{\\sigma})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119859;</mi><mi>&#956;</mi></msub><mo>,</mo><msub><mi>&#119859;</mi><mi>&#963;</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(\\mathbf{z}_{\\mu},\\mathbf{z}_{\\sigma})</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "representation",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets</span>\nFor an E2E STA system, we create the speech form of caption to construct training speech-audio pairs.\nWe use AudioCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib11\" title=\"\">11</a>]</cite> and a text-to-speech model VITS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib12\" title=\"\">12</a>]</cite> to convert caption into corresponding speech.\nSince AudioCaps is a subset of the sound event classification dataset, AudioSet&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib13\" title=\"\">13</a>]</cite>, the event labels are accessible for representation learning.</p>\n\n",
                "matched_terms": [
                    "representation",
                    "classification",
                    "speech",
                    "caption",
                    "audiocaps"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 1: Semantic representation learning</span>\nWe conduct representation learning&#8212;while simultaneously pre-training the bridge network&#8212;by appending a classifier for sound event classification, using labels from AudioSet&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib13\" title=\"\">13</a>]</cite>.\nThe bridge network is pre-trained with a learning rate of <math alttext=\"3.2\\times 10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\"><semantics><mrow><mn>3.2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">3.2\\times 10^{-3}</annotation></semantics></math>, a maximum of <math alttext=\"500\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m2\" intent=\":literal\"><semantics><mn>500</mn><annotation encoding=\"application/x-tex\">500</annotation></semantics></math> epochs and an early stopping of <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m3\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math> epochs.\nThe evaluation metric is mean average precision (mAP).</p>\n\n",
                "matched_terms": [
                    "representation",
                    "classification"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Metrics</span> Common objective audio generation metrics, including FAD, FD, KL divergence, inception score (IS) and CLAP similarity, are employed.\nWe also perform human subjective evaluation, where evaluators listen to 10 samples from each model and score them on two dimensions: overall audio quality (OVL) and relevance to the input (REL).\nAll evaluators are screened for no hearing loss and have university-level education from prestigious institutions.\nThey use designated headphones during evaluation.</p>\n\n",
                "matched_terms": [
                    "input",
                    "clap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first evaluate the effectiveness of spoken sound event semantic representation extracted by different speech encoders and bridge networks.\nThen, we compare STAR with cascaded baselines in terms of latency and generation performance, with ablations on the architecture.\n</p>\n\n",
                "matched_terms": [
                    "representation",
                    "performance",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The E2E system mitigates the latency introduced by the cascaded system.\nWe measured the time required to extract semantic representation from each speech input.\nFor the cascaded system, this includes the latency of the ASR model and text encoder, while for the E2E system, it includes the latency of the speech encoder and bridge network.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#S4.T2\" title=\"In 4 Experimental Setup &#8227; STAR: Speech-to-Audio Generation via Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the average results on test set.\nThe cascaded system requires an average delay of at least <math alttext=\"156ms\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>156</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">156ms</annotation></semantics></math>, whereas STAR processes the speech signal directly with only <math alttext=\"36ms\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>36</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">36ms</annotation></semantics></math> latency (<math alttext=\"\\approx 76.9\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mrow><mn>76.9</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\approx 76.9\\%</annotation></semantics></math> reduction), significantly reducing latency and holding the potential to enhance interactivity.</p>\n\n",
                "matched_terms": [
                    "representation",
                    "speech",
                    "input",
                    "encoder",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Results of different STA systems are presented in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#S4.T2\" title=\"In 4 Experimental Setup &#8227; STAR: Speech-to-Audio Generation via Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nBoth objective and subjective evaluation show that STAR outperforms all cascaded systems.\nWe observe that the ASR results contain inevitable errors while some TTA models are quite sensitive to the text input, resulting in unsatisfactory performance.\nIn contrast, as an E2E system, STAR takes speech embedding directly as the input to the generation model, mitigating the error accumulation problem.\nOverall, the performance demonstrates the feasibility of E2E STA, to support more seamless human-machine interaction.\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "performance",
                    "results",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We further conducted ablation studies on the impacts of representation learning and speech encoder selection, as shown in the lower half of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#S4.T2\" title=\"In 4 Experimental Setup &#8227; STAR: Speech-to-Audio Generation via Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nWhen we directly adopt HuBERT features as the input (w/o BridgeNet), the speech processing latency reduces slightly but the generation performance degrades noticeably.\nThis validates the effectiveness of the bridge network in mapping speech embedding to spoken sound event semantics.\nReplacing HuBERT features with DAC results in much worse performance.\nThis corresponds to the linear probing results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#S5.SS1\" title=\"5.1 Representation Learning Evaluation &#8227; 5 Results and Analysis &#8227; STAR: Speech-to-Audio Generation via Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, indicating that STA prefers semantic features than acoustic features as only the speech content is the effective information for generation.</p>\n\n",
                "matched_terms": [
                    "representation",
                    "speech",
                    "dac",
                    "hubert",
                    "input",
                    "linear",
                    "performance",
                    "probing",
                    "encoder",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work proposes STAR, an E2E STA system that leverages representation learning to create sound effects from spoken descriptions.\nThrough comprehensive representation learning experiments, we demonstrate that the combination of pre-trained speech encoders and a bridge network can extract sufficient semantic information for audio generation.\nUtilizing a Q-Former bridge network and a two-stage training strategy, STAR significantly reduces the processing latency while outperforming the cascaded baseline systems.\nAblation studies validate the necessity of our model architecture.\nWe hope STAR will contribute to the design of future E2E speech dialogue models, endowing them with omni generation abilities.\n</p>\n\n",
                "matched_terms": [
                    "representation",
                    "speech",
                    "qformer"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "STAR: Speech-to-Audio Generation via Representation Learning",
        "caption": "Table 2: Generation performance of different systems.\n“Processing latency” refers to the representation processing delay for one speech input, with the results representing the mean and variance. OVL = overall quality. REL = relevance to input.",
        "body": "System\nProcessing\nGeneration\nObjective\nSubjective\n\n\n\nLatency (ms) ↓\\downarrow\n\n\nTime (s) ↓\\downarrow\n\n\nFAD ↓\\downarrow\n\n\nFD ↓\\downarrow\n\n\nKL ↓\\downarrow\n\n\nIS ↑\\uparrow\n\n\nCLAP ↑\\uparrow\n\n\nOVL ↑\\uparrow\n\n\nREL ↑\\uparrow\n\n\n\nCascade (“W” refers to the WHISPER ASR model)\n\n\n\n\nW + AudioLDM 2\n\n265.83 ±72.89\n\n\n52.78 ±1.66\n\n2.40\n20.92\n1.62\n8.38\n0.377\n\n3.54 ±1.02\n\n\n3.54 ±1.15\n\n\n\nW + TANGO 2\n\n156.33 ±62.16\n\n\n14.62 ±0.10\n\n2.57\n19.89\n1.22\n8.75\n0.466\n\n3.39 ±0.88\n\n\n3.78 ±1.12\n\n\n\nW + AUDIT\n\n187.86 ±75.75\n\n\n9.13 ±0.43\n\n6.20\n41.89\n1.88\n6.26\n0.293\n\n2.74 ±1.08\n\n\n3.16 ±1.25\n\n\n\nEnd-to-End (Ours)\n\n\nSTAR\n\n35.93 ±14.50\n\n\n2.76 ±0.03\n\n2.23\n15.42\n1.48\n12.22\n0.422\n\n4.22 ±0.92\n\n\n4.30 ±0.88\n\n\n\n    w/o BridgeNet\n\n32.83 ±26.29\n\n\n2.90 ±0.10\n\n3.63\n16.20\n1.68\n9.54\n0.390\n-\n-\n\n\n    DAC Encoder\n\n58.68 ±30.44\n\n\n2.77 ±0.03\n\n5.49\n29.69\n4.21\n6.62\n0.112\n-\n-",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">System</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Processing</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Generation</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" colspan=\"5\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Objective</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Subjective</span></th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Latency (ms)</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Time (s)</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">FAD </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">FD </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">KL </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">IS </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CLAP </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m7\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">OVL </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m8\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">REL </span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m9\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"10\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Cascade (&#8220;W&#8221; refers to the WHISPER ASR model)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">W + AudioLDM 2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">265.83 </span><span class=\"ltx_text\" style=\"font-size:80%;\">&#177;72.89</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">52.78 </span><span class=\"ltx_text\" style=\"font-size:80%;\">&#177;1.66</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.40</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">20.92</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.62</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.38</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.377</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.54 </span><span class=\"ltx_text\" style=\"font-size:80%;\">&#177;1.02</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.54 </span><span class=\"ltx_text\" style=\"font-size:80%;\">&#177;1.15</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">W + TANGO 2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">156.33 </span><span class=\"ltx_text\" style=\"font-size:80%;\">&#177;62.16</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">14.62 </span><span class=\"ltx_text\" style=\"font-size:80%;\">&#177;0.10</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.57</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">19.89</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1.22</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">8.75</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.466</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.39 </span><span class=\"ltx_text\" style=\"font-size:80%;\">&#177;0.88</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.78 </span><span class=\"ltx_text\" style=\"font-size:80%;\">&#177;1.12</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">W + AUDIT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">187.86 </span><span class=\"ltx_text\" style=\"font-size:80%;\">&#177;75.75</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">9.13 </span><span class=\"ltx_text\" style=\"font-size:80%;\">&#177;0.43</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.20</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">41.89</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.88</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.26</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.293</span></td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">2.74 </span><span class=\"ltx_text\" style=\"font-size:80%;\">&#177;1.08</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">3.16 </span><span class=\"ltx_text\" style=\"font-size:80%;\">&#177;1.25</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"10\"><span class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">End-to-End (Ours)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">STAR</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">35.93 </span><span class=\"ltx_text\" style=\"font-size:80%;\">&#177;14.50</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2.76</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text\" style=\"font-size:80%;\">&#177;0.03</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2.23</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">15.42</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.48</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">12.22</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.422</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.22</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text\" style=\"font-size:80%;\">&#177;0.92</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">4.30</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text\" style=\"font-size:80%;\">&#177;0.88</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8202;&#8195; w/o BridgeNet</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">\n<span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">32.83</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text\" style=\"font-size:80%;\">&#177;26.29</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">2.90 </span><span class=\"ltx_text\" style=\"font-size:80%;\">&#177;0.10</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.63</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">16.20</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.68</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">9.54</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.390</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">&#8194;&#8202;&#8195; DAC Encoder</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">58.68 </span><span class=\"ltx_text\" style=\"font-size:80%;\">&#177;30.44</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">2.77 </span><span class=\"ltx_text\" style=\"font-size:80%;\">&#177;0.03</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">5.49</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">29.69</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">4.21</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">6.62</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.112</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text\" style=\"font-size:90%;\">-</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "representation",
            "overall",
            "±3044",
            "quality",
            "±6216",
            "subjective",
            "±043",
            "±092",
            "input",
            "↓downarrow",
            "latency”",
            "±7289",
            "ours",
            "bridgenet",
            "variance",
            "refers",
            "encoder",
            "representing",
            "objective",
            "speech",
            "±1450",
            "rel",
            "star",
            "one",
            "“w”",
            "±7575",
            "ovl",
            "mean",
            "latency",
            "clap",
            "results",
            "whisper",
            "generation",
            "system",
            "relevance",
            "model",
            "audit",
            "±125",
            "endtoend",
            "±003",
            "±2629",
            "cascade",
            "±115",
            "dac",
            "fad",
            "±010",
            "±108",
            "±112",
            "systems",
            "performance",
            "↑uparrow",
            "delay",
            "±166",
            "asr",
            "time",
            "“processing",
            "different",
            "±102",
            "processing",
            "tango",
            "±088",
            "audioldm"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The E2E system mitigates the latency introduced by the cascaded system.\nWe measured the time required to extract semantic representation from each speech input.\nFor the cascaded system, this includes the latency of the ASR model and text encoder, while for the E2E system, it includes the latency of the speech encoder and bridge network.\n<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#S4.T2\" title=\"In 4 Experimental Setup &#8227; STAR: Speech-to-Audio Generation via Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a> presents the average results on test set.\nThe cascaded system requires an average delay of at least <math alttext=\"156ms\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>156</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">156ms</annotation></semantics></math>, whereas STAR processes the speech signal directly with only <math alttext=\"36ms\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>36</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">36ms</annotation></semantics></math> latency (<math alttext=\"\\approx 76.9\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mrow><mn>76.9</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\approx 76.9\\%</annotation></semantics></math> reduction), significantly reducing latency and holding the potential to enhance interactivity.</p>\n\n",
            "<p class=\"ltx_p\">Results of different STA systems are presented in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#S4.T2\" title=\"In 4 Experimental Setup &#8227; STAR: Speech-to-Audio Generation via Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nBoth objective and subjective evaluation show that STAR outperforms all cascaded systems.\nWe observe that the ASR results contain inevitable errors while some TTA models are quite sensitive to the text input, resulting in unsatisfactory performance.\nIn contrast, as an E2E system, STAR takes speech embedding directly as the input to the generation model, mitigating the error accumulation problem.\nOverall, the performance demonstrates the feasibility of E2E STA, to support more seamless human-machine interaction.\n</p>\n\n",
            "<p class=\"ltx_p\">We further conducted ablation studies on the impacts of representation learning and speech encoder selection, as shown in the lower half of <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#S4.T2\" title=\"In 4 Experimental Setup &#8227; STAR: Speech-to-Audio Generation via Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>.\nWhen we directly adopt HuBERT features as the input (w/o BridgeNet), the speech processing latency reduces slightly but the generation performance degrades noticeably.\nThis validates the effectiveness of the bridge network in mapping speech embedding to spoken sound event semantics.\nReplacing HuBERT features with DAC results in much worse performance.\nThis corresponds to the linear probing results in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#S5.SS1\" title=\"5.1 Representation Learning Evaluation &#8227; 5 Results and Analysis &#8227; STAR: Speech-to-Audio Generation via Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">Section</span>&#160;<span class=\"ltx_text ltx_ref_tag\">5.1</span></a>, indicating that STA prefers semantic features than acoustic features as only the speech content is the effective information for generation.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">This work presents STAR, the first end-to-end speech-to-audio generation framework, designed to enhance efficiency and address error propagation inherent in cascaded systems.\nUnlike prior approaches relying on text or vision, STAR leverages speech as it constitutes a natural modality for interaction.\nAs an initial step to validate the feasibility of the system, we demonstrate through representation learning experiments that spoken sound event semantics can be effectively extracted from raw speech, capturing both auditory events and scene cues.\nLeveraging the semantic representations, STAR incorporates a bridge network for representation mapping and a two-stage training strategy to achieve end-to-end synthesis.\nWith a <math alttext=\"76.9\\%\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mrow><mn>76.9</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">76.9\\%</annotation></semantics></math> reduction in speech processing latency, STAR demonstrates superior generation performance over the cascaded systems.\nOverall, STAR establishes speech as a direct interaction signal for audio generation, thereby bridging representation learning and multimodal synthesis.\nGenerated samples are available at <a class=\"ltx_ref ltx_href ltx_font_italic\" href=\"https://zeyuxie29.github.io/STAR/\" style=\"--ltx-fg-color:#00FFFF;\" title=\"\">https://zeyuxie29.github.io/STAR</a>.</p>\n\n",
                "matched_terms": [
                    "representation",
                    "overall",
                    "speech",
                    "generation",
                    "system",
                    "star",
                    "endtoend",
                    "processing",
                    "latency",
                    "systems",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Index Terms<span class=\"ltx_text ltx_font_upright\">&#8212;&#8201;</span></span>\nSpeech-to-audio generation, representation learning, end-to-end generation, bridge network</p>\n\n",
                "matched_terms": [
                    "representation",
                    "generation",
                    "endtoend"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While significant progress in audio generation, speech &#8212; a natural and fundamental form of human communication and an interaction-oriented input modality &#8212; has received little attention. For instance, given a spoken query such as &#8220;the sound of birds&#8221;, no current system can directly produce the corresponding audio.\nTo address this gap, we propose the speech-to-audio (STA) task, which aims to understand event information within speech and generate corresponding audio.\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation",
                    "system",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Naturally, one would argue that such task can be achieved by a cascaded system consisting of automatic speech recognition (ASR) and text-to-audio (TTA) models.\nHowever, the cascaded system introduces unnecessary latency, which can significantly hinder the human-computer interaction experience.\nOur analysis reveals that at least <math alttext=\"156ms\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m1\" intent=\":literal\"><semantics><mrow><mn>156</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">156ms</annotation></semantics></math> of delay is required to process a speech input in a cascaded system, while our end-to-end (E2E) approach reduces the latency to <math alttext=\"36ms\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m2\" intent=\":literal\"><semantics><mrow><mn>36</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">36ms</annotation></semantics></math> (<math alttext=\"\\approx 76.9\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S1.p2.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mrow><mn>76.9</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\approx 76.9\\%</annotation></semantics></math> reduction).\nFurthermore, E2E models offer the key advantage of unified training with speech language models, enabling seamless integration of audio generation into human-computer interaction.\nThis unlocks new capabilities for systems like Speech SpeechGPT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib1\" title=\"\">1</a>]</cite> and Qwen-Omni&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib2\" title=\"\">2</a>]</cite>, overcoming the fundamental limitations of cascaded systems.\nBy design, E2E systems deliver low latency, native integration, and reduced cascading errors, establishing a new track for audio language models.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation",
                    "system",
                    "endtoend",
                    "one",
                    "input",
                    "latency",
                    "systems",
                    "delay",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Another pertinent issue concerns the feasibility of the E2E STA system &#8212; namely, whether sufficient information can be <span class=\"ltx_text ltx_font_bold\">directly extracted</span> from speech signals to drive audio generation.\nSpecifically, can we extract speech representations that contain semantic information of sound events like &#8220;dog barking&#8221; or &#8220;bell ringing&#8221;?\nSuch representations are referred to as <span class=\"ltx_text ltx_framed ltx_framed_underline\">spoken sound event semantics</span>.\nTo this end, we conducted a series of representation learning experiments to evaluate the cross-modal information extraction capability of different encoders.\nThe results suggest that self-supervised pre-training speech encoders, such as HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib3\" title=\"\">3</a>]</cite> and WavLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib4\" title=\"\">4</a>]</cite>, <span class=\"ltx_text ltx_font_bold\">are capable of extracting spoken sound event semantics from speech signals</span>.\nOn the strength of this evidence, we developed STAR (<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">S</span>peech-<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">T</span>o-<span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">A</span>udio generation via <span class=\"ltx_text\" style=\"--ltx-fg-color:#8C8C8C;\">R</span>epresentation learning).</p>\n\n",
                "matched_terms": [
                    "representation",
                    "generation",
                    "speech",
                    "system",
                    "star",
                    "different",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To better bridge the speech and audio modalities, we draw on insights from large-model fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib5\" title=\"\">5</a>]</cite> to introduce a bridge network and a two-stage training strategy.\nSpecifically, our framework consists of a speech encoder for extracting speech embeddings, a bridge network that maps them to spoken sound event semantics, and a flow matching model for generation.\nThe two-stage training comprises (1) representation learning for speech-audio semantic alignment, and (2) generative model training.\nExperimental results demonstrate that our end-to-end system significantly reduces speech processing latency while surpassing the generation performance of cascaded systems.\nThis also validates the effectiveness of leveraging speech as a direct input modality to guide audio generation, making it a pioneering exploration for integration with audio LLMs.\nOur contributions are as follows:\n</p>\n\n",
                "matched_terms": [
                    "representation",
                    "speech",
                    "generation",
                    "system",
                    "model",
                    "endtoend",
                    "input",
                    "processing",
                    "latency",
                    "systems",
                    "performance",
                    "encoder",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recognize the potential of the speech-to-audio generation task and have designed the first E2E system STAR;</p>\n\n",
                "matched_terms": [
                    "generation",
                    "system",
                    "star"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Significantly reduces speech processing latency from <math alttext=\"156ms\" class=\"ltx_Math\" display=\"inline\" id=\"S1.I1.i4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>156</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">156ms</annotation></semantics></math> to <math alttext=\"36ms\" class=\"ltx_Math\" display=\"inline\" id=\"S1.I1.i4.p1.m2\" intent=\":literal\"><semantics><mrow><mn>36</mn><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>m</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow><annotation encoding=\"application/x-tex\">36ms</annotation></semantics></math> (<math alttext=\"\\approx 76.9\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S1.I1.i4.p1.m3\" intent=\":literal\"><semantics><mrow><mi/><mo>&#8776;</mo><mrow><mn>76.9</mn><mo>%</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\approx 76.9\\%</annotation></semantics></math> reduction), while surpassing the generation performance of cascaded systems.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation",
                    "processing",
                    "latency",
                    "systems",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Representation learning was conducted to validate the feasibility of end-to-end STA, as it is essential for extracting sufficient semantic information from speech &#8212; particularly information pertaining to sound event types.\nHence we utilize a combination of speech encoder and bridge network to (1) extract high-level compact features from raw speech signals and (2) map features to spoken sound event semantic representation that\ncontain sound event information, respectively.</p>\n\n",
                "matched_terms": [
                    "representation",
                    "speech",
                    "endtoend",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To facilitate the narrative, we first provide the following definitions:\nA <span class=\"ltx_text ltx_framed ltx_framed_underline\">caption</span> refers to textual content that describes sound events.\nIn STA, the input is the speech waveform that reads the caption out, hereafter referred to as <span class=\"ltx_text ltx_framed ltx_framed_underline\">speech</span>.\nThe generated content is <span class=\"ltx_text ltx_framed ltx_framed_underline\">audio</span>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "refers",
                    "input"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A speech encoder is necessary to extract compact embedding from redundant raw speech signals.\nWe investigate the efficacy of different types of encoders: one type includes HuBERT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib3\" title=\"\">3</a>]</cite> and WavLM&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib4\" title=\"\">4</a>]</cite>, which are pre-trained with masked language modeling objectives; the other type consists of codec models, such as the descript-audio-codec (DAC)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib6\" title=\"\">6</a>]</cite>, which is trained using an autoencoding approach.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "one",
                    "dac",
                    "different",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The extracted speech embedding is denoted as <math alttext=\"\\mathcal{\\hat{S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\mathcal{\\hat{S}}</annotation></semantics></math>.\nTo align <math alttext=\"\\mathcal{\\hat{S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\mathcal{\\hat{S}}</annotation></semantics></math> with sound event semantic representations <math alttext=\"\\mathcal{S}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><annotation encoding=\"application/x-tex\">\\mathcal{S}</annotation></semantics></math> for guiding audio generation, we employ a bridge network for mapping.\nTwo architectures are investigated:\n(1) MLP directly projects <math alttext=\"\\mathcal{\\hat{S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\"><semantics><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\mathcal{\\hat{S}}</annotation></semantics></math> by fully-connected layers and an average pooling operation;\n(2) Q-Former&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib7\" title=\"\">7</a>]</cite> encodes <math alttext=\"\\mathcal{\\hat{S}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\"><semantics><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#119982;</mi><mo>^</mo></mover><annotation encoding=\"application/x-tex\">\\mathcal{\\hat{S}}</annotation></semantics></math> through cross-attention with a fixed number of learnable query embeddings.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "generation"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In our representation learning experiments, the downstream task of sound event classification is utilized to evaluate the quality of the mapped representation, since accurate sound event identification is paramount for audio generation.</p>\n\n",
                "matched_terms": [
                    "representation",
                    "generation",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The representation learning results (see Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#S5.SS1\" title=\"5.1 Representation Learning Evaluation &#8227; 5 Results and Analysis &#8227; STAR: Speech-to-Audio Generation via Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">5.1</span></a>) demonstrate that spoken sound event semantics can be effectively and directly extracted from speech signals, thus enabling the development of an E2E STA system.\nAs shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#S1.F2\" title=\"In 1 Introduction &#8227; STAR: Speech-to-Audio Generation via Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>&#160;<span class=\"ltx_text ltx_ref_tag\">2</span></a>, STAR combines the speech encoder and bridge network from representation experiments to extract semantic representations, and further incorporates a generation module composed of flow matching and a variational autoencoder (VAE).</p>\n\n",
                "matched_terms": [
                    "representation",
                    "speech",
                    "system",
                    "generation",
                    "star",
                    "encoder",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The VAE is employed to extract representations from audio to reduce the computation burden.\nThe VAE encoder compresses the audio <math alttext=\"\\mathcal{A}\\in\\mathbb{R}^{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{A}\\in\\mathbb{R}^{T}</annotation></semantics></math> into the latent representation <math alttext=\"\\mathbf{z}\\in\\mathbb{R}^{\\frac{T}{R}\\times D}=[\\mathbf{z}_{\\mu},\\mathbf{z}_{\\sigma}]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mi>&#119859;</mi><mo>&#8712;</mo><msup><mi>&#8477;</mi><mrow><mfrac><mi>T</mi><mi>R</mi></mfrac><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><mi>D</mi></mrow></msup><mo>=</mo><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119859;</mi><mi>&#956;</mi></msub><mo>,</mo><msub><mi>&#119859;</mi><mi>&#963;</mi></msub><mo stretchy=\"false\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathbf{z}\\in\\mathbb{R}^{\\frac{T}{R}\\times D}=[\\mathbf{z}_{\\mu},\\mathbf{z}_{\\sigma}]</annotation></semantics></math> with a compression ratio <math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mi>R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math>.\nThe VAE decoder reconstructs the audio <math alttext=\"\\mathcal{\\tilde{A}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mover accent=\"true\"><mi class=\"ltx_font_mathcaligraphic\">&#119964;</mi><mo>~</mo></mover><annotation encoding=\"application/x-tex\">\\mathcal{\\tilde{A}}</annotation></semantics></math> based on samples drawn from the distribution <math alttext=\"\\mathcal{N}(\\mathbf{z}_{\\mu},\\mathbf{z}_{\\sigma})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119977;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>&#119859;</mi><mi>&#956;</mi></msub><mo>,</mo><msub><mi>&#119859;</mi><mi>&#963;</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{N}(\\mathbf{z}_{\\mu},\\mathbf{z}_{\\sigma})</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "representation",
                    "encoder"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Datasets</span>\nFor an E2E STA system, we create the speech form of caption to construct training speech-audio pairs.\nWe use AudioCaps&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib11\" title=\"\">11</a>]</cite> and a text-to-speech model VITS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib12\" title=\"\">12</a>]</cite> to convert caption into corresponding speech.\nSince AudioCaps is a subset of the sound event classification dataset, AudioSet&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib13\" title=\"\">13</a>]</cite>, the event labels are accessible for representation learning.</p>\n\n",
                "matched_terms": [
                    "representation",
                    "system",
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Stage 1: Semantic representation learning</span>\nWe conduct representation learning&#8212;while simultaneously pre-training the bridge network&#8212;by appending a classifier for sound event classification, using labels from AudioSet&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib13\" title=\"\">13</a>]</cite>.\nThe bridge network is pre-trained with a learning rate of <math alttext=\"3.2\\times 10^{-3}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m1\" intent=\":literal\"><semantics><mrow><mn>3.2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">&#215;</mo><msup><mn>10</mn><mrow><mo>&#8722;</mo><mn>3</mn></mrow></msup></mrow><annotation encoding=\"application/x-tex\">3.2\\times 10^{-3}</annotation></semantics></math>, a maximum of <math alttext=\"500\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m2\" intent=\":literal\"><semantics><mn>500</mn><annotation encoding=\"application/x-tex\">500</annotation></semantics></math> epochs and an early stopping of <math alttext=\"20\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p2.m3\" intent=\":literal\"><semantics><mn>20</mn><annotation encoding=\"application/x-tex\">20</annotation></semantics></math> epochs.\nThe evaluation metric is mean average precision (mAP).</p>\n\n",
                "matched_terms": [
                    "representation",
                    "mean"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Baseline</span>\nWe incorporate cascade systems for comparison, combining ASR and TTA.\nFor ASR, we use WHISPER&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib14\" title=\"\">14</a>]</cite>, while for TTA, we experiment several mainstream models: AudioLDM 2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib15\" title=\"\">15</a>]</cite>, TANGO 2&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib16\" title=\"\">16</a>]</cite> and AUDIT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib17\" title=\"\">17</a>]</cite>. We adopt default settings for these models.\nFor STAR, we use an NFE of 20 and a guidance scale of <math alttext=\"5.0\" class=\"ltx_Math\" display=\"inline\" id=\"S4.p4.m1\" intent=\":literal\"><semantics><mn>5.0</mn><annotation encoding=\"application/x-tex\">5.0</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "audit",
                    "star",
                    "cascade",
                    "audioldm",
                    "tango",
                    "systems",
                    "whisper",
                    "asr"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Metrics</span> Common objective audio generation metrics, including FAD, FD, KL divergence, inception score (IS) and CLAP similarity, are employed.\nWe also perform human subjective evaluation, where evaluators listen to 10 samples from each model and score them on two dimensions: overall audio quality (OVL) and relevance to the input (REL).\nAll evaluators are screened for no hearing loss and have university-level education from prestigious institutions.\nThey use designated headphones during evaluation.</p>\n\n",
                "matched_terms": [
                    "overall",
                    "generation",
                    "relevance",
                    "model",
                    "quality",
                    "rel",
                    "subjective",
                    "input",
                    "ovl",
                    "fad",
                    "clap",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We first evaluate the effectiveness of spoken sound event semantic representation extracted by different speech encoders and bridge networks.\nThen, we compare STAR with cascaded baselines in terms of latency and generation performance, with ablations on the architecture.\n</p>\n\n",
                "matched_terms": [
                    "representation",
                    "generation",
                    "speech",
                    "star",
                    "different",
                    "latency",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The spoken sound event semantic representation capabilities are shown in <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#S4.T1\" title=\"In 4 Experimental Setup &#8227; STAR: Speech-to-Audio Generation via Representation Learning\"><span class=\"ltx_text ltx_ref_tag\">Table</span>&#160;<span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nWe also incorporate text encoders with the caption input as a topline since the input is the original caption.\nTwo text encoders are evaluated: audio-oriented CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib18\" title=\"\">18</a>]</cite>, and general-purpose Flan-T5&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib19\" title=\"\">19</a>]</cite>.\nBoth have been shown effective in TTA&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.17164v1#bib.bib16\" title=\"\">16</a>]</cite>.\nResults show that semantic representations are comparable to the caption representation topline, indicating <span class=\"ltx_text ltx_font_bold\">the spoken sound event semantics can be effectively\nand directly extracted from speech signals</span>.\nSpecifically, HuBERT performs slightly better than WavLM.\nBoth outperforms DAC, as they focus on semantic information, whereas DAC emphasizes acoustic details.\nThe Q-Former exhibits superior performance across different representations compared to MLP.\nTherefore, we adopt HuBERT and Q-Former in the following experiments by default.</p>\n\n",
                "matched_terms": [
                    "representation",
                    "speech",
                    "dac",
                    "different",
                    "input",
                    "clap",
                    "performance",
                    "results"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This work proposes STAR, an E2E STA system that leverages representation learning to create sound effects from spoken descriptions.\nThrough comprehensive representation learning experiments, we demonstrate that the combination of pre-trained speech encoders and a bridge network can extract sufficient semantic information for audio generation.\nUtilizing a Q-Former bridge network and a two-stage training strategy, STAR significantly reduces the processing latency while outperforming the cascaded baseline systems.\nAblation studies validate the necessity of our model architecture.\nWe hope STAR will contribute to the design of future E2E speech dialogue models, endowing them with omni generation abilities.\n</p>\n\n",
                "matched_terms": [
                    "representation",
                    "generation",
                    "speech",
                    "system",
                    "model",
                    "star",
                    "processing",
                    "latency",
                    "systems"
                ]
            }
        ]
    }
}