{
    "S3.T1": {
        "source_file": "Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis",
        "caption": "\\fontspec_if_language:nTFENG\\addfontfeatureLanguage=EnglishTable 1: Category Distribution of the Annotated Bangla Corpus",
        "body": "Category\nPercentage\n\n\n\n\nReduplication\n66.3%\n\n\nRepetition\n32.9%\n\n\nNeither\n0.8%",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Category</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Percentage</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Reduplication</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">66.3%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Repetition</th>\n<td class=\"ltx_td ltx_align_center\">32.9%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Neither</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.8%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "fontspeciflanguagentfengaddfontfeaturelanguageenglishtable",
            "bangla",
            "category",
            "corpus",
            "reduplication",
            "neither",
            "repetition",
            "percentage",
            "distribution",
            "annotated"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The final distribution of the annotated corpus, which shows a significant imbalance, is presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#S3.T1\" title=\"\\fontspec_if_language:nTFENG\\addfontfeatureLanguage=EnglishTable 1 &#8227; \\fontspec_if_language:nTFENG\\addfontfeatureLanguage=English3.1.3 LLM-Aided Initial Labeling and Expert Annotation &#8227; \\fontspec_if_language:nTFENG\\addfontfeatureLanguage=English3.1 Corpus Creation: The Bangla Repetition Corpus &#8227; \\fontspec_if_language:nTFENG\\addfontfeatureLanguage=English3 Methodology &#8227; Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_ERROR undefined\">\\fontspec_if_language:nTF</span>ENG<span class=\"ltx_ERROR undefined\">\\addfontfeature</span>Language=English1</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Automatic Speech Recognition (ASR) transcripts, especially in low-resource languages like Bangla, contain a critical ambiguity: word-word repetitions can be either Repetition Disfluency (unintentional ASR error/hesitation) or Morphological Reduplication (a deliberate grammatical construct). Standard disfluency correction fails by erroneously deleting valid linguistic information. To solve this, we introduce the first publicly available, 20,000-row Bangla corpus, manually annotated to explicitly distinguish between these two phenomena in noisy ASR transcripts. We benchmark this novel resource using two paradigms: state-of-the-art multilingual Large Language Models (LLMs) and task-specific fine-tuning of encoder models. LLMs achieve competitive performance (up to 82.68% accuracy) with few-shot prompting. However, fine-tuning proves superior, with the language-specific BanglaBERT model achieving the highest accuracy of 84.78% and an F1 score of 0.677. This establishes a strong, linguistically-informed baseline and provides essential data for developing sophisticated, semantic-preserving text normalization systems for Bangla.</p>\n\n",
                "matched_terms": [
                    "bangla",
                    "corpus",
                    "reduplication",
                    "repetition",
                    "annotated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Developing models to resolve such language-specific ambiguities requires large-scale, high-quality annotated data. Despite its massive speaker base, Bangla is a low-resource language in NLP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib17\" title=\"\">17</a>]</cite> due to a scarcity of standardized, publicly available datasets. For the specific task of distinguishing repetition disfluency from morphological reduplication in Bangla, no publicly available annotated corpus existed prior to this work. This resource gap has been the primary impediment to developing and rigorously evaluating computational systems for this task, hindering the shift from generic, one-size-fits-all NLP solutions toward models sensitive to the unique grammatical structures of low-resource languages.</p>\n\n",
                "matched_terms": [
                    "bangla",
                    "corpus",
                    "reduplication",
                    "repetition",
                    "annotated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Corpus Creation:</span> We introduce the first publicly available, <math alttext=\"\\mathbf{20,000}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.I2.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>&#120784;&#120782;</mn><mo>,</mo><mn>&#120782;&#120782;&#120782;</mn></mrow><annotation encoding=\"application/x-tex\">\\mathbf{20,000}</annotation></semantics></math>-row Bangla corpus, manually annotated to explicitly distinguish between Repetition Disfluency and Morphological Reduplication in noisy ASR transcripts. Furthermore, we provide a fine-grained linguistic analysis by subcategorizing all Morphological Reduplication instances into nine distinct semantic and functional classes.</p>\n\n",
                "matched_terms": [
                    "bangla",
                    "corpus",
                    "reduplication",
                    "repetition",
                    "annotated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our research is situated at the intersection of speech processing, computational linguistics, and low-resource NLP. We contextualize our contribution by reviewing the distinct treatment of repetition as an error in disfluency correction versus a meaningful construct in morphological reduplication, and by outlining the standard methodological paradigms our work builds upon.</p>\n\n",
                "matched_terms": [
                    "repetition",
                    "reduplication"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, a fundamental limitation of the dominant DC paradigm is its inherently subtractive nature that treats all repetitions as &#8220;noise&#8221; to be deleted&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib11\" title=\"\">11</a>]</cite>. This approach is incompatible with languages like Bangla, where repetition is also a productive grammatical device. In linguistics, morphological reduplication is a rule-governed process where a word is repeated to encode specific semantic nuances, such as continuity, iterativity, or intensity&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib16\" title=\"\">16</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib1\" title=\"\">1</a>]</cite>. This creates a critical structural ambiguity where the surface form &#8216;word-word&#8216; can be either an error or a meaningful linguistic construct.</p>\n\n",
                "matched_terms": [
                    "repetition",
                    "reduplication",
                    "bangla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Developing robust NLP solutions for Bangla is hindered by a scarcity of standardized datasets, a common challenge for low-resource languages despite their large speaker populations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib17\" title=\"\">17</a>]</cite>. This has motivated broad efforts to create foundational resources and models for the Indic language family&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib13\" title=\"\">13</a>]</cite>. Our approach to corpus creation aligns with pragmatic solutions to this data gap: we leverage noisy, auto-generated ASR transcripts from YouTube&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib9\" title=\"\">9</a>]</cite>. This strategy is effective because the inherent flaws of ASR systems provide a naturalistic distribution of the very phenomena knowingly spurious repetitions, speaker hesitations, and correctly transcribed reduplications, required to train a robust real-world classifier.</p>\n\n",
                "matched_terms": [
                    "bangla",
                    "distribution",
                    "corpus"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Bangla Repetition Corpus was synthesized from real-world, noisy Automatic Speech Recognition (ASR) transcripts, ensuring a naturalistic distribution of both errors and grammatical forms. This process involved four steps: Scalable Data Acquisition, Automated Filtering, Expert Annotation, and Fine-Grained Sub-categorization.</p>\n\n",
                "matched_terms": [
                    "repetition",
                    "bangla",
                    "distribution",
                    "corpus"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the cleaned, large-scale corpus of ASR text, we automatically extracted the ambiguous cases of contiguous word repetition:</p>\n\n",
                "matched_terms": [
                    "repetition",
                    "corpus"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Repetition Isolation:</span> The corpus was processed in memory-efficient chunks. Consecutive, identical word pairs (<math alttext=\"\\text{word}_{i}=\\text{word}_{i-1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I2.i1.p1.m1\" intent=\":literal\"><semantics><mrow><msub><mtext>word</mtext><mi>i</mi></msub><mo>=</mo><msub><mtext>word</mtext><mrow><mi>i</mi><mo>&#8722;</mo><mn>1</mn></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\text{word}_{i}=\\text{word}_{i-1}</annotation></semantics></math>) were identified after tokenization using a regular expression that explicitly handles Bengali, English, and numerical tokens (<span class=\"ltx_text ltx_markedasmath ltx_font_typewriter\">[0&#774;980-0&#774;9FF]+|[a-zA-Z0-9]+</span>), while excluding purely numerical repetitions.</p>\n\n",
                "matched_terms": [
                    "repetition",
                    "corpus"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM-Aided Initial Labeling:</span> To accelerate the annotation of the vast corpus, the sentences were first processed using the leading commercial model, GPT-4o, as an initial categorization engine. The model utilized the structured, few-shot prompt strategy, providing preliminary labels (Reduplication, Repetition, or Neither) for the entire set.</p>\n\n",
                "matched_terms": [
                    "repetition",
                    "neither",
                    "reduplication",
                    "corpus"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Expert Manual Review and Verification:</span> The corpus with the preliminary LLM labels was then subjected to a rigorous manual review by expert Bengali speakers. This critical step served to correct any inaccuracies introduced by the LLM and to ensure linguistic fidelity, particularly for nuanced cases where the distinction between a complex reduplication form and a speaker hesitation was ambiguous.</p>\n\n",
                "matched_terms": [
                    "reduplication",
                    "corpus"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Reduplication (Grammatical):</span> The repetition is an intentional, rule-governed morphological process that conveys semantic nuances such as iterativity, continuity, intensity, or plurality. <em class=\"ltx_emph ltx_font_italic\">Example:</em> <span class=\"ltx_ERROR undefined\">\\banglafont</span>&#8220;&#2437;&#2434;&#2453;&#2455;&#2497;&#2482;&#2507; <span class=\"ltx_text ltx_font_bold\">&#2453;&#2480;&#2503; &#2453;&#2480;&#2503;</span> &#2438;&#2478;&#2480;&#2494; &#2447;&#2453;&#2463;&#2497; &#2438;&#2472;&#2509;&#2465;&#2494;&#2480;&#2488;&#2509;&#2463;&#2509;&#2479;&#2494;&#2472;&#2509;&#2465;&#2495;&#2434; &#2465;&#2503;&#2477;&#2503;&#2482;&#2474; &#2453;&#2480;&#2494; &#2458;&#2503;&#2487;&#2509;&#2463;&#2494; &#2453;&#2480;&#2476;&#2507;&#8221; (<em class=\"ltx_emph ltx_font_italic\">Onkogulo <math alttext=\"\\mathbf{kore\\text{ }kore}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I4.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119844;&#119848;&#119851;&#119838;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext class=\"ltx_mathvariant_italic\">&#160;</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#119844;&#119848;&#119851;&#119838;</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbf{kore\\text{ }kore}</annotation></semantics></math> amra&#8230;</em>)</p>\n\n",
                "matched_terms": [
                    "repetition",
                    "reduplication"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the primary classification, all instances identified as Morphological Reduplication underwent a second stage of fine-grained annotation to determine their specific semantic function. This was accomplished using a Large Language Model guided by a carefully constructed few-shot prompt that defined nine distinct subcategories:\n<span class=\"ltx_text ltx_font_bold\">Intensity/Emphasis</span>, <span class=\"ltx_text ltx_font_bold\">Frequency/Iteration</span>, <span class=\"ltx_text ltx_font_bold\">Continuity/Ongoing Action</span>, <span class=\"ltx_text ltx_font_bold\">Plurality/Multiplicity</span>, <span class=\"ltx_text ltx_font_bold\">Distributive/Separateness</span>, <span class=\"ltx_text ltx_font_bold\">Vagueness/Approximation</span>, <span class=\"ltx_text ltx_font_bold\">Echo Word/Rhyming</span>, <span class=\"ltx_text ltx_font_bold\">Reciprocal/Correlative</span>, and <span class=\"ltx_text ltx_font_bold\">Onomatopoeia</span>.\nFor instance, in the sentence <span class=\"ltx_ERROR undefined\">\\banglafont</span>&#8220;&#8230;&#2447;&#2453;&#2509;&#2488; &#2447;&#2480; &#2477;&#2509;&#2479;&#2494;&#2482;&#2497; &#2474;&#2503;&#2479;&#2492;&#2503; &#2455;&#2503;&#2459;&#2495; &#2453;&#2468; &#2453;&#2468; &#2476;&#2482;&#2507;&#8230;&#8221; (&#8230;we got the values of x, tell me what what&#8230;), the repeated word <span class=\"ltx_ERROR undefined\">\\banglafont</span>&#8220;&#2453;&#2468; &#2453;&#2468;&#8221; (<span class=\"ltx_text ltx_font_italic\">koto koto</span>) implies an iterative query for multiple values, leading to its classification as <span class=\"ltx_text ltx_font_bold\">Frequency / Iteration</span>. This two-tiered annotation process enriches the corpus, providing a detailed linguistic layer for future research.</p>\n\n",
                "matched_terms": [
                    "reduplication",
                    "corpus"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Explicit Context:</span> For few-shot tests, the prompt included explicit linguistic definitions and examples for the three target categories (Reduplication, Repetition, and Neither) to guide the LLMs&#8217; in-context learning capability.</p>\n\n",
                "matched_terms": [
                    "repetition",
                    "reduplication",
                    "neither"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We established robust performance baselines by conducting task-specific fine-tuning on three prominent Transformer-based encoder models: a Bangla-specific model and two high-performing multilingual models. The task was framed as a three-way sequence classification (sentence-level classification into Reduplication, Repetition, or Neither).</p>\n\n",
                "matched_terms": [
                    "repetition",
                    "reduplication",
                    "neither"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The zero-shot performance of the leading LLMs (e.g., Gemini 2.5 Flash at 78.51%) demonstrates that massive multilingual pre-training imparts a strong baseline capability to resolve word repetition ambiguity, likely leveraging latent knowledge across the Indo-European family. For the top models, few-shot prompting was the most effective method, consistently boosting accuracy to over 81%. This confirms that explicit in-context examples are necessary to guide the LLMs toward the subtle grammatical cues that distinguish morphological reduplication from disfluency. Claude 4, for instance, achieved the highest LLM performance at 82.68%. However, providing examples was inconsistent for the lower-tier models, sometimes degrading performance, which suggests that their internal representations are less robustly aligned with the task, and their ability to generalize from few-shot examples is limited.</p>\n\n",
                "matched_terms": [
                    "repetition",
                    "reduplication"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Metric Disparity Analysis:</span> A key observation is the substantial disparity between Precision (<math alttext=\"\\mathbf{0.901}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I2.i3.p1.m1\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.901</mn><annotation encoding=\"application/x-tex\">\\mathbf{0.901}</annotation></semantics></math>) and Recall (<math alttext=\"\\mathbf{0.646}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I2.i3.p1.m2\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.646</mn><annotation encoding=\"application/x-tex\">\\mathbf{0.646}</annotation></semantics></math>) for the Fine-Tuned BanglaBERT model. This disparity reflects the consequence of the highly imbalanced dataset (66.3% Reduplication vs. 32.9% Repetition). The high precision is highly desirable for normalization, as it indicates the model is extremely conservative, successfully avoiding False Positives (i.e., erroneously deleting meaningful Reduplication instances). Conversely, the lower recall shows that the model still misses a significant number of true Repetition Disfluency instances (False Negatives), allowing noise to remain in the transcript. This conservatism represents a strategic trade-off, prioritizing semantic preservation over comprehensive noise removal.</p>\n\n",
                "matched_terms": [
                    "repetition",
                    "reduplication"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper addresses the critical ambiguity between grammatical Morphological Reduplication and erroneous Repetition Disfluency in Bangla ASR transcripts. To solve this, we introduce the first publicly available, annotated corpus for this classification task. Our experiments demonstrate that task-specific fine-tuning is superior to few-shot prompting of large language models. The language-specific BanglaBERT model established the strongest performance baseline, achieving an accuracy of <span class=\"ltx_text ltx_font_bold\">84.78%</span>. This work provides the essential data and a validated benchmark, paving the way for developing robust, semantic-preserving text normalization systems for Bangla.</p>\n\n",
                "matched_terms": [
                    "bangla",
                    "corpus",
                    "reduplication",
                    "repetition",
                    "annotated"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The primary limitation of this work stems from the high degree of dataset imbalance, with Reduplication instances significantly outnumbering Repetition instances (66.3% vs. 32.9%). While fine-tuning improved the F1 score, a substantial gap remains between precision and recall (e.g., BanglaBERT Fine Tuned: Precision 0.901, Recall 0.646), especially for the minority classes, suggesting that models may still be prone to bias towards the dominant Reduplication category. Future work must focus on mitigating this bias:</p>\n\n",
                "matched_terms": [
                    "repetition",
                    "category",
                    "reduplication"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, the corpus is derived exclusively from educational content on YouTube. While this domain is rich in ASR errors and clear speech, it may not fully capture the linguistic variability, disfluency patterns, and reduplication nuances found in other spontaneous speech domains (e.g., political talk shows, casual vlogs, etc.), which could limit the generalizability of our model beyond this specific context. Future corpus expansion should target a more diverse range of conversational speech domains.</p>\n\n",
                "matched_terms": [
                    "reduplication",
                    "corpus"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis",
        "caption": "\\fontspec_if_language:nTFENG\\addfontfeatureLanguage=EnglishTable 2: Accuracy (%) of Different Prompting Techniques on the Bangla Corpus. Bold values indicate the peak accuracy achieved by each model across the different prompting strategies.",
        "body": "Models\nZero-shot (%)\nOne-shot (%)\nFew-shot (%)\n\n\n\n\nClaude 4 sonnet\n76.41\n80.29\n82.68\n\n\nGPT-4o\n78.50\n80.50\n82.10\n\n\nGemini 2.5 Flash\n78.51\n76.72\n81.49\n\n\nPhi-4\n61.19\n60.30\n62.39\n\n\nGemma 3.4b\n63.88\n61.10\n46.20\n\n\nLlama 3 8b Instruct\n56.12\n54.33\n53.73\n\n\nMistral 7b instruct\n43.88\n66.27\n62.09",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Models</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Zero-shot (%)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">One-shot (%)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Few-shot (%)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Claude 4 sonnet</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">76.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">80.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">82.68</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">GPT-4o</th>\n<td class=\"ltx_td ltx_align_center\">78.50</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">80.50</span></td>\n<td class=\"ltx_td ltx_align_center\">82.10</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Gemini 2.5 Flash</th>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">78.51</span></td>\n<td class=\"ltx_td ltx_align_center\">76.72</td>\n<td class=\"ltx_td ltx_align_center\">81.49</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Phi-4</th>\n<td class=\"ltx_td ltx_align_center\">61.19</td>\n<td class=\"ltx_td ltx_align_center\">60.30</td>\n<td class=\"ltx_td ltx_align_center\">62.39</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Gemma 3.4b</th>\n<td class=\"ltx_td ltx_align_center\">63.88</td>\n<td class=\"ltx_td ltx_align_center\">61.10</td>\n<td class=\"ltx_td ltx_align_center\">46.20</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Llama 3 8b Instruct</th>\n<td class=\"ltx_td ltx_align_center\">56.12</td>\n<td class=\"ltx_td ltx_align_center\">54.33</td>\n<td class=\"ltx_td ltx_align_center\">53.73</td>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">Mistral 7b instruct</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">43.88</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">66.27</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">62.09</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "gemini",
            "corpus",
            "techniques",
            "accuracy",
            "different",
            "prompting",
            "llama",
            "oneshot",
            "achieved",
            "gpt4o",
            "each",
            "bold",
            "fewshot",
            "sonnet",
            "peak",
            "fontspeciflanguagentfengaddfontfeaturelanguageenglishtable",
            "bangla",
            "strategies",
            "mistral",
            "zeroshot",
            "gemma",
            "34b",
            "across",
            "indicate",
            "models",
            "phi4",
            "values",
            "instruct",
            "flash",
            "model",
            "claude"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#S4.T2\" title=\"\\fontspec_if_language:nTFENG\\addfontfeatureLanguage=EnglishTable 2 &#8227; \\fontspec_if_language:nTFENG\\addfontfeatureLanguage=English4.1 LLM Benchmarking with Prompting Strategies &#8227; \\fontspec_if_language:nTFENG\\addfontfeatureLanguage=English4 Results &#8227; Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_ERROR undefined\">\\fontspec_if_language:nTF</span>ENG<span class=\"ltx_ERROR undefined\">\\addfontfeature</span>Language=English2</span></a> presents the accuracy of different multilingual LLMs across the three prompting strategies.</p>\n\n",
            "<p class=\"ltx_p\">The consistently high performance of Claude 4, GPT-4o, and Gemini 2.5 Flash across all prompting configurations established them as the top-performing models (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#S4.T2\" title=\"\\fontspec_if_language:nTFENG\\addfontfeatureLanguage=EnglishTable 2 &#8227; \\fontspec_if_language:nTFENG\\addfontfeatureLanguage=English4.1 LLM Benchmarking with Prompting Strategies &#8227; \\fontspec_if_language:nTFENG\\addfontfeatureLanguage=English4 Results &#8227; Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_ERROR undefined\">\\fontspec_if_language:nTF</span>ENG<span class=\"ltx_ERROR undefined\">\\addfontfeature</span>Language=English2</span></a>). Conversely, the relatively low and inconsistent results from open-source models like Gemma 3.4b and Llama 3 8b Instruct underscored the challenge of generalizing complex linguistic rules in a low-resource setting without substantial specialized pre-training.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Automatic Speech Recognition (ASR) transcripts, especially in low-resource languages like Bangla, contain a critical ambiguity: word-word repetitions can be either Repetition Disfluency (unintentional ASR error/hesitation) or Morphological Reduplication (a deliberate grammatical construct). Standard disfluency correction fails by erroneously deleting valid linguistic information. To solve this, we introduce the first publicly available, 20,000-row Bangla corpus, manually annotated to explicitly distinguish between these two phenomena in noisy ASR transcripts. We benchmark this novel resource using two paradigms: state-of-the-art multilingual Large Language Models (LLMs) and task-specific fine-tuning of encoder models. LLMs achieve competitive performance (up to 82.68% accuracy) with few-shot prompting. However, fine-tuning proves superior, with the language-specific BanglaBERT model achieving the highest accuracy of 84.78% and an F1 score of 0.677. This establishes a strong, linguistically-informed baseline and provides essential data for developing sophisticated, semantic-preserving text normalization systems for Bangla.</p>\n\n",
                "matched_terms": [
                    "bangla",
                    "corpus",
                    "models",
                    "accuracy",
                    "prompting",
                    "model",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This structural ambiguity, visually detailed in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#S1.F1\" title=\"\\fontspec_if_language:nTFENG\\addfontfeatureLanguage=EnglishFigure 1 &#8227; \\fontspec_if_language:nTFENG\\addfontfeatureLanguage=English1 Introduction &#8227; Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_ERROR undefined\">\\fontspec_if_language:nTF</span>ENG<span class=\"ltx_ERROR undefined\">\\addfontfeature</span>Language=English1</span></a>, presents a formidable challenge. Generic disfluency detection models designed with a &#8220;subtractive&#8221; philosophy would fail by erroneously stripping away valid linguistic information, catastrophically altering the semantic content of the text. Resolving this requires a fine-grained, context-aware classification model.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Developing models to resolve such language-specific ambiguities requires large-scale, high-quality annotated data. Despite its massive speaker base, Bangla is a low-resource language in NLP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib17\" title=\"\">17</a>]</cite> due to a scarcity of standardized, publicly available datasets. For the specific task of distinguishing repetition disfluency from morphological reduplication in Bangla, no publicly available annotated corpus existed prior to this work. This resource gap has been the primary impediment to developing and rigorously evaluating computational systems for this task, hindering the shift from generic, one-size-fits-all NLP solutions toward models sensitive to the unique grammatical structures of low-resource languages.</p>\n\n",
                "matched_terms": [
                    "models",
                    "bangla",
                    "corpus"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Corpus Creation:</span> We introduce the first publicly available, <math alttext=\"\\mathbf{20,000}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.I2.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>&#120784;&#120782;</mn><mo>,</mo><mn>&#120782;&#120782;&#120782;</mn></mrow><annotation encoding=\"application/x-tex\">\\mathbf{20,000}</annotation></semantics></math>-row Bangla corpus, manually annotated to explicitly distinguish between Repetition Disfluency and Morphological Reduplication in noisy ASR transcripts. Furthermore, we provide a fine-grained linguistic analysis by subcategorizing all Morphological Reduplication instances into nine distinct semantic and functional classes.</p>\n\n",
                "matched_terms": [
                    "bangla",
                    "corpus"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM Benchmarking:</span> We benchmark state-of-the-art multilingual Large Language Models (LLMs) (GPT, Gemini, Claude families) under zero-shot, one-shot, and few-shot prompting. LLMs achieve a competitive performance up to <math alttext=\"\\mathbf{82.68\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.I2.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">82.68</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">\\mathbf{82.68\\%}</annotation></semantics></math> accuracy with few-shot prompting.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "claude",
                    "models",
                    "accuracy",
                    "prompting",
                    "oneshot",
                    "zeroshot",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine-Tuning Analysis:</span> We empirically demonstrate the superiority of task-specific fine-tuning. The language-specific BanglaBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib5\" title=\"\">5</a>]</cite> model achieves the highest performance with an accuracy of <math alttext=\"\\mathbf{84.78\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.I2.i3.p1.m1\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">84.78</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">\\mathbf{84.78\\%}</annotation></semantics></math> and an F1 score of <math alttext=\"\\mathbf{0.677}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.I2.i3.p1.m2\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.677</mn><annotation encoding=\"application/x-tex\">\\mathbf{0.677}</annotation></semantics></math>, establishing a strong, linguistically-informed baseline for developing semantic-preserving text normalization systems for Bangla.</p>\n\n",
                "matched_terms": [
                    "model",
                    "accuracy",
                    "bangla"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Computational Disfluency Correction (DC) is a critical post-processing step for ASR, designed to improve transcript readability by identifying and removing phenomena like filled pauses, self-corrections, and repetitions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib18\" title=\"\">18</a>]</cite>. The field has evolved from classic sequence tagging to sophisticated Transformer-based architectures, with large-scale corpora like DISCO enabling high F1 scores in high-resource languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib12\" title=\"\">12</a>]</cite>. For low-resource languages, including Bengali, the lack of labeled data has spurred techniques like zero-shot learning with multilingual encoders and synthetic data augmentation via adversarial training to improve performance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib19\" title=\"\">19</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "techniques",
                    "zeroshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This specific challenge is recognized across the Indo-Aryan language family. Recent parallel work has successfully benchmarked this classification task in Hindi, Marathi, and Telugu, achieving Macro F1 scores up to 85.62% and demonstrating the necessity of context-aware models that can distinguish grammatical reduplication from disfluent structures like the Reparandum-Interregnum-Repair (RiR) pattern&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib2\" title=\"\">2</a>]</cite>. While early computational work on reduplication in Indic languages relied on rule-based systems or finite-state transducers&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib8\" title=\"\">8</a>]</cite>, our work addresses this problem using modern neural architectures. We bridge the gap between the subtractive ASR-processing paradigm and principled linguistic analysis by creating a resource to train models for this nuanced classification task.</p>\n\n",
                "matched_terms": [
                    "models",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Developing robust NLP solutions for Bangla is hindered by a scarcity of standardized datasets, a common challenge for low-resource languages despite their large speaker populations&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib17\" title=\"\">17</a>]</cite>. This has motivated broad efforts to create foundational resources and models for the Indic language family&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib13\" title=\"\">13</a>]</cite>. Our approach to corpus creation aligns with pragmatic solutions to this data gap: we leverage noisy, auto-generated ASR transcripts from YouTube&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib9\" title=\"\">9</a>]</cite>. This strategy is effective because the inherent flaws of ASR systems provide a naturalistic distribution of the very phenomena knowingly spurious repetitions, speaker hesitations, and correctly transcribed reduplications, required to train a robust real-world classifier.</p>\n\n",
                "matched_terms": [
                    "models",
                    "bangla",
                    "corpus"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the classification task itself, we evaluate the two dominant paradigms for applying pre-trained models: in-context learning &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib20\" title=\"\">20</a>]</cite> via prompting and task-specific fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib15\" title=\"\">15</a>]</cite>. The former tests the ability of massive LLMs to perform the task with zero or few examples, while the latter adapts the weights of smaller, pre-trained encoder models to the specific dataset. Our experiments provide a direct comparison of these approaches, utilizing both general multilingual models (mBERT, XLM-RoBERTa) and the language-specific BanglaBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib5\" title=\"\">5</a>]</cite> to establish a strong, linguistically-informed baseline.</p>\n\n",
                "matched_terms": [
                    "prompting",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our methodological framework is structured around three key phases: <span class=\"ltx_text ltx_font_bold\">Corpus Creation</span>, <span class=\"ltx_text ltx_font_bold\">LLM Benchmarking (Prompting)</span>, and <span class=\"ltx_text ltx_font_bold\">Task-Specific Fine-Tuning</span>. The comprehensive workflow for the corpus creation phase is illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#S2.F2\" title=\"\\fontspec_if_language:nTFENG\\addfontfeatureLanguage=EnglishFigure 2 &#8227; \\fontspec_if_language:nTFENG\\addfontfeatureLanguage=English2.2 Methodological Context and Modeling Paradigms &#8227; \\fontspec_if_language:nTFENG\\addfontfeatureLanguage=English2 Related Works &#8227; Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_ERROR undefined\">\\fontspec_if_language:nTF</span>ENG<span class=\"ltx_ERROR undefined\">\\addfontfeature</span>Language=English2</span></a>. This overall design establishes a strong performance baseline by rigorously comparing the capabilities of in-context learning against transfer learning for this fine-grained linguistic classification task.</p>\n\n",
                "matched_terms": [
                    "prompting",
                    "corpus"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The Bangla Repetition Corpus was synthesized from real-world, noisy Automatic Speech Recognition (ASR) transcripts, ensuring a naturalistic distribution of both errors and grammatical forms. This process involved four steps: Scalable Data Acquisition, Automated Filtering, Expert Annotation, and Fine-Grained Sub-categorization.</p>\n\n",
                "matched_terms": [
                    "bangla",
                    "corpus"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM-Aided Initial Labeling:</span> To accelerate the annotation of the vast corpus, the sentences were first processed using the leading commercial model, GPT-4o, as an initial categorization engine. The model utilized the structured, few-shot prompt strategy, providing preliminary labels (Reduplication, Repetition, or Neither) for the entire set.</p>\n\n",
                "matched_terms": [
                    "model",
                    "gpt4o",
                    "fewshot",
                    "corpus"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the primary classification, all instances identified as Morphological Reduplication underwent a second stage of fine-grained annotation to determine their specific semantic function. This was accomplished using a Large Language Model guided by a carefully constructed few-shot prompt that defined nine distinct subcategories:\n<span class=\"ltx_text ltx_font_bold\">Intensity/Emphasis</span>, <span class=\"ltx_text ltx_font_bold\">Frequency/Iteration</span>, <span class=\"ltx_text ltx_font_bold\">Continuity/Ongoing Action</span>, <span class=\"ltx_text ltx_font_bold\">Plurality/Multiplicity</span>, <span class=\"ltx_text ltx_font_bold\">Distributive/Separateness</span>, <span class=\"ltx_text ltx_font_bold\">Vagueness/Approximation</span>, <span class=\"ltx_text ltx_font_bold\">Echo Word/Rhyming</span>, <span class=\"ltx_text ltx_font_bold\">Reciprocal/Correlative</span>, and <span class=\"ltx_text ltx_font_bold\">Onomatopoeia</span>.\nFor instance, in the sentence <span class=\"ltx_ERROR undefined\">\\banglafont</span>&#8220;&#8230;&#2447;&#2453;&#2509;&#2488; &#2447;&#2480; &#2477;&#2509;&#2479;&#2494;&#2482;&#2497; &#2474;&#2503;&#2479;&#2492;&#2503; &#2455;&#2503;&#2459;&#2495; &#2453;&#2468; &#2453;&#2468; &#2476;&#2482;&#2507;&#8230;&#8221; (&#8230;we got the values of x, tell me what what&#8230;), the repeated word <span class=\"ltx_ERROR undefined\">\\banglafont</span>&#8220;&#2453;&#2468; &#2453;&#2468;&#8221; (<span class=\"ltx_text ltx_font_italic\">koto koto</span>) implies an iterative query for multiple values, leading to its classification as <span class=\"ltx_text ltx_font_bold\">Frequency / Iteration</span>. This two-tiered annotation process enriches the corpus, providing a detailed linguistic layer for future research.</p>\n\n",
                "matched_terms": [
                    "model",
                    "fewshot",
                    "values",
                    "corpus"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We established a prompting baseline by evaluating seven state-of-the-art Large Language Models (LLMs) on this classification task. This set included leading proprietary models (GPT-4o, Claude 4, and Gemini 2.5 Flash) and several prominent open-source alternatives (Gemma 3, Mistral 7b instruct, Llama 3 8b Instruct, and Phi-4). The models were tested under Zero-shot, One-shot, and Few-shot (<math alttext=\"\\text{N}\\leq 5\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.SSS1.p1.m1\" intent=\":literal\"><semantics><mrow><mtext>N</mtext><mo>&#8804;</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">\\text{N}\\leq 5</annotation></semantics></math>) conditions.</p>\n\n",
                "matched_terms": [
                    "gemma",
                    "gemini",
                    "claude",
                    "models",
                    "llama",
                    "prompting",
                    "oneshot",
                    "gpt4o",
                    "phi4",
                    "instruct",
                    "flash",
                    "mistral",
                    "zeroshot",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We established robust performance baselines by conducting task-specific fine-tuning on three prominent Transformer-based encoder models: a Bangla-specific model and two high-performing multilingual models. The task was framed as a three-way sequence classification (sentence-level classification into Reduplication, Repetition, or Neither).</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Models:</span> We selected BanglaBERT (a language-specific model pre-trained on a vast Bengali corpus), XLM-RoBERTa (base), and mBERT (two widely-used multilingual models).</p>\n\n",
                "matched_terms": [
                    "models",
                    "model",
                    "corpus"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The zero-shot performance of the leading LLMs (e.g., Gemini 2.5 Flash at 78.51%) demonstrates that massive multilingual pre-training imparts a strong baseline capability to resolve word repetition ambiguity, likely leveraging latent knowledge across the Indo-European family. For the top models, few-shot prompting was the most effective method, consistently boosting accuracy to over 81%. This confirms that explicit in-context examples are necessary to guide the LLMs toward the subtle grammatical cues that distinguish morphological reduplication from disfluency. Claude 4, for instance, achieved the highest LLM performance at 82.68%. However, providing examples was inconsistent for the lower-tier models, sometimes degrading performance, which suggests that their internal representations are less robustly aligned with the task, and their ability to generalize from few-shot examples is limited.</p>\n\n",
                "matched_terms": [
                    "gemini",
                    "across",
                    "claude",
                    "models",
                    "accuracy",
                    "prompting",
                    "achieved",
                    "flash",
                    "zeroshot",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Top-Performing:</span> Claude 4, GPT-4o, Gemini 2.5 Flash.</p>\n\n",
                "matched_terms": [
                    "flash",
                    "gpt4o",
                    "gemini",
                    "claude"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Mid-Tier:</span> Mistral 7b instruct, Phi-4.</p>\n\n",
                "matched_terms": [
                    "mistral",
                    "phi4",
                    "instruct"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Low-Performing:</span> Gemma 3 4b, Llama 3 8b Instruct.</p>\n\n",
                "matched_terms": [
                    "gemma",
                    "instruct",
                    "llama"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fine-tuning the encoder models on our custom Bangla dataset led to substantial improvements in all metrics, significantly surpassing the highest performance achieved by the LLMs through prompting. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#S4.F3\" title=\"\\fontspec_if_language:nTFENG\\addfontfeatureLanguage=EnglishFigure 3 &#8227; \\fontspec_if_language:nTFENG\\addfontfeatureLanguage=English4.2 Impact of Task-Specific Fine-Tuning &#8227; \\fontspec_if_language:nTFENG\\addfontfeatureLanguage=English4 Results &#8227; Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_ERROR undefined\">\\fontspec_if_language:nTF</span>ENG<span class=\"ltx_ERROR undefined\">\\addfontfeature</span>Language=English3</span></a> illustrates the sharp gain in accuracy for all three models after fine-tuning.</p>\n\n",
                "matched_terms": [
                    "bangla",
                    "models",
                    "accuracy",
                    "prompting",
                    "achieved"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">BanglaBERT <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib5\" title=\"\">5</a>]</cite> Superiority and Cross-Linguistic Context:</span> BanglaBERT achieved the highest performance across the board after fine-tuning, with an accuracy of <span class=\"ltx_text ltx_font_bold\">84.78%</span> and the highest precision (<math alttext=\"\\mathbf{0.901}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I2.i1.p1.m1\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.901</mn><annotation encoding=\"application/x-tex\">\\mathbf{0.901}</annotation></semantics></math>) and F1 score (<math alttext=\"\\mathbf{0.677}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I2.i1.p1.m2\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.677</mn><annotation encoding=\"application/x-tex\">\\mathbf{0.677}</annotation></semantics></math>). This result is competitive and consistent with state-of-the-art Macro F1 scores achieved in parallel research on this specific reduplication/repetition classification task in related Indo-Aryan languages, such as Hindi (up to 85.62%) and Marathi (up to 84.82%)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib2\" title=\"\">2</a>]</cite>. This highlights the value of using a language-specific model for a nuanced task in a low-resource language.</p>\n\n",
                "matched_terms": [
                    "model",
                    "accuracy",
                    "across",
                    "achieved"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine-Tuning vs. Prompting:</span> The best fine-tuned model (BanglaBERT, <math alttext=\"84.78\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I2.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>84.78</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">84.78\\%</annotation></semantics></math> accuracy) significantly outperformed the best-prompted LLM (Claude 4, <math alttext=\"82.68\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I2.i2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>82.68</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">82.68\\%</annotation></semantics></math> accuracy), demonstrating that for this specific, linguistically-motivated classification task, the comprehensive parameter updates of fine-tuning are more effective than in-context learning.</p>\n\n",
                "matched_terms": [
                    "prompting",
                    "accuracy",
                    "model",
                    "claude"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper addresses the critical ambiguity between grammatical Morphological Reduplication and erroneous Repetition Disfluency in Bangla ASR transcripts. To solve this, we introduce the first publicly available, annotated corpus for this classification task. Our experiments demonstrate that task-specific fine-tuning is superior to few-shot prompting of large language models. The language-specific BanglaBERT model established the strongest performance baseline, achieving an accuracy of <span class=\"ltx_text ltx_font_bold\">84.78%</span>. This work provides the essential data and a validated benchmark, paving the way for developing robust, semantic-preserving text normalization systems for Bangla.</p>\n\n",
                "matched_terms": [
                    "bangla",
                    "corpus",
                    "models",
                    "accuracy",
                    "prompting",
                    "model",
                    "fewshot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Synthetic Data Augmentation:</span> We plan to leverage modern generative techniques to create a more balanced training environment. This includes using Large Language Models (LLMs) specifically as Disfluency Generators to create natural and diverse synthetic disfluent sentences for the minority class&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib7\" title=\"\">7</a>]</cite>. This strategy has been shown to be effective in capturing real-world disfluencies in low-resource settings&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib14\" title=\"\">14</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "techniques",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Adversarial Training:</span> To improve the robustness of the fine-tuned model against noisy, real-world ASR outputs and enhance performance across all classes, we intend to implement Adversarial Training during the fine-tuning phase. This technique has previously yielded significant F1 improvements for Disfluency Correction tasks in Bengali and other Indian languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib3\" title=\"\">3</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, the corpus is derived exclusively from educational content on YouTube. While this domain is rich in ASR errors and clear speech, it may not fully capture the linguistic variability, disfluency patterns, and reduplication nuances found in other spontaneous speech domains (e.g., political talk shows, casual vlogs, etc.), which could limit the generalizability of our model beyond this specific context. Future corpus expansion should target a more diverse range of conversational speech domains.</p>\n\n",
                "matched_terms": [
                    "model",
                    "corpus"
                ]
            }
        ]
    },
    "S4.T3": {
        "source_file": "Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis",
        "caption": "\\fontspec_if_language:nTFENG\\addfontfeatureLanguage=EnglishTable 3: Fine-tuning Results: Accuracy, Precision, Recall, and F1 Score. The fine-tuning process yields substantial gains in accuracy for all models (approx. 24-46 percentage points). BanglaBERT emerges as the strongest performer, achieving the highest Accuracy (84.78%\\mathbf{84.78\\%}) and a superior F1 Score (0.677\\mathbf{0.677}). The high Precision (0.901\\mathbf{0.901}) achieved by BanglaBERT is crucial, indicating a strong ability to preserve grammatically meaningful Reduplication instances, prioritizing semantic integrity over comprehensive disfluency removal.",
        "body": "Model Name\nType\nAccuracy (%)\nPrecision\nRecall\nF1 Score\n\n\n\n\nBanglaBERT\nBase\n59.70\n0.394\n0.395\n0.381\n\n\nFine Tuned\n84.78\n0.901\n0.646\n0.677\n\n\nXLM-RoBERTa\nBase\n37.31\n0.267\n0.331\n0.190\n\n\nFine Tuned\n83.28\n0.556\n0.580\n0.566\n\n\nmBert\nBase\n37.31\n0.124\n0.333\n0.181\n\n\nFine Tuned\n83.28\n0.553\n0.581\n0.565",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model Name</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Type</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Accuracy (%)</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Precision</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Recall</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">F1 Score</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">BanglaBERT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Base</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">59.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.394</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.395</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.381</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Fine Tuned</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">84.78</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.901</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.646</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.677</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">XLM-RoBERTa</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Base</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">37.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.267</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.331</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.190</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Fine Tuned</td>\n<td class=\"ltx_td ltx_align_center\">83.28</td>\n<td class=\"ltx_td ltx_align_center\">0.556</td>\n<td class=\"ltx_td ltx_align_center\">0.580</td>\n<td class=\"ltx_td ltx_align_center\">0.566</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">mBert</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Base</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">37.31</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.124</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.333</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.181</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Fine Tuned</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">83.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.553</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.581</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.565</td>\n</tr>\n</tbody>\n</table>\n\n",
        "informative_terms_identified": [
            "ability",
            "emerges",
            "superior",
            "reduplication",
            "xlmroberta",
            "high",
            "base",
            "strong",
            "fine",
            "achieving",
            "instances",
            "semantic",
            "grammatically",
            "disfluency",
            "over",
            "recall",
            "strongest",
            "0901mathbf0901",
            "accuracy",
            "mbert",
            "results",
            "yields",
            "tuned",
            "achieved",
            "points",
            "prioritizing",
            "substantial",
            "removal",
            "meaningful",
            "fontspeciflanguagentfengaddfontfeaturelanguageenglishtable",
            "score",
            "performer",
            "name",
            "banglabert",
            "process",
            "approx",
            "highest",
            "preserve",
            "precision",
            "indicating",
            "gains",
            "crucial",
            "8478mathbf8478",
            "all",
            "models",
            "finetuning",
            "model",
            "comprehensive",
            "percentage",
            "integrity",
            "0677mathbf0677",
            "type"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The numerical results for accuracy and other key metrics are detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#S4.T3\" title=\"\\fontspec_if_language:nTFENG\\addfontfeatureLanguage=EnglishTable 3 &#8227; \\fontspec_if_language:nTFENG\\addfontfeatureLanguage=English4.2 Impact of Task-Specific Fine-Tuning &#8227; \\fontspec_if_language:nTFENG\\addfontfeatureLanguage=English4 Results &#8227; Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_ERROR undefined\">\\fontspec_if_language:nTF</span>ENG<span class=\"ltx_ERROR undefined\">\\addfontfeature</span>Language=English3</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Automatic Speech Recognition (ASR) transcripts, especially in low-resource languages like Bangla, contain a critical ambiguity: word-word repetitions can be either Repetition Disfluency (unintentional ASR error/hesitation) or Morphological Reduplication (a deliberate grammatical construct). Standard disfluency correction fails by erroneously deleting valid linguistic information. To solve this, we introduce the first publicly available, 20,000-row Bangla corpus, manually annotated to explicitly distinguish between these two phenomena in noisy ASR transcripts. We benchmark this novel resource using two paradigms: state-of-the-art multilingual Large Language Models (LLMs) and task-specific fine-tuning of encoder models. LLMs achieve competitive performance (up to 82.68% accuracy) with few-shot prompting. However, fine-tuning proves superior, with the language-specific BanglaBERT model achieving the highest accuracy of 84.78% and an F1 score of 0.677. This establishes a strong, linguistically-informed baseline and provides essential data for developing sophisticated, semantic-preserving text normalization systems for Bangla.</p>\n\n",
                "matched_terms": [
                    "score",
                    "superior",
                    "models",
                    "accuracy",
                    "finetuning",
                    "model",
                    "highest",
                    "strong",
                    "banglabert",
                    "achieving",
                    "disfluency",
                    "reduplication"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This often results in a significant Word Error Rate (WER). A major, persistent source of error stems from speech disfluencies, which are interruptions in the smooth flow of speech, including filled pauses, hesitations, self-corrections, and most relevantly, repetitions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib18\" title=\"\">18</a>]</cite>. Disfluencies are natural and frequent; one study found a <math alttext=\"\\mathbf{50\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mn>&#120787;&#120782;</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">\\mathbf{50\\%}</annotation></semantics></math> probability of a disfluency in a 10-13 word sentence&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib12\" title=\"\">12</a>]</cite>. Their presence creates noisy transcripts that are difficult to read and detrimental to downstream Natural Language Processing (NLP) tasks such as machine translation or information extraction&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib11\" title=\"\">11</a>]</cite>. Consequently, automatic Disfluency Correction (DC) is a critical research area, aiming to &#8220;clean&#8221; ASR outputs&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib10\" title=\"\">10</a>]</cite>. High-quality DC corpora, such as DISCO, have enabled benchmark F1 scores up to <math alttext=\"94.29\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S1.SS1.p2.m2\" intent=\":literal\"><semantics><mrow><mn>94.29</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">94.29\\%</annotation></semantics></math> in languages like Hindi, confirming DC&#8217;s vital post-processing role&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib4\" title=\"\">4</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "results",
                    "disfluency"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Morphological Reduplication:</span> A deliberate, rule-governed, and grammatically significant process where a word is repeated to convey a specific semantic nuance, such as continuity, iterativity, intensity, or plurality&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib16\" title=\"\">16</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib1\" title=\"\">1</a>]</cite>. <em class=\"ltx_emph ltx_font_italic\">Example:</em> <span class=\"ltx_ERROR undefined\">\\banglafont</span>&#8220;&#2437;&#2434;&#2453;&#2455;&#2497;&#2482;&#2507; <span class=\"ltx_text ltx_font_bold\">&#2453;&#2480;&#2503; &#2453;&#2480;&#2503;</span> &#2438;&#2478;&#2480;&#2494; &#2447;&#2453;&#2463;&#2497; &#2438;&#2472;&#2509;&#2465;&#2494;&#2480;&#2488;&#2509;&#2463;&#2509;&#2479;&#2494;&#2472;&#2509;&#2465;&#2495;&#2434; &#2465;&#2503;&#2477;&#2503;&#2482;&#2474; &#2453;&#2480;&#2494; &#2458;&#2503;&#2487;&#2509;&#2463;&#2494; &#2453;&#2480;&#2476;&#2507;&#8221; (<em class=\"ltx_emph ltx_font_italic\">Onkogulo kore kore&#8230;</em>). The repeated phrase <span class=\"ltx_ERROR undefined\">\\banglafont</span>&#8220;&#2453;&#2480;&#2503; &#2453;&#2480;&#2503;&#8221; (<em class=\"ltx_emph ltx_font_italic\">kore kore</em>) is a crucial construction conveying an iterative nature and must be preserved.</p>\n\n",
                "matched_terms": [
                    "crucial",
                    "process",
                    "semantic",
                    "grammatically",
                    "reduplication"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This structural ambiguity, visually detailed in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#S1.F1\" title=\"\\fontspec_if_language:nTFENG\\addfontfeatureLanguage=EnglishFigure 1 &#8227; \\fontspec_if_language:nTFENG\\addfontfeatureLanguage=English1 Introduction &#8227; Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_ERROR undefined\">\\fontspec_if_language:nTF</span>ENG<span class=\"ltx_ERROR undefined\">\\addfontfeature</span>Language=English1</span></a>, presents a formidable challenge. Generic disfluency detection models designed with a &#8220;subtractive&#8221; philosophy would fail by erroneously stripping away valid linguistic information, catastrophically altering the semantic content of the text. Resolving this requires a fine-grained, context-aware classification model.</p>\n\n",
                "matched_terms": [
                    "disfluency",
                    "model",
                    "models",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Developing models to resolve such language-specific ambiguities requires large-scale, high-quality annotated data. Despite its massive speaker base, Bangla is a low-resource language in NLP&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib17\" title=\"\">17</a>]</cite> due to a scarcity of standardized, publicly available datasets. For the specific task of distinguishing repetition disfluency from morphological reduplication in Bangla, no publicly available annotated corpus existed prior to this work. This resource gap has been the primary impediment to developing and rigorously evaluating computational systems for this task, hindering the shift from generic, one-size-fits-all NLP solutions toward models sensitive to the unique grammatical structures of low-resource languages.</p>\n\n",
                "matched_terms": [
                    "base",
                    "disfluency",
                    "reduplication",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Corpus Creation:</span> We introduce the first publicly available, <math alttext=\"\\mathbf{20,000}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.I2.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mn>&#120784;&#120782;</mn><mo>,</mo><mn>&#120782;&#120782;&#120782;</mn></mrow><annotation encoding=\"application/x-tex\">\\mathbf{20,000}</annotation></semantics></math>-row Bangla corpus, manually annotated to explicitly distinguish between Repetition Disfluency and Morphological Reduplication in noisy ASR transcripts. Furthermore, we provide a fine-grained linguistic analysis by subcategorizing all Morphological Reduplication instances into nine distinct semantic and functional classes.</p>\n\n",
                "matched_terms": [
                    "all",
                    "instances",
                    "semantic",
                    "disfluency",
                    "reduplication"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM Benchmarking:</span> We benchmark state-of-the-art multilingual Large Language Models (LLMs) (GPT, Gemini, Claude families) under zero-shot, one-shot, and few-shot prompting. LLMs achieve a competitive performance up to <math alttext=\"\\mathbf{82.68\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.I2.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">82.68</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">\\mathbf{82.68\\%}</annotation></semantics></math> accuracy with few-shot prompting.</p>\n\n",
                "matched_terms": [
                    "models",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine-Tuning Analysis:</span> We empirically demonstrate the superiority of task-specific fine-tuning. The language-specific BanglaBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib5\" title=\"\">5</a>]</cite> model achieves the highest performance with an accuracy of <math alttext=\"\\mathbf{84.78\\%}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.I2.i3.p1.m1\" intent=\":literal\"><semantics><mrow><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">84.78</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">\\mathbf{84.78\\%}</annotation></semantics></math> and an F1 score of <math alttext=\"\\mathbf{0.677}\" class=\"ltx_Math\" display=\"inline\" id=\"S1.I2.i3.p1.m2\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.677</mn><annotation encoding=\"application/x-tex\">\\mathbf{0.677}</annotation></semantics></math>, establishing a strong, linguistically-informed baseline for developing semantic-preserving text normalization systems for Bangla.</p>\n\n",
                "matched_terms": [
                    "8478mathbf8478",
                    "score",
                    "0677mathbf0677",
                    "accuracy",
                    "finetuning",
                    "model",
                    "banglabert",
                    "strong",
                    "highest"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our research is situated at the intersection of speech processing, computational linguistics, and low-resource NLP. We contextualize our contribution by reviewing the distinct treatment of repetition as an error in disfluency correction versus a meaningful construct in morphological reduplication, and by outlining the standard methodological paradigms our work builds upon.</p>\n\n",
                "matched_terms": [
                    "meaningful",
                    "disfluency",
                    "reduplication"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Computational Disfluency Correction (DC) is a critical post-processing step for ASR, designed to improve transcript readability by identifying and removing phenomena like filled pauses, self-corrections, and repetitions&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib18\" title=\"\">18</a>]</cite>. The field has evolved from classic sequence tagging to sophisticated Transformer-based architectures, with large-scale corpora like DISCO enabling high F1 scores in high-resource languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib12\" title=\"\">12</a>]</cite>. For low-resource languages, including Bengali, the lack of labeled data has spurred techniques like zero-shot learning with multilingual encoders and synthetic data augmentation via adversarial training to improve performance&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib19\" title=\"\">19</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "disfluency",
                    "high"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">However, a fundamental limitation of the dominant DC paradigm is its inherently subtractive nature that treats all repetitions as &#8220;noise&#8221; to be deleted&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib11\" title=\"\">11</a>]</cite>. This approach is incompatible with languages like Bangla, where repetition is also a productive grammatical device. In linguistics, morphological reduplication is a rule-governed process where a word is repeated to encode specific semantic nuances, such as continuity, iterativity, or intensity&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib16\" title=\"\">16</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib1\" title=\"\">1</a>]</cite>. This creates a critical structural ambiguity where the surface form &#8216;word-word&#8216; can be either an error or a meaningful linguistic construct.</p>\n\n",
                "matched_terms": [
                    "meaningful",
                    "all",
                    "process",
                    "semantic",
                    "reduplication"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This specific challenge is recognized across the Indo-Aryan language family. Recent parallel work has successfully benchmarked this classification task in Hindi, Marathi, and Telugu, achieving Macro F1 scores up to 85.62% and demonstrating the necessity of context-aware models that can distinguish grammatical reduplication from disfluent structures like the Reparandum-Interregnum-Repair (RiR) pattern&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib2\" title=\"\">2</a>]</cite>. While early computational work on reduplication in Indic languages relied on rule-based systems or finite-state transducers&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib6\" title=\"\">6</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib8\" title=\"\">8</a>]</cite>, our work addresses this problem using modern neural architectures. We bridge the gap between the subtractive ASR-processing paradigm and principled linguistic analysis by creating a resource to train models for this nuanced classification task.</p>\n\n",
                "matched_terms": [
                    "models",
                    "reduplication",
                    "achieving"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the classification task itself, we evaluate the two dominant paradigms for applying pre-trained models: in-context learning &#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib20\" title=\"\">20</a>]</cite> via prompting and task-specific fine-tuning&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib15\" title=\"\">15</a>]</cite>. The former tests the ability of massive LLMs to perform the task with zero or few examples, while the latter adapts the weights of smaller, pre-trained encoder models to the specific dataset. Our experiments provide a direct comparison of these approaches, utilizing both general multilingual models (mBERT, XLM-RoBERTa) and the language-specific BanglaBERT&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib5\" title=\"\">5</a>]</cite> to establish a strong, linguistically-informed baseline.</p>\n\n",
                "matched_terms": [
                    "ability",
                    "models",
                    "finetuning",
                    "xlmroberta",
                    "mbert",
                    "banglabert",
                    "strong"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our methodological framework is structured around three key phases: <span class=\"ltx_text ltx_font_bold\">Corpus Creation</span>, <span class=\"ltx_text ltx_font_bold\">LLM Benchmarking (Prompting)</span>, and <span class=\"ltx_text ltx_font_bold\">Task-Specific Fine-Tuning</span>. The comprehensive workflow for the corpus creation phase is illustrated in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#S2.F2\" title=\"\\fontspec_if_language:nTFENG\\addfontfeatureLanguage=EnglishFigure 2 &#8227; \\fontspec_if_language:nTFENG\\addfontfeatureLanguage=English2.2 Methodological Context and Modeling Paradigms &#8227; \\fontspec_if_language:nTFENG\\addfontfeatureLanguage=English2 Related Works &#8227; Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_ERROR undefined\">\\fontspec_if_language:nTF</span>ENG<span class=\"ltx_ERROR undefined\">\\addfontfeature</span>Language=English2</span></a>. This overall design establishes a strong performance baseline by rigorously comparing the capabilities of in-context learning against transfer learning for this fine-grained linguistic classification task.</p>\n\n",
                "matched_terms": [
                    "strong",
                    "finetuning",
                    "comprehensive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Efficient Link Retrieval:</span> Video Uniform Resource Locators (URLs) were collected using the <span class=\"ltx_ERROR undefined\">\\fontspec_if_language:nTF</span><span class=\"ltx_text ltx_font_typewriter\">ENG<span class=\"ltx_ERROR undefined\">\\addfontfeature</span>Language=Englishyt-dlp</span> utility. To maximize throughput, the retrieval process was parallelized using concurrent threads. We processed content from the channels&#8217; &#8220;videos&#8221; tabs in batches of <math alttext=\"\\mathbf{2000}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I1.i1.p1.m1\" intent=\":literal\"><semantics><mn>&#120784;&#120782;&#120782;&#120782;</mn><annotation encoding=\"application/x-tex\">\\mathbf{2000}</annotation></semantics></math> to quickly compile a comprehensive list of video links.</p>\n\n",
                "matched_terms": [
                    "comprehensive",
                    "process"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The large set of approximately <math alttext=\"\\mathbf{29,000}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.SSS3.p1.m1\" intent=\":literal\"><semantics><mrow><mn>&#120784;&#120791;</mn><mo>,</mo><mn>&#120782;&#120782;&#120782;</mn></mrow><annotation encoding=\"application/x-tex\">\\mathbf{29,000}</annotation></semantics></math> filtered sentences underwent a two-stage labeling process, combining the scalability of generative models with the precision of expert human review.</p>\n\n",
                "matched_terms": [
                    "models",
                    "process",
                    "precision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">LLM-Aided Initial Labeling:</span> To accelerate the annotation of the vast corpus, the sentences were first processed using the leading commercial model, GPT-4o, as an initial categorization engine. The model utilized the structured, few-shot prompt strategy, providing preliminary labels (Reduplication, Repetition, or Neither) for the entire set.</p>\n\n",
                "matched_terms": [
                    "reduplication",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Final Corpus Selection:</span> Following the comprehensive manual verification and cleaning, all sentences that could not be unambiguously classified (often due to extreme ASR noise or context fragmentation) were discarded. This process finalized the core corpus, resulting in a set of <math alttext=\"\\mathbf{20,000}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I3.i3.p1.m1\" intent=\":literal\"><semantics><mrow><mn>&#120784;&#120782;</mn><mo>,</mo><mn>&#120782;&#120782;&#120782;</mn></mrow><annotation encoding=\"application/x-tex\">\\mathbf{20,000}</annotation></semantics></math> gold-standard rows, which were subsequently used for training and evaluation.</p>\n\n",
                "matched_terms": [
                    "comprehensive",
                    "process",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Reduplication (Grammatical):</span> The repetition is an intentional, rule-governed morphological process that conveys semantic nuances such as iterativity, continuity, intensity, or plurality. <em class=\"ltx_emph ltx_font_italic\">Example:</em> <span class=\"ltx_ERROR undefined\">\\banglafont</span>&#8220;&#2437;&#2434;&#2453;&#2455;&#2497;&#2482;&#2507; <span class=\"ltx_text ltx_font_bold\">&#2453;&#2480;&#2503; &#2453;&#2480;&#2503;</span> &#2438;&#2478;&#2480;&#2494; &#2447;&#2453;&#2463;&#2497; &#2438;&#2472;&#2509;&#2465;&#2494;&#2480;&#2488;&#2509;&#2463;&#2509;&#2479;&#2494;&#2472;&#2509;&#2465;&#2495;&#2434; &#2465;&#2503;&#2477;&#2503;&#2482;&#2474; &#2453;&#2480;&#2494; &#2458;&#2503;&#2487;&#2509;&#2463;&#2494; &#2453;&#2480;&#2476;&#2507;&#8221; (<em class=\"ltx_emph ltx_font_italic\">Onkogulo <math alttext=\"\\mathbf{kore\\text{ }kore}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.I4.i1.p1.m1\" intent=\":literal\"><semantics><mrow><mi>&#119844;&#119848;&#119851;&#119838;</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mtext class=\"ltx_mathvariant_italic\">&#160;</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>&#119844;&#119848;&#119851;&#119838;</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbf{kore\\text{ }kore}</annotation></semantics></math> amra&#8230;</em>)</p>\n\n",
                "matched_terms": [
                    "reduplication",
                    "process",
                    "semantic"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Following the primary classification, all instances identified as Morphological Reduplication underwent a second stage of fine-grained annotation to determine their specific semantic function. This was accomplished using a Large Language Model guided by a carefully constructed few-shot prompt that defined nine distinct subcategories:\n<span class=\"ltx_text ltx_font_bold\">Intensity/Emphasis</span>, <span class=\"ltx_text ltx_font_bold\">Frequency/Iteration</span>, <span class=\"ltx_text ltx_font_bold\">Continuity/Ongoing Action</span>, <span class=\"ltx_text ltx_font_bold\">Plurality/Multiplicity</span>, <span class=\"ltx_text ltx_font_bold\">Distributive/Separateness</span>, <span class=\"ltx_text ltx_font_bold\">Vagueness/Approximation</span>, <span class=\"ltx_text ltx_font_bold\">Echo Word/Rhyming</span>, <span class=\"ltx_text ltx_font_bold\">Reciprocal/Correlative</span>, and <span class=\"ltx_text ltx_font_bold\">Onomatopoeia</span>.\nFor instance, in the sentence <span class=\"ltx_ERROR undefined\">\\banglafont</span>&#8220;&#8230;&#2447;&#2453;&#2509;&#2488; &#2447;&#2480; &#2477;&#2509;&#2479;&#2494;&#2482;&#2497; &#2474;&#2503;&#2479;&#2492;&#2503; &#2455;&#2503;&#2459;&#2495; &#2453;&#2468; &#2453;&#2468; &#2476;&#2482;&#2507;&#8230;&#8221; (&#8230;we got the values of x, tell me what what&#8230;), the repeated word <span class=\"ltx_ERROR undefined\">\\banglafont</span>&#8220;&#2453;&#2468; &#2453;&#2468;&#8221; (<span class=\"ltx_text ltx_font_italic\">koto koto</span>) implies an iterative query for multiple values, leading to its classification as <span class=\"ltx_text ltx_font_bold\">Frequency / Iteration</span>. This two-tiered annotation process enriches the corpus, providing a detailed linguistic layer for future research.</p>\n\n",
                "matched_terms": [
                    "all",
                    "model",
                    "process",
                    "instances",
                    "semantic",
                    "reduplication"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Structured Prediction:</span> All prompts enforced a Structured JSON-in, JSON-out format, requiring the model to output a single, valid JSON object containing only the predicted category, which minimizes parsing errors and enforces a consistent response structure.</p>\n\n",
                "matched_terms": [
                    "model",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We established robust performance baselines by conducting task-specific fine-tuning on three prominent Transformer-based encoder models: a Bangla-specific model and two high-performing multilingual models. The task was framed as a three-way sequence classification (sentence-level classification into Reduplication, Repetition, or Neither).</p>\n\n",
                "matched_terms": [
                    "models",
                    "reduplication",
                    "model",
                    "finetuning"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Models:</span> We selected BanglaBERT (a language-specific model pre-trained on a vast Bengali corpus), XLM-RoBERTa (base), and mBERT (two widely-used multilingual models).</p>\n\n",
                "matched_terms": [
                    "models",
                    "xlmroberta",
                    "mbert",
                    "base",
                    "banglabert",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Training Parameters:</span> All models were fine-tuned for 3 epochs with a Batch Size of 16 and a Learning Rate of 2e-5. A Weight Decay of 0.01 was applied, and the Max Length was set to 128 tokens.</p>\n\n",
                "matched_terms": [
                    "models",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The zero-shot performance of the leading LLMs (e.g., Gemini 2.5 Flash at 78.51%) demonstrates that massive multilingual pre-training imparts a strong baseline capability to resolve word repetition ambiguity, likely leveraging latent knowledge across the Indo-European family. For the top models, few-shot prompting was the most effective method, consistently boosting accuracy to over 81%. This confirms that explicit in-context examples are necessary to guide the LLMs toward the subtle grammatical cues that distinguish morphological reduplication from disfluency. Claude 4, for instance, achieved the highest LLM performance at 82.68%. However, providing examples was inconsistent for the lower-tier models, sometimes degrading performance, which suggests that their internal representations are less robustly aligned with the task, and their ability to generalize from few-shot examples is limited.</p>\n\n",
                "matched_terms": [
                    "ability",
                    "models",
                    "accuracy",
                    "highest",
                    "strong",
                    "achieved",
                    "disfluency",
                    "reduplication",
                    "over"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The consistently high performance of Claude 4, GPT-4o, and Gemini 2.5 Flash across all prompting configurations established them as the top-performing models (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#S4.T2\" title=\"\\fontspec_if_language:nTFENG\\addfontfeatureLanguage=EnglishTable 2 &#8227; \\fontspec_if_language:nTFENG\\addfontfeatureLanguage=English4.1 LLM Benchmarking with Prompting Strategies &#8227; \\fontspec_if_language:nTFENG\\addfontfeatureLanguage=English4 Results &#8227; Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_ERROR undefined\">\\fontspec_if_language:nTF</span>ENG<span class=\"ltx_ERROR undefined\">\\addfontfeature</span>Language=English2</span></a>). Conversely, the relatively low and inconsistent results from open-source models like Gemma 3.4b and Llama 3 8b Instruct underscored the challenge of generalizing complex linguistic rules in a low-resource setting without substantial specialized pre-training.</p>\n\n",
                "matched_terms": [
                    "all",
                    "models",
                    "high",
                    "results",
                    "substantial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Fine-tuning the encoder models on our custom Bangla dataset led to substantial improvements in all metrics, significantly surpassing the highest performance achieved by the LLMs through prompting. Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#S4.F3\" title=\"\\fontspec_if_language:nTFENG\\addfontfeatureLanguage=EnglishFigure 3 &#8227; \\fontspec_if_language:nTFENG\\addfontfeatureLanguage=English4.2 Impact of Task-Specific Fine-Tuning &#8227; \\fontspec_if_language:nTFENG\\addfontfeatureLanguage=English4 Results &#8227; Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_ERROR undefined\">\\fontspec_if_language:nTF</span>ENG<span class=\"ltx_ERROR undefined\">\\addfontfeature</span>Language=English3</span></a> illustrates the sharp gain in accuracy for all three models after fine-tuning.</p>\n\n",
                "matched_terms": [
                    "all",
                    "models",
                    "accuracy",
                    "finetuning",
                    "highest",
                    "achieved",
                    "substantial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">BanglaBERT <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib5\" title=\"\">5</a>]</cite> Superiority and Cross-Linguistic Context:</span> BanglaBERT achieved the highest performance across the board after fine-tuning, with an accuracy of <span class=\"ltx_text ltx_font_bold\">84.78%</span> and the highest precision (<math alttext=\"\\mathbf{0.901}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I2.i1.p1.m1\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.901</mn><annotation encoding=\"application/x-tex\">\\mathbf{0.901}</annotation></semantics></math>) and F1 score (<math alttext=\"\\mathbf{0.677}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I2.i1.p1.m2\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.677</mn><annotation encoding=\"application/x-tex\">\\mathbf{0.677}</annotation></semantics></math>). This result is competitive and consistent with state-of-the-art Macro F1 scores achieved in parallel research on this specific reduplication/repetition classification task in related Indo-Aryan languages, such as Hindi (up to 85.62%) and Marathi (up to 84.82%)&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib2\" title=\"\">2</a>]</cite>. This highlights the value of using a language-specific model for a nuanced task in a low-resource language.</p>\n\n",
                "matched_terms": [
                    "0901mathbf0901",
                    "score",
                    "0677mathbf0677",
                    "accuracy",
                    "finetuning",
                    "model",
                    "banglabert",
                    "achieved",
                    "highest",
                    "precision"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Fine-Tuning vs. Prompting:</span> The best fine-tuned model (BanglaBERT, <math alttext=\"84.78\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I2.i2.p1.m1\" intent=\":literal\"><semantics><mrow><mn>84.78</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">84.78\\%</annotation></semantics></math> accuracy) significantly outperformed the best-prompted LLM (Claude 4, <math alttext=\"82.68\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I2.i2.p1.m2\" intent=\":literal\"><semantics><mrow><mn>82.68</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">82.68\\%</annotation></semantics></math> accuracy), demonstrating that for this specific, linguistically-motivated classification task, the comprehensive parameter updates of fine-tuning are more effective than in-context learning.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "finetuning",
                    "banglabert",
                    "comprehensive",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Metric Disparity Analysis:</span> A key observation is the substantial disparity between Precision (<math alttext=\"\\mathbf{0.901}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I2.i3.p1.m1\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.901</mn><annotation encoding=\"application/x-tex\">\\mathbf{0.901}</annotation></semantics></math>) and Recall (<math alttext=\"\\mathbf{0.646}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I2.i3.p1.m2\" intent=\":literal\"><semantics><mn class=\"ltx_mathvariant_bold\" mathvariant=\"bold\">0.646</mn><annotation encoding=\"application/x-tex\">\\mathbf{0.646}</annotation></semantics></math>) for the Fine-Tuned BanglaBERT model. This disparity reflects the consequence of the highly imbalanced dataset (66.3% Reduplication vs. 32.9% Repetition). The high precision is highly desirable for normalization, as it indicates the model is extremely conservative, successfully avoiding False Positives (i.e., erroneously deleting meaningful Reduplication instances). Conversely, the lower recall shows that the model still misses a significant number of true Repetition Disfluency instances (False Negatives), allowing noise to remain in the transcript. This conservatism represents a strategic trade-off, prioritizing semantic preservation over comprehensive noise removal.</p>\n\n",
                "matched_terms": [
                    "0901mathbf0901",
                    "removal",
                    "meaningful",
                    "model",
                    "substantial",
                    "high",
                    "banglabert",
                    "comprehensive",
                    "instances",
                    "semantic",
                    "prioritizing",
                    "disfluency",
                    "reduplication",
                    "over",
                    "precision",
                    "recall"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Consistent Gains:</span> XLM-RoBERTa and mBERT both showed similar and substantial gains, reaching an accuracy of <math alttext=\"83.28\\%\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I2.i4.p1.m1\" intent=\":literal\"><semantics><mrow><mn>83.28</mn><mo>%</mo></mrow><annotation encoding=\"application/x-tex\">83.28\\%</annotation></semantics></math>. However, their F1 scores remained notably lower than BanglaBERT, reinforcing the advantage of specialized language pre-training.</p>\n\n",
                "matched_terms": [
                    "gains",
                    "accuracy",
                    "xlmroberta",
                    "mbert",
                    "banglabert",
                    "substantial"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper addresses the critical ambiguity between grammatical Morphological Reduplication and erroneous Repetition Disfluency in Bangla ASR transcripts. To solve this, we introduce the first publicly available, annotated corpus for this classification task. Our experiments demonstrate that task-specific fine-tuning is superior to few-shot prompting of large language models. The language-specific BanglaBERT model established the strongest performance baseline, achieving an accuracy of <span class=\"ltx_text ltx_font_bold\">84.78%</span>. This work provides the essential data and a validated benchmark, paving the way for developing robust, semantic-preserving text normalization systems for Bangla.</p>\n\n",
                "matched_terms": [
                    "strongest",
                    "superior",
                    "models",
                    "accuracy",
                    "finetuning",
                    "model",
                    "banglabert",
                    "achieving",
                    "disfluency",
                    "reduplication"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The primary limitation of this work stems from the high degree of dataset imbalance, with Reduplication instances significantly outnumbering Repetition instances (66.3% vs. 32.9%). While fine-tuning improved the F1 score, a substantial gap remains between precision and recall (e.g., BanglaBERT Fine Tuned: Precision 0.901, Recall 0.646), especially for the minority classes, suggesting that models may still be prone to bias towards the dominant Reduplication category. Future work must focus on mitigating this bias:</p>\n\n",
                "matched_terms": [
                    "score",
                    "models",
                    "finetuning",
                    "reduplication",
                    "high",
                    "banglabert",
                    "tuned",
                    "fine",
                    "instances",
                    "substantial",
                    "precision",
                    "recall"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Synthetic Data Augmentation:</span> We plan to leverage modern generative techniques to create a more balanced training environment. This includes using Large Language Models (LLMs) specifically as Disfluency Generators to create natural and diverse synthetic disfluent sentences for the minority class&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib7\" title=\"\">7</a>]</cite>. This strategy has been shown to be effective in capturing real-world disfluencies in low-resource settings&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib14\" title=\"\">14</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "disfluency",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Adversarial Training:</span> To improve the robustness of the fine-tuned model against noisy, real-world ASR outputs and enhance performance across all classes, we intend to implement Adversarial Training during the fine-tuning phase. This technique has previously yielded significant F1 improvements for Disfluency Correction tasks in Bengali and other Indian languages&#160;<cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.13159v1#bib.bib3\" title=\"\">3</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "disfluency",
                    "finetuning",
                    "model",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Furthermore, the corpus is derived exclusively from educational content on YouTube. While this domain is rich in ASR errors and clear speech, it may not fully capture the linguistic variability, disfluency patterns, and reduplication nuances found in other spontaneous speech domains (e.g., political talk shows, casual vlogs, etc.), which could limit the generalizability of our model beyond this specific context. Future corpus expansion should target a more diverse range of conversational speech domains.</p>\n\n",
                "matched_terms": [
                    "disfluency",
                    "model",
                    "reduplication"
                ]
            }
        ]
    }
}