{
    "S1.F1": {
        "source_file": "ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos",
        "caption": "Figure 1: TikTok video with overlay text claiming “Someone captured the missile in the Beirut blast,” and noisy Arabic audio and explosion visuals. Features below show why ShortCheck marked it Checkworthy.",
        "body": "Field\n\n\nValue\n\n\n\n\noverlay_text\n\n\nSomeone captured the | missile in the Beirut blast\n\n\n\n\ntranscript\n\n\nIt’s a shame (in Arabic)\n\n\n\n\nvideo_summary\n\n\nThe video captures footage of the 2020 Beirut blast, showing destruction and chaos in an urban area, with explosions visible throughout.\n\n\n\n\nbuzzword_detected\n\n\nFalse\n\n\n\n\ntranscript_verdict\n\n\nhostile\n\n\n\n\nsummary_verdict\n\n\ncontentious-issue\n\n\n\n\noverlay_verdict\n\n\nhostile\n\n\n\n\nCheckworthiness\n\n\nTrue",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_minipage ltx_align_middle\" style=\"width:172.5pt;\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Field</span></td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\"><span class=\"ltx_text ltx_font_bold\">Value</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">overlay_text</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">Someone captured the | missile in the Beirut blast</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">transcript</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">It&#8217;s a shame (in Arabic)</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">video_summary</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">The video captures footage of the 2020 Beirut blast, showing destruction and chaos in an urban area, with explosions visible throughout.</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">buzzword_detected</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">False</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">transcript_verdict</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">hostile</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">summary_verdict</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">contentious-issue</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">overlay_verdict</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\">hostile</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">Checkworthiness</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:113.8pt;\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#006400;\">True</span></span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "missile",
            "visible",
            "true",
            "overlaytext",
            "explosions",
            "blast”",
            "“someone",
            "noisy",
            "urban",
            "chaos",
            "contentiousissue",
            "false",
            "transcript",
            "area",
            "video",
            "explosion",
            "text",
            "shortcheck",
            "why",
            "value",
            "shame",
            "arabic",
            "beirut",
            "below",
            "checkworthiness",
            "marked",
            "field",
            "audio",
            "someone",
            "footage",
            "overlayverdict",
            "it’s",
            "hostile",
            "checkworthy",
            "tiktok",
            "features",
            "showing",
            "transcriptverdict",
            "buzzworddetected",
            "destruction",
            "videosummary",
            "summaryverdict",
            "captured",
            "captures",
            "claiming",
            "visuals",
            "blast",
            "show",
            "overlay",
            "throughout"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The popularity of short-form video platforms such as <em class=\"ltx_emph ltx_font_italic\">TikTok</em>, <em class=\"ltx_emph ltx_font_italic\">YouTube Shorts</em> and <em class=\"ltx_emph ltx_font_italic\">Instagram Reels</em> has transformed how information is produced, consumed, and spread.\nWith billions of monthly active users, these platforms create fertile ground for the spread of misinformation on sensitive topics including politics, health, and social issues.\nUnlike traditional text or image-based content, these videos may include multiple modalities such as speech, text overlays, music, and visuals, often edited in ways that obscure meaning or context, though not all of these elements are always present; for example, some videos contain only on-screen text without audio, while others show just a speaker without any additional graphics or overlays.\nFor example, Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows a TikTok screenshot where the overlay text makes the claim, while the audio transcript (translated to &#8220;it&#8217;s a shame&#8221;) does not provide any useful information. The video summary notes an urban explosion, indicating potentially contentious content for fact-checkers.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Short-form video platforms like TikTok present unique challenges for misinformation detection due to their multimodal, dynamic, and noisy content.\nWe present <span class=\"ltx_text ltx_font_smallcaps\">ShortCheck</span>, a modular, inference-only pipeline with a user-friendly interface that automatically identifies checkworthy short-form videos to help human fact-checkers.\nThe system integrates speech transcription, OCR, object and deepfake detection, video-to-text summarization, and claim verification.\n<span class=\"ltx_text ltx_font_smallcaps\">ShortCheck</span>&#160;is validated by evaluating it on two manually annotated datasets with TikTok videos in a multilingual setting.\nThe pipeline achieves promising results with F1-weighted score over 70%.</p>\n\n",
                "matched_terms": [
                    "shortcheck",
                    "checkworthy",
                    "noisy",
                    "tiktok",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text ltx_font_smallcaps\">ShortCheck</span>\n  <span class=\"ltx_text ltx_font_bold\">: Checkworthiness Detection of Multilingual Short-Form Videos</span>\n</p>\n\n",
                "matched_terms": [
                    "shortcheck",
                    "checkworthiness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The multimodal complexity of short videos makes automated fact-checking technically challenging and manual efforts are increasingly unsustainable, especially for under-resourced fact-checkers facing unprecedented content scale and funding cuts.\nIn this demo, we present a prototype designed to automate the identification of potentially checkworthy videos, significantly reducing the time required by human fact-checkers.\nOur prototype is easy to use and can predict checkworthiness in over 30 major languages.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Intersection of languages supported by Meta Llama3 <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://ai.meta.com/blog/meta-llama-3/\" title=\"\">https://ai.meta.com/blog/meta-llama-3/</a> and OpenAI Whisper <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/openai/whisper\" title=\"\">https://github.com/openai/whisper</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "checkworthy",
                    "checkworthiness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most existing misinformation detection systems are designed for structured, single-modality content such as news articles, social media posts, or deepfake detection, with a primary focus on either text, audio, transcriptions or visual modalities. We summarize the existing fact-checking systems and their modalities in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. However, these approaches are not suited for short-form video content found on platforms like TikTok or YouTube Shorts due to the casual unstructured nature of the content.</p>\n\n",
                "matched_terms": [
                    "text",
                    "tiktok",
                    "audio",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Short-form videos pose unique challenges due to their <span class=\"ltx_text ltx_font_italic\">limited multimodal generalization</span>. They often blend speech, text, music, and visuals in non-linear, asynchronous ways that traditional unimodal models struggle to interpret <cite class=\"ltx_cite ltx_citemacro_cite\">Alam et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib4\" title=\"\">2022</a>); Yao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib38\" title=\"\">2023</a>); Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib18\" title=\"\">2022</a>); Singhal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib33\" title=\"\">2019</a>)</cite>. These videos also exhibit <span class=\"ltx_text ltx_font_italic\">noisy or incomplete modality signals</span>: some lack audio, others feature distorted overlays or rapid cuts, making existing detectors brittle in real-world conditions <cite class=\"ltx_cite ltx_citemacro_cite\">Jindal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib22\" title=\"\">2020</a>); Venktesh et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib36\" title=\"\">2024</a>)</cite>. Furthermore, most models offer <span class=\"ltx_text ltx_font_italic\">low interpretability</span>, returning binary predictions without justifications. This hinders adoption in professional workflows that require transparent, evidence-backed reasoning <cite class=\"ltx_cite ltx_citemacro_cite\">Schlichtkrull et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib29\" title=\"\">2023</a>); Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib18\" title=\"\">2022</a>); Alam et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib4\" title=\"\">2022</a>); Venktesh and Setty (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib37\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "noisy",
                    "audio",
                    "visuals"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper introduces a demonstration system for detecting checkworthy TikTok videos.\nOur key contributions are:</p>\n\n",
                "matched_terms": [
                    "tiktok",
                    "checkworthy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Two <span class=\"ltx_text ltx_font_bold\">new multilingual annotated datasets</span> of TikTok videos labeled for checkworthiness.</p>\n\n",
                "matched_terms": [
                    "tiktok",
                    "checkworthiness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multimodal misinformation has led to techniques that fuse text, audio, and visual cues <cite class=\"ltx_cite ltx_citemacro_citep\">(Alam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib4\" title=\"\">2022</a>)</cite>. <span class=\"ltx_text ltx_font_smallcaps\">FakingRecipe</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Bu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib13\" title=\"\">2024</a>)</cite> investigates manipulation strategies in TikTok videos, complementing earlier datasets like <span class=\"ltx_text ltx_font_smallcaps\">SpotFake</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Singhal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib33\" title=\"\">2019</a>)</cite> and <span class=\"ltx_text ltx_font_smallcaps\">NewsBag</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Jindal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib22\" title=\"\">2020</a>)</cite>. <cite class=\"ltx_cite ltx_citemacro_citet\">Yao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib38\" title=\"\">2023</a>)</cite> proposed an end-to-end video fact-checking system with explanation generation. Our work builds on these by offering a modular, interpretable pipeline designed for integration into professional fact-checkers&#8217; workflows.</p>\n\n",
                "matched_terms": [
                    "text",
                    "tiktok",
                    "audio",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent systems have bridged research and application by supporting live or end-to-end fact-checking across modalities. <span class=\"ltx_text ltx_font_smallcaps\">BRENDA</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Botnevik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib12\" title=\"\">2020</a>)</cite> provides real-time claim detection for long-form text. <span class=\"ltx_text ltx_font_smallcaps\">FactCheckEditor</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Setty, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib31\" title=\"\">2024b</a>)</cite> and <span class=\"ltx_text ltx_font_smallcaps\">PodFC</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Setty, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib32\" title=\"\">2025</a>)</cite> support multilingual verification, with the latter tailored for long-form audio. <span class=\"ltx_text ltx_font_smallcaps\">LiveFC</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Venktesh and Setty, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib37\" title=\"\">2025</a>)</cite> handles real-time fact-checking of audio streams, combining transcription, claim detection, evidence retrieval, and verdict generation. However, as summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, most systems focus on single-modality inputs (usually text) and single-claim granularity, with limited support for video or multimodal content.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our system, <span class=\"ltx_text ltx_font_smallcaps\">ShortCheck</span>, differs in its emphasis on short-form video platforms like TikTok. It uniquely supports all four modalities: text, audio, image, and video, along with multilingual processing and capable of processing long text with more than single claims. While it does not support full automated fact-checking, it offers interpretable, modular verdict signals suitable for assisting professional fact-checkers in verification.</p>\n\n",
                "matched_terms": [
                    "shortcheck",
                    "text",
                    "tiktok",
                    "audio",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given that the checkworthiness of a TikTok video can be inherently subjective, we base our approach on established best practices followed by professional fact-checkers. In particular, we consulted the guidelines of Faktisk.no<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>A Norwegian non-profit organization accredited by the International Fact-Checking Network (IFCN).</span></span></span> This also aligns with the definition of fact-checkworthiness in the literature<cite class=\"ltx_cite ltx_citemacro_cite\">Jaradat et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib21\" title=\"\">2018</a>); Barr&#243;n-Cede&#241;o et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib8\" title=\"\">2024</a>)</cite></p>\n\n",
                "matched_terms": [
                    "tiktok",
                    "video",
                    "checkworthiness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a modular, inference-only pipeline for detecting potentially misleading or checkworthy content in short-form videos, particularly those published on platforms like TikTok. The system assigns each video one of two categorical labels: <span class=\"ltx_text ltx_font_typewriter\">Checkworthy</span>, or <span class=\"ltx_text ltx_font_typewriter\">Not_Checkworthy</span>. Unlike prior work that builds monolithic or end-to-end models, our design emphasizes modularity, interpretability, and adaptability. Each component in the pipeline can be independently replaced, which makes the system robust to failures in specific modalities and easier to maintain in production settings.</p>\n\n",
                "matched_terms": [
                    "tiktok",
                    "checkworthy",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pipeline comprises feature extraction modules tailored to speech, text, visuals, and metadata, whose outputs are aggregated by a rule-based engine for final video classification. Additional modules, such as object detection for weapons, were tested but excluded due to limited contribution.</p>\n\n",
                "matched_terms": [
                    "text",
                    "visuals",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first stage involves extracting visible on-screen text through Optical Character Recognition (OCR), using the EasyOCR library<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/JaidedAI/EasyOCR\" title=\"\">https://github.com/JaidedAI/EasyOCR</a></span></span></span>. This module captures embedded captions or textual overlays, which are common in TikTok videos. However, OCR performance is often challenged by stylized fonts, rapid transitions, and visually noisy frames.</p>\n\n",
                "matched_terms": [
                    "visible",
                    "text",
                    "noisy",
                    "captures",
                    "tiktok"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Speech transcription is handled by OpenAI Whisper <cite class=\"ltx_cite ltx_citemacro_cite\">Radford et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib28\" title=\"\">2022</a>)</cite>, which offers robust multilingual transcription and performs well in noisy audio environments. However, overlapping music or sound effects, which are prevalent in entertainment-oriented videos, can still degrade transcription quality.</p>\n\n",
                "matched_terms": [
                    "noisy",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To obtain a visual semantic summary, video frames are sampled and passed through LLaVA <cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib26\" title=\"\">2023b</a>)</cite>, a vision-language model that generates frame-level captions describing people, objects, and scenes. These captions are subsequently summarized and contextualized using Meta&#8217;s LLaMA 3 model, which also performs high-level semantic classification. The model predicts whether a video is political, hostile, benign, or promotional in nature. All models are hosted via Ollama, a lightweight, local model serving platform that supports REST-based inference.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"ollama.com\" title=\"\">ollama.com</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "hostile",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further refine the decision-making process, we incorporate a claim detection and external fact-checking module. This component leverages fine-tuned transformer models for claim detection and natural language inference (NLI), applied to both the transcript and the visual summary of the video. Following the approach proposed by <cite class=\"ltx_cite ltx_citemacro_cite\">Setty (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib31\" title=\"\">2024b</a>)</cite>, the module identifies declarative, factual statements and attempts to verify them against a fact-checking evidence database. While this does not fact-check the entire content of the video, it provides fact-checkers early signals indicating whether the video may contain verifiably false or misleading information.</p>\n\n",
                "matched_terms": [
                    "false",
                    "video",
                    "transcript"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, the system aggregates the outputs from all modules using a rule-based logic engine. A scoring system considers multiple module outputs, including the presence of claims, ideological language, or political classification. If the cumulative score exceeds a threshold, the video is marked as <span class=\"ltx_text ltx_font_typewriter\">Checkworthy</span>. Advertisements are detected and filtered early in the pipeline and override all other scores. Videos that do not meet either criterion are labeled as <span class=\"ltx_text ltx_font_typewriter\">Not_Checkworthy</span>. This decision process is entirely transparent and configurable, enabling future refinement without retraining models.</p>\n\n",
                "matched_terms": [
                    "marked",
                    "checkworthy",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This dataset includes 249 TikTok videos curated by Faktisk for an emotional analysis study on political trolling via buzzwords like &#8220;Stem FRP&#8221;.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.faktisk.no/artikkel/faktisk-analyse-av-tiktok-menn-mest-negative/109375\" title=\"\">https://www.faktisk.no/artikkel/faktisk-analyse-av-tiktok-menn-mest-negative/109375</a></span></span></span> We manually annotated each video for checkworthiness using the guidelines in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#S3\" title=\"3 System Overview &#8227; ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "tiktok",
                    "video",
                    "checkworthiness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The model performs well on both Norwegian and English TikTok videos, with notable differences in class-wise behavior. For Norwegian content, the system achieves strong recall for <span class=\"ltx_text ltx_font_typewriter\">Checkworthy</span> instances (0.85) and high overall accuracy (0.88), indicating robust performance in identifying relevant claims. In contrast, the English dataset shows higher precision for <span class=\"ltx_text ltx_font_typewriter\">Checkworthy</span> (0.82) but lower recall (0.58), suggesting the model is more conservative in flagging English videos as checkworthy. Combined macro-averaged scores are comparable across languages (F1: 0.72 for Norwegian, 0.74 for English), highlighting the pipeline&#8217;s cross-lingual generalizability with slightly better balance in the Norwegian case.</p>\n\n",
                "matched_terms": [
                    "tiktok",
                    "checkworthy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The ablation study shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#S4.T5\" title=\"Table 5 &#8227; TikTok Videos from Fact-Checking Websites: &#8227; 4.2 Datasets &#8227; 4 Experimental Evaluation &#8227; ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> demonstrates that textual modules are the most influential components in determining checkworthiness. The removal of the <span class=\"ltx_text ltx_font_italic\">Transcript Verdict</span> and <span class=\"ltx_text ltx_font_italic\">Buzzword</span> modules resulted in the largest decreases in recall and F1-score, highlighting the critical role of spoken content and ideological language. In contrast, excluding modules such as <span class=\"ltx_text ltx_font_italic\">Weapon Detection</span>, <span class=\"ltx_text ltx_font_italic\">Fact Check</span>, or <span class=\"ltx_text ltx_font_italic\">Video-to-Text Verdict</span> had minimal impact, indicating their limited standalone contribution. Notably, the <span class=\"ltx_text ltx_font_italic\">Weapon Detection</span> module slightly reduced overall performance, likely because such content appears infrequently; as a result, it was omitted from the final pipeline. These findings reinforce the system&#8217;s reliance on semantic and linguistic features rather than visual or metadata-based cues. Finally since removing individual modules does not show a huge drop in performance, the overall performance is attributed to contribution of modules.</p>\n\n",
                "matched_terms": [
                    "show",
                    "features",
                    "transcript",
                    "checkworthiness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented\n<span class=\"ltx_text ltx_font_smallcaps\">ShortCheck</span>, a modular, multilingual pipeline for detecting checkworthy content in short-form videos. The system integrates multiple modalities: text, audio, video, and image, and achieves strong performance on manually annotated TikTok datasets in both Norwegian and English. Through ablation studies, we showed that transcript and ideological language signals contribute most to checkworthiness, while visual features like deepfake detection and object recognition offer limited standalone utility.</p>\n\n",
                "matched_terms": [
                    "text",
                    "shortcheck",
                    "checkworthy",
                    "tiktok",
                    "features",
                    "transcript",
                    "checkworthiness",
                    "audio",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In future work, we plan to extend <span class=\"ltx_text ltx_font_smallcaps\">ShortCheck</span>&#160;with full fact-checking while testing on more datasets, including multilingual video content from X and YouTube. We will evaluate individual modules in greater depth, improve robustness in noisy or low-resource settings, and integrate feedback from fact-checkers to refine outputs and interpretability. Finally, we aim to explore advanced multimodal fusion techniques beyond rule-based aggregation to boost accuracy and generalization.</p>\n\n",
                "matched_terms": [
                    "shortcheck",
                    "noisy",
                    "video"
                ]
            }
        ]
    },
    "S1.T1": {
        "source_file": "ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos",
        "caption": "Table 1: Fact-checking systems categorized by input modality, granularity, multilinguality, and fact-checking support.",
        "body": "System\nText\nAudio\nImage\nVideo\nGranularity\nM.lingual\nFull FC\n\n\n\nBRENDA Botnevik et al. (2020)\n\n✓\n✗\n✗\n✗\nLong Text\n✗\n✓\n\n\n\nFLEEK Bayat et al. (2023)\n\n✓\n✗\n✗\n✗\nSingle Claim\n✗\n✓\n\n\n\nQACheck Pan and et al. (2023)\n\n✓\n✗\n✗\n✗\nSingle Claim\n✗\n✓\n\n\n\nClaimLens Devasier et al. (2024)\n\n✓\n✗\n✗\n✗\nSingle Claim\n✗\n✗\n\n\n\nTruthReader Li et al. (2024)\n\n✓\n✗\n✗\n✗\nLong Text\n✗\n✓\n\n\n\nOpenFactCheck Iqbal et al. (2024)\n\n✓\n✗\n✗\n✗\nLong Text\n✗\n✓\n\n\n\nFactCheckEditor Setty (2024a)\n\n✓\n✗\n✗\n✗\nLong Text\n✓\n✓\n\n\n\nLoki Li et al. (2025)\n\n✓\n✗\n✗\n✗\nSingle Claim\n✓\n✓\n\n\n\nAudioCWD Ivanov et al. (2024)\n\n✗\n✓\n✗\n✗\nSingle Claim\n✗\n✗\n\n\n\nLiveFC Venktesh and Setty (2025)\n\n✓\n✓\n✗\n✗\nLong Text\n✗\n✓\n\n\n\nPodFC Setty (2025)\n\n✓\n✗\n✓\n✗\nLong Text\n✓\n✓\n\n\n\nFauxtography Zlatkova et al. (2019)\n\n✓\n✗\n✓\n✗\nSingle Claim\n✗\n✓\n\n\n\nAVerImaTeC Cao et al. (2025)\n\n✓\n✗\n✓\n✗\nSingle Claim\n✗\n✓\n\n\n\nCER Barone et al. (2025)\n\n✓\n✓\n✓\n✓\nSingle Claim\n✗\n✓\n\n\n\nCOVID-VTS Liu et al. (2023a)\n\n✓\n✓\n✓\n✓\nSingle Claim\n✗\n✗\n\n\n\nShortCheck (ours)\n✓\n✓\n✓\n✓\nLong Text\n✓\n✗",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">System</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Text</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Audio</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Image</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Video</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Granularity</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">M.lingual</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Full FC</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_text ltx_font_smallcaps\">BRENDA</span>&#160;<cite class=\"ltx_cite ltx_citemacro_citet\">Botnevik et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib12\" title=\"\">2020</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Long Text</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">&#10003;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text ltx_font_smallcaps\">FLEEK</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Bayat et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib9\" title=\"\">2023</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">Single Claim</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text ltx_font_smallcaps\">QACheck</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Pan and et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib27\" title=\"\">2023</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">Single Claim</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text ltx_font_smallcaps\">ClaimLens</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Devasier et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib15\" title=\"\">2024</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">Single Claim</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text ltx_font_smallcaps\">TruthReader</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib23\" title=\"\">2024</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">Long Text</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text ltx_font_smallcaps\">OpenFactCheck</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Iqbal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib19\" title=\"\">2024</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">Long Text</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text ltx_font_smallcaps\">FactCheckEditor</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Setty (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib30\" title=\"\">2024a</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">Long Text</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text ltx_font_smallcaps\">Loki</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib24\" title=\"\">2025</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">Single Claim</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text ltx_font_smallcaps\">AudioCWD</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Ivanov et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib20\" title=\"\">2024</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">Single Claim</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text ltx_font_smallcaps\">LiveFC</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Venktesh and Setty (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib37\" title=\"\">2025</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">Long Text</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text ltx_font_smallcaps\">PodFC</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Setty (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib32\" title=\"\">2025</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">Long Text</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text ltx_font_smallcaps\">Fauxtography</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Zlatkova et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib39\" title=\"\">2019</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">Single Claim</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text ltx_font_smallcaps\">AVerImaTeC</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Cao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib14\" title=\"\">2025</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">Single Claim</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text ltx_font_smallcaps\">CER</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Barone et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib7\" title=\"\">2025</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">Single Claim</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">\n<span class=\"ltx_text ltx_font_smallcaps\">COVID-VTS</span>&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib25\" title=\"\">2023a</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center\">Single Claim</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\">&#10007;</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">\n<span class=\"ltx_text ltx_font_smallcaps\">ShortCheck</span>&#160;(ours)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Long Text</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">&#10003;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">&#10007;</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "covidvts",
            "fauxtography",
            "categorized",
            "setty",
            "cer",
            "pan",
            "long",
            "factchecking",
            "zlatkova",
            "video",
            "text",
            "2023a",
            "shortcheck",
            "factcheckeditor",
            "botnevik",
            "truthreader",
            "image",
            "claimlens",
            "multilinguality",
            "venktesh",
            "audio",
            "cao",
            "mlingual",
            "brenda",
            "system",
            "modality",
            "bayat",
            "podfc",
            "systems",
            "iqbal",
            "livefc",
            "claim",
            "single",
            "qacheck",
            "devasier",
            "2024a",
            "openfactcheck",
            "audiocwd",
            "ivanov",
            "granularity",
            "input",
            "barone",
            "ours",
            "loki",
            "averimatec",
            "fleek",
            "liu",
            "support",
            "full"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">Most existing misinformation detection systems are designed for structured, single-modality content such as news articles, social media posts, or deepfake detection, with a primary focus on either text, audio, transcriptions or visual modalities. We summarize the existing fact-checking systems and their modalities in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. However, these approaches are not suited for short-form video content found on platforms like TikTok or YouTube Shorts due to the casual unstructured nature of the content.</p>\n\n",
            "<p class=\"ltx_p\">Recent systems have bridged research and application by supporting live or end-to-end fact-checking across modalities. <span class=\"ltx_text ltx_font_smallcaps\">BRENDA</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Botnevik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib12\" title=\"\">2020</a>)</cite> provides real-time claim detection for long-form text. <span class=\"ltx_text ltx_font_smallcaps\">FactCheckEditor</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Setty, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib31\" title=\"\">2024b</a>)</cite> and <span class=\"ltx_text ltx_font_smallcaps\">PodFC</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Setty, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib32\" title=\"\">2025</a>)</cite> support multilingual verification, with the latter tailored for long-form audio. <span class=\"ltx_text ltx_font_smallcaps\">LiveFC</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Venktesh and Setty, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib37\" title=\"\">2025</a>)</cite> handles real-time fact-checking of audio streams, combining transcription, claim detection, evidence retrieval, and verdict generation. However, as summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, most systems focus on single-modality inputs (usually text) and single-claim granularity, with limited support for video or multimodal content.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Short-form video platforms like TikTok present unique challenges for misinformation detection due to their multimodal, dynamic, and noisy content.\nWe present <span class=\"ltx_text ltx_font_smallcaps\">ShortCheck</span>, a modular, inference-only pipeline with a user-friendly interface that automatically identifies checkworthy short-form videos to help human fact-checkers.\nThe system integrates speech transcription, OCR, object and deepfake detection, video-to-text summarization, and claim verification.\n<span class=\"ltx_text ltx_font_smallcaps\">ShortCheck</span>&#160;is validated by evaluating it on two manually annotated datasets with TikTok videos in a multilingual setting.\nThe pipeline achieves promising results with F1-weighted score over 70%.</p>\n\n",
                "matched_terms": [
                    "shortcheck",
                    "claim",
                    "system",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The popularity of short-form video platforms such as <em class=\"ltx_emph ltx_font_italic\">TikTok</em>, <em class=\"ltx_emph ltx_font_italic\">YouTube Shorts</em> and <em class=\"ltx_emph ltx_font_italic\">Instagram Reels</em> has transformed how information is produced, consumed, and spread.\nWith billions of monthly active users, these platforms create fertile ground for the spread of misinformation on sensitive topics including politics, health, and social issues.\nUnlike traditional text or image-based content, these videos may include multiple modalities such as speech, text overlays, music, and visuals, often edited in ways that obscure meaning or context, though not all of these elements are always present; for example, some videos contain only on-screen text without audio, while others show just a speaker without any additional graphics or overlays.\nFor example, Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows a TikTok screenshot where the overlay text makes the claim, while the audio transcript (translated to &#8220;it&#8217;s a shame&#8221;) does not provide any useful information. The video summary notes an urban explosion, indicating potentially contentious content for fact-checkers.</p>\n\n",
                "matched_terms": [
                    "text",
                    "audio",
                    "claim",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Short-form videos pose unique challenges due to their <span class=\"ltx_text ltx_font_italic\">limited multimodal generalization</span>. They often blend speech, text, music, and visuals in non-linear, asynchronous ways that traditional unimodal models struggle to interpret <cite class=\"ltx_cite ltx_citemacro_cite\">Alam et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib4\" title=\"\">2022</a>); Yao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib38\" title=\"\">2023</a>); Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib18\" title=\"\">2022</a>); Singhal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib33\" title=\"\">2019</a>)</cite>. These videos also exhibit <span class=\"ltx_text ltx_font_italic\">noisy or incomplete modality signals</span>: some lack audio, others feature distorted overlays or rapid cuts, making existing detectors brittle in real-world conditions <cite class=\"ltx_cite ltx_citemacro_cite\">Jindal et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib22\" title=\"\">2020</a>); Venktesh et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib36\" title=\"\">2024</a>)</cite>. Furthermore, most models offer <span class=\"ltx_text ltx_font_italic\">low interpretability</span>, returning binary predictions without justifications. This hinders adoption in professional workflows that require transparent, evidence-backed reasoning <cite class=\"ltx_cite ltx_citemacro_cite\">Schlichtkrull et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib29\" title=\"\">2023</a>); Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib18\" title=\"\">2022</a>); Alam et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib4\" title=\"\">2022</a>); Venktesh and Setty (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib37\" title=\"\">2025</a>)</cite>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "setty",
                    "venktesh",
                    "audio",
                    "modality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While multimodal fact-checking is gaining traction, the gap between research prototypes and deployable, interpretable tools for short-form video remains substantial. Bridging this divide requires not only improved multimodal understanding but also system outputs that align with the needs of human fact-checkers in high-throughput environments.</p>\n\n",
                "matched_terms": [
                    "system",
                    "factchecking",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Automated fact-checking has progressed with the rise of benchmark datasets. The FEVER dataset by <cite class=\"ltx_cite ltx_citemacro_citet\">Thorne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib35\" title=\"\">2018</a>)</cite> established foundational tasks in claim verification, with follow-up surveys and taxonomies by <cite class=\"ltx_cite ltx_citemacro_citet\">Thorne and Vlachos (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib34\" title=\"\">2018</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib18\" title=\"\">2022</a>)</cite>. Real-world datasets such as <span class=\"ltx_text ltx_font_smallcaps\">MultiFC</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Augenstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib6\" title=\"\">2019</a>)</cite>, <span class=\"ltx_text ltx_font_smallcaps\">AVeriTeC</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Schlichtkrull et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib29\" title=\"\">2023</a>)</cite>, and <span class=\"ltx_text ltx_font_smallcaps\">QuanTemp</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Venktesh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib36\" title=\"\">2024</a>)</cite> have expanded the scope to include diverse, evidence-backed, and numerically grounded claims.</p>\n\n",
                "matched_terms": [
                    "factchecking",
                    "claim",
                    "venktesh"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multimodal misinformation has led to techniques that fuse text, audio, and visual cues <cite class=\"ltx_cite ltx_citemacro_citep\">(Alam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib4\" title=\"\">2022</a>)</cite>. <span class=\"ltx_text ltx_font_smallcaps\">FakingRecipe</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Bu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib13\" title=\"\">2024</a>)</cite> investigates manipulation strategies in TikTok videos, complementing earlier datasets like <span class=\"ltx_text ltx_font_smallcaps\">SpotFake</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Singhal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib33\" title=\"\">2019</a>)</cite> and <span class=\"ltx_text ltx_font_smallcaps\">NewsBag</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Jindal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib22\" title=\"\">2020</a>)</cite>. <cite class=\"ltx_cite ltx_citemacro_citet\">Yao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib38\" title=\"\">2023</a>)</cite> proposed an end-to-end video fact-checking system with explanation generation. Our work builds on these by offering a modular, interpretable pipeline designed for integration into professional fact-checkers&#8217; workflows.</p>\n\n",
                "matched_terms": [
                    "text",
                    "system",
                    "factchecking",
                    "audio",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our system, <span class=\"ltx_text ltx_font_smallcaps\">ShortCheck</span>, differs in its emphasis on short-form video platforms like TikTok. It uniquely supports all four modalities: text, audio, image, and video, along with multilingual processing and capable of processing long text with more than single claims. While it does not support full automated fact-checking, it offers interpretable, modular verdict signals suitable for assisting professional fact-checkers in verification.</p>\n\n",
                "matched_terms": [
                    "shortcheck",
                    "text",
                    "system",
                    "full",
                    "support",
                    "single",
                    "image",
                    "long",
                    "factchecking",
                    "audio",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given that the checkworthiness of a TikTok video can be inherently subjective, we base our approach on established best practices followed by professional fact-checkers. In particular, we consulted the guidelines of Faktisk.no<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>A Norwegian non-profit organization accredited by the International Fact-Checking Network (IFCN).</span></span></span> This also aligns with the definition of fact-checkworthiness in the literature<cite class=\"ltx_cite ltx_citemacro_cite\">Jaradat et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib21\" title=\"\">2018</a>); Barr&#243;n-Cede&#241;o et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib8\" title=\"\">2024</a>)</cite></p>\n\n",
                "matched_terms": [
                    "factchecking",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A short video is defined as up to 10 minutes long.</p>\n\n",
                "matched_terms": [
                    "video",
                    "long"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a modular, inference-only pipeline for detecting potentially misleading or checkworthy content in short-form videos, particularly those published on platforms like TikTok. The system assigns each video one of two categorical labels: <span class=\"ltx_text ltx_font_typewriter\">Checkworthy</span>, or <span class=\"ltx_text ltx_font_typewriter\">Not_Checkworthy</span>. Unlike prior work that builds monolithic or end-to-end models, our design emphasizes modularity, interpretability, and adaptability. Each component in the pipeline can be independently replaced, which makes the system robust to failures in specific modalities and easier to maintain in production settings.</p>\n\n",
                "matched_terms": [
                    "system",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pipeline comprises feature extraction modules tailored to speech, text, visuals, and metadata, whose outputs are aggregated by a rule-based engine for final video classification. Additional modules, such as object detection for weapons, were tested but excluded due to limited contribution.</p>\n\n",
                "matched_terms": [
                    "text",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To obtain a visual semantic summary, video frames are sampled and passed through LLaVA <cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib26\" title=\"\">2023b</a>)</cite>, a vision-language model that generates frame-level captions describing people, objects, and scenes. These captions are subsequently summarized and contextualized using Meta&#8217;s LLaMA 3 model, which also performs high-level semantic classification. The model predicts whether a video is political, hostile, benign, or promotional in nature. All models are hosted via Ollama, a lightweight, local model serving platform that supports REST-based inference.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"ollama.com\" title=\"\">ollama.com</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "liu",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further refine the decision-making process, we incorporate a claim detection and external fact-checking module. This component leverages fine-tuned transformer models for claim detection and natural language inference (NLI), applied to both the transcript and the visual summary of the video. Following the approach proposed by <cite class=\"ltx_cite ltx_citemacro_cite\">Setty (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib31\" title=\"\">2024b</a>)</cite>, the module identifies declarative, factual statements and attempts to verify them against a fact-checking evidence database. While this does not fact-check the entire content of the video, it provides fact-checkers early signals indicating whether the video may contain verifiably false or misleading information.</p>\n\n",
                "matched_terms": [
                    "factchecking",
                    "claim",
                    "setty",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, the system aggregates the outputs from all modules using a rule-based logic engine. A scoring system considers multiple module outputs, including the presence of claims, ideological language, or political classification. If the cumulative score exceeds a threshold, the video is marked as <span class=\"ltx_text ltx_font_typewriter\">Checkworthy</span>. Advertisements are detected and filtered early in the pipeline and override all other scores. Videos that do not meet either criterion are labeled as <span class=\"ltx_text ltx_font_typewriter\">Not_Checkworthy</span>. This decision process is entirely transparent and configurable, enabling future refinement without retraining models.</p>\n\n",
                "matched_terms": [
                    "system",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented\n<span class=\"ltx_text ltx_font_smallcaps\">ShortCheck</span>, a modular, multilingual pipeline for detecting checkworthy content in short-form videos. The system integrates multiple modalities: text, audio, video, and image, and achieves strong performance on manually annotated TikTok datasets in both Norwegian and English. Through ablation studies, we showed that transcript and ideological language signals contribute most to checkworthiness, while visual features like deepfake detection and object recognition offer limited standalone utility.</p>\n\n",
                "matched_terms": [
                    "text",
                    "shortcheck",
                    "system",
                    "image",
                    "audio",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In future work, we plan to extend <span class=\"ltx_text ltx_font_smallcaps\">ShortCheck</span>&#160;with full fact-checking while testing on more datasets, including multilingual video content from X and YouTube. We will evaluate individual modules in greater depth, improve robustness in noisy or low-resource settings, and integrate feedback from fact-checkers to refine outputs and interpretability. Finally, we aim to explore advanced multimodal fusion techniques beyond rule-based aggregation to boost accuracy and generalization.</p>\n\n",
                "matched_terms": [
                    "shortcheck",
                    "factchecking",
                    "full",
                    "video"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos",
        "caption": "Table 2: Results on TikTok videos in Norwegian and English. CW = Checkworthy, NCW = Not Checkworthy. Combined metrics are macro-averages. Metrics: Precision (P), Recall (R), Accuracy (Acc) and F1-weighted (F1-W)",
        "body": "Dataset\nCW\nNCW\nCombined (Macro)\n\n\n\nP\nR\nF1-W\nP\nR\nF1-W\nP\nR\nF1-W\nAcc\n\n\nNorwegian influencer\n0.64\n0.85\n0.73\n0.95\n0.92\n0.93\n0.74\n0.73\n0.72\n0.88\n\n\nFact-checking websites\n0.82\n0.58\n0.68\n0.72\n0.90\n0.80\n0.77\n0.74\n0.74\n0.76",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">CW</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">NCW</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold\">Combined (Macro)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">P</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">R</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">F1-W</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">P</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">R</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">F1-W</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">P</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">R</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">F1-W</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Acc</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Norwegian influencer</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.64</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.92</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.93</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.88</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Fact-checking websites</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.76</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "combined",
            "ncw",
            "videos",
            "factchecking",
            "recall",
            "f1weighted",
            "accuracy",
            "results",
            "websites",
            "metrics",
            "checkworthy",
            "tiktok",
            "influencer",
            "macroaverages",
            "acc",
            "precision",
            "dataset",
            "norwegian",
            "english",
            "f1w",
            "macro",
            "not"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Short-form video platforms like TikTok present unique challenges for misinformation detection due to their multimodal, dynamic, and noisy content.\nWe present <span class=\"ltx_text ltx_font_smallcaps\">ShortCheck</span>, a modular, inference-only pipeline with a user-friendly interface that automatically identifies checkworthy short-form videos to help human fact-checkers.\nThe system integrates speech transcription, OCR, object and deepfake detection, video-to-text summarization, and claim verification.\n<span class=\"ltx_text ltx_font_smallcaps\">ShortCheck</span>&#160;is validated by evaluating it on two manually annotated datasets with TikTok videos in a multilingual setting.\nThe pipeline achieves promising results with F1-weighted score over 70%.</p>\n\n",
                "matched_terms": [
                    "checkworthy",
                    "f1weighted",
                    "tiktok",
                    "results",
                    "videos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The popularity of short-form video platforms such as <em class=\"ltx_emph ltx_font_italic\">TikTok</em>, <em class=\"ltx_emph ltx_font_italic\">YouTube Shorts</em> and <em class=\"ltx_emph ltx_font_italic\">Instagram Reels</em> has transformed how information is produced, consumed, and spread.\nWith billions of monthly active users, these platforms create fertile ground for the spread of misinformation on sensitive topics including politics, health, and social issues.\nUnlike traditional text or image-based content, these videos may include multiple modalities such as speech, text overlays, music, and visuals, often edited in ways that obscure meaning or context, though not all of these elements are always present; for example, some videos contain only on-screen text without audio, while others show just a speaker without any additional graphics or overlays.\nFor example, Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows a TikTok screenshot where the overlay text makes the claim, while the audio transcript (translated to &#8220;it&#8217;s a shame&#8221;) does not provide any useful information. The video summary notes an urban explosion, indicating potentially contentious content for fact-checkers.</p>\n\n",
                "matched_terms": [
                    "tiktok",
                    "videos",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The multimodal complexity of short videos makes automated fact-checking technically challenging and manual efforts are increasingly unsustainable, especially for under-resourced fact-checkers facing unprecedented content scale and funding cuts.\nIn this demo, we present a prototype designed to automate the identification of potentially checkworthy videos, significantly reducing the time required by human fact-checkers.\nOur prototype is easy to use and can predict checkworthiness in over 30 major languages.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Intersection of languages supported by Meta Llama3 <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://ai.meta.com/blog/meta-llama-3/\" title=\"\">https://ai.meta.com/blog/meta-llama-3/</a> and OpenAI Whisper <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/openai/whisper\" title=\"\">https://github.com/openai/whisper</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "factchecking",
                    "checkworthy",
                    "videos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most existing misinformation detection systems are designed for structured, single-modality content such as news articles, social media posts, or deepfake detection, with a primary focus on either text, audio, transcriptions or visual modalities. We summarize the existing fact-checking systems and their modalities in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. However, these approaches are not suited for short-form video content found on platforms like TikTok or YouTube Shorts due to the casual unstructured nature of the content.</p>\n\n",
                "matched_terms": [
                    "tiktok",
                    "factchecking",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While multimodal fact-checking is gaining traction, the gap between research prototypes and deployable, interpretable tools for short-form video remains substantial. Bridging this divide requires not only improved multimodal understanding but also system outputs that align with the needs of human fact-checkers in high-throughput environments.</p>\n\n",
                "matched_terms": [
                    "factchecking",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This paper introduces a demonstration system for detecting checkworthy TikTok videos.\nOur key contributions are:</p>\n\n",
                "matched_terms": [
                    "tiktok",
                    "checkworthy",
                    "videos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Two <span class=\"ltx_text ltx_font_bold\">new multilingual annotated datasets</span> of TikTok videos labeled for checkworthiness.</p>\n\n",
                "matched_terms": [
                    "tiktok",
                    "videos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A <span class=\"ltx_text ltx_font_bold\">demo interface</span> that allows fact-checkers to upload videos, inspect intermediate results, and link claims to existing fact-checks.</p>\n\n",
                "matched_terms": [
                    "results",
                    "videos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Automated fact-checking has progressed with the rise of benchmark datasets. The FEVER dataset by <cite class=\"ltx_cite ltx_citemacro_citet\">Thorne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib35\" title=\"\">2018</a>)</cite> established foundational tasks in claim verification, with follow-up surveys and taxonomies by <cite class=\"ltx_cite ltx_citemacro_citet\">Thorne and Vlachos (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib34\" title=\"\">2018</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib18\" title=\"\">2022</a>)</cite>. Real-world datasets such as <span class=\"ltx_text ltx_font_smallcaps\">MultiFC</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Augenstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib6\" title=\"\">2019</a>)</cite>, <span class=\"ltx_text ltx_font_smallcaps\">AVeriTeC</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Schlichtkrull et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib29\" title=\"\">2023</a>)</cite>, and <span class=\"ltx_text ltx_font_smallcaps\">QuanTemp</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Venktesh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib36\" title=\"\">2024</a>)</cite> have expanded the scope to include diverse, evidence-backed, and numerically grounded claims.</p>\n\n",
                "matched_terms": [
                    "factchecking",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multimodal misinformation has led to techniques that fuse text, audio, and visual cues <cite class=\"ltx_cite ltx_citemacro_citep\">(Alam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib4\" title=\"\">2022</a>)</cite>. <span class=\"ltx_text ltx_font_smallcaps\">FakingRecipe</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Bu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib13\" title=\"\">2024</a>)</cite> investigates manipulation strategies in TikTok videos, complementing earlier datasets like <span class=\"ltx_text ltx_font_smallcaps\">SpotFake</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Singhal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib33\" title=\"\">2019</a>)</cite> and <span class=\"ltx_text ltx_font_smallcaps\">NewsBag</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Jindal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib22\" title=\"\">2020</a>)</cite>. <cite class=\"ltx_cite ltx_citemacro_citet\">Yao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib38\" title=\"\">2023</a>)</cite> proposed an end-to-end video fact-checking system with explanation generation. Our work builds on these by offering a modular, interpretable pipeline designed for integration into professional fact-checkers&#8217; workflows.</p>\n\n",
                "matched_terms": [
                    "tiktok",
                    "factchecking",
                    "videos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our system, <span class=\"ltx_text ltx_font_smallcaps\">ShortCheck</span>, differs in its emphasis on short-form video platforms like TikTok. It uniquely supports all four modalities: text, audio, image, and video, along with multilingual processing and capable of processing long text with more than single claims. While it does not support full automated fact-checking, it offers interpretable, modular verdict signals suitable for assisting professional fact-checkers in verification.</p>\n\n",
                "matched_terms": [
                    "tiktok",
                    "factchecking",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given that the checkworthiness of a TikTok video can be inherently subjective, we base our approach on established best practices followed by professional fact-checkers. In particular, we consulted the guidelines of Faktisk.no<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>A Norwegian non-profit organization accredited by the International Fact-Checking Network (IFCN).</span></span></span> This also aligns with the definition of fact-checkworthiness in the literature<cite class=\"ltx_cite ltx_citemacro_cite\">Jaradat et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib21\" title=\"\">2018</a>); Barr&#243;n-Cede&#241;o et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib8\" title=\"\">2024</a>)</cite></p>\n\n",
                "matched_terms": [
                    "tiktok",
                    "factchecking",
                    "norwegian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Claims related to celebrity gossip, sports, advertisements etc are not considered checkworthy.</p>\n\n",
                "matched_terms": [
                    "checkworthy",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a modular, inference-only pipeline for detecting potentially misleading or checkworthy content in short-form videos, particularly those published on platforms like TikTok. The system assigns each video one of two categorical labels: <span class=\"ltx_text ltx_font_typewriter\">Checkworthy</span>, or <span class=\"ltx_text ltx_font_typewriter\">Not_Checkworthy</span>. Unlike prior work that builds monolithic or end-to-end models, our design emphasizes modularity, interpretability, and adaptability. Each component in the pipeline can be independently replaced, which makes the system robust to failures in specific modalities and easier to maintain in production settings.</p>\n\n",
                "matched_terms": [
                    "tiktok",
                    "checkworthy",
                    "videos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first stage involves extracting visible on-screen text through Optical Character Recognition (OCR), using the EasyOCR library<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/JaidedAI/EasyOCR\" title=\"\">https://github.com/JaidedAI/EasyOCR</a></span></span></span>. This module captures embedded captions or textual overlays, which are common in TikTok videos. However, OCR performance is often challenged by stylized fonts, rapid transitions, and visually noisy frames.</p>\n\n",
                "matched_terms": [
                    "tiktok",
                    "videos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further refine the decision-making process, we incorporate a claim detection and external fact-checking module. This component leverages fine-tuned transformer models for claim detection and natural language inference (NLI), applied to both the transcript and the visual summary of the video. Following the approach proposed by <cite class=\"ltx_cite ltx_citemacro_cite\">Setty (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib31\" title=\"\">2024b</a>)</cite>, the module identifies declarative, factual statements and attempts to verify them against a fact-checking evidence database. While this does not fact-check the entire content of the video, it provides fact-checkers early signals indicating whether the video may contain verifiably false or misleading information.</p>\n\n",
                "matched_terms": [
                    "factchecking",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, the system aggregates the outputs from all modules using a rule-based logic engine. A scoring system considers multiple module outputs, including the presence of claims, ideological language, or political classification. If the cumulative score exceeds a threshold, the video is marked as <span class=\"ltx_text ltx_font_typewriter\">Checkworthy</span>. Advertisements are detected and filtered early in the pipeline and override all other scores. Videos that do not meet either criterion are labeled as <span class=\"ltx_text ltx_font_typewriter\">Not_Checkworthy</span>. This decision process is entirely transparent and configurable, enabling future refinement without retraining models.</p>\n\n",
                "matched_terms": [
                    "checkworthy",
                    "videos",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This dataset includes 249 TikTok videos curated by Faktisk for an emotional analysis study on political trolling via buzzwords like &#8220;Stem FRP&#8221;.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.faktisk.no/artikkel/faktisk-analyse-av-tiktok-menn-mest-negative/109375\" title=\"\">https://www.faktisk.no/artikkel/faktisk-analyse-av-tiktok-menn-mest-negative/109375</a></span></span></span> We manually annotated each video for checkworthiness using the guidelines in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#S3\" title=\"3 System Overview &#8227; ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "tiktok",
                    "dataset",
                    "videos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This dataset, curated by <cite class=\"ltx_cite ltx_citemacro_cite\">Bu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib13\" title=\"\">2024</a>)</cite>, was compiled from fact-checking platforms including Snopes, PolitiFact, FactCheck.org, and Health Feedback. While the majority of content is in English, some posts and modalities appear in other languages. We annotated a sample of 254 videos from this collection, following the same guidelines described before.</p>\n\n",
                "matched_terms": [
                    "factchecking",
                    "english",
                    "dataset",
                    "videos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The model performs well on both Norwegian and English TikTok videos, with notable differences in class-wise behavior. For Norwegian content, the system achieves strong recall for <span class=\"ltx_text ltx_font_typewriter\">Checkworthy</span> instances (0.85) and high overall accuracy (0.88), indicating robust performance in identifying relevant claims. In contrast, the English dataset shows higher precision for <span class=\"ltx_text ltx_font_typewriter\">Checkworthy</span> (0.82) but lower recall (0.58), suggesting the model is more conservative in flagging English videos as checkworthy. Combined macro-averaged scores are comparable across languages (F1: 0.72 for Norwegian, 0.74 for English), highlighting the pipeline&#8217;s cross-lingual generalizability with slightly better balance in the Norwegian case.</p>\n\n",
                "matched_terms": [
                    "english",
                    "checkworthy",
                    "tiktok",
                    "combined",
                    "accuracy",
                    "norwegian",
                    "videos",
                    "precision",
                    "dataset",
                    "recall"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The ablation study shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#S4.T5\" title=\"Table 5 &#8227; TikTok Videos from Fact-Checking Websites: &#8227; 4.2 Datasets &#8227; 4 Experimental Evaluation &#8227; ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> demonstrates that textual modules are the most influential components in determining checkworthiness. The removal of the <span class=\"ltx_text ltx_font_italic\">Transcript Verdict</span> and <span class=\"ltx_text ltx_font_italic\">Buzzword</span> modules resulted in the largest decreases in recall and F1-score, highlighting the critical role of spoken content and ideological language. In contrast, excluding modules such as <span class=\"ltx_text ltx_font_italic\">Weapon Detection</span>, <span class=\"ltx_text ltx_font_italic\">Fact Check</span>, or <span class=\"ltx_text ltx_font_italic\">Video-to-Text Verdict</span> had minimal impact, indicating their limited standalone contribution. Notably, the <span class=\"ltx_text ltx_font_italic\">Weapon Detection</span> module slightly reduced overall performance, likely because such content appears infrequently; as a result, it was omitted from the final pipeline. These findings reinforce the system&#8217;s reliance on semantic and linguistic features rather than visual or metadata-based cues. Finally since removing individual modules does not show a huge drop in performance, the overall performance is attributed to contribution of modules.</p>\n\n",
                "matched_terms": [
                    "not",
                    "recall"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The models were evaluated in a zero-shot setting, without any fine-tuning on the target dataset. Among them, <span class=\"ltx_text ltx_font_italic\">EfficientNET</span> outperformed all others across evaluation metrics, achieving an accuracy of 0.612 and a remarkably high precision of 0.992, indicating highly reliable positive predictions. However, its recall of 0.573 suggests it still misses nearly half of actual deepfakes. <span class=\"ltx_text ltx_font_italic\">MesoNET</span> and <span class=\"ltx_text ltx_font_italic\">Wvolf/ViT</span> perform very poorly.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "metrics",
                    "precision",
                    "dataset",
                    "recall"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Upload TikTok videos or paste URLs.</p>\n\n",
                "matched_terms": [
                    "tiktok",
                    "videos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented\n<span class=\"ltx_text ltx_font_smallcaps\">ShortCheck</span>, a modular, multilingual pipeline for detecting checkworthy content in short-form videos. The system integrates multiple modalities: text, audio, video, and image, and achieves strong performance on manually annotated TikTok datasets in both Norwegian and English. Through ablation studies, we showed that transcript and ideological language signals contribute most to checkworthiness, while visual features like deepfake detection and object recognition offer limited standalone utility.</p>\n\n",
                "matched_terms": [
                    "english",
                    "checkworthy",
                    "tiktok",
                    "videos",
                    "norwegian"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In future work, we plan to extend <span class=\"ltx_text ltx_font_smallcaps\">ShortCheck</span>&#160;with full fact-checking while testing on more datasets, including multilingual video content from X and YouTube. We will evaluate individual modules in greater depth, improve robustness in noisy or low-resource settings, and integrate feedback from fact-checkers to refine outputs and interpretability. Finally, we aim to explore advanced multimodal fusion techniques beyond rule-based aggregation to boost accuracy and generalization.</p>\n\n",
                "matched_terms": [
                    "factchecking",
                    "accuracy"
                ]
            }
        ]
    },
    "S4.T3": {
        "source_file": "ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos",
        "caption": "Table 3: Dataset composition by language and checkworthiness class. CW: Checkworthy and NCW: Not Checkworthy.",
        "body": "Dataset\nCW\nNCW\nTotal\n\n\nNorwegian influencer\n33\n204\n237\n\n\nFact-checking websites\n114\n140\n254\n\n\nTotal\n147\n344\n491",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">CW</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">NCW</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Total</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Norwegian influencer</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">204</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">237</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fact-checking websites</td>\n<td class=\"ltx_td ltx_align_center\">114</td>\n<td class=\"ltx_td ltx_align_center\">140</td>\n<td class=\"ltx_td ltx_align_center\">254</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Total</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">147</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">344</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">491</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "language",
            "checkworthy",
            "composition",
            "influencer",
            "class",
            "total",
            "ncw",
            "norwegian",
            "checkworthiness",
            "factchecking",
            "websites",
            "dataset",
            "not"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We evaluate <span class=\"ltx_text ltx_font_smallcaps\">ShortCheck</span>&#160;on two manually annotated dataset. Summary of dataset statistics is shown in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#S4.T3\" title=\"Table 3 &#8227; TikTok Videos from Fact-Checking Websites: &#8227; 4.2 Datasets &#8227; 4 Experimental Evaluation &#8227; ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos\"><span class=\"ltx_text ltx_ref_tag\">3</span></a></p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">The multimodal complexity of short videos makes automated fact-checking technically challenging and manual efforts are increasingly unsustainable, especially for under-resourced fact-checkers facing unprecedented content scale and funding cuts.\nIn this demo, we present a prototype designed to automate the identification of potentially checkworthy videos, significantly reducing the time required by human fact-checkers.\nOur prototype is easy to use and can predict checkworthiness in over 30 major languages.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span>Intersection of languages supported by Meta Llama3 <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://ai.meta.com/blog/meta-llama-3/\" title=\"\">https://ai.meta.com/blog/meta-llama-3/</a> and OpenAI Whisper <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/openai/whisper\" title=\"\">https://github.com/openai/whisper</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "factchecking",
                    "checkworthy",
                    "checkworthiness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most existing misinformation detection systems are designed for structured, single-modality content such as news articles, social media posts, or deepfake detection, with a primary focus on either text, audio, transcriptions or visual modalities. We summarize the existing fact-checking systems and their modalities in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. However, these approaches are not suited for short-form video content found on platforms like TikTok or YouTube Shorts due to the casual unstructured nature of the content.</p>\n\n",
                "matched_terms": [
                    "factchecking",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While multimodal fact-checking is gaining traction, the gap between research prototypes and deployable, interpretable tools for short-form video remains substantial. Bridging this divide requires not only improved multimodal understanding but also system outputs that align with the needs of human fact-checkers in high-throughput environments.</p>\n\n",
                "matched_terms": [
                    "factchecking",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Automated fact-checking has progressed with the rise of benchmark datasets. The FEVER dataset by <cite class=\"ltx_cite ltx_citemacro_citet\">Thorne et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib35\" title=\"\">2018</a>)</cite> established foundational tasks in claim verification, with follow-up surveys and taxonomies by <cite class=\"ltx_cite ltx_citemacro_citet\">Thorne and Vlachos (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib34\" title=\"\">2018</a>)</cite> and <cite class=\"ltx_cite ltx_citemacro_citet\">Guo et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib18\" title=\"\">2022</a>)</cite>. Real-world datasets such as <span class=\"ltx_text ltx_font_smallcaps\">MultiFC</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Augenstein et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib6\" title=\"\">2019</a>)</cite>, <span class=\"ltx_text ltx_font_smallcaps\">AVeriTeC</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Schlichtkrull et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib29\" title=\"\">2023</a>)</cite>, and <span class=\"ltx_text ltx_font_smallcaps\">QuanTemp</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Venktesh et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib36\" title=\"\">2024</a>)</cite> have expanded the scope to include diverse, evidence-backed, and numerically grounded claims.</p>\n\n",
                "matched_terms": [
                    "factchecking",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our system, <span class=\"ltx_text ltx_font_smallcaps\">ShortCheck</span>, differs in its emphasis on short-form video platforms like TikTok. It uniquely supports all four modalities: text, audio, image, and video, along with multilingual processing and capable of processing long text with more than single claims. While it does not support full automated fact-checking, it offers interpretable, modular verdict signals suitable for assisting professional fact-checkers in verification.</p>\n\n",
                "matched_terms": [
                    "factchecking",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given that the checkworthiness of a TikTok video can be inherently subjective, we base our approach on established best practices followed by professional fact-checkers. In particular, we consulted the guidelines of Faktisk.no<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>A Norwegian non-profit organization accredited by the International Fact-Checking Network (IFCN).</span></span></span> This also aligns with the definition of fact-checkworthiness in the literature<cite class=\"ltx_cite ltx_citemacro_cite\">Jaradat et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib21\" title=\"\">2018</a>); Barr&#243;n-Cede&#241;o et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib8\" title=\"\">2024</a>)</cite></p>\n\n",
                "matched_terms": [
                    "factchecking",
                    "norwegian",
                    "checkworthiness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Claims related to celebrity gossip, sports, advertisements etc are not considered checkworthy.</p>\n\n",
                "matched_terms": [
                    "checkworthy",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further refine the decision-making process, we incorporate a claim detection and external fact-checking module. This component leverages fine-tuned transformer models for claim detection and natural language inference (NLI), applied to both the transcript and the visual summary of the video. Following the approach proposed by <cite class=\"ltx_cite ltx_citemacro_cite\">Setty (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib31\" title=\"\">2024b</a>)</cite>, the module identifies declarative, factual statements and attempts to verify them against a fact-checking evidence database. While this does not fact-check the entire content of the video, it provides fact-checkers early signals indicating whether the video may contain verifiably false or misleading information.</p>\n\n",
                "matched_terms": [
                    "language",
                    "factchecking",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, the system aggregates the outputs from all modules using a rule-based logic engine. A scoring system considers multiple module outputs, including the presence of claims, ideological language, or political classification. If the cumulative score exceeds a threshold, the video is marked as <span class=\"ltx_text ltx_font_typewriter\">Checkworthy</span>. Advertisements are detected and filtered early in the pipeline and override all other scores. Videos that do not meet either criterion are labeled as <span class=\"ltx_text ltx_font_typewriter\">Not_Checkworthy</span>. This decision process is entirely transparent and configurable, enabling future refinement without retraining models.</p>\n\n",
                "matched_terms": [
                    "language",
                    "checkworthy",
                    "not"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This dataset includes 249 TikTok videos curated by Faktisk for an emotional analysis study on political trolling via buzzwords like &#8220;Stem FRP&#8221;.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.faktisk.no/artikkel/faktisk-analyse-av-tiktok-menn-mest-negative/109375\" title=\"\">https://www.faktisk.no/artikkel/faktisk-analyse-av-tiktok-menn-mest-negative/109375</a></span></span></span> We manually annotated each video for checkworthiness using the guidelines in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#S3\" title=\"3 System Overview &#8227; ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "checkworthiness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This dataset, curated by <cite class=\"ltx_cite ltx_citemacro_cite\">Bu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib13\" title=\"\">2024</a>)</cite>, was compiled from fact-checking platforms including Snopes, PolitiFact, FactCheck.org, and Health Feedback. While the majority of content is in English, some posts and modalities appear in other languages. We annotated a sample of 254 videos from this collection, following the same guidelines described before.</p>\n\n",
                "matched_terms": [
                    "factchecking",
                    "dataset"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The model performs well on both Norwegian and English TikTok videos, with notable differences in class-wise behavior. For Norwegian content, the system achieves strong recall for <span class=\"ltx_text ltx_font_typewriter\">Checkworthy</span> instances (0.85) and high overall accuracy (0.88), indicating robust performance in identifying relevant claims. In contrast, the English dataset shows higher precision for <span class=\"ltx_text ltx_font_typewriter\">Checkworthy</span> (0.82) but lower recall (0.58), suggesting the model is more conservative in flagging English videos as checkworthy. Combined macro-averaged scores are comparable across languages (F1: 0.72 for Norwegian, 0.74 for English), highlighting the pipeline&#8217;s cross-lingual generalizability with slightly better balance in the Norwegian case.</p>\n\n",
                "matched_terms": [
                    "dataset",
                    "norwegian",
                    "checkworthy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The ablation study shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#S4.T5\" title=\"Table 5 &#8227; TikTok Videos from Fact-Checking Websites: &#8227; 4.2 Datasets &#8227; 4 Experimental Evaluation &#8227; ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> demonstrates that textual modules are the most influential components in determining checkworthiness. The removal of the <span class=\"ltx_text ltx_font_italic\">Transcript Verdict</span> and <span class=\"ltx_text ltx_font_italic\">Buzzword</span> modules resulted in the largest decreases in recall and F1-score, highlighting the critical role of spoken content and ideological language. In contrast, excluding modules such as <span class=\"ltx_text ltx_font_italic\">Weapon Detection</span>, <span class=\"ltx_text ltx_font_italic\">Fact Check</span>, or <span class=\"ltx_text ltx_font_italic\">Video-to-Text Verdict</span> had minimal impact, indicating their limited standalone contribution. Notably, the <span class=\"ltx_text ltx_font_italic\">Weapon Detection</span> module slightly reduced overall performance, likely because such content appears infrequently; as a result, it was omitted from the final pipeline. These findings reinforce the system&#8217;s reliance on semantic and linguistic features rather than visual or metadata-based cues. Finally since removing individual modules does not show a huge drop in performance, the overall performance is attributed to contribution of modules.</p>\n\n",
                "matched_terms": [
                    "language",
                    "not",
                    "checkworthiness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented\n<span class=\"ltx_text ltx_font_smallcaps\">ShortCheck</span>, a modular, multilingual pipeline for detecting checkworthy content in short-form videos. The system integrates multiple modalities: text, audio, video, and image, and achieves strong performance on manually annotated TikTok datasets in both Norwegian and English. Through ablation studies, we showed that transcript and ideological language signals contribute most to checkworthiness, while visual features like deepfake detection and object recognition offer limited standalone utility.</p>\n\n",
                "matched_terms": [
                    "language",
                    "norwegian",
                    "checkworthy",
                    "checkworthiness"
                ]
            }
        ]
    },
    "S4.T4": {
        "source_file": "ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos",
        "caption": "Table 4: Evaluation scores of DeepFake detection models. Precision (P), Recall (R), Accuracy (A), and F1-weighted (F1-w)",
        "body": "Model\nA\nP\nR\nF1-W\n\n\nMesoNET\n0.114\n0.808\n0.019\n0.038\n\n\nWvolf/ViT\n0.100\n0.000\n0.000\n0.000\n\n\nEfficientNET\n0.612\n0.992\n0.573\n0.727",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">A</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">P</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">R</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">F1-W</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">MesoNET</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.114</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.808</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.019</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.038</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Wvolf/ViT</td>\n<td class=\"ltx_td ltx_align_center\">0.100</td>\n<td class=\"ltx_td ltx_align_center\">0.000</td>\n<td class=\"ltx_td ltx_align_center\">0.000</td>\n<td class=\"ltx_td ltx_align_center\">0.000</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\">EfficientNET</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\">0.612</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\">0.992</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\">0.573</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\"><span class=\"ltx_text ltx_font_bold\">0.727</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "wvolfvit",
            "deepfake",
            "detection",
            "evaluation",
            "f1weighted",
            "models",
            "accuracy",
            "efficientnet",
            "mesonet",
            "f1w",
            "model",
            "scores",
            "precision",
            "recall"
        ],
        "citing_paragraphs": [],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Short-form video platforms like TikTok present unique challenges for misinformation detection due to their multimodal, dynamic, and noisy content.\nWe present <span class=\"ltx_text ltx_font_smallcaps\">ShortCheck</span>, a modular, inference-only pipeline with a user-friendly interface that automatically identifies checkworthy short-form videos to help human fact-checkers.\nThe system integrates speech transcription, OCR, object and deepfake detection, video-to-text summarization, and claim verification.\n<span class=\"ltx_text ltx_font_smallcaps\">ShortCheck</span>&#160;is validated by evaluating it on two manually annotated datasets with TikTok videos in a multilingual setting.\nThe pipeline achieves promising results with F1-weighted score over 70%.</p>\n\n",
                "matched_terms": [
                    "deepfake",
                    "f1weighted",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most existing misinformation detection systems are designed for structured, single-modality content such as news articles, social media posts, or deepfake detection, with a primary focus on either text, audio, transcriptions or visual modalities. We summarize the existing fact-checking systems and their modalities in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. However, these approaches are not suited for short-form video content found on platforms like TikTok or YouTube Shorts due to the casual unstructured nature of the content.</p>\n\n",
                "matched_terms": [
                    "deepfake",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Deepfake detection models like <span class=\"ltx_text ltx_font_smallcaps\">MesoNet</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Afchar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib2\" title=\"\">2018a</a>)</cite> target facial forgeries using lightweight CNNs, making them suitable for integration into broader video authenticity pipelines.</p>\n\n",
                "matched_terms": [
                    "mesonet",
                    "deepfake",
                    "models",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To detect synthetic media, we incorporated a deepfake detection module into the pipeline. Since many methods rely on identity-specific data or high-quality frontal imagery, we evaluated their generalization to TikTok&#8217;s unconstrained, user-generated content <cite class=\"ltx_cite ltx_citemacro_cite\">Abbas and Taeihagh (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib1\" title=\"\">2024</a>)</cite>. We tested three zero-shot models: <span class=\"ltx_text ltx_font_italic\">MesoNet</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Afchar et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib3\" title=\"\">2018b</a>)</cite>, a mesoscopic CNN; <span class=\"ltx_text ltx_font_italic\">EfficientNet</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Bonettini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib11\" title=\"\">2021</a>); Dolhansky et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib16\" title=\"\">2020</a>)</cite>, an attention-based model from the DFDC challenge; and <span class=\"ltx_text ltx_font_italic\">Wvolf/ViT<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_upright\">4</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_upright\" href=\"https://huggingface.co/Wvolf/ViT_Deepfake_Detection\" title=\"\">https://huggingface.co/Wvolf/ViT_Deepfake_Detection</a></span></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">Bonettini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib10\" title=\"\">2020</a>)</cite>, selected for its plug-and-play accessibility. Their outputs contributed as signals in the rule-based decision logic.</p>\n\n",
                "matched_terms": [
                    "deepfake",
                    "detection",
                    "models",
                    "mesonet",
                    "model",
                    "efficientnet"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To obtain a visual semantic summary, video frames are sampled and passed through LLaVA <cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib26\" title=\"\">2023b</a>)</cite>, a vision-language model that generates frame-level captions describing people, objects, and scenes. These captions are subsequently summarized and contextualized using Meta&#8217;s LLaMA 3 model, which also performs high-level semantic classification. The model predicts whether a video is political, hostile, benign, or promotional in nature. All models are hosted via Ollama, a lightweight, local model serving platform that supports REST-based inference.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"ollama.com\" title=\"\">ollama.com</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further refine the decision-making process, we incorporate a claim detection and external fact-checking module. This component leverages fine-tuned transformer models for claim detection and natural language inference (NLI), applied to both the transcript and the visual summary of the video. Following the approach proposed by <cite class=\"ltx_cite ltx_citemacro_cite\">Setty (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib31\" title=\"\">2024b</a>)</cite>, the module identifies declarative, factual statements and attempts to verify them against a fact-checking evidence database. While this does not fact-check the entire content of the video, it provides fact-checkers early signals indicating whether the video may contain verifiably false or misleading information.</p>\n\n",
                "matched_terms": [
                    "models",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, the system aggregates the outputs from all modules using a rule-based logic engine. A scoring system considers multiple module outputs, including the presence of claims, ideological language, or political classification. If the cumulative score exceeds a threshold, the video is marked as <span class=\"ltx_text ltx_font_typewriter\">Checkworthy</span>. Advertisements are detected and filtered early in the pipeline and override all other scores. Videos that do not meet either criterion are labeled as <span class=\"ltx_text ltx_font_typewriter\">Not_Checkworthy</span>. This decision process is entirely transparent and configurable, enabling future refinement without retraining models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "scores"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present the overall results and ablation studies. In addition, we also present the effectiveness of some of the modules such as DeepFake detection. We plan to evaluate other modules in detail in future work.</p>\n\n",
                "matched_terms": [
                    "deepfake",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The model performs well on both Norwegian and English TikTok videos, with notable differences in class-wise behavior. For Norwegian content, the system achieves strong recall for <span class=\"ltx_text ltx_font_typewriter\">Checkworthy</span> instances (0.85) and high overall accuracy (0.88), indicating robust performance in identifying relevant claims. In contrast, the English dataset shows higher precision for <span class=\"ltx_text ltx_font_typewriter\">Checkworthy</span> (0.82) but lower recall (0.58), suggesting the model is more conservative in flagging English videos as checkworthy. Combined macro-averaged scores are comparable across languages (F1: 0.72 for Norwegian, 0.74 for English), highlighting the pipeline&#8217;s cross-lingual generalizability with slightly better balance in the Norwegian case.</p>\n\n",
                "matched_terms": [
                    "accuracy",
                    "model",
                    "scores",
                    "precision",
                    "recall"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The ablation study shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#S4.T5\" title=\"Table 5 &#8227; TikTok Videos from Fact-Checking Websites: &#8227; 4.2 Datasets &#8227; 4 Experimental Evaluation &#8227; ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> demonstrates that textual modules are the most influential components in determining checkworthiness. The removal of the <span class=\"ltx_text ltx_font_italic\">Transcript Verdict</span> and <span class=\"ltx_text ltx_font_italic\">Buzzword</span> modules resulted in the largest decreases in recall and F1-score, highlighting the critical role of spoken content and ideological language. In contrast, excluding modules such as <span class=\"ltx_text ltx_font_italic\">Weapon Detection</span>, <span class=\"ltx_text ltx_font_italic\">Fact Check</span>, or <span class=\"ltx_text ltx_font_italic\">Video-to-Text Verdict</span> had minimal impact, indicating their limited standalone contribution. Notably, the <span class=\"ltx_text ltx_font_italic\">Weapon Detection</span> module slightly reduced overall performance, likely because such content appears infrequently; as a result, it was omitted from the final pipeline. These findings reinforce the system&#8217;s reliance on semantic and linguistic features rather than visual or metadata-based cues. Finally since removing individual modules does not show a huge drop in performance, the overall performance is attributed to contribution of modules.</p>\n\n",
                "matched_terms": [
                    "recall",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The models were evaluated in a zero-shot setting, without any fine-tuning on the target dataset. Among them, <span class=\"ltx_text ltx_font_italic\">EfficientNET</span> outperformed all others across evaluation metrics, achieving an accuracy of 0.612 and a remarkably high precision of 0.992, indicating highly reliable positive predictions. However, its recall of 0.573 suggests it still misses nearly half of actual deepfakes. <span class=\"ltx_text ltx_font_italic\">MesoNET</span> and <span class=\"ltx_text ltx_font_italic\">Wvolf/ViT</span> perform very poorly.</p>\n\n",
                "matched_terms": [
                    "evaluation",
                    "models",
                    "accuracy",
                    "efficientnet",
                    "mesonet",
                    "wvolfvit",
                    "precision",
                    "recall"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented\n<span class=\"ltx_text ltx_font_smallcaps\">ShortCheck</span>, a modular, multilingual pipeline for detecting checkworthy content in short-form videos. The system integrates multiple modalities: text, audio, video, and image, and achieves strong performance on manually annotated TikTok datasets in both Norwegian and English. Through ablation studies, we showed that transcript and ideological language signals contribute most to checkworthiness, while visual features like deepfake detection and object recognition offer limited standalone utility.</p>\n\n",
                "matched_terms": [
                    "deepfake",
                    "detection"
                ]
            }
        ]
    },
    "S4.T5": {
        "source_file": "ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos",
        "caption": "Table 5: Ablation study showing the performance change when individual modules are removed. Values represent the change from the full system (Baseline). Red indicates a drop in performance. Metrics: Precision (P), Recall (R), Accuracy (Acc) and F1-weighted (F1-W).",
        "body": "Removed\nP\nR\nAcc\nF1-W\n\n\nWeapon detection\n+0.005\n+0.002\n+0.004\n+0.004\n\n\nVideo summary\n+0.001\n-0.008\n0.000\n-0.002\n\n\nTranscript\n+0.027\n-0.076\n0.000\n-0.024\n\n\nBuzzword\n-0.024\n-0.050\n-0.021\n-0.033\n\n\nOCR\n-0.005\n-0.030\n-0.003\n-0.004\n\n\nFact Check\n+0.013\n-0.040\n+0.004\n-0.009\n\n\nAll modules\n0.737\n0.727\n0.884\n0.720",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Removed</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">P</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">R</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Acc</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">F1-W</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Weapon detection</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#006400;\">+0.005</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#006400;\">+0.002</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#006400;\">+0.004</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#006400;\">+0.004</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Video summary</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#006400;\">+0.001</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">-0.008</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">0.000</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">-0.002</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Transcript</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#006400;\">+0.027</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">-0.076</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#000000;\">0.000</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">-0.024</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Buzzword</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">-0.024</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">-0.050</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">-0.021</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">-0.033</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">OCR</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">-0.005</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">-0.030</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">-0.003</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">-0.004</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Fact Check</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#006400;\">+0.013</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">-0.040</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#006400;\">+0.004</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"--ltx-fg-color:#FF0000;\">-0.009</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_t\">All modules</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.737</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.727</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.884</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.720</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "change",
            "drop",
            "summary",
            "study",
            "individual",
            "transcript",
            "removed",
            "video",
            "recall",
            "f1weighted",
            "accuracy",
            "buzzword",
            "metrics",
            "indicates",
            "system",
            "performance",
            "ablation",
            "detection",
            "showing",
            "ocr",
            "acc",
            "weapon",
            "from",
            "precision",
            "baseline",
            "represent",
            "all",
            "fact",
            "modules",
            "values",
            "f1w",
            "when",
            "full",
            "red",
            "check"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The ablation study shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#S4.T5\" title=\"Table 5 &#8227; TikTok Videos from Fact-Checking Websites: &#8227; 4.2 Datasets &#8227; 4 Experimental Evaluation &#8227; ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> demonstrates that textual modules are the most influential components in determining checkworthiness. The removal of the <span class=\"ltx_text ltx_font_italic\">Transcript Verdict</span> and <span class=\"ltx_text ltx_font_italic\">Buzzword</span> modules resulted in the largest decreases in recall and F1-score, highlighting the critical role of spoken content and ideological language. In contrast, excluding modules such as <span class=\"ltx_text ltx_font_italic\">Weapon Detection</span>, <span class=\"ltx_text ltx_font_italic\">Fact Check</span>, or <span class=\"ltx_text ltx_font_italic\">Video-to-Text Verdict</span> had minimal impact, indicating their limited standalone contribution. Notably, the <span class=\"ltx_text ltx_font_italic\">Weapon Detection</span> module slightly reduced overall performance, likely because such content appears infrequently; as a result, it was omitted from the final pipeline. These findings reinforce the system&#8217;s reliance on semantic and linguistic features rather than visual or metadata-based cues. Finally since removing individual modules does not show a huge drop in performance, the overall performance is attributed to contribution of modules.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Short-form video platforms like TikTok present unique challenges for misinformation detection due to their multimodal, dynamic, and noisy content.\nWe present <span class=\"ltx_text ltx_font_smallcaps\">ShortCheck</span>, a modular, inference-only pipeline with a user-friendly interface that automatically identifies checkworthy short-form videos to help human fact-checkers.\nThe system integrates speech transcription, OCR, object and deepfake detection, video-to-text summarization, and claim verification.\n<span class=\"ltx_text ltx_font_smallcaps\">ShortCheck</span>&#160;is validated by evaluating it on two manually annotated datasets with TikTok videos in a multilingual setting.\nThe pipeline achieves promising results with F1-weighted score over 70%.</p>\n\n",
                "matched_terms": [
                    "system",
                    "detection",
                    "f1weighted",
                    "ocr",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The popularity of short-form video platforms such as <em class=\"ltx_emph ltx_font_italic\">TikTok</em>, <em class=\"ltx_emph ltx_font_italic\">YouTube Shorts</em> and <em class=\"ltx_emph ltx_font_italic\">Instagram Reels</em> has transformed how information is produced, consumed, and spread.\nWith billions of monthly active users, these platforms create fertile ground for the spread of misinformation on sensitive topics including politics, health, and social issues.\nUnlike traditional text or image-based content, these videos may include multiple modalities such as speech, text overlays, music, and visuals, often edited in ways that obscure meaning or context, though not all of these elements are always present; for example, some videos contain only on-screen text without audio, while others show just a speaker without any additional graphics or overlays.\nFor example, Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> shows a TikTok screenshot where the overlay text makes the claim, while the audio transcript (translated to &#8220;it&#8217;s a shame&#8221;) does not provide any useful information. The video summary notes an urban explosion, indicating potentially contentious content for fact-checkers.</p>\n\n",
                "matched_terms": [
                    "video",
                    "all",
                    "transcript",
                    "summary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Most existing misinformation detection systems are designed for structured, single-modality content such as news articles, social media posts, or deepfake detection, with a primary focus on either text, audio, transcriptions or visual modalities. We summarize the existing fact-checking systems and their modalities in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>. However, these approaches are not suited for short-form video content found on platforms like TikTok or YouTube Shorts due to the casual unstructured nature of the content.</p>\n\n",
                "matched_terms": [
                    "video",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">While multimodal fact-checking is gaining traction, the gap between research prototypes and deployable, interpretable tools for short-form video remains substantial. Bridging this divide requires not only improved multimodal understanding but also system outputs that align with the needs of human fact-checkers in high-throughput environments.</p>\n\n",
                "matched_terms": [
                    "system",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A <span class=\"ltx_text ltx_font_bold\">modular multimodal pipeline</span> that integrates OCR, transcription, video-to-text captioning, semantic classification, retrieval and fact-checking modules.</p>\n\n",
                "matched_terms": [
                    "ocr",
                    "modules"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Multimodal misinformation has led to techniques that fuse text, audio, and visual cues <cite class=\"ltx_cite ltx_citemacro_citep\">(Alam et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib4\" title=\"\">2022</a>)</cite>. <span class=\"ltx_text ltx_font_smallcaps\">FakingRecipe</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Bu et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib13\" title=\"\">2024</a>)</cite> investigates manipulation strategies in TikTok videos, complementing earlier datasets like <span class=\"ltx_text ltx_font_smallcaps\">SpotFake</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Singhal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib33\" title=\"\">2019</a>)</cite> and <span class=\"ltx_text ltx_font_smallcaps\">NewsBag</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Jindal et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib22\" title=\"\">2020</a>)</cite>. <cite class=\"ltx_cite ltx_citemacro_citet\">Yao et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib38\" title=\"\">2023</a>)</cite> proposed an end-to-end video fact-checking system with explanation generation. Our work builds on these by offering a modular, interpretable pipeline designed for integration into professional fact-checkers&#8217; workflows.</p>\n\n",
                "matched_terms": [
                    "system",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Deepfake detection models like <span class=\"ltx_text ltx_font_smallcaps\">MesoNet</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Afchar et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib2\" title=\"\">2018a</a>)</cite> target facial forgeries using lightweight CNNs, making them suitable for integration into broader video authenticity pipelines.</p>\n\n",
                "matched_terms": [
                    "video",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent systems have bridged research and application by supporting live or end-to-end fact-checking across modalities. <span class=\"ltx_text ltx_font_smallcaps\">BRENDA</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Botnevik et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib12\" title=\"\">2020</a>)</cite> provides real-time claim detection for long-form text. <span class=\"ltx_text ltx_font_smallcaps\">FactCheckEditor</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Setty, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib31\" title=\"\">2024b</a>)</cite> and <span class=\"ltx_text ltx_font_smallcaps\">PodFC</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Setty, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib32\" title=\"\">2025</a>)</cite> support multilingual verification, with the latter tailored for long-form audio. <span class=\"ltx_text ltx_font_smallcaps\">LiveFC</span> <cite class=\"ltx_cite ltx_citemacro_citep\">(Venktesh and Setty, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib37\" title=\"\">2025</a>)</cite> handles real-time fact-checking of audio streams, combining transcription, claim detection, evidence retrieval, and verdict generation. However, as summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#S1.T1\" title=\"Table 1 &#8227; 1 Introduction &#8227; ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, most systems focus on single-modality inputs (usually text) and single-claim granularity, with limited support for video or multimodal content.</p>\n\n",
                "matched_terms": [
                    "video",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our system, <span class=\"ltx_text ltx_font_smallcaps\">ShortCheck</span>, differs in its emphasis on short-form video platforms like TikTok. It uniquely supports all four modalities: text, audio, image, and video, along with multilingual processing and capable of processing long text with more than single claims. While it does not support full automated fact-checking, it offers interpretable, modular verdict signals suitable for assisting professional fact-checkers in verification.</p>\n\n",
                "matched_terms": [
                    "system",
                    "full",
                    "video",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We propose a modular, inference-only pipeline for detecting potentially misleading or checkworthy content in short-form videos, particularly those published on platforms like TikTok. The system assigns each video one of two categorical labels: <span class=\"ltx_text ltx_font_typewriter\">Checkworthy</span>, or <span class=\"ltx_text ltx_font_typewriter\">Not_Checkworthy</span>. Unlike prior work that builds monolithic or end-to-end models, our design emphasizes modularity, interpretability, and adaptability. Each component in the pipeline can be independently replaced, which makes the system robust to failures in specific modalities and easier to maintain in production settings.</p>\n\n",
                "matched_terms": [
                    "system",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The pipeline comprises feature extraction modules tailored to speech, text, visuals, and metadata, whose outputs are aggregated by a rule-based engine for final video classification. Additional modules, such as object detection for weapons, were tested but excluded due to limited contribution.</p>\n\n",
                "matched_terms": [
                    "modules",
                    "video",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The first stage involves extracting visible on-screen text through Optical Character Recognition (OCR), using the EasyOCR library<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/JaidedAI/EasyOCR\" title=\"\">https://github.com/JaidedAI/EasyOCR</a></span></span></span>. This module captures embedded captions or textual overlays, which are common in TikTok videos. However, OCR performance is often challenged by stylized fonts, rapid transitions, and visually noisy frames.</p>\n\n",
                "matched_terms": [
                    "ocr",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To detect synthetic media, we incorporated a deepfake detection module into the pipeline. Since many methods rely on identity-specific data or high-quality frontal imagery, we evaluated their generalization to TikTok&#8217;s unconstrained, user-generated content <cite class=\"ltx_cite ltx_citemacro_cite\">Abbas and Taeihagh (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib1\" title=\"\">2024</a>)</cite>. We tested three zero-shot models: <span class=\"ltx_text ltx_font_italic\">MesoNet</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Afchar et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib3\" title=\"\">2018b</a>)</cite>, a mesoscopic CNN; <span class=\"ltx_text ltx_font_italic\">EfficientNet</span> <cite class=\"ltx_cite ltx_citemacro_cite\">Bonettini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib11\" title=\"\">2021</a>); Dolhansky et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib16\" title=\"\">2020</a>)</cite>, an attention-based model from the DFDC challenge; and <span class=\"ltx_text ltx_font_italic\">Wvolf/ViT<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_upright\">4</span></span><a class=\"ltx_ref ltx_url ltx_font_typewriter ltx_font_upright\" href=\"https://huggingface.co/Wvolf/ViT_Deepfake_Detection\" title=\"\">https://huggingface.co/Wvolf/ViT_Deepfake_Detection</a></span></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">Bonettini et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib10\" title=\"\">2020</a>)</cite>, selected for its plug-and-play accessibility. Their outputs contributed as signals in the rule-based decision logic.</p>\n\n",
                "matched_terms": [
                    "from",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To obtain a visual semantic summary, video frames are sampled and passed through LLaVA <cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib26\" title=\"\">2023b</a>)</cite>, a vision-language model that generates frame-level captions describing people, objects, and scenes. These captions are subsequently summarized and contextualized using Meta&#8217;s LLaMA 3 model, which also performs high-level semantic classification. The model predicts whether a video is political, hostile, benign, or promotional in nature. All models are hosted via Ollama, a lightweight, local model serving platform that supports REST-based inference.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"ollama.com\" title=\"\">ollama.com</a></span></span></span></p>\n\n",
                "matched_terms": [
                    "all",
                    "video",
                    "summary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In addition to these core modules, we incorporate a rule-based system for detecting ideological buzzwords and coded language, often referred to as dog whistles. This module operates on both OCR and transcript outputs, scanning for terms known to encode political or ideological meaning in subtle ways. The detection rules are informed by prior literature on dog-whistle communication <cite class=\"ltx_cite ltx_citemacro_cite\">Albertson (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib5\" title=\"\">2015</a>)</cite> and curated datasets from organizations such as Faktisk.no.</p>\n\n",
                "matched_terms": [
                    "system",
                    "detection",
                    "ocr",
                    "modules",
                    "transcript",
                    "from"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To further refine the decision-making process, we incorporate a claim detection and external fact-checking module. This component leverages fine-tuned transformer models for claim detection and natural language inference (NLI), applied to both the transcript and the visual summary of the video. Following the approach proposed by <cite class=\"ltx_cite ltx_citemacro_cite\">Setty (<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#bib.bib31\" title=\"\">2024b</a>)</cite>, the module identifies declarative, factual statements and attempts to verify them against a fact-checking evidence database. While this does not fact-check the entire content of the video, it provides fact-checkers early signals indicating whether the video may contain verifiably false or misleading information.</p>\n\n",
                "matched_terms": [
                    "detection",
                    "video",
                    "transcript",
                    "summary"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Finally, the system aggregates the outputs from all modules using a rule-based logic engine. A scoring system considers multiple module outputs, including the presence of claims, ideological language, or political classification. If the cumulative score exceeds a threshold, the video is marked as <span class=\"ltx_text ltx_font_typewriter\">Checkworthy</span>. Advertisements are detected and filtered early in the pipeline and override all other scores. Videos that do not meet either criterion are labeled as <span class=\"ltx_text ltx_font_typewriter\">Not_Checkworthy</span>. This decision process is entirely transparent and configurable, enabling future refinement without retraining models.</p>\n\n",
                "matched_terms": [
                    "system",
                    "all",
                    "modules",
                    "from",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All models used in the pipeline are inference-only and require no task-specific fine-tuning. We use prompt engineering to adapt general-purpose models to specific sub-tasks. The LLaMA 3 and LLaVA models are deployed using Ollama, which allows for lightweight, local hosting and fast prototyping. Custom prompts, temperature settings, stop sequences, and token limits are adjusted to ensure consistent outputs across modules.</p>\n\n",
                "matched_terms": [
                    "modules",
                    "all"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">A key design goal of our approach is interpretability. Each module exposes intermediate outputs that are human-readable and can be inspected by fact-checkers. This transparency builds trust and enables feedback-driven improvement of the system. The modular design also ensures that any individual component, such as the OCR engine or the semantic classifier, can be replaced or updated without disrupting the entire pipeline. This is especially important for deployment in evolving information ecosystems where content formats and threat types change rapidly.</p>\n\n",
                "matched_terms": [
                    "system",
                    "ocr",
                    "individual",
                    "change"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">This dataset includes 249 TikTok videos curated by Faktisk for an emotional analysis study on political trolling via buzzwords like &#8220;Stem FRP&#8221;.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote6\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">6</sup><span class=\"ltx_tag ltx_tag_note\">6</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.faktisk.no/artikkel/faktisk-analyse-av-tiktok-menn-mest-negative/109375\" title=\"\">https://www.faktisk.no/artikkel/faktisk-analyse-av-tiktok-menn-mest-negative/109375</a></span></span></span> We manually annotated each video for checkworthiness using the guidelines in Section&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.20467v1#S3\" title=\"3 System Overview &#8227; ShortCheck: Checkworthiness Detection of Multilingual Short-Form Videos\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</p>\n\n",
                "matched_terms": [
                    "study",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this section, we present the overall results and ablation studies. In addition, we also present the effectiveness of some of the modules such as DeepFake detection. We plan to evaluate other modules in detail in future work.</p>\n\n",
                "matched_terms": [
                    "modules",
                    "ablation",
                    "detection"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The model performs well on both Norwegian and English TikTok videos, with notable differences in class-wise behavior. For Norwegian content, the system achieves strong recall for <span class=\"ltx_text ltx_font_typewriter\">Checkworthy</span> instances (0.85) and high overall accuracy (0.88), indicating robust performance in identifying relevant claims. In contrast, the English dataset shows higher precision for <span class=\"ltx_text ltx_font_typewriter\">Checkworthy</span> (0.82) but lower recall (0.58), suggesting the model is more conservative in flagging English videos as checkworthy. Combined macro-averaged scores are comparable across languages (F1: 0.72 for Norwegian, 0.74 for English), highlighting the pipeline&#8217;s cross-lingual generalizability with slightly better balance in the Norwegian case.</p>\n\n",
                "matched_terms": [
                    "system",
                    "performance",
                    "accuracy",
                    "precision",
                    "recall"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The models were evaluated in a zero-shot setting, without any fine-tuning on the target dataset. Among them, <span class=\"ltx_text ltx_font_italic\">EfficientNET</span> outperformed all others across evaluation metrics, achieving an accuracy of 0.612 and a remarkably high precision of 0.992, indicating highly reliable positive predictions. However, its recall of 0.573 suggests it still misses nearly half of actual deepfakes. <span class=\"ltx_text ltx_font_italic\">MesoNET</span> and <span class=\"ltx_text ltx_font_italic\">Wvolf/ViT</span> perform very poorly.</p>\n\n",
                "matched_terms": [
                    "all",
                    "accuracy",
                    "metrics",
                    "precision",
                    "recall"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We presented\n<span class=\"ltx_text ltx_font_smallcaps\">ShortCheck</span>, a modular, multilingual pipeline for detecting checkworthy content in short-form videos. The system integrates multiple modalities: text, audio, video, and image, and achieves strong performance on manually annotated TikTok datasets in both Norwegian and English. Through ablation studies, we showed that transcript and ideological language signals contribute most to checkworthiness, while visual features like deepfake detection and object recognition offer limited standalone utility.</p>\n\n",
                "matched_terms": [
                    "system",
                    "performance",
                    "ablation",
                    "detection",
                    "transcript",
                    "video"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In future work, we plan to extend <span class=\"ltx_text ltx_font_smallcaps\">ShortCheck</span>&#160;with full fact-checking while testing on more datasets, including multilingual video content from X and YouTube. We will evaluate individual modules in greater depth, improve robustness in noisy or low-resource settings, and integrate feedback from fact-checkers to refine outputs and interpretability. Finally, we aim to explore advanced multimodal fusion techniques beyond rule-based aggregation to boost accuracy and generalization.</p>\n\n",
                "matched_terms": [
                    "full",
                    "accuracy",
                    "individual",
                    "modules",
                    "from",
                    "video"
                ]
            }
        ]
    }
}