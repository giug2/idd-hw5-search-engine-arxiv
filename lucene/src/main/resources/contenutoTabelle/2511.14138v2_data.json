{
    "S3.T2": {
        "source_file": "FxSearcher: gradient-free text-driven audio transformation",
        "caption": "Table 2: Ablation study of guiding prompt on FxSearcher performance across speech and overall quality metrics.",
        "body": "Method\nSpeech\nOverall\n\n\n\nWER ↓\\downarrow\n\n\nPESQ↑\\uparrow\n\n\nFAD↓\\downarrow\n\n\nCLAP↑\\uparrow\n\n\nMOS↑\\uparrow\n\n\nGemini-WR↑\\uparrow\n\n\n\nFxSearcher\n37.5\n1.09\n11.23\n0.456\n3.47\n51.3\n\n\n\nw/o TguideT_{\\text{guide}}\n\n53.0\n1.06\n14.19\n0.482\n2.99\n48.7",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_tt\" rowspan=\"2\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Method</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Speech</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_tt\" colspan=\"4\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Overall</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">WER </span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">PESQ</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">FAD</span><math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8595;</mo><annotation encoding=\"application/x-tex\">\\downarrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">CLAP</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m4\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">MOS</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m5\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Gemini-WR</span><math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m6\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8593;</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">FxSearcher</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">37.5</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">1.09</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">11.23</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.456</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.47</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">51.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">w/o </span><math alttext=\"T_{\\text{guide}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T2.m7\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.900em\">T</mi><mtext mathsize=\"0.900em\">guide</mtext></msub><annotation encoding=\"application/x-tex\">T_{\\text{guide}}</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">53.0</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">1.06</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">14.19</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.482</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.99</span></td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">48.7</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "overall",
            "guiding",
            "quality",
            "ablation",
            "↓downarrow",
            "mos↑uparrow",
            "speech",
            "across",
            "metrics",
            "wer",
            "fad↓downarrow",
            "tguidettextguide",
            "geminiwr↑uparrow",
            "performance",
            "pesq↑uparrow",
            "prompt",
            "study",
            "fxsearcher",
            "clap↑uparrow",
            "method"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Guiding Prompt.</span> Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14138v2#S3.T2\" title=\"Table 2 &#8227; 3.3 Main Result &#8227; 3 EXPERIMENTS &#8227; FxSearcher: gradient-free text-driven audio transformation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the effectiveness of the guiding prompt. Using the guiding prompt significantly improves speech clarity and quality (lower WER, higher PESQ), and prevents excessive deviation from the original audio (lower FAD).\nThe acoustic stability is also enhanced, as detailed in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14138v2#S3.F4\" title=\"Figure 4 &#8227; 3.3 Main Result &#8227; 3 EXPERIMENTS &#8227; FxSearcher: gradient-free text-driven audio transformation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nThe guiding prompt leads to a smoother and more predictable distribution, as evidenced by its lower mean and standard deviation.\nThe raw target score (<math alttext=\"S_{\\text{target}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>S</mi><mtext>target</mtext></msub><annotation encoding=\"application/x-tex\">S_{\\text{target}}</annotation></semantics></math>) is higher without the guiding prompt, as the optimization is focused on a single objective.\nWhile an AI judge shows a slight preference for the guided result, the effect on human listeners is decisive.\nThe MOS evaluation confirms a significant preference for audio generated with the guide, validating its direct impact on perceptual quality.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Achieving diverse and high-quality audio transformations from text prompts remains challenging, as existing methods are fundamentally constrained by their reliance on a limited set of differentiable audio effects. This paper proposes <span class=\"ltx_text ltx_font_bold\">FxSearcher</span>, a novel gradient-free framework that discovers the optimal configuration of audio effects (FX) to transform a source signal according to a text prompt. Our method employs Bayesian Optimization and CLAP-based score function to perform this search efficiently. Furthermore, a guiding prompt is introduced to prevent undesirable artifacts and enhance human preference. To objectively evaluate our method, we propose an AI-based evaluation framework. The results demonstrate that the highest scores achieved by our method on these metrics align closely with human preferences. Demos are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hojoonki.github.io/FxSearcher/\" title=\"\">https://hojoonki.github.io/FxSearcher/</a>.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "guiding",
                    "fxsearcher",
                    "metrics",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these issues, we propose &#160;<span class=\"ltx_text ltx_font_bold\">FxSearcher</span>, a novel framework that employs a gradient-free optimization approach. As depicted in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14138v2#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; FxSearcher: gradient-free text-driven audio transformation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our framework enables the integration of any audio FX regardless of their differentiability. To achieve this, we employ Bayesian Optimization&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jones1998efficient</span>]</cite>, efficiently navigating the parameter space of audio FX. The optimization is guided by a score function based on the CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">elizalde2023clap</span>]</cite>, which measures the semantic similarity between the transformed audio and a target text prompt. Since we find that optimizing for the target text prompt only often produces overly processed audio, we enhance the score function by introducing a guiding prompt strategy. Furthermore, we propose a set of AI-based objective metrics to validate the performance of our method. Combining these metrics with a Mean Opinion Score (MOS) test provides a more comprehensive and robust evaluation of the transformed audio. The main contributions of our paper are as follows:</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "guiding",
                    "fxsearcher",
                    "metrics",
                    "method",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We demonstrate that the guiding prompt strategy acts as an effective regularizer, preventing undesirable artifacts and leading to human-preferable results.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "guiding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our method achieves the highest scores across both our newly proposed AI-based objective metrics and standard human listening tests.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "method",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The goal of our framework is to identify optimal audio effect parameters using natural language descriptions and audio. As illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14138v2#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; FxSearcher: gradient-free text-driven audio transformation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, FxSearcher operates in a closed-loop optimization driven by a Bayesian Optimization&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jones1998efficient</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">frazier2018tutorial</span>]</cite>. In each iteration, the algorithm proposes candidate parameters that are applied to a source audio by a predefined Audio FX Chain&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">SonicBloom2017EffectsChain</span>]</cite>. The resulting audio is then evaluated by a score function utilizing CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">elizalde2023clap</span>]</cite>. This function measures semantic alignment with a target prompt describing the desired attributes, and a guiding prompt is used to steer the optimization toward a more human-preferable sonic direction. The final score provides feedback to the Bayesian Optimization algorithm, guiding its subsequent search for a better solution.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "guiding",
                    "fxsearcher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our score function is designed to guide the optimization toward results that are not only textually relevant but also human-preferable. For this, we compute a holistic score from two complementary prompts. The primary <span class=\"ltx_text ltx_font_bold\">target prompt</span>, <math alttext=\"T_{\\text{target}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\"><semantics><msub><mi>T</mi><mtext>target</mtext></msub><annotation encoding=\"application/x-tex\">T_{\\text{target}}</annotation></semantics></math>, provides the literal description of the desired sound.\nA <span class=\"ltx_text ltx_font_bold\">guiding prompt</span>, <math alttext=\"T_{\\text{guide}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>T</mi><mtext>guide</mtext></msub><annotation encoding=\"application/x-tex\">T_{\\text{guide}}</annotation></semantics></math>, which is a description of common undesirable artifacts (<span class=\"ltx_text ltx_font_italic\">A harsh, distorted, muddy, unclear, oversaturated, unpleasant sound.</span>), is used to refine the search and steer the result toward a more human-preferable quality.\nBased on these prompts, we calculate a <span class=\"ltx_text ltx_font_bold\">target score</span> (<math alttext=\"S_{\\text{target}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m3\" intent=\":literal\"><semantics><msub><mi>S</mi><mtext>target</mtext></msub><annotation encoding=\"application/x-tex\">S_{\\text{target}}</annotation></semantics></math>) and a <span class=\"ltx_text ltx_font_bold\">guiding score</span> (<math alttext=\"S_{\\text{guide}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m4\" intent=\":literal\"><semantics><msub><mi>S</mi><mtext>guide</mtext></msub><annotation encoding=\"application/x-tex\">S_{\\text{guide}}</annotation></semantics></math>) using CLAP as follow:</p>\n\n",
                "matched_terms": [
                    "tguidettextguide",
                    "prompt",
                    "guiding",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:319.7pt;height:98.3pt;vertical-align:-46.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_tt ltx_rowspan ltx_rowspan_2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Domain</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_tt ltx_rowspan ltx_rowspan_2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Method</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_rowspan ltx_rowspan_2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Time (s)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_4\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metric</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">CLAP</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">MOS</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">QWEN</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Gemini-WR</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_t ltx_rowspan ltx_rowspan_3\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Speech</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">LLM2FX</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">71.7</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.232</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">1.77</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">2.32</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">38.2</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Text2FX</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">197.4</span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.527</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">2.28</span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">2.38</span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">51.3</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">FxSearcher</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">71.9</span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.447</span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">3.48</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">2.73</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">61.8</span> / 48.7</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t ltx_rowspan ltx_rowspan_3\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Instrumental</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">LLM2FX</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">71.9</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.341</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">2.70</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">3.14</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">28.4</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Text2FX</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">165.5</span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.561</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">3.19</span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">3.03</span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">33.8</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">FxSearcher</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">71.9</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.464</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">3.46</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">3.18</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">71.6 / 66.2</span></span></span>\n</span></span>\n</span></span></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "method",
                    "fxsearcher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Objective Audio Quality Metrics.</span> We measure Fr&#233;chet Audio Distance (FAD)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kilgour2018fr</span>]</cite> for distributional similarity and Loudness&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">series2011algorithms</span>]</cite> and its standard deviation for stability. For speech, we add Word Error Rate (WER, via Whisper-large-v3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2023robust</span>]</cite>) and Perceptual Evaluation of Speech Quality (PESQ)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">recommendation2001perceptual</span>]</cite> for intelligibility.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "metrics",
                    "quality",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance comparison between FxSearcher and the baseline is summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14138v2#S3.T1\" title=\"Table 1 &#8227; 3.1 Experimental Details &#8227; 3 EXPERIMENTS &#8227; FxSearcher: gradient-free text-driven audio transformation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nIn terms of computational efficiency, FxSearcher shows a significant advantage over Text2FX and operates at a comparable speed to LLM2FX.\nRegarding the objective CLAP score, Text2FX achieves the highest, while LLM2FX yields the lowest.\nThe poor performance of LLM2FX is attributed to its operational mechanism, which does not consider the input audio.\nThe strong performance of Text2FX is anticipated, as its gradient-based method is explicitly designed to maximize this single metric.\nBlack-box approaches like ours, in contrast, explore the parameter space more broadly without such direct guidance&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nesterov2013introductory</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">banker2025gradient</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "method",
                    "fxsearcher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the human evaluation (MOS), FxSearcher achieves a significantly higher score, indicating that its results are more perceptually aligned with user expectations. We further validate this finding through our AI evaluations. FxSearcher achieves the highest scores on both AI-based metrics: the QWEN score and the win rate from Gemini 2.5 Flash. We observe that the correlation between the CLAP score and human preference diminishes at higher score ranges. This combined evidence suggests that while the CLAP score is a useful guide, it serves as an incomplete proxy for perceptual quality. Therefore, by employing the suite of AI-based metrics that we have introduced, it is possible to achieve results that more accurately reflect human perception.</p>\n\n",
                "matched_terms": [
                    "metrics",
                    "quality",
                    "fxsearcher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">FX Chain.</span> Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14138v2#S3.T3\" title=\"Table 3 &#8227; 3.4 Ablation Study &#8227; 3 EXPERIMENTS &#8227; FxSearcher: gradient-free text-driven audio transformation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> investigates how the diversity of the available FX pool impacts performance. We start with a default chain (Equalizer&#8594;Reverb) and progressively expand it by sequentially adding distinct effects. All effects used in this chain are standard, indifferentiable versions.\nThe results show a clear, monotonic improvement as the FX chain grows. With each new effect introduced, both the CLAP and QWEN scores consistently increase. This performance gain is attributed to two main factors. The first is the isolated contribution of each component, which is especially pronounced upon the inclusion of PitchShift. The second factor is the richness of the expanded sonic palette, which provides more degrees of freedom, enabling FxSearcher to discover more effective and nuanced parameter combinations to satisfy the target prompt.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "performance",
                    "fxsearcher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we introduce FxSearcher, a novel gradient-free framework that controls FX to transform audio from text prompts. The strong preference for our method in both human and AI evaluations stems from its core advantage: compatibility with any FX plugin, which unlocks a far greater sonic diversity. We also demonstrated that the guiding prompt strategy improves audio quality and stability. We expect FxSearcher to become an intuitive tool that can democratize complex audio editing for experts and novices alike.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "guiding",
                    "quality",
                    "fxsearcher",
                    "study",
                    "method"
                ]
            }
        ]
    },
    "S3.F4": {
        "source_file": "FxSearcher: gradient-free text-driven audio transformation",
        "caption": "Fig. 4: Effect of the guiding prompt on loudness distribution. The plot (left) shows the density of loudness values (in LUFS), while the table (right) summarizes their mean and standard deviation.",
        "body": "Method\nLoudness\nStd\n\n\nFxSearcher\n-20.70\n8.57\n\n\n\n- w/o TguideT_{\\text{guide}}\n\n-18.88\n9.52",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_minipage ltx_align_middle\" style=\"width:208.1pt;\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Loudness</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">Std</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">FxSearcher</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-20.70</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">8.57</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:80%;\">- w/o </span><math alttext=\"T_{\\text{guide}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.F4.m1\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.800em\">T</mi><mtext mathsize=\"0.800em\">guide</mtext></msub><annotation encoding=\"application/x-tex\">T_{\\text{guide}}</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">-18.88</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:80%;\">9.52</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "guiding",
            "std",
            "right",
            "shows",
            "loudness",
            "standard",
            "left",
            "deviation",
            "mean",
            "distribution",
            "tguidettextguide",
            "values",
            "lufs",
            "effect",
            "prompt",
            "fig",
            "fxsearcher",
            "summarizes",
            "plot",
            "method",
            "while",
            "their",
            "density"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Guiding Prompt.</span> Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14138v2#S3.T2\" title=\"Table 2 &#8227; 3.3 Main Result &#8227; 3 EXPERIMENTS &#8227; FxSearcher: gradient-free text-driven audio transformation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a> shows the effectiveness of the guiding prompt. Using the guiding prompt significantly improves speech clarity and quality (lower WER, higher PESQ), and prevents excessive deviation from the original audio (lower FAD).\nThe acoustic stability is also enhanced, as detailed in Figure&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14138v2#S3.F4\" title=\"Figure 4 &#8227; 3.3 Main Result &#8227; 3 EXPERIMENTS &#8227; FxSearcher: gradient-free text-driven audio transformation\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>.\nThe guiding prompt leads to a smoother and more predictable distribution, as evidenced by its lower mean and standard deviation.\nThe raw target score (<math alttext=\"S_{\\text{target}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.m1\" intent=\":literal\"><semantics><msub><mi>S</mi><mtext>target</mtext></msub><annotation encoding=\"application/x-tex\">S_{\\text{target}}</annotation></semantics></math>) is higher without the guiding prompt, as the optimization is focused on a single objective.\nWhile an AI judge shows a slight preference for the guided result, the effect on human listeners is decisive.\nThe MOS evaluation confirms a significant preference for audio generated with the guide, validating its direct impact on perceptual quality.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Achieving diverse and high-quality audio transformations from text prompts remains challenging, as existing methods are fundamentally constrained by their reliance on a limited set of differentiable audio effects. This paper proposes <span class=\"ltx_text ltx_font_bold\">FxSearcher</span>, a novel gradient-free framework that discovers the optimal configuration of audio effects (FX) to transform a source signal according to a text prompt. Our method employs Bayesian Optimization and CLAP-based score function to perform this search efficiently. Furthermore, a guiding prompt is introduced to prevent undesirable artifacts and enhance human preference. To objectively evaluate our method, we propose an AI-based evaluation framework. The results demonstrate that the highest scores achieved by our method on these metrics align closely with human preferences. Demos are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hojoonki.github.io/FxSearcher/\" title=\"\">https://hojoonki.github.io/FxSearcher/</a>.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "guiding",
                    "fxsearcher",
                    "method",
                    "their"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Recent research on FX control is categorized into two main approaches. The first is a gradient-based method, influenced by Differentiable Digital Signal Processing (DDSP)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">engel2020ddsp</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hayes2024review</span>]</cite>. This allows FX to be integrated into deep learning frameworks, enabling end-to-end training via gradient descent. However, due to the differentiability constraint, these methods cannot fully reproduce the complex dynamics of commercial-grade FX&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">ramirez2021differentiable</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kuznetsov2020differentiable</span>]</cite>, potentially limiting expressive power&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">engel2020ddsp</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hayes2024review</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">hayes2021neural</span>]</cite>. For instance, while Text2FX&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chu2025text2fx</span>]</cite> effectively implements FX using gradient descent optimization, its reliance on differentiability confines it to a limited subset of FX like equalizers and reverbs, resulting in a lack of sonic diversity. The other approach&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">doh2025can</span>]</cite> bypasses the gradient constraint by leveraging the rich knowledge of Large Language Models (LLMs) to generate FX parameters. Yet, these models operate solely on the text modality, disregarding the source audio entirely.</p>\n\n",
                "matched_terms": [
                    "method",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these issues, we propose &#160;<span class=\"ltx_text ltx_font_bold\">FxSearcher</span>, a novel framework that employs a gradient-free optimization approach. As depicted in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14138v2#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; FxSearcher: gradient-free text-driven audio transformation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our framework enables the integration of any audio FX regardless of their differentiability. To achieve this, we employ Bayesian Optimization&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jones1998efficient</span>]</cite>, efficiently navigating the parameter space of audio FX. The optimization is guided by a score function based on the CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">elizalde2023clap</span>]</cite>, which measures the semantic similarity between the transformed audio and a target text prompt. Since we find that optimizing for the target text prompt only often produces overly processed audio, we enhance the score function by introducing a guiding prompt strategy. Furthermore, we propose a set of AI-based objective metrics to validate the performance of our method. Combining these metrics with a Mean Opinion Score (MOS) test provides a more comprehensive and robust evaluation of the transformed audio. The main contributions of our paper are as follows:</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "guiding",
                    "fxsearcher",
                    "mean",
                    "method",
                    "their"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We demonstrate that the guiding prompt strategy acts as an effective regularizer, preventing undesirable artifacts and leading to human-preferable results.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "guiding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our method achieves the highest scores across both our newly proposed AI-based objective metrics and standard human listening tests.</p>\n\n",
                "matched_terms": [
                    "standard",
                    "method"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The goal of our framework is to identify optimal audio effect parameters using natural language descriptions and audio. As illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14138v2#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; FxSearcher: gradient-free text-driven audio transformation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, FxSearcher operates in a closed-loop optimization driven by a Bayesian Optimization&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jones1998efficient</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">frazier2018tutorial</span>]</cite>. In each iteration, the algorithm proposes candidate parameters that are applied to a source audio by a predefined Audio FX Chain&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">SonicBloom2017EffectsChain</span>]</cite>. The resulting audio is then evaluated by a score function utilizing CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">elizalde2023clap</span>]</cite>. This function measures semantic alignment with a target prompt describing the desired attributes, and a guiding prompt is used to steer the optimization toward a more human-preferable sonic direction. The final score provides feedback to the Bayesian Optimization algorithm, guiding its subsequent search for a better solution.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "guiding",
                    "effect",
                    "fxsearcher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our score function is designed to guide the optimization toward results that are not only textually relevant but also human-preferable. For this, we compute a holistic score from two complementary prompts. The primary <span class=\"ltx_text ltx_font_bold\">target prompt</span>, <math alttext=\"T_{\\text{target}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\"><semantics><msub><mi>T</mi><mtext>target</mtext></msub><annotation encoding=\"application/x-tex\">T_{\\text{target}}</annotation></semantics></math>, provides the literal description of the desired sound.\nA <span class=\"ltx_text ltx_font_bold\">guiding prompt</span>, <math alttext=\"T_{\\text{guide}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>T</mi><mtext>guide</mtext></msub><annotation encoding=\"application/x-tex\">T_{\\text{guide}}</annotation></semantics></math>, which is a description of common undesirable artifacts (<span class=\"ltx_text ltx_font_italic\">A harsh, distorted, muddy, unclear, oversaturated, unpleasant sound.</span>), is used to refine the search and steer the result toward a more human-preferable quality.\nBased on these prompts, we calculate a <span class=\"ltx_text ltx_font_bold\">target score</span> (<math alttext=\"S_{\\text{target}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m3\" intent=\":literal\"><semantics><msub><mi>S</mi><mtext>target</mtext></msub><annotation encoding=\"application/x-tex\">S_{\\text{target}}</annotation></semantics></math>) and a <span class=\"ltx_text ltx_font_bold\">guiding score</span> (<math alttext=\"S_{\\text{guide}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m4\" intent=\":literal\"><semantics><msub><mi>S</mi><mtext>guide</mtext></msub><annotation encoding=\"application/x-tex\">S_{\\text{guide}}</annotation></semantics></math>) using CLAP as follow:</p>\n\n",
                "matched_terms": [
                    "tguidettextguide",
                    "prompt",
                    "guiding"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\text{sim}(\\cdot,\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><mtext>sim</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo rspace=\"0em\">,</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{sim}(\\cdot,\\cdot)</annotation></semantics></math> denotes the cosine similarity, and <math alttext=\"CLAP_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m6\" intent=\":literal\"><semantics><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>P</mi><mi>A</mi></msub></mrow><annotation encoding=\"application/x-tex\">CLAP_{A}</annotation></semantics></math> and <math alttext=\"CLAP_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m7\" intent=\":literal\"><semantics><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>P</mi><mi>T</mi></msub></mrow><annotation encoding=\"application/x-tex\">CLAP_{T}</annotation></semantics></math> represent the audio and text CLAP encoders, respectively.\nTo simultaneously increase similarity to the target description while decreasing similarity to undesirable characteristics, we aim to maximize the difference between the target and guiding scores. Therefore, the final objective score <math alttext=\"S_{\\text{final}}^{(t)}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m8\" intent=\":literal\"><semantics><msubsup><mi>S</mi><mtext>final</mtext><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">S_{\\text{final}}^{(t)}</annotation></semantics></math> is formulated as:</p>\n\n",
                "matched_terms": [
                    "guiding",
                    "while"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">FX Chain.</span> The FX chain is configured to apply a total of six effectors sequentially, using Spotify&#8217;s Pedalboard &#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sobot_peter_2023_7817838</span>]</cite> with an order determined by typical signal flow in sound engineering&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">SonicBloom2017EffectsChain</span>]</cite>: Equalizer <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Distortion <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> BitCrush <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> PitchShift <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Delay <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Reverb. The Equalizer first shapes the tone, followed by Distortion for adding texture. BitCrush and PitchShift then introduce more dramatic changes, while Delay and Reverb finalize the sound by imparting a sense of space. The audio FX chain is controlled by 26 parameters, which are divided into two main types. The first group consists of 22 parameters dedicated to configuring the effect settings: 15 for the Equalizer, 3 for Reverb, and 1 for each of the four other effects. The second group contains the 4 remaining parameters, which control the activation (on/off) of those four optional effects.</p>\n\n",
                "matched_terms": [
                    "while",
                    "effect"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:319.7pt;height:98.3pt;vertical-align:-46.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_tt ltx_rowspan ltx_rowspan_2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Domain</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_tt ltx_rowspan ltx_rowspan_2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Method</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_rowspan ltx_rowspan_2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Time (s)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_4\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metric</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">CLAP</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">MOS</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">QWEN</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Gemini-WR</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_t ltx_rowspan ltx_rowspan_3\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Speech</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">LLM2FX</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">71.7</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.232</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">1.77</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">2.32</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">38.2</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Text2FX</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">197.4</span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.527</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">2.28</span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">2.38</span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">51.3</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">FxSearcher</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">71.9</span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.447</span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">3.48</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">2.73</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">61.8</span> / 48.7</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t ltx_rowspan ltx_rowspan_3\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Instrumental</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">LLM2FX</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">71.9</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.341</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">2.70</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">3.14</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">28.4</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Text2FX</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">165.5</span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.561</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">3.19</span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">3.03</span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">33.8</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">FxSearcher</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">71.9</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.464</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">3.46</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">3.18</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">71.6 / 66.2</span></span></span>\n</span></span>\n</span></span></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</p>\n\n",
                "matched_terms": [
                    "method",
                    "fxsearcher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Objective Audio Quality Metrics.</span> We measure Fr&#233;chet Audio Distance (FAD)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">kilgour2018fr</span>]</cite> for distributional similarity and Loudness&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">series2011algorithms</span>]</cite> and its standard deviation for stability. For speech, we add Word Error Rate (WER, via Whisper-large-v3&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">radford2023robust</span>]</cite>) and Perceptual Evaluation of Speech Quality (PESQ)&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">recommendation2001perceptual</span>]</cite> for intelligibility.</p>\n\n",
                "matched_terms": [
                    "standard",
                    "deviation",
                    "loudness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance comparison between FxSearcher and the baseline is summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14138v2#S3.T1\" title=\"Table 1 &#8227; 3.1 Experimental Details &#8227; 3 EXPERIMENTS &#8227; FxSearcher: gradient-free text-driven audio transformation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nIn terms of computational efficiency, FxSearcher shows a significant advantage over Text2FX and operates at a comparable speed to LLM2FX.\nRegarding the objective CLAP score, Text2FX achieves the highest, while LLM2FX yields the lowest.\nThe poor performance of LLM2FX is attributed to its operational mechanism, which does not consider the input audio.\nThe strong performance of Text2FX is anticipated, as its gradient-based method is explicitly designed to maximize this single metric.\nBlack-box approaches like ours, in contrast, explore the parameter space more broadly without such direct guidance&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nesterov2013introductory</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">banker2025gradient</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "method",
                    "while",
                    "shows",
                    "fxsearcher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the human evaluation (MOS), FxSearcher achieves a significantly higher score, indicating that its results are more perceptually aligned with user expectations. We further validate this finding through our AI evaluations. FxSearcher achieves the highest scores on both AI-based metrics: the QWEN score and the win rate from Gemini 2.5 Flash. We observe that the correlation between the CLAP score and human preference diminishes at higher score ranges. This combined evidence suggests that while the CLAP score is a useful guide, it serves as an incomplete proxy for perceptual quality. Therefore, by employing the suite of AI-based metrics that we have introduced, it is possible to achieve results that more accurately reflect human perception.</p>\n\n",
                "matched_terms": [
                    "while",
                    "fxsearcher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">FX Chain.</span> Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14138v2#S3.T3\" title=\"Table 3 &#8227; 3.4 Ablation Study &#8227; 3 EXPERIMENTS &#8227; FxSearcher: gradient-free text-driven audio transformation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> investigates how the diversity of the available FX pool impacts performance. We start with a default chain (Equalizer&#8594;Reverb) and progressively expand it by sequentially adding distinct effects. All effects used in this chain are standard, indifferentiable versions.\nThe results show a clear, monotonic improvement as the FX chain grows. With each new effect introduced, both the CLAP and QWEN scores consistently increase. This performance gain is attributed to two main factors. The first is the isolated contribution of each component, which is especially pronounced upon the inclusion of PitchShift. The second factor is the richness of the expanded sonic palette, which provides more degrees of freedom, enabling FxSearcher to discover more effective and nuanced parameter combinations to satisfy the target prompt.</p>\n\n",
                "matched_terms": [
                    "standard",
                    "effect",
                    "prompt",
                    "fxsearcher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we introduce FxSearcher, a novel gradient-free framework that controls FX to transform audio from text prompts. The strong preference for our method in both human and AI evaluations stems from its core advantage: compatibility with any FX plugin, which unlocks a far greater sonic diversity. We also demonstrated that the guiding prompt strategy improves audio quality and stability. We expect FxSearcher to become an intuitive tool that can democratize complex audio editing for experts and novices alike.</p>\n\n",
                "matched_terms": [
                    "method",
                    "prompt",
                    "guiding",
                    "fxsearcher"
                ]
            }
        ]
    },
    "S3.T3": {
        "source_file": "FxSearcher: gradient-free text-driven audio transformation",
        "caption": "Table 3: Results of the ablation study on the FX chain. The final configuration(+Delay) represents our full FxSearcher framework.",
        "body": "FX Chain\nCLAP\nQWEN\n\n\nSpeech\nInstrumental\nSpeech\nInstrumental\n\n\n\nEqualizer →\\rightarrow Reverb\n\n0.389\n0.428\n2.32\n3.11\n\n\n+ Distortion\n0.397\n0.439\n2.31\n3.14\n\n\n+ BitCrush\n0.409\n0.437\n2.45\n3.16\n\n\n+ PitchShift\n0.445\n0.457\n2.62\n3.15\n\n\n+ Delay(Ours)\n0.447\n0.464\n2.73\n3.18",
        "html_code": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">FX Chain</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">CLAP</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">QWEN</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Speech</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Instrumental</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Speech</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Instrumental</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">Equalizer </span><math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.T3.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\" stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> Reverb</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.389</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.428</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.11</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">+ Distortion</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.397</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.439</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.31</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.14</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">+ BitCrush</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.409</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.437</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.45</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.16</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">+ PitchShift</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.445</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">0.457</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">2.62</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">3.15</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">+ Delay(Ours)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.447</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">0.464</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">2.73</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">3.18</span></td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "reverb",
            "configurationdelay",
            "ablation",
            "distortion",
            "→rightarrow",
            "our",
            "speech",
            "instrumental",
            "represents",
            "clap",
            "equalizer",
            "results",
            "delayours",
            "qwen",
            "pitchshift",
            "bitcrush",
            "full",
            "final",
            "study",
            "chain",
            "fxsearcher",
            "framework"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">FX Chain.</span> Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14138v2#S3.T3\" title=\"Table 3 &#8227; 3.4 Ablation Study &#8227; 3 EXPERIMENTS &#8227; FxSearcher: gradient-free text-driven audio transformation\"><span class=\"ltx_text ltx_ref_tag\">3</span></a> investigates how the diversity of the available FX pool impacts performance. We start with a default chain (Equalizer&#8594;Reverb) and progressively expand it by sequentially adding distinct effects. All effects used in this chain are standard, indifferentiable versions.\nThe results show a clear, monotonic improvement as the FX chain grows. With each new effect introduced, both the CLAP and QWEN scores consistently increase. This performance gain is attributed to two main factors. The first is the isolated contribution of each component, which is especially pronounced upon the inclusion of PitchShift. The second factor is the richness of the expanded sonic palette, which provides more degrees of freedom, enabling FxSearcher to discover more effective and nuanced parameter combinations to satisfy the target prompt.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Achieving diverse and high-quality audio transformations from text prompts remains challenging, as existing methods are fundamentally constrained by their reliance on a limited set of differentiable audio effects. This paper proposes <span class=\"ltx_text ltx_font_bold\">FxSearcher</span>, a novel gradient-free framework that discovers the optimal configuration of audio effects (FX) to transform a source signal according to a text prompt. Our method employs Bayesian Optimization and CLAP-based score function to perform this search efficiently. Furthermore, a guiding prompt is introduced to prevent undesirable artifacts and enhance human preference. To objectively evaluate our method, we propose an AI-based evaluation framework. The results demonstrate that the highest scores achieved by our method on these metrics align closely with human preferences. Demos are available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hojoonki.github.io/FxSearcher/\" title=\"\">https://hojoonki.github.io/FxSearcher/</a>.</p>\n\n",
                "matched_terms": [
                    "framework",
                    "results",
                    "our",
                    "fxsearcher"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To address these issues, we propose &#160;<span class=\"ltx_text ltx_font_bold\">FxSearcher</span>, a novel framework that employs a gradient-free optimization approach. As depicted in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14138v2#S1.F1\" title=\"Figure 1 &#8227; 1 Introduction &#8227; FxSearcher: gradient-free text-driven audio transformation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>, our framework enables the integration of any audio FX regardless of their differentiability. To achieve this, we employ Bayesian Optimization&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jones1998efficient</span>]</cite>, efficiently navigating the parameter space of audio FX. The optimization is guided by a score function based on the CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">elizalde2023clap</span>]</cite>, which measures the semantic similarity between the transformed audio and a target text prompt. Since we find that optimizing for the target text prompt only often produces overly processed audio, we enhance the score function by introducing a guiding prompt strategy. Furthermore, we propose a set of AI-based objective metrics to validate the performance of our method. Combining these metrics with a Mean Opinion Score (MOS) test provides a more comprehensive and robust evaluation of the transformed audio. The main contributions of our paper are as follows:</p>\n\n",
                "matched_terms": [
                    "framework",
                    "fxsearcher",
                    "our",
                    "clap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To the best of our knowledge, we propose the first gradient-free optimization framework for text-driven audio transformation.</p>\n\n",
                "matched_terms": [
                    "framework",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The goal of our framework is to identify optimal audio effect parameters using natural language descriptions and audio. As illustrated in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14138v2#S1.F2\" title=\"Figure 2 &#8227; 1 Introduction &#8227; FxSearcher: gradient-free text-driven audio transformation\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, FxSearcher operates in a closed-loop optimization driven by a Bayesian Optimization&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">jones1998efficient</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">frazier2018tutorial</span>]</cite>. In each iteration, the algorithm proposes candidate parameters that are applied to a source audio by a predefined Audio FX Chain&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">SonicBloom2017EffectsChain</span>]</cite>. The resulting audio is then evaluated by a score function utilizing CLAP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">elizalde2023clap</span>]</cite>. This function measures semantic alignment with a target prompt describing the desired attributes, and a guiding prompt is used to steer the optimization toward a more human-preferable sonic direction. The final score provides feedback to the Bayesian Optimization algorithm, guiding its subsequent search for a better solution.</p>\n\n",
                "matched_terms": [
                    "final",
                    "fxsearcher",
                    "chain",
                    "framework",
                    "clap",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a source audio <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m1\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> and a target text prompt <math alttext=\"T_{\\text{target}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>T</mi><mtext>target</mtext></msub><annotation encoding=\"application/x-tex\">T_{\\text{target}}</annotation></semantics></math>, our primary objective is to find an optimal set of FX parameters, <math alttext=\"X^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m3\" intent=\":literal\"><semantics><msup><mi>X</mi><mo>&#8727;</mo></msup><annotation encoding=\"application/x-tex\">X^{*}</annotation></semantics></math>, and the corresponding transformed audio, <math alttext=\"A_{\\text{FX}}^{*}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m4\" intent=\":literal\"><semantics><msubsup><mi>A</mi><mtext>FX</mtext><mo>&#8727;</mo></msubsup><annotation encoding=\"application/x-tex\">A_{\\text{FX}}^{*}</annotation></semantics></math>.\nOur pipeline iteratively optimizes the parameters of an audio FX chain. This sequential process can be formally represented as a single function, <math alttext=\"\\operatorname{FX}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m5\" intent=\":literal\"><semantics><mrow><mi>FX</mi><mo>&#8289;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\operatorname{FX}(\\cdot)</annotation></semantics></math>. At iteration <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m6\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, the chain takes the source audio <math alttext=\"A\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m7\" intent=\":literal\"><semantics><mi>A</mi><annotation encoding=\"application/x-tex\">A</annotation></semantics></math> and a set of parameters <math alttext=\"X_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m8\" intent=\":literal\"><semantics><msub><mi>X</mi><mi>t</mi></msub><annotation encoding=\"application/x-tex\">X_{t}</annotation></semantics></math> to generate the transformed audio <math alttext=\"A_{\\text{FX}}^{(t)}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS2.p1.m9\" intent=\":literal\"><semantics><msubsup><mi>A</mi><mtext>FX</mtext><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">A_{\\text{FX}}^{(t)}</annotation></semantics></math> as</p>\n\n",
                "matched_terms": [
                    "chain",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our score function is designed to guide the optimization toward results that are not only textually relevant but also human-preferable. For this, we compute a holistic score from two complementary prompts. The primary <span class=\"ltx_text ltx_font_bold\">target prompt</span>, <math alttext=\"T_{\\text{target}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m1\" intent=\":literal\"><semantics><msub><mi>T</mi><mtext>target</mtext></msub><annotation encoding=\"application/x-tex\">T_{\\text{target}}</annotation></semantics></math>, provides the literal description of the desired sound.\nA <span class=\"ltx_text ltx_font_bold\">guiding prompt</span>, <math alttext=\"T_{\\text{guide}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m2\" intent=\":literal\"><semantics><msub><mi>T</mi><mtext>guide</mtext></msub><annotation encoding=\"application/x-tex\">T_{\\text{guide}}</annotation></semantics></math>, which is a description of common undesirable artifacts (<span class=\"ltx_text ltx_font_italic\">A harsh, distorted, muddy, unclear, oversaturated, unpleasant sound.</span>), is used to refine the search and steer the result toward a more human-preferable quality.\nBased on these prompts, we calculate a <span class=\"ltx_text ltx_font_bold\">target score</span> (<math alttext=\"S_{\\text{target}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m3\" intent=\":literal\"><semantics><msub><mi>S</mi><mtext>target</mtext></msub><annotation encoding=\"application/x-tex\">S_{\\text{target}}</annotation></semantics></math>) and a <span class=\"ltx_text ltx_font_bold\">guiding score</span> (<math alttext=\"S_{\\text{guide}}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m4\" intent=\":literal\"><semantics><msub><mi>S</mi><mtext>guide</mtext></msub><annotation encoding=\"application/x-tex\">S_{\\text{guide}}</annotation></semantics></math>) using CLAP as follow:</p>\n\n",
                "matched_terms": [
                    "results",
                    "our",
                    "clap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"\\text{sim}(\\cdot,\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m5\" intent=\":literal\"><semantics><mrow><mtext>sim</mtext><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo rspace=\"0em\">,</mo><mo lspace=\"0em\" rspace=\"0em\">&#8901;</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{sim}(\\cdot,\\cdot)</annotation></semantics></math> denotes the cosine similarity, and <math alttext=\"CLAP_{A}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m6\" intent=\":literal\"><semantics><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>P</mi><mi>A</mi></msub></mrow><annotation encoding=\"application/x-tex\">CLAP_{A}</annotation></semantics></math> and <math alttext=\"CLAP_{T}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m7\" intent=\":literal\"><semantics><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>A</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><msub><mi>P</mi><mi>T</mi></msub></mrow><annotation encoding=\"application/x-tex\">CLAP_{T}</annotation></semantics></math> represent the audio and text CLAP encoders, respectively.\nTo simultaneously increase similarity to the target description while decreasing similarity to undesirable characteristics, we aim to maximize the difference between the target and guiding scores. Therefore, the final objective score <math alttext=\"S_{\\text{final}}^{(t)}\" class=\"ltx_Math\" display=\"inline\" id=\"S2.SS3.p1.m8\" intent=\":literal\"><semantics><msubsup><mi>S</mi><mtext>final</mtext><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><annotation encoding=\"application/x-tex\">S_{\\text{final}}^{(t)}</annotation></semantics></math> is formulated as:</p>\n\n",
                "matched_terms": [
                    "final",
                    "clap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">FX Chain.</span> The FX chain is configured to apply a total of six effectors sequentially, using Spotify&#8217;s Pedalboard &#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">sobot_peter_2023_7817838</span>]</cite> with an order determined by typical signal flow in sound engineering&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">SonicBloom2017EffectsChain</span>]</cite>: Equalizer <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Distortion <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> BitCrush <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> PitchShift <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Delay <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> Reverb. The Equalizer first shapes the tone, followed by Distortion for adding texture. BitCrush and PitchShift then introduce more dramatic changes, while Delay and Reverb finalize the sound by imparting a sense of space. The audio FX chain is controlled by 26 parameters, which are divided into two main types. The first group consists of 22 parameters dedicated to configuring the effect settings: 15 for the Equalizer, 3 for Reverb, and 1 for each of the four other effects. The second group contains the 4 remaining parameters, which control the activation (on/off) of those four optional effects.</p>\n\n",
                "matched_terms": [
                    "reverb",
                    "bitcrush",
                    "chain",
                    "distortion",
                    "→rightarrow",
                    "equalizer",
                    "pitchshift"
                ]
            },
            {
                "html": "<p class=\"ltx_p ltx_align_center\">\n  <span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:319.7pt;height:98.3pt;vertical-align:-46.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(0.0pt,0.0pt) scale(1,1) ;\">\n<span class=\"ltx_p\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_tt ltx_rowspan ltx_rowspan_2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Domain</span></span>\n<span class=\"ltx_td ltx_align_left ltx_border_tt ltx_rowspan ltx_rowspan_2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Method</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_rowspan ltx_rowspan_2\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Time (s)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_4\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Evaluation Metric</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">CLAP</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">MOS</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">QWEN</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">Gemini-WR</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_t ltx_rowspan ltx_rowspan_3\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Speech</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">LLM2FX</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">71.7</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.232</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">1.77</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">2.32</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">38.2</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Text2FX</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">197.4</span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.527</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">2.28</span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">2.38</span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">51.3</span></span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">FxSearcher</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">71.9</span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.447</span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">3.48</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">2.73</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">61.8</span> / 48.7</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t ltx_rowspan ltx_rowspan_3\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Instrumental</span>\n<span class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">LLM2FX</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">71.9</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.341</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">2.70</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">3.14</span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">28.4</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Text2FX</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">165.5</span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">0.561</span></span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">3.19</span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">3.03</span>\n<span class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">33.8</span></span>\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">FxSearcher</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">71.9</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.464</span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">3.46</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">3.18</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span class=\"ltx_text ltx_font_bold\">71.6 / 66.2</span></span></span>\n</span></span>\n</span></span></span>\n  <span class=\"ltx_text\" style=\"font-size:90%;\"/>\n</p>\n\n",
                "matched_terms": [
                    "speech",
                    "fxsearcher",
                    "instrumental",
                    "clap",
                    "qwen"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Dataset.</span> We construct a dataset consisting of source audio and target prompts. Audio samples for the speech domain and instrumental domain are drawn from LibriSpeech&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">panayotov2015librispeech</span>]</cite> and a public dataset<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.kaggle.com/datasets/abdulvahap/music-instrunment-sounds-for-classification\" title=\"\">https://www.kaggle.com/datasets/abdulvahap/music-instrunment-sounds-for-classification</a></span></span></span>, respectively. We prepare 150 text prompts for the evaluation. 120 prompts are automatically generated using GPT-5&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://openai.com/index/introducing-gpt-5/\" title=\"\">https://openai.com/index/introducing-gpt-5/</a></span></span></span>-60 for the speech domain and 60 for the instrumental domain, while the remaining 30 are designed by the researchers.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "instrumental"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Configuration.</span>\nFor our experiments, the maximum number of search iterations is set to 100, and the early stopping patience is set to 30. For the comparison, Text2FX&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">chu2025text2fx</span>]</cite> and LLM2FX&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">doh2025can</span>]</cite> are reproduced in our environment. Also, we utilize a pretrained CLAP model&#160;<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/laion/clap-htsat-unfused\" title=\"\">https://huggingface.co/laion/clap-htsat-unfused</a></span></span></span> for optimization and evaluation. All experiments are conducted on a single NVIDIA RTX 3090 GPU.</p>\n\n",
                "matched_terms": [
                    "our",
                    "clap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance comparison between FxSearcher and the baseline is summarized in Table&#160;<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.14138v2#S3.T1\" title=\"Table 1 &#8227; 3.1 Experimental Details &#8227; 3 EXPERIMENTS &#8227; FxSearcher: gradient-free text-driven audio transformation\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>.\nIn terms of computational efficiency, FxSearcher shows a significant advantage over Text2FX and operates at a comparable speed to LLM2FX.\nRegarding the objective CLAP score, Text2FX achieves the highest, while LLM2FX yields the lowest.\nThe poor performance of LLM2FX is attributed to its operational mechanism, which does not consider the input audio.\nThe strong performance of Text2FX is anticipated, as its gradient-based method is explicitly designed to maximize this single metric.\nBlack-box approaches like ours, in contrast, explore the parameter space more broadly without such direct guidance&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">nesterov2013introductory</span>, <span class=\"ltx_ref ltx_missing_citation ltx_ref_self\">banker2025gradient</span>]</cite>.</p>\n\n",
                "matched_terms": [
                    "fxsearcher",
                    "clap"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the human evaluation (MOS), FxSearcher achieves a significantly higher score, indicating that its results are more perceptually aligned with user expectations. We further validate this finding through our AI evaluations. FxSearcher achieves the highest scores on both AI-based metrics: the QWEN score and the win rate from Gemini 2.5 Flash. We observe that the correlation between the CLAP score and human preference diminishes at higher score ranges. This combined evidence suggests that while the CLAP score is a useful guide, it serves as an incomplete proxy for perceptual quality. Therefore, by employing the suite of AI-based metrics that we have introduced, it is possible to achieve results that more accurately reflect human perception.</p>\n\n",
                "matched_terms": [
                    "fxsearcher",
                    "clap",
                    "qwen",
                    "results",
                    "our"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we introduce FxSearcher, a novel gradient-free framework that controls FX to transform audio from text prompts. The strong preference for our method in both human and AI evaluations stems from its core advantage: compatibility with any FX plugin, which unlocks a far greater sonic diversity. We also demonstrated that the guiding prompt strategy improves audio quality and stability. We expect FxSearcher to become an intuitive tool that can democratize complex audio editing for experts and novices alike.</p>\n\n",
                "matched_terms": [
                    "framework",
                    "fxsearcher",
                    "our",
                    "study"
                ]
            }
        ]
    }
}