{
    "S4.T1": {
        "source_file": "SpeechJudge: Towards Human-Level Judgment for Speech Naturalness",
        "caption": "Table 1: Protocols of different models for naturalness judgment.",
        "body": "Prompts of AudioLLMs\n\n\n\n\n\n•\n\nWe are comparing the naturalness of two models’ outputs. The models need to speak the target text accurately and naturally.\n\n•\n\nTarget text: {tt}, Output A: {a1a_{1}}, Output B: {a2a_{2}}. Analyze the two outputs above, and score them with number from 1 to 10. Note:\n\n∘\\circ\n\nPlease evaluate the naturalness of both audio outputs based on the following criteria: Prosody and Intonation, Pacing and Rhythm, Articulation and Clarity, and Overall Naturalness.\n\n∘\\circ\n\nAfter conducting a detailed analysis of each criterion, using the following output template to highlight your conclusion: Output A: X, Output B: X.",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\">Prompts of AudioLLMs</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_border_bb\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_itemize ltx_framed ltx_framed_rectangle\" id=\"S4.I1\" style=\"--ltx-border-color:#000000;padding:3pt;border-width:0.4pt;\">\n<span class=\"ltx_item\" id=\"S4.I1.i1\" style=\"list-style-type:none;\"><span class=\"ltx_tag ltx_tag_item\">&#8226;</span>\n<span class=\"ltx_para\" id=\"S4.I1.i1.p1\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">We are comparing the naturalness of two models&#8217; outputs. The models need to speak the target text accurately and naturally.</span></span>\n</span></span>\n<span class=\"ltx_item\" id=\"S4.I1.i2\" style=\"list-style-type:none;\"><span class=\"ltx_tag ltx_tag_item\">&#8226;</span>\n<span class=\"ltx_para\" id=\"S4.I1.i2.p1\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;\">Target text: {</span><math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.p1.m1\" intent=\":literal\"><semantics><mi mathsize=\"0.800em\">t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\">}, Output A: {</span><math alttext=\"a_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.p1.m2\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.800em\">a</mi><mn mathsize=\"0.800em\">1</mn></msub><annotation encoding=\"application/x-tex\">a_{1}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\">}, Output B: {</span><math alttext=\"a_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.p1.m3\" intent=\":literal\"><semantics><msub><mi mathsize=\"0.800em\">a</mi><mn mathsize=\"0.800em\">2</mn></msub><annotation encoding=\"application/x-tex\">a_{2}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:80%;\">}. Analyze the two outputs above, and score them with number from 1 to 10. Note:</span></span>\n<span class=\"ltx_itemize\" id=\"S4.I1.i2.I1\">\n<span class=\"ltx_item\" id=\"S4.I1.i2.I1.i1\" style=\"list-style-type:none;\"><span class=\"ltx_tag ltx_tag_item\"><math alttext=\"\\circ\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.I1.i1.m1\" intent=\":literal\"><semantics><mo>&#8728;</mo><annotation encoding=\"application/x-tex\">\\circ</annotation></semantics></math></span>\n<span class=\"ltx_para\" id=\"S4.I1.i2.I1.i1.p1\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;--ltx-fg-color:#2954AD;\">Please evaluate the naturalness of both audio outputs based on the following criteria: Prosody and Intonation, Pacing and Rhythm, Articulation and Clarity, and Overall Naturalness.</span><span class=\"ltx_text\" style=\"font-size:80%;\"/></span>\n</span></span>\n<span class=\"ltx_item\" id=\"S4.I1.i2.I1.i2\" style=\"list-style-type:none;\"><span class=\"ltx_tag ltx_tag_item\"><math alttext=\"\\circ\" class=\"ltx_Math\" display=\"inline\" id=\"S4.I1.i2.I1.i2.m1\" intent=\":literal\"><semantics><mo>&#8728;</mo><annotation encoding=\"application/x-tex\">\\circ</annotation></semantics></math></span>\n<span class=\"ltx_para\" id=\"S4.I1.i2.I1.i2.p1\">\n<span class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:80%;--ltx-fg-color:#2954AD;\">After conducting a detailed analysis of each criterion,</span><span class=\"ltx_text\" style=\"font-size:80%;\"> using the following output template to highlight your conclusion: Output A: X, Output B: X.</span></span>\n</span></span>\n</span>\n</span></span>\n</span>\n</span>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "protocols",
            "naturalness",
            "overall",
            "evaluate",
            "pacing",
            "articulation",
            "models’",
            "text",
            "based",
            "output",
            "clarity",
            "speak",
            "outputs",
            "target",
            "please",
            "score",
            "them",
            "intonation",
            "each",
            "analyze",
            "template",
            "from",
            "analysis",
            "both",
            "audiollms",
            "need",
            "following",
            "a2a2",
            "criterion",
            "prosody",
            "above",
            "prompts",
            "number",
            "comparing",
            "rhythm",
            "conducting",
            "detailed",
            "your",
            "models",
            "judgment",
            "different",
            "naturally",
            "two",
            "accurately",
            "criteria",
            "highlight",
            "conclusion",
            "note",
            "after",
            "a1a1",
            "audio",
            "∘circ"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:80%;\">We use the protocols of Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T1\" style=\"font-size:80%;\" title=\"Table 1 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:80%;\"> to establish judgment rules for different models. The results of AudioLLMs here are obtained using the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">plain</span>\n  <span class=\"ltx_text\" style=\"font-size:80%;\"> prompt of Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T1\" style=\"font-size:80%;\" title=\"Table 1 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:80%;\">.</span>\n</p>\n\n",
            "<p class=\"ltx_p\">We test the naturalness judgment capability of various models based on SpeechJudge-Eval. We consider four different categories of models, whose evaluation protocols are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T1\" title=\"Table 1 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>:</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AudioLLMs</span>, which are employed to test their speech naturalness understanding capabilities in a <span class=\"ltx_text ltx_font_italic\">zero-shot</span> manner<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>We assume that the adopted AudioLLMs have not been directly trained on the speech naturalness judgment task. Their performance on this benchmark is therefore considered a <span class=\"ltx_text ltx_font_italic\">zero-shot</span> capability.</span></span></span>. We include the open-source Phi-4-Multimodal <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib1\" title=\"\">1</a>]</cite>, Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>]</cite>, Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib28\" title=\"\">28</a>]</cite>, Gemma-3n <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib26\" title=\"\">26</a>]</cite>, Voxtral <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib32\" title=\"\">32</a>]</cite>, MiDashengLM <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib12\" title=\"\">12</a>]</cite>, Mimo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib53\" title=\"\">53</a>]</cite>, and the closed-source Gemini-2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>]</cite> and GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib22\" title=\"\">22</a>]</cite>. We use the <span class=\"ltx_text ltx_font_italic\">plain</span> prompt of Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T1\" title=\"Table 1 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> to instruct the model to pairwise score the naturalness of two audios. We use their grading to determine the naturalness preference.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SFT Stage</span>&#8195;We consider SFT as a &#8220;cold start&#8221; stage to improve the Qwen2.5-Omni&#8217;s instruction-following, reasoning, and speech naturalness understanding capabilities. We select Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>]</cite>&#8212;one of the leading closed-source models on SpeechJudge-Eval (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T2\" title=\"Table 2 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>)&#8212;to serve as a teacher model, and instruct it to generate the CoT data. Specifically, for each sample <math alttext=\"d=(t,a_{1},a_{2},y_{\\mathcal{H}})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo>,</mo><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">d=(t,a_{1},a_{2},y_{\\mathcal{H}})</annotation></semantics></math> from SpeechJudge-Data, we use the CoT prompt from Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T1\" title=\"Table 1 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (denoted as <math alttext=\"\\mathbf{I}_{CoT}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><msub><mi>&#119816;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{I}_{CoT}</annotation></semantics></math>) to instruct Gemini-2.5-Flash to generate a rationale-based output (denoted as <math alttext=\"\\mathbf{O}_{teacher}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m3\" intent=\":literal\"><semantics><msub><mi>&#119822;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{O}_{teacher}</annotation></semantics></math>). We then extract the preference judgment (<math alttext=\"y_{\\mathcal{M}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m4\" intent=\":literal\"><semantics><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi></msub><annotation encoding=\"application/x-tex\">y_{\\mathcal{M}}</annotation></semantics></math>) from this output. For samples where Gemini-2.5-Flash&#8217;s preference is consistent with the human (i.e., <math alttext=\"y_{\\mathcal{M}}=y_{\\mathcal{H}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi></msub><mo>=</mo><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></msub></mrow><annotation encoding=\"application/x-tex\">y_{\\mathcal{M}}=y_{\\mathcal{H}}</annotation></semantics></math>), we concatenate the CoT prompt and the model&#8217;s output, <math alttext=\"[\\mathbf{I}_{CoT},\\mathbf{O}_{teacher}]\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119816;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><mo>,</mo><msub><mi>&#119822;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[\\mathbf{I}_{CoT},\\mathbf{O}_{teacher}]</annotation></semantics></math>, to create a data point for our SFT dataset. Conversely, we consider the sample <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m7\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> a challenging case and reserve the prompt <math alttext=\"\\mathbf{I}_{CoT}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m8\" intent=\":literal\"><semantics><msub><mi>&#119816;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{I}_{CoT}</annotation></semantics></math> for the second-stage RL dataset. During the SFT stage, for each training sample <math alttext=\"[\\mathbf{I}_{CoT},\\mathbf{O}_{teacher}]\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m9\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119816;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><mo>,</mo><msub><mi>&#119822;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[\\mathbf{I}_{CoT},\\mathbf{O}_{teacher}]</annotation></semantics></math>, we perform the next token prediction only on the segment <math alttext=\"\\mathbf{O}_{teacher}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m10\" intent=\":literal\"><semantics><msub><mi>&#119822;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{O}_{teacher}</annotation></semantics></math>.</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AudioLLMs</span>: We use the plain prompt of Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T1\" title=\"Table 1 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> to instruct AudioLLMs to pairwise score the naturalness of two audios. For the closed-source models, we use the official API released by Google<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span><a class=\"ltx_ref ltx_href\" href=\"https://ai.google.dev/gemini-api/docs/models\" title=\"\">https://ai.google.dev/gemini-api/docs/models</a></span></span></span> for Gemini and OpenAI<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span><a class=\"ltx_ref ltx_href\" href=\"https://platform.openai.com/docs/models\" title=\"\">https://platform.openai.com/docs/models</a></span></span></span> for GPT. We use the model variants <span class=\"ltx_text ltx_font_typewriter\">gemini-2.5-flash</span>, <span class=\"ltx_text ltx_font_typewriter\">gemini-2.5-pro</span>, <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini-audio-preview-2024-12-17</span>, and <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-audio-preview-2025-06-03</span> for Gemini-2.5-Flash, Gemini-2.5-Pro, GPT-4o mini Audio, and GPT-4o Audio.</p>\n\n",
            "<p class=\"ltx_p\">In this study, we investigate whether using the CoT from Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T1\" title=\"Table 1 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> helps AudioLLMs better judge speech naturalness. Interestingly, we find that some closed-source AudioLLMs, such as Gemini-2.5-Flash, improve their performance on SpeechJudge-Eval through this thinking and reasoning process. However, this strategy often does not work for existing open-source AudioLLMs. For example, the results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S10.T5\" title=\"Table 5 &#8227; 10 More Evaluation Results of Existing AudioLLMs &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> show that while Gemini-2.5-Flash consistently improves with the CoT prompt, Kimi-Audio-7B-Instruct, which is already the leading open-source model on SpeechJudge-Eval (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T2\" title=\"Table 2 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), actually sees a decline in performance when using the CoT prompt.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Aligning large generative models with human feedback is a critical challenge. In speech synthesis, this is particularly pronounced due to the lack of a large-scale human preference dataset, which hinders the development of models that truly align with human perception. To address this, we introduce <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SpeechJudge</span>, a comprehensive suite comprising a dataset, a benchmark, and a reward model centered on <span class=\"ltx_text ltx_font_italic\">naturalness</span>&#8212;one of the most fundamental subjective metrics for speech synthesis. First, we present <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SpeechJudge-Data</span>, a large-scale human feedback corpus of 99K speech pairs. The dataset is constructed using a diverse set of advanced zero-shot text-to-speech (TTS) models across diverse speech styles and multiple languages, with human annotations for both intelligibility and naturalness preference. From this, we establish <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SpeechJudge-Eval</span>, a challenging benchmark for speech naturalness judgment. Our evaluation reveals that existing metrics and AudioLLMs struggle with this task; the leading model, Gemini-2.5-Flash, achieves less than 70% agreement with human judgment, highlighting a significant gap for improvement. To bridge this gap, we develop <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SpeechJudge-GRM</span>, a generative reward model (GRM) based on Qwen2.5-Omni-7B. It is trained on SpeechJudge-Data via a two-stage post-training process: Supervised Fine-Tuning (SFT) with Chain-of-Thought rationales followed by Reinforcement Learning (RL) with GRPO on challenging cases. On the SpeechJudge-Eval benchmark, the proposed SpeechJudge-GRM demonstrates superior performance, achieving 77.2% accuracy (and 79.4% after inference-time scaling @10) compared to a classic Bradley-Terry reward model (72.7%). Furthermore, SpeechJudge-GRM can be also employed as a reward function during the post-training of speech generation models to facilitate their alignment with human preferences.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "models",
                    "judgment",
                    "from",
                    "both",
                    "audiollms",
                    "after",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The collection and integration of human feedback corpora for model alignment has become a critical stage in the development of modern large-scale generative models, proving indispensable in domains such as text <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib44\" title=\"\">44</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib37\" title=\"\">37</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib3\" title=\"\">3</a>]</cite>, image <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib54\" title=\"\">54</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib30\" title=\"\">30</a>]</cite>, and video generation <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib55\" title=\"\">55</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib33\" title=\"\">33</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">1. A Large-scale Human Feedback Dataset: <span class=\"ltx_text ltx_font_italic\">SpeechJudge-Data</span>.</span>\nWe recruit human annotators to provide feedback on synthesized speeches, with a focus on assessing two fundamental speech aspects: <span class=\"ltx_text ltx_font_italic\">intelligibility</span> and <span class=\"ltx_text ltx_font_italic\">naturalness</span>. For data synthesis, we employ a diverse set of advanced, open-source zero-shot TTS models with varying architectures (such as CosyVoice2 <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib14\" title=\"\">14</a>]</cite>, Ints <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>]</cite>, F5-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib7\" title=\"\">7</a>]</cite>, and MaskGCT <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib52\" title=\"\">52</a>]</cite>) to produce the compared speech pairs. We prepare speech references in both regular and expressive styles, construct multilingual target texts, and cover both monolingual and cross-lingual synthesis scenarios to ensure data diversity (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.SS1\" title=\"3.1 Dataset Construction &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>). We instruct human annotators to perform two tasks based on a speech pair (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F1\" title=\"Figure 1 &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>): (a) pointwise annotation of text accuracy to assess intelligibility, and (b) pairwise preference annotation to judge relative speech naturalness. This extensive effort, involving 69 labelers over two months, results in 99K annotated pairs, with each pair receiving an average of 2.49 annotations from different labelers. We believe the SpeechJudge-Data can serve as a valuable corpus for alignment research in speech synthesis (e.g., DPO alignment <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib39\" title=\"\">39</a>]</cite> or reward modeling <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib44\" title=\"\">44</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib37\" title=\"\">37</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib3\" title=\"\">3</a>]</cite> in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5\" title=\"5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>).</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "each",
                    "text",
                    "models",
                    "different",
                    "two",
                    "from",
                    "both",
                    "target",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2. An Evaluation Benchmark for Speech Naturalness Judgment: <span class=\"ltx_text ltx_font_italic\">SpeechJudge-Eval</span>.</span>\nWe design a dedicated evaluation benchmark for the task of speech naturalness judgment. The task is structured as follows: given a target text and two corresponding speech samples, a model needs to judge which one is more natural. To construct the evaluation set, we select a subset from the SpeechJudge-Data where human annotators demonstrated high inter-annotator agreement, ensuring a high-quality ground truth. We assess the naturalness judgment capabilities of a wide range of metrics and models, including Word Error Rate (WER) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib38\" title=\"\">38</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib16\" title=\"\">16</a>]</cite>, Fr&#233;chet Audio Distance (FAD) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib27\" title=\"\">27</a>]</cite>, MOS predictors <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib41\" title=\"\">41</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib40\" title=\"\">40</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib48\" title=\"\">48</a>]</cite>, Deepfake Detectors <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib49\" title=\"\">49</a>]</cite>, and AudioLLMs <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib28\" title=\"\">28</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib53\" title=\"\">53</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib22\" title=\"\">22</a>]</cite>. Our evaluations reveal that even the most capable model&#8212;specifically, Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>]</cite> in our experiments&#8212;achieved less than 70% agreement with human preferences. This finding highlights a significant performance gap and underscores the substantial room for research and improvement in automated speech naturalness judgment.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "text",
                    "models",
                    "judgment",
                    "two",
                    "from",
                    "target",
                    "audiollms",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">3. A Generative Reward Model for Speech Naturalness: <span class=\"ltx_text ltx_font_italic\">SpeechJudge-GRM</span>.</span>\nTo develop a reward model that more effectively captures human preferences, we develop SpeechJudge-GRM, a generative reward model (GRM) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib59\" title=\"\">59</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib34\" title=\"\">34</a>]</cite> trained on the SpeechJudge-Data. Specifically, we base our model on Qwen2.5-Omni-7B <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>]</cite> and design a two-stage post-training process. During the first stage, we perform SFT as the &#8220;cold start&#8221; to improve the model&#8217;s instruction-following and rationale-based reasoning capabilities. To achieve this, we leverage Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>]</cite> to generate Chain-of-Thought (CoT) data for speech naturalness judgment task. In the second stage, we focus on more challenging cases of SpeechJudge-Data, which we define as instances where Gemini-2.5-Flash fails to make the correct judgment. Treating the human-annotated labels as the verifiable reward <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib34\" title=\"\">34</a>]</cite>, we apply the GRPO-based RL stage <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib42\" title=\"\">42</a>]</cite>. Our experiments demonstrate that when trained on the same data, SpeechJudge-GRM significantly outperformed the classic Bradley-Terry reward model (BTRM) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib39\" title=\"\">39</a>]</cite>, achieving a higher accuracy in predicting human preferences (77.2% for SpeechJudge-GRM vs. 72.7% for SpeechJudge-BTRM, Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Methodology &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Besides, SpeechJudge-GRM also supports inference-time scaling and offers explainability through its CoT outputs. Furthermore, SpeechJudge-GRM can also be employed as an objective naturalness metric for sample selection (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F5\" title=\"Figure 5 &#8227; 5.1 Methodology &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) or as a reward function in RL algorithms to enhance the quality of existing speech generation models (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6\" title=\"Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>).</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "models",
                    "judgment",
                    "outputs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AudioLLM as a Judge</span>&#8195;Using LLMs as automated quality evaluators is a prominent topic in the textual LLM field, popularized by the &#8220;LLM-as-a-judge&#8221; paradigm <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib64\" title=\"\">64</a>]</cite>. This approach has recently been extended to the speech domain. A concurrent work, AudioJudge <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib36\" title=\"\">36</a>]</cite>, evaluates the capabilities and limitations of using AudioLLMs for speech quality assessment and paralinguistic understanding by prompt engineering. Furthermore, many studies have focused on fine-tuning AudioLLMs to elicit their understanding capabilities for specific tasks. Examples include discriminating the human-likeness of audio <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib51\" title=\"\">51</a>]</cite>, understanding low-level acoustic qualities <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib50\" title=\"\">50</a>]</cite>, and enhancing the assessment of instruction-following in spoken dialogue systems <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib17\" title=\"\">17</a>]</cite>. However, how to improve the ability of AudioLLMs to understand and judge speech naturalness, and how to use the quality assessment capabilities of AudioLLMs as a reward to improve the post-training of speech generation models themselves, remain significantly underexplored.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "models",
                    "audio",
                    "audiollms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work is grounded in <span class=\"ltx_text ltx_font_bold\">SpeechJudge-Data</span>, a large-scale human feedback corpus for assessing the <span class=\"ltx_text ltx_font_italic\">intelligibility</span> and <span class=\"ltx_text ltx_font_italic\">naturalness</span> of synthesized speech. Formally, we aim to construct a dataset <math alttext=\"\\mathcal{D}=\\{(t,a_{1},a_{2})\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{D}=\\{(t,a_{1},a_{2})\\}</annotation></semantics></math>, where each triplet comprises a pair of synthesized speech samples <math alttext=\"(a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{1},a_{2})</annotation></semantics></math> and the corresponding target text <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>. We instruct annotators to provide pointwise intelligibility and pairwise naturalness preference annotations based on <math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math> (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F1\" title=\"Figure 1 &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "each",
                    "text",
                    "target",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a diverse set of recent advanced zero-shot TTS models to prepare the dataset <math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math>. Formally, for each sample <math alttext=\"(t,a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(t,a_{1},a_{2})</annotation></semantics></math>, we denote the synthesized speech <math alttext=\"a_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>a</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">a_{i}</annotation></semantics></math> as being produced by the model <math alttext=\"\\mathcal{M}_{tts}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{M}_{tts}</annotation></semantics></math>, i.e., <math alttext=\"a_{i}\\sim\\mathcal{M}_{tts}(a_{ref},t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>a</mi><mi>i</mi></msub><mo>&#8764;</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">a_{i}\\sim\\mathcal{M}_{tts}(a_{ref},t)</annotation></semantics></math>, where <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math> is the reference speech.</p>\n\n",
                "matched_terms": [
                    "models",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model Selection</span>&#8195;For <math alttext=\"\\mathcal{M}_{tts}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{M}_{tts}</annotation></semantics></math>, we select the following six models of three architectures to enrich the distribution of the synthetic data (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F2.sf1\" title=\"Figure 2(a) &#8227; Figure 2 &#8227; 3.1 Dataset Construction &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>): (1) <span class=\"ltx_text ltx_font_bold\">AR-based</span>: ARS <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib52\" title=\"\">52</a>]</cite>, CosyVoice2 <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib14\" title=\"\">14</a>]</cite>, CosyVoice2-INTP <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>]</cite>, and Ints-INTP <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>]</cite>. The latter two are released by <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>]</cite> as intelligibility-enhanced models. (2) <span class=\"ltx_text ltx_font_bold\">FM-based</span>: F5-TTS. (3) <span class=\"ltx_text ltx_font_bold\">MGM-based</span>: MaskGCT <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib52\" title=\"\">52</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "following",
                    "models",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt Construction</span>&#8195;To build diverse prompts <math alttext=\"(a_{ref},t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{ref},t)</annotation></semantics></math> for TTS, for <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math>, we adopt both <span class=\"ltx_text ltx_font_bold\">regular</span> and <span class=\"ltx_text ltx_font_bold\">expressive</span> speech samples. The regular samples are randomly selected from the Emilia-Large dataset <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib19\" title=\"\">19</a>]</cite>. The expressive samples are sourced from corpora rich in paralinguistics, including the emotional corpora: ParaSpeechCaps <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib9\" title=\"\">9</a>]</cite>, the accented corpora: L2-Arctic <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib63\" title=\"\">63</a>]</cite> and KeSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib46\" title=\"\">46</a>]</cite>, the whisper samples from an in-house corpus, and the character voices from video games Genshin Impact <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib43\" title=\"\">43</a>]</cite>. We display the detailed distribution of speech references in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F2.sf2\" title=\"Figure 2(b) &#8227; Figure 2 &#8227; 3.1 Dataset Construction &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a>.</p>\n\n",
                "matched_terms": [
                    "detailed",
                    "from",
                    "both",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The target text <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> paired with each <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m2\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math> is constructed as follows: For regular <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m3\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math> samples, we randomly sample transcriptions from the Emilia-Large dataset <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib19\" title=\"\">19</a>]</cite>. These are then refined using DeepSeek-V3 <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib10\" title=\"\">10</a>]</cite> to correct typos and normalize punctuations. For expressive <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m4\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math> samples, we instruct DeepSeek-V3 to generate several scripts in different writing styles, tailored to the topic of <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m5\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math> (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S7.SS1\" title=\"7.1 Details of Prompt Construction &#8227; 7 Details of SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">7.1</span></a> for more details). The languages of the target texts included Chinese (<span class=\"ltx_text ltx_font_italic\">zh</span>), English (<span class=\"ltx_text ltx_font_italic\">en</span>), and Chinese-English code-switching (<span class=\"ltx_text ltx_font_italic\">mixed</span>). For the combinations <math alttext=\"(a_{ref},t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{ref},t)</annotation></semantics></math>, we include both monolingual settings (<span class=\"ltx_text ltx_font_italic\">en2en</span> and <span class=\"ltx_text ltx_font_italic\">zh2zh</span>) and cross-lingual settings (<span class=\"ltx_text ltx_font_italic\">zh2en</span>, <span class=\"ltx_text ltx_font_italic\">en2zh</span>, <span class=\"ltx_text ltx_font_italic\">zh2mixed</span>, and <span class=\"ltx_text ltx_font_italic\">en2mixed</span>), where <span class=\"ltx_text ltx_font_italic\">zh2en</span> denotes Chinese <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m7\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math> with English <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m8\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, and similarly for others. The distribution of the language settings of <math alttext=\"(a_{ref},t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m9\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{ref},t)</annotation></semantics></math> is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F2.sf3\" title=\"Figure 2(c) &#8227; Figure 2 &#8227; 3.1 Dataset Construction &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2(c)</span></a>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "text",
                    "different",
                    "from",
                    "both",
                    "target"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Pair Construction</span>&#8195;To ensure the diversity of the <math alttext=\"(a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{1},a_{2})</annotation></semantics></math> pairs being compared, we follow <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>]</cite> and adopt both intra-model (i.e.,<math alttext=\"a_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m2\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">a_{1}</annotation></semantics></math> and <math alttext=\"a_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m3\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">a_{2}</annotation></semantics></math> being generated by the same model) and inter-model pairs (i.e., <math alttext=\"a_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m4\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">a_{1}</annotation></semantics></math> and <math alttext=\"a_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m5\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">a_{2}</annotation></semantics></math> being generated by the different models). The distribution of the speech pair is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S7.F7\" title=\"Figure 7 &#8227; 7.1 Details of Prompt Construction &#8227; 7 Details of SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "a2a2",
                    "models",
                    "different",
                    "both",
                    "a1a1"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a sample <math alttext=\"(t,a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(t,a_{1},a_{2})</annotation></semantics></math>, human annotators are instructed to perform both pointwise intelligibility and pairwise naturalness annotations (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F1\" title=\"Figure 1 &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). For intelligibility, annotators perform a binary classification to determine whether the speech (<math alttext=\"a_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">a_{1}</annotation></semantics></math> and <math alttext=\"a_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">a_{2}</annotation></semantics></math>) accurately reads the text <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> without any content insertion, omission, or mispronunciation. For naturalness, they perform a five-scale Comparative Mean Opinion Score (CMOS) annotation to determine which of the two audio clips (<math alttext=\"a_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">a_{1}</annotation></semantics></math> or <math alttext=\"a_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m6\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">a_{2}</annotation></semantics></math>) sounds more natural and human-like. We recruit human annotators and provide them with training. The detailed annotation guidelines are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S8\" title=\"8 Human Annotation Details &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "a2a2",
                    "detailed",
                    "text",
                    "score",
                    "two",
                    "accurately",
                    "both",
                    "them",
                    "a1a1",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Statistics</span>&#8195;We recruit 69 annotators and conduct annotations over two months. The resulting constructed dataset <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math>, which we denote as SpeechJudge-Data (raw), contains 99K <math alttext=\"(t,a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(t,a_{1},a_{2})</annotation></semantics></math> samples, with each sample receiving an average of 2.49 annotations from different labelers. The market value of this annotation scale is estimated at over 500K RMB (about 70K USD). Based on the raw dataset, we also construct several subsets for analysis and reward model training. We provide detailed descriptions of each subset and its applications in the following sections and in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S7.SS2\" title=\"7.2 Subsets of SpeechJudge-Data &#8227; 7 Details of SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "detailed",
                    "different",
                    "two",
                    "from",
                    "analysis",
                    "following",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human Agreement Analysis</span>&#8195;We analyze the human annotations for naturalness in this section; discussions regarding intelligibility are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S8.SS2\" title=\"8.2 Intelligibility Annotation Analysis &#8227; 8 Human Annotation Details &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">8.2</span></a>. For naturalness annotations, we evaluate the inter-annotator agreement across our constructed dataset. To simplify the analysis, given the sample <math alttext=\"(t,a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(t,a_{1},a_{2})</annotation></semantics></math>, we transform the five-scale naturalness scale (CMOS) into a ternary classification system: either <math alttext=\"a_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">a_{1}</annotation></semantics></math> is better, <math alttext=\"a_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">a_{2}</annotation></semantics></math> is better, or their quality is a Tie. Based on this simplified classification, we categorize the annotation results into four distinct levels of agreement<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><span class=\"ltx_text ltx_font_bold\">Note</span>: Each sample of SpeechJudge-Data is independently annotated by a minimum of two and a maximum of three annotators (Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S8\" title=\"8 Human Annotation Details &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>).</span></span></span>: (1) <span class=\"ltx_text ltx_font_bold\">Full Agreement (FA)</span>: A consensus is reached among all annotators, with all ratings pointing to the same outcome (e.g., &#8220;2A&#8221;, &#8220;3A&#8221;, &#8220;2B&#8221;, &#8220;3B&#8221;). We use &#8220;2A&#8221; to indicate that two annotators both rated <math alttext=\"a_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m4\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">a_{1}</annotation></semantics></math> as better, while &#8220;3B&#8221; denotes three annotators all rating <math alttext=\"a_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m5\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">a_{2}</annotation></semantics></math> as better. (2) <span class=\"ltx_text ltx_font_bold\">Weak Agreement (WA)</span>: This level captures cases where two annotators agree on a specific polarity, while the third annotator marks a Tie (e.g., &#8220;2A+1T&#8221;, &#8220;2B+1T&#8221;). We also include the &#8220;2T+1A&#8221; and &#8220;2T+1B&#8221; cases in this level. (3) <span class=\"ltx_text ltx_font_bold\">Weak Disagreement (WD)</span>: This occurs when two annotators&#8217; ratings share the same polarity, but the third&#8217;s rating is the opposite (e.g., &#8220;2A+B&#8221;, &#8220;2B+A&#8221;). (4) <span class=\"ltx_text ltx_font_bold\">Full Disagreement (FD)</span>: This represents a complete lack of consensus, where all three annotators provide different classifications, denoted as &#8220;1A+1B+1T&#8221;.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "each",
                    "a2a2",
                    "evaluate",
                    "analyze",
                    "different",
                    "two",
                    "analysis",
                    "both",
                    "a1a1",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Human Annotation &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we demonstrate the distribution of these human agreement levels for the SpeechJudge-Data and its two subsets, <span class=\"ltx_text ltx_font_italic\">regular</span> and <span class=\"ltx_text ltx_font_italic\">expressive</span> (which are defined by their speech references). The figure shows that about 70% of the entire dataset falls into the Full Agreement (51.5%) or Weak Agreement (17.2%) levels. Furthermore, we observe that the <span class=\"ltx_text ltx_font_italic\">expressive</span> subset has a lower agreement level than the <span class=\"ltx_text ltx_font_italic\">regular</span> subset, which suggests that human evaluation of expressive speech generation is inherently a more challenging problem. Besides this sample-level agreement analysis, we also analyze the reliability of individual annotators, and we will discuss this in the Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S8.SS1\" title=\"8.1 Individual Annotator Reliability &#8227; 8 Human Annotation Details &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">8.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "two",
                    "analyze"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate speech naturalness, existing studies typically organize their own listening tests, which often have inconsistent settings across different papers <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib28\" title=\"\">28</a>]</cite>. Alternatively, previous researchers use proxy MOS predictors, such as UTMOS <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib41\" title=\"\">41</a>]</cite>, as an objective metric. However, it remains an underexplored problem whether these metrics can accurately judge the naturalness of more advanced speech generation models <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib52\" title=\"\">52</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>]</cite> and align with human preferences. Motivated by this, we construct a benchmark, <span class=\"ltx_text ltx_font_bold\">SpeechJudge-Eval</span>, specifically for the speech naturalness judgment task.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "evaluate",
                    "models",
                    "judgment",
                    "different",
                    "accurately"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:80%;\">We instruct AudioLLMs using two modes of prompt: </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">plain</span>\n  <span class=\"ltx_text\" style=\"font-size:80%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">CoT</span>\n  <span class=\"ltx_text\" style=\"font-size:80%;\">. The text in </span>\n  <span class=\"ltx_text\" style=\"font-size:80%;--ltx-fg-color:#2954AD;\">blue</span>\n  <span class=\"ltx_text\" style=\"font-size:80%;\"> is only employed during the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">CoT</span>\n  <span class=\"ltx_text\" style=\"font-size:80%;\"> mode.</span>\n</p>\n\n",
                "matched_terms": [
                    "text",
                    "two",
                    "audiollms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task Formulation</span>&#8195;We formulate the naturalness judgment task as a pairwise comparison, specifically a <span class=\"ltx_text ltx_font_italic\">win-or-lose</span> binary classification task: Given a target text <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> and a corresponding audio pair <math alttext=\"(a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{1},a_{2})</annotation></semantics></math>, a model needs to determine which audio has better naturalness. This results in a binary choice: either <math alttext=\"a_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">a_{1}</annotation></semantics></math> is better or <math alttext=\"a_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">a_{2}</annotation></semantics></math> is better. We use the human answer as the ground truth, and use <span class=\"ltx_text ltx_font_bold\">Accuracy</span> to measure the judgment performance of a model <math alttext=\"\\mathcal{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><annotation encoding=\"application/x-tex\">\\mathcal{M}</annotation></semantics></math> on the evaluation set <math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "a2a2",
                    "text",
                    "judgment",
                    "target",
                    "a1a1",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Data</span>&#8195;We sample a subset of the SpeechJudge-Data to create the evaluation set for SpeechJudge-Eval. Specifically, we first select a subset that contains only <span class=\"ltx_text ltx_font_italic\">preference data</span> (i.e., we filter out samples with the &#8220;Tie&#8221; annotation), and then choose only those with full-agreement-level (FA) samples to ensure a high-quality ground truth. We perform sampling from both the <span class=\"ltx_text ltx_font_italic\">regular</span> and <span class=\"ltx_text ltx_font_italic\">expressive</span> subsets of SpeechJudge-Data and proportionally cover the three target text languages (<span class=\"ltx_text ltx_font_italic\">zh</span>, <span class=\"ltx_text ltx_font_italic\">en</span>, and <span class=\"ltx_text ltx_font_italic\">mixed</span>) within each subset. The final SpeechJudge-Eval dataset consists of 1,000 samples. The construction details of SpeechJudge-Eval and its distribution can be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S7.SS2\" title=\"7.2 Subsets of SpeechJudge-Data &#8227; 7 Details of SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "text",
                    "from",
                    "both",
                    "target"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Objective metrics</span>, such as WER <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib38\" title=\"\">38</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib16\" title=\"\">16</a>]</cite>, SIM <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib6\" title=\"\">6</a>]</cite>, and FAD <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib27\" title=\"\">27</a>]</cite> in audio generation tasks. We assume that a better value of these metrics (e.g., lower for WER and FAD; higher for SIM) indicates better naturalness.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MOS Predictors</span>, including DNSMOS <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib40\" title=\"\">40</a>]</cite>, UTMOS <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib41\" title=\"\">41</a>]</cite>, and predictors from audiobox-aesthetics (CE, CU, PC, and PQ) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib48\" title=\"\">48</a>]</cite>. We assume that a higher MOS score corresponds to better naturalness.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "from",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Deepfake detectors</span>, which are typically pre-trained on a binary classification task to predict whether an audio is fake or not <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib49\" title=\"\">49</a>]</cite>. We assume that an audio with a lower fake probability should have better naturalness.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance of different models on SpeechJudge-Eval is presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T2\" title=\"Table 2 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. A key observation is that speech naturalness judgment is a highly challenging task. The leading model, Gemini-2.5-Flash, still only achieves less than 70% agreement with human preferences. When comparing different models, we find that: (1) common objective metrics and MOS predictors show only a weak correlation with human preferences, often achieving less than 60% accuracy and sometimes performing at the level of a random guess (around 50%). (2) While deepfake detectors are highly effective at distinguishing between machine-generated and human-recorded speech <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib49\" title=\"\">49</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib25\" title=\"\">25</a>]</cite>, their ability to do so is not well-aligned with the naturalness objective when comparing two generated samples. (3) AudioLLMs demonstrate significant potential for this task. While some models, such as Gemma-3n and GPT-4o mini Audio, perform at a chance level, a number of others achieve an accuracy exceeding 60%. This promising performance motivates us to further leverage these AudioLLMs for the design of a reward model for speech naturalness.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "models",
                    "judgment",
                    "different",
                    "two",
                    "audiollms",
                    "number",
                    "comparing",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the proposed SpeechJudge-Data, we further explore how to train a reward model capable of accurately capturing human preferences. Specifically, we propose <span class=\"ltx_text ltx_font_bold\">SpeechJudge-GRM</span>, where we leverage the inherent audio understanding capabilities of AudioLLMs (specifically, Qwen2.5-Omni-7B <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>]</cite>) to elicit their speech naturalness judgment capability. Compared to the classic BTRM <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib4\" title=\"\">4</a>]</cite>, the key strengths of GRM are its ability to enable Chain-of-Thought (CoT) reasoning and its support for test-time computation via majority voting, which ultimately leads to improved preference judgment performance <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib59\" title=\"\">59</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "judgment",
                    "accurately",
                    "audiollms",
                    "audio",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We develop SpeechJudge-GRM based on Qwen2.5-Omni-7B (Thinker) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>]</cite>. Inspired by the powerful capabilities of RL with the verifiable reward (RLVR) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib42\" title=\"\">42</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib11\" title=\"\">11</a>]</cite>, our natural initial approach is to treat the human preference <math alttext=\"y_{\\mathcal{H}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></msub><annotation encoding=\"application/x-tex\">y_{\\mathcal{H}}</annotation></semantics></math> for the pair <math alttext=\"(a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{1},a_{2})</annotation></semantics></math> as a verifiable reward, and launch a RLVR training based on Qwen2.5-Omni.\nHowever, in practice, we find that the instruction-following reasoning capabilities of Qwen2.5-Omni are very weak (more detailed discussions can be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S10\" title=\"10 More Evaluation Results of Existing AudioLLMs &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>). Therefore, we adopt a two-stage post-training process (&#8220;SFT + RL&#8221;) to develop SpeechJudge-GRM (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F4\" title=\"Figure 4 &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). We describe the details as follows.</p>\n\n",
                "matched_terms": [
                    "based",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:80%;\">Our evaluation is conducted on SpeechJudge-Eval (like Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T2\" style=\"font-size:80%;\" title=\"Table 2 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:80%;\">). </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">w/ Voting@10</span>\n  <span class=\"ltx_text\" style=\"font-size:80%;\">: For each prompt, the GRM generates 10 outputs, and we use the majority voting from these 10 outputs as the final result.</span>\n</p>\n\n",
                "matched_terms": [
                    "each",
                    "from",
                    "outputs"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RL Stage</span>&#8195;We treat the annotated human preference as a verifiable reward, and, building on the SFT model, we further trained it using the GRPO algorithm <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib42\" title=\"\">42</a>]</cite>. Specifically, for each sample <math alttext=\"d=(t,a_{1},a_{2},y_{\\mathcal{H}})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo>,</mo><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">d=(t,a_{1},a_{2},y_{\\mathcal{H}})</annotation></semantics></math> in the RL dataset, we adopt the CoT prompt to instruct the policy model to conduct multiple rollouts during each iteration. For the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th rollout, we parse the model&#8217;s preference for <math alttext=\"(a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{1},a_{2})</annotation></semantics></math>, denoted as <math alttext=\"y_{\\mathcal{M}}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m4\" intent=\":literal\"><semantics><msubsup><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">y_{\\mathcal{M}}^{i}</annotation></semantics></math>. Following <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib34\" title=\"\">34</a>]</cite>, we use an accuracy-based rule to calculate the reward: the reward is 1 if <math alttext=\"y_{\\mathcal{M}}^{i}=y_{\\mathcal{H}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m5\" intent=\":literal\"><semantics><mrow><msubsup><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mi>i</mi></msubsup><mo>=</mo><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></msub></mrow><annotation encoding=\"application/x-tex\">y_{\\mathcal{M}}^{i}=y_{\\mathcal{H}}</annotation></semantics></math>, and -1 otherwise. In other words, during the RL stage, we only constrain the model&#8217;s final naturalness judgment to align with human preferences, allowing the model to autonomously optimize its reasoning and rationale generation capabilities.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "each",
                    "judgment",
                    "following"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We denote the training dataset of SpeechJudge-GRM as SpeechJudge-Data (train). Its construction process is as follows (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S7.SS2\" title=\"7.2 Subsets of SpeechJudge-Data &#8227; 7 Details of SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a> for more details). Based on the raw SpeechJudge-Data, we first filter out all samples at the Full Disagreement (FD) level. For the other samples&#8212;at the FA, WA, and WD levels&#8212;we apply a majority voting principle among annotators to determine the final label for each. We then further exclude samples with a &#8220;Tie&#8221; label, using only the remaining preference data to form the SpeechJudge-Data (train).\nWe use LoRA <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib20\" title=\"\">20</a>]</cite> to fine-tune the GRM during both the SFT and RL stages. Other experimental setup details are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S11\" title=\"11 Training Details of SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "each",
                    "both",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To verify the effectiveness of SpeechJudge-GRM for naturalness judgment, we evaluate it on the SpeechJudge-Eval benchmark. We develop SpeechJudge-BTRM as a baseline, which utilizes the BTRM paradigm <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib39\" title=\"\">39</a>]</cite> by adding a linear layer on Qwen2.5-Omni-7B (Thinker) to produce a single scalar reward prediction. SpeechJudge-BTRM also uses LoRA fine-tuning and uses the same training data as SpeechJudge-GRM.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "evaluate",
                    "judgment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the results of Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Methodology &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we can observe that: (1) The SpeechJudge-BTRM achieves a 72.7% agreement with human preferences on SpeechJudge-Eval, a level of performance comparable to the initial development of BTRMs in the textual LLM RLHF field <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib44\" title=\"\">44</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib37\" title=\"\">37</a>]</cite>. (2) After conducting SFT training with the CoT data, the accuracy of SpeechJudge-GRM (SFT) reaches 75.3%. Besides, further RLVR training improves the final model SpeechJudge-GRM (SFT+RL) to an accuracy of 77.2%. (3) Due to the generative nature of the GRM, we can further enhance the accuracy of SpeechJudge-GRM using inference-time scaling. For example, by using majority voting across 10 outputs instead of just one, the accuracy is improved by approximately 2 percentage points (75.3% <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> 77.6%; 77.2% <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> 79.4%). These results collectively verify the effectiveness of our proposed SpeechJudge-GRM for judging speech naturalness.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "conducting",
                    "outputs",
                    "from",
                    "after"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigate the effect of SpeechJudge-based reward models for high-quality sample selection. We use the hard cases from SeedTTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib2\" title=\"\">2</a>]</cite> and the code-switching cases from Amphion-TTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib60\" title=\"\">60</a>]</cite> as target texts. For each text, we instruct the Qwen2.5-Omni-7B (Talker) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib57\" title=\"\">57</a>]</cite> to generate 100 speeches. We then ask human subjects to compare the best-of-100 output&#8212;as selected by either SpeechJudge-BTRM or SpeechJudge-GRM&#8212;against a randomly sampled output. The evaluation measures the win/lose/tie ratios based on speech naturalness. From Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F5\" title=\"Figure 5 &#8227; 5.1 Methodology &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that the best-of-100 samples selected by both SpeechJudge-BTRM and SpeechJudge-GRM are more likely to outperform a randomly selected sample from the same set. This finding demonstrates the advantage of using the SpeechJudge-Data corpus for training human-aligned reward model. Furthermore, SpeechJudge-GRM exhibits better performance than SpeechJudge-BTRM, which highlights the superiority of the proposed GRM.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "each",
                    "text",
                    "models",
                    "output",
                    "from",
                    "both",
                    "target",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigate the effect of using SpeechJudge-GRM as a reward function for post-training of TTS model. Specifically, we develop a new zero-shot TTS model, <span class=\"ltx_text ltx_font_bold\">Qwen2.5-0.5B-TTS</span>, to serve as the base model, which was not involved in the construction of the SpeechJudge-Data. This model is based on Qwen2.5-0.5B <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib57\" title=\"\">57</a>]</cite>, adopts the classic two-stage &#8220;AR+Diffusion&#8221; architecture <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib13\" title=\"\">13</a>]</cite>, uses the speech tokenizer from DualCodec <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib31\" title=\"\">31</a>]</cite>, and is pre-trained on the Emilia dataset <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib19\" title=\"\">19</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "from",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on this pre-trained model, we design four comparative methods:\n(1) <span class=\"ltx_text ltx_font_bold\">w/ INTP</span>: We use the intelligibility preference dataset, INTP <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>]</cite>, to perform offline DPO alignment <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib39\" title=\"\">39</a>]</cite>.\n(2) <span class=\"ltx_text ltx_font_bold\">w/ SpeechJudge-Data</span>: We use the SpeechJudge-Data (train) to perform offline DPO alignment.\n(3) <span class=\"ltx_text ltx_font_bold\">w/ SpeechJudge-GRM (offline)</span>: We use SpeechJudge-GRM as an offline preference data annotator. We take all speech pairs from the INTP dataset and re-annotate their preference labels using SpeechJudge-GRM, then perform offline DPO alignment on the resulting data.\n(4) <span class=\"ltx_text ltx_font_bold\">w/ SpeechJudge-GRM (online)</span>: We use SpeechJudge-GRM as a reward function for the online DPO algorithm <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib18\" title=\"\">18</a>]</cite>. The training data consists of only the prompts from INTP (i.e., the target texts and speech references for zero-shot TTS).</p>\n\n",
                "matched_terms": [
                    "from",
                    "target",
                    "based",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use SeedTTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib2\" title=\"\">2</a>]</cite> and Amphion-TTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib60\" title=\"\">60</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib62\" title=\"\">62</a>]</cite> as evaluation sets. We present the objective results (WER and SIM) in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S12.T6\" title=\"Table 6 &#8227; 12.2 Objective Results &#8227; 12 Sample Selection and Post-Training based on SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and the subjective results in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6\" title=\"Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.\nWe observe that both intelligibility and naturalness are enhanced for all the four methods after post-training. Additionally, the post-training method based on SpeechJudge-GRM achieves a greater improvement in naturalness (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6.sf1\" title=\"Figure 6(a) &#8227; Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6(a)</span></a>). Besides, the SpeechJudge-based methods could match or lead to a slight improvement in speaker similarity (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6.sf2\" title=\"Figure 6(b) &#8227; Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6(b)</span></a>).</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "after",
                    "both",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we address the challenge of aligning speech synthesis with human perception of naturalness by introducing SpeechJudge, a suite including a large-scale dataset (SpeechJudge-Data), a challenging benchmark (SpeechJudge-Eval), and a generative reward model (SpeechJudge-GRM). Our benchmark reveals that even top AudioLLMs struggle with naturalness judgment, achieving less than 70% human agreement. In contrast, our proposed SpeechJudge-GRM reaches 77.2% accuracy (up to 79.4% with inference-time scaling @10), outperforming the classic Bradley-Terry reward models (72.7%). We also demonstrate its practical utility as a reward function to effectively enhance TTS model naturalness via post-training. While our primary focus is on naturalness, future work could extend this framework to other subjective attributes like speaker similarity and emotional expressiveness, potentially through multi-objective reward modeling. By releasing our resources, we aim to catalyze research in building more human-aligned speech generation systems.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "models",
                    "judgment",
                    "audiollms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the target texts paired with the expressive speech references, we use DeepSeek-V3 to generate several scripts in different writing styles based on the speech reference&#8217;s text, the prompt used is listed below.</p>\n\n",
                "matched_terms": [
                    "text",
                    "different",
                    "target",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct several subsets based on SpeechJudge-Data (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S7.F8\" title=\"Figure 8 &#8227; 7.2 Subsets of SpeechJudge-Data &#8227; 7 Details of SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>). We begin with the <span class=\"ltx_text ltx_font_bold\">SpeechJudge-Data (raw)</span> corpus, containing 99K pairs, where each pair is annotated by multiple labelers as a five-scale naturalness CMOS. We aggregate these annotations via a majority vote for each pair, and subsequently discard all &#8220;Tie&#8221; pairs, yielding the 79K-pair human <span class=\"ltx_text ltx_font_italic\">preference</span> data, denoted as <span class=\"ltx_text ltx_font_bold\">SpeechJudge-Data (pref)</span>.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "each",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During our preliminary analysis based on SpeechJudge-Data (pref), we observe that a significant disparity in intelligibility between two speech samples can overshadow the subtler quality of naturalness, biasing human preference toward the more comprehensible sample. To mitigate this confounding factor and create a more high-quality dataset focused specifically on naturalness, we further refine the data. Specifically, we retain only pairs where the absolute WER gap of those is below 12%. This process results in the 44K-pair high-quality <span class=\"ltx_text ltx_font_bold\">SpeechJudge-Data (hq)</span> subset, ensuring that its preference labels are more reflective of genuine differences in naturalness.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "analysis",
                    "two",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From SpeechJudge-Data (hq), we construct our benchmark, <span class=\"ltx_text ltx_font_bold\">SpeechJudge-Eval</span>, by applying stratified sampling to FA-level pairs, resulting in 1,000 pairs; its composition is detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S7.T4\" title=\"Table 4 &#8227; 7.2 Subsets of SpeechJudge-Data &#8227; 7 Details of SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. Similarly, we use the same strategy to construct a validation set of the same size, <span class=\"ltx_text ltx_font_bold\">SpeechJudge-Data (dev)</span>. The remaining 42K pairs, <span class=\"ltx_text ltx_font_bold\">SpeechJudge-Data (train)</span>, constitute the training set for our reward models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "from",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">All personnel underwent standardized training based on a detailed annotation manual. Initially, we conducted a pilot study among researchers to refine the guidelines for clarity and unambiguity. To ensure annotation quality, each sample <math alttext=\"(t,a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S8.SS0.SSS0.Px2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(t,a_{1},a_{2})</annotation></semantics></math> was independently annotated by two individuals. A third annotator was introduced if any disagreements. The complete annotation guidelines are attached below.</p>\n\n",
                "matched_terms": [
                    "each",
                    "detailed",
                    "clarity",
                    "two",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For a given sample annotated by a group of <math alttext=\"M\" class=\"ltx_Math\" display=\"inline\" id=\"S8.SS1.p2.m1\" intent=\":literal\"><semantics><mi>M</mi><annotation encoding=\"application/x-tex\">M</annotation></semantics></math> annotators, the agreement score for annotator <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S8.SS1.p2.m2\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math> is calculated as the fraction of the other <math alttext=\"M-1\" class=\"ltx_Math\" display=\"inline\" id=\"S8.SS1.p2.m3\" intent=\":literal\"><semantics><mrow><mi>M</mi><mo>&#8722;</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">M-1</annotation></semantics></math> annotators who assigned the exact same label. An annotator&#8217;s final reliability score is the average of these scores across all samples they evaluated. We excluded participants who annotated fewer than 10 samples from this analysis.</p>\n\n",
                "matched_terms": [
                    "from",
                    "analysis",
                    "score"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide a detailed analysis of the relationship between the mostly common used objective intelligibility metric, Word Error Rate (WER), and the subjective human judgments of intelligibility. Our goal is to determine the extent to which WER can serve as a reliable proxy for human perception.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "detailed"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use all the speech samples from SpeechJudge-Data (raw) for this analysis. We visualize the relationship between WER and the subjective text accuracy in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S8.F10\" title=\"Figure 10 &#8227; 8.2 Intelligibility Annotation Analysis &#8227; 8 Human Annotation Details &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.\nFor the regular speeches (the orange curve), we observe a consistent negative correlation: as the WER increases, its perceived text accuracy steadily declines. For the expressive speeches (the green curve), the similar trend holds for expressive speech when WER is under about 12%. When WER is over the threshold, however, the correlation between WER and the subjective text accuracy weakens significantly. We think this divergence is sourced from that the greater stylistic variations in expressive speech pose a substantial challenge to the robustness of ASR systems compared to the regular samples.</p>\n\n",
                "matched_terms": [
                    "text",
                    "from",
                    "analysis"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the evaluation on the SpeechJudge-Eval Benchmark of Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T2\" title=\"Table 2 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we adopt the following protocol for each model:</p>\n\n",
                "matched_terms": [
                    "following",
                    "each"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Using AudioLLM as a judge models, prompt engineering strategies are usually believed crucial for improving the performance <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib64\" title=\"\">64</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib36\" title=\"\">36</a>]</cite>. Some common prompt engineering strategies include using the CoT prompts to activate the model&#8217;s thinking and reasoning abilities <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib59\" title=\"\">59</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib34\" title=\"\">34</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib36\" title=\"\">36</a>]</cite>, or employing few-shot evaluation formats <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib64\" title=\"\">64</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib53\" title=\"\">53</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "prompts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on our preliminary qualitative analysis, we believe the reason why the open-source AudioLLMs do not work well with the CoT prompt is that their foundational capabilities are relatively weak. These weaknesses include instruction-following (such as format-following), multiple-audio understanding, long-text generation, and reasoning abilities. This is also why, when we developed SpeechJudge-GRM, we did not directly apply RLVR on top of Qwen2.5-Omni-7B. Instead, we used an initial SFT stage as a cold start.</p>\n\n",
                "matched_terms": [
                    "analysis",
                    "based",
                    "audiollms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RL Stage</span>&#8195;We use the 17K samples (as described above) to conduct DAPO <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib58\" title=\"\">58</a>]</cite>, which is an enhanced variant of GRPO <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib42\" title=\"\">42</a>]</cite>. We utilize the <span class=\"ltx_text ltx_font_typewriter\">ms-swift<span class=\"ltx_note ltx_role_footnote\" id=\"footnote12\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">12</span></span><a class=\"ltx_ref ltx_href ltx_font_serif\" href=\"https://github.com/modelscope/ms-swift\" title=\"\">https://github.com/modelscope/ms-swift</a></span></span></span></span> toolkit to launch the training process. We initialize the policy model with the SFT model and use LoRA training with a rank of 64. The number of rollouts for each prompt is set to 8, and the batch size is 32. The learning rate is 5e-6. We select the best checkpoint on SpeechJudge-Data (dev) as the final SpeechJudge-GRM model, i.e., SpeechJudge-GRM (SFT+RL).</p>\n\n",
                "matched_terms": [
                    "each",
                    "number",
                    "above"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the construction of SpeechJudge-Data, we hired human labelers from a data crowdsourcing company. To verify the effectiveness of our training for them and to ensure the high quality of both the dataset and the resulting SpeechJudge-GRM, the human subjects for the final sample selection and TTS post-training experiments (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.SS3\" title=\"5.3 High-Quality Sample Selection based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.SS4\" title=\"5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>) were all experienced speech generation researchers. All these researchers had extensive audio backgrounds, with a minimum of two years of experience in speech synthesis.</p>\n\n",
                "matched_terms": [
                    "two",
                    "from",
                    "both",
                    "them",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We randomly selected the subjective evaluation samples from both SeedTTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib2\" title=\"\">2</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote13\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_tag ltx_tag_note\">13</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/BytedanceSpeech/seed-tts-eval\" title=\"\">https://github.com/BytedanceSpeech/seed-tts-eval</a></span></span></span> and Amphion-TTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib62\" title=\"\">62</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\">14</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/datasets/amphion/Amphion-TTS-Eval\" title=\"\">https://huggingface.co/datasets/amphion/Amphion-TTS-Eval</a></span></span></span>. The evaluation set for each system in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6\" title=\"Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> consists of 70 samples, while the set for each system in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F5\" title=\"Figure 5 &#8227; 5.1 Methodology &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> contains 100 samples. Each audio sample in these evaluations received at least three independent ratings. These subjective evaluation results show that the annotation quality of SpeechJudge-Data largely aligns with the judgments of professional researchers.</p>\n\n",
                "matched_terms": [
                    "each",
                    "from",
                    "both",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding the SIM metric, both w/ INTP and w/ SpeechJudge-GRM (offline) either match or slightly outperform the baseline model, while the other two methods show a slight decline. However, the objective SIM results appear to be in slight conflict with the subjective speaker similarity results in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6.sf2\" title=\"Figure 6(b) &#8227; Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6(b)</span></a>. For instance, in the subjective evaluation, w/ INTP actually shows a decrease in speaker similarity (Win: 24.30%, Lose: 32.90%).</p>\n\n",
                "matched_terms": [
                    "both",
                    "two"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Through follow-up interviews with the subjects who participated in our subjective evaluation, we gathered additional qualitative insights. Participants consistently reported that the synthesized samples, both before and after post-training, demonstrated excellent speaker similarity, closely matching the reference speaker&#8217;s timbre and style. In most cases, participants found it challenging to distinguish any significant differences in similarity, leading them to prefer selecting &#8220;Tie&#8221;. For example, in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6.sf2\" title=\"Figure 6(b) &#8227; Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6(b)</span></a>, all four methods have the highest &#8220;Tie&#8221; proportion, each exceeding 40%. This demonstrates that post-training methods centered on naturalness (SpeechJudge-based) or intelligibility (INTP-based) are not yet fully aligned with speaker similarity, which requires further research into speaker similarity alignment.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "each",
                    "both",
                    "them",
                    "after"
                ]
            }
        ]
    },
    "S4.T2": {
        "source_file": "SpeechJudge: Towards Human-Level Judgment for Speech Naturalness",
        "caption": "Table 2: Accuracy of speech naturalness judgment across different models on SpeechJudge-Eval.",
        "body": "Model\nRegular\nExpressive\nTotal\n\n\nObjective Metrics\n\n\nWER\n59.3\n57.0\n57.9\n\n\nSIM\n47.5\n42.5\n44.5\n\n\nFAD\n50.3\n47.5\n48.6\n\n\nMOS Predictor\n\n\nDNSMOS\n61.0\n55.8\n57.9\n\n\nUTMOS\n54.0\n53.5\n53.7\n\n\nContent Enjoyment (CE)\n69.3\n55.2\n60.8\n\n\nContent Usefulness (CU)\n61.3\n54.7\n57.3\n\n\nProduction Complexity (PC)\n39.3\n48.7\n44.9\n\n\nProduction Quality (PQ)\n61.3\n54.3\n57.1\n\n\nDeepfake Detectors\n\n\nAASIST\n40.5\n50.8\n46.7\n\n\nADV\n35.3\n40.3\n38.3\n\n\nAudioLLMs (Open-source)\n\n\nPhi-4-Multimodal\n54.8\n58.5\n57.0\n\n\nQwen2.5-Omni-7B\n62.0\n59.7\n60.6\n\n\nKimi-Audio-7B-Instruct\n65.5\n68.0\n67.0\n\n\nGemma-3n-E4B-it\n49.0\n47.7\n48.2\n\n\nVoxtral-Mini-3B-2507\n60.0\n53.3\n56.0\n\n\nMiDashengLM\n58.8\n63.5\n61.6\n\n\nMiMo-Audio-7B-Instruct\n61.3\n49.3\n54.1\n\n\nAudioLLMs (Closed-source)\n\n\nGemini-2.5-Flash\n73.5\n66.2\n69.1\n\n\nGemini-2.5-Pro\n73.0\n62.2\n66.5\n\n\nGPT-4o mini Audio\n56.3\n46.7\n50.5\n\n\nGPT-4o Audio\n71.5\n64.7\n67.4",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Regular</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Expressive</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Total</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6E6E6;\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"--ltx-bg-color:#E6E6E6;\">Objective Metrics</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">WER</td>\n<td class=\"ltx_td ltx_align_center\">59.3</td>\n<td class=\"ltx_td ltx_align_center\">57.0</td>\n<td class=\"ltx_td ltx_align_center\">57.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">SIM</td>\n<td class=\"ltx_td ltx_align_center\">47.5</td>\n<td class=\"ltx_td ltx_align_center\">42.5</td>\n<td class=\"ltx_td ltx_align_center\">44.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">FAD</td>\n<td class=\"ltx_td ltx_align_center\">50.3</td>\n<td class=\"ltx_td ltx_align_center\">47.5</td>\n<td class=\"ltx_td ltx_align_center\">48.6</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6E6E6;\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"--ltx-bg-color:#E6E6E6;\">MOS Predictor</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">DNSMOS</td>\n<td class=\"ltx_td ltx_align_center\">61.0</td>\n<td class=\"ltx_td ltx_align_center\">55.8</td>\n<td class=\"ltx_td ltx_align_center\">57.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">UTMOS</td>\n<td class=\"ltx_td ltx_align_center\">54.0</td>\n<td class=\"ltx_td ltx_align_center\">53.5</td>\n<td class=\"ltx_td ltx_align_center\">53.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Content Enjoyment (CE)</td>\n<td class=\"ltx_td ltx_align_center\">69.3</td>\n<td class=\"ltx_td ltx_align_center\">55.2</td>\n<td class=\"ltx_td ltx_align_center\">60.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Content Usefulness (CU)</td>\n<td class=\"ltx_td ltx_align_center\">61.3</td>\n<td class=\"ltx_td ltx_align_center\">54.7</td>\n<td class=\"ltx_td ltx_align_center\">57.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Production Complexity (PC)</td>\n<td class=\"ltx_td ltx_align_center\">39.3</td>\n<td class=\"ltx_td ltx_align_center\">48.7</td>\n<td class=\"ltx_td ltx_align_center\">44.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Production Quality (PQ)</td>\n<td class=\"ltx_td ltx_align_center\">61.3</td>\n<td class=\"ltx_td ltx_align_center\">54.3</td>\n<td class=\"ltx_td ltx_align_center\">57.1</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6E6E6;\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"--ltx-bg-color:#E6E6E6;\">Deepfake Detectors</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">AASIST</td>\n<td class=\"ltx_td ltx_align_center\">40.5</td>\n<td class=\"ltx_td ltx_align_center\">50.8</td>\n<td class=\"ltx_td ltx_align_center\">46.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">ADV</td>\n<td class=\"ltx_td ltx_align_center\">35.3</td>\n<td class=\"ltx_td ltx_align_center\">40.3</td>\n<td class=\"ltx_td ltx_align_center\">38.3</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6E6E6;\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"--ltx-bg-color:#E6E6E6;\">AudioLLMs (Open-source)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Phi-4-Multimodal</td>\n<td class=\"ltx_td ltx_align_center\">54.8</td>\n<td class=\"ltx_td ltx_align_center\">58.5</td>\n<td class=\"ltx_td ltx_align_center\">57.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Qwen2.5-Omni-7B</td>\n<td class=\"ltx_td ltx_align_center\">62.0</td>\n<td class=\"ltx_td ltx_align_center\">59.7</td>\n<td class=\"ltx_td ltx_align_center\">60.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Kimi-Audio-7B-Instruct</td>\n<td class=\"ltx_td ltx_align_center\">65.5</td>\n<td class=\"ltx_td ltx_align_center\">68.0</td>\n<td class=\"ltx_td ltx_align_center\">67.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Gemma-3n-E4B-it</td>\n<td class=\"ltx_td ltx_align_center\">49.0</td>\n<td class=\"ltx_td ltx_align_center\">47.7</td>\n<td class=\"ltx_td ltx_align_center\">48.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Voxtral-Mini-3B-2507</td>\n<td class=\"ltx_td ltx_align_center\">60.0</td>\n<td class=\"ltx_td ltx_align_center\">53.3</td>\n<td class=\"ltx_td ltx_align_center\">56.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MiDashengLM</td>\n<td class=\"ltx_td ltx_align_center\">58.8</td>\n<td class=\"ltx_td ltx_align_center\">63.5</td>\n<td class=\"ltx_td ltx_align_center\">61.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">MiMo-Audio-7B-Instruct</td>\n<td class=\"ltx_td ltx_align_center\">61.3</td>\n<td class=\"ltx_td ltx_align_center\">49.3</td>\n<td class=\"ltx_td ltx_align_center\">54.1</td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6E6E6;\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"4\"><span class=\"ltx_text ltx_font_bold ltx_font_italic\" style=\"--ltx-bg-color:#E6E6E6;\">AudioLLMs (Closed-source)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Gemini-2.5-Flash</td>\n<td class=\"ltx_td ltx_align_center\">73.5</td>\n<td class=\"ltx_td ltx_align_center\">66.2</td>\n<td class=\"ltx_td ltx_align_center\">69.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Gemini-2.5-Pro</td>\n<td class=\"ltx_td ltx_align_center\">73.0</td>\n<td class=\"ltx_td ltx_align_center\">62.2</td>\n<td class=\"ltx_td ltx_align_center\">66.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">GPT-4o mini Audio</td>\n<td class=\"ltx_td ltx_align_center\">56.3</td>\n<td class=\"ltx_td ltx_align_center\">46.7</td>\n<td class=\"ltx_td ltx_align_center\">50.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">GPT-4o Audio</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">71.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">64.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">67.4</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "naturalness",
            "regular",
            "quality",
            "complexity",
            "gemma3ne4bit",
            "production",
            "sim",
            "utmos",
            "enjoyment",
            "content",
            "dnsmos",
            "voxtralmini3b2507",
            "adv",
            "gpt4o",
            "objective",
            "speech",
            "across",
            "metrics",
            "gemini25pro",
            "aasist",
            "gemini25flash",
            "wer",
            "audiollms",
            "usefulness",
            "predictor",
            "deepfake",
            "model",
            "mos",
            "closedsource",
            "fad",
            "opensource",
            "midashenglm",
            "accuracy",
            "qwen25omni7b",
            "detectors",
            "mini",
            "models",
            "judgment",
            "different",
            "total",
            "phi4multimodal",
            "expressive",
            "speechjudgeeval",
            "mimoaudio7binstruct",
            "audio",
            "kimiaudio7binstruct"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">The performance of different models on SpeechJudge-Eval is presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T2\" title=\"Table 2 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. A key observation is that speech naturalness judgment is a highly challenging task. The leading model, Gemini-2.5-Flash, still only achieves less than 70% agreement with human preferences. When comparing different models, we find that: (1) common objective metrics and MOS predictors show only a weak correlation with human preferences, often achieving less than 60% accuracy and sometimes performing at the level of a random guess (around 50%). (2) While deepfake detectors are highly effective at distinguishing between machine-generated and human-recorded speech <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib49\" title=\"\">49</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib25\" title=\"\">25</a>]</cite>, their ability to do so is not well-aligned with the naturalness objective when comparing two generated samples. (3) AudioLLMs demonstrate significant potential for this task. While some models, such as Gemma-3n and GPT-4o mini Audio, perform at a chance level, a number of others achieve an accuracy exceeding 60%. This promising performance motivates us to further leverage these AudioLLMs for the design of a reward model for speech naturalness.</p>\n\n",
            "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:80%;\">Our evaluation is conducted on SpeechJudge-Eval (like Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T2\" style=\"font-size:80%;\" title=\"Table 2 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:80%;\">). </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">w/ Voting@10</span>\n  <span class=\"ltx_text\" style=\"font-size:80%;\">: For each prompt, the GRM generates 10 outputs, and we use the majority voting from these 10 outputs as the final result.</span>\n</p>\n\n",
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SFT Stage</span>&#8195;We consider SFT as a &#8220;cold start&#8221; stage to improve the Qwen2.5-Omni&#8217;s instruction-following, reasoning, and speech naturalness understanding capabilities. We select Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>]</cite>&#8212;one of the leading closed-source models on SpeechJudge-Eval (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T2\" title=\"Table 2 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>)&#8212;to serve as a teacher model, and instruct it to generate the CoT data. Specifically, for each sample <math alttext=\"d=(t,a_{1},a_{2},y_{\\mathcal{H}})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo>,</mo><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">d=(t,a_{1},a_{2},y_{\\mathcal{H}})</annotation></semantics></math> from SpeechJudge-Data, we use the CoT prompt from Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T1\" title=\"Table 1 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (denoted as <math alttext=\"\\mathbf{I}_{CoT}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><msub><mi>&#119816;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{I}_{CoT}</annotation></semantics></math>) to instruct Gemini-2.5-Flash to generate a rationale-based output (denoted as <math alttext=\"\\mathbf{O}_{teacher}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m3\" intent=\":literal\"><semantics><msub><mi>&#119822;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{O}_{teacher}</annotation></semantics></math>). We then extract the preference judgment (<math alttext=\"y_{\\mathcal{M}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m4\" intent=\":literal\"><semantics><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi></msub><annotation encoding=\"application/x-tex\">y_{\\mathcal{M}}</annotation></semantics></math>) from this output. For samples where Gemini-2.5-Flash&#8217;s preference is consistent with the human (i.e., <math alttext=\"y_{\\mathcal{M}}=y_{\\mathcal{H}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi></msub><mo>=</mo><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></msub></mrow><annotation encoding=\"application/x-tex\">y_{\\mathcal{M}}=y_{\\mathcal{H}}</annotation></semantics></math>), we concatenate the CoT prompt and the model&#8217;s output, <math alttext=\"[\\mathbf{I}_{CoT},\\mathbf{O}_{teacher}]\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119816;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><mo>,</mo><msub><mi>&#119822;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[\\mathbf{I}_{CoT},\\mathbf{O}_{teacher}]</annotation></semantics></math>, to create a data point for our SFT dataset. Conversely, we consider the sample <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m7\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> a challenging case and reserve the prompt <math alttext=\"\\mathbf{I}_{CoT}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m8\" intent=\":literal\"><semantics><msub><mi>&#119816;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{I}_{CoT}</annotation></semantics></math> for the second-stage RL dataset. During the SFT stage, for each training sample <math alttext=\"[\\mathbf{I}_{CoT},\\mathbf{O}_{teacher}]\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m9\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119816;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><mo>,</mo><msub><mi>&#119822;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[\\mathbf{I}_{CoT},\\mathbf{O}_{teacher}]</annotation></semantics></math>, we perform the next token prediction only on the segment <math alttext=\"\\mathbf{O}_{teacher}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m10\" intent=\":literal\"><semantics><msub><mi>&#119822;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{O}_{teacher}</annotation></semantics></math>.</p>\n\n",
            "<p class=\"ltx_p\">During the evaluation on the SpeechJudge-Eval Benchmark of Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T2\" title=\"Table 2 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we adopt the following protocol for each model:</p>\n\n",
            "<p class=\"ltx_p\">In this study, we investigate whether using the CoT from Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T1\" title=\"Table 1 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> helps AudioLLMs better judge speech naturalness. Interestingly, we find that some closed-source AudioLLMs, such as Gemini-2.5-Flash, improve their performance on SpeechJudge-Eval through this thinking and reasoning process. However, this strategy often does not work for existing open-source AudioLLMs. For example, the results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S10.T5\" title=\"Table 5 &#8227; 10 More Evaluation Results of Existing AudioLLMs &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> show that while Gemini-2.5-Flash consistently improves with the CoT prompt, Kimi-Audio-7B-Instruct, which is already the leading open-source model on SpeechJudge-Eval (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T2\" title=\"Table 2 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), actually sees a decline in performance when using the CoT prompt.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Aligning large generative models with human feedback is a critical challenge. In speech synthesis, this is particularly pronounced due to the lack of a large-scale human preference dataset, which hinders the development of models that truly align with human perception. To address this, we introduce <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SpeechJudge</span>, a comprehensive suite comprising a dataset, a benchmark, and a reward model centered on <span class=\"ltx_text ltx_font_italic\">naturalness</span>&#8212;one of the most fundamental subjective metrics for speech synthesis. First, we present <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SpeechJudge-Data</span>, a large-scale human feedback corpus of 99K speech pairs. The dataset is constructed using a diverse set of advanced zero-shot text-to-speech (TTS) models across diverse speech styles and multiple languages, with human annotations for both intelligibility and naturalness preference. From this, we establish <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SpeechJudge-Eval</span>, a challenging benchmark for speech naturalness judgment. Our evaluation reveals that existing metrics and AudioLLMs struggle with this task; the leading model, Gemini-2.5-Flash, achieves less than 70% agreement with human judgment, highlighting a significant gap for improvement. To bridge this gap, we develop <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SpeechJudge-GRM</span>, a generative reward model (GRM) based on Qwen2.5-Omni-7B. It is trained on SpeechJudge-Data via a two-stage post-training process: Supervised Fine-Tuning (SFT) with Chain-of-Thought rationales followed by Reinforcement Learning (RL) with GRPO on challenging cases. On the SpeechJudge-Eval benchmark, the proposed SpeechJudge-GRM demonstrates superior performance, achieving 77.2% accuracy (and 79.4% after inference-time scaling @10) compared to a classic Bradley-Terry reward model (72.7%). Furthermore, SpeechJudge-GRM can be also employed as a reward function during the post-training of speech generation models to facilitate their alignment with human preferences.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "speech",
                    "qwen25omni7b",
                    "model",
                    "across",
                    "models",
                    "metrics",
                    "judgment",
                    "gemini25flash",
                    "audiollms",
                    "accuracy",
                    "speechjudgeeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The collection and integration of human feedback corpora for model alignment has become a critical stage in the development of modern large-scale generative models, proving indispensable in domains such as text <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib44\" title=\"\">44</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib37\" title=\"\">37</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib3\" title=\"\">3</a>]</cite>, image <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib54\" title=\"\">54</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib30\" title=\"\">30</a>]</cite>, and video generation <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib55\" title=\"\">55</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib33\" title=\"\">33</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the field of speech synthesis, <span class=\"ltx_text ltx_font_bold ltx_font_italic\">naturalness</span> has long been a cornerstone subjective metric for quality assessment <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib28\" title=\"\">28</a>]</cite>, representing one of the most general-purpose indicators of performance <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib47\" title=\"\">47</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib45\" title=\"\">45</a>]</cite>. Prior research has explored automated speech assessment through MOS predictors <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib41\" title=\"\">41</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib21\" title=\"\">21</a>]</cite> and constructed the human feedback corpora for specific attributes like the low-level acoustic quality <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib50\" title=\"\">50</a>]</cite>. However, a large-scale human feedback corpus centered on the holistic quality of naturalness&#8212;and a corresponding reward model trained to capture these preferences&#8212;remains a notably underexplored area. To fill this void, this paper focuses on the dimension of speech naturalness and present a three-part contribution:</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "speech",
                    "model",
                    "quality",
                    "mos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">1. A Large-scale Human Feedback Dataset: <span class=\"ltx_text ltx_font_italic\">SpeechJudge-Data</span>.</span>\nWe recruit human annotators to provide feedback on synthesized speeches, with a focus on assessing two fundamental speech aspects: <span class=\"ltx_text ltx_font_italic\">intelligibility</span> and <span class=\"ltx_text ltx_font_italic\">naturalness</span>. For data synthesis, we employ a diverse set of advanced, open-source zero-shot TTS models with varying architectures (such as CosyVoice2 <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib14\" title=\"\">14</a>]</cite>, Ints <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>]</cite>, F5-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib7\" title=\"\">7</a>]</cite>, and MaskGCT <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib52\" title=\"\">52</a>]</cite>) to produce the compared speech pairs. We prepare speech references in both regular and expressive styles, construct multilingual target texts, and cover both monolingual and cross-lingual synthesis scenarios to ensure data diversity (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.SS1\" title=\"3.1 Dataset Construction &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>). We instruct human annotators to perform two tasks based on a speech pair (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F1\" title=\"Figure 1 &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>): (a) pointwise annotation of text accuracy to assess intelligibility, and (b) pairwise preference annotation to judge relative speech naturalness. This extensive effort, involving 69 labelers over two months, results in 99K annotated pairs, with each pair receiving an average of 2.49 annotations from different labelers. We believe the SpeechJudge-Data can serve as a valuable corpus for alignment research in speech synthesis (e.g., DPO alignment <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib39\" title=\"\">39</a>]</cite> or reward modeling <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib44\" title=\"\">44</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib37\" title=\"\">37</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib3\" title=\"\">3</a>]</cite> in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5\" title=\"5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>).</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "speech",
                    "regular",
                    "models",
                    "different",
                    "opensource",
                    "expressive",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2. An Evaluation Benchmark for Speech Naturalness Judgment: <span class=\"ltx_text ltx_font_italic\">SpeechJudge-Eval</span>.</span>\nWe design a dedicated evaluation benchmark for the task of speech naturalness judgment. The task is structured as follows: given a target text and two corresponding speech samples, a model needs to judge which one is more natural. To construct the evaluation set, we select a subset from the SpeechJudge-Data where human annotators demonstrated high inter-annotator agreement, ensuring a high-quality ground truth. We assess the naturalness judgment capabilities of a wide range of metrics and models, including Word Error Rate (WER) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib38\" title=\"\">38</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib16\" title=\"\">16</a>]</cite>, Fr&#233;chet Audio Distance (FAD) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib27\" title=\"\">27</a>]</cite>, MOS predictors <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib41\" title=\"\">41</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib40\" title=\"\">40</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib48\" title=\"\">48</a>]</cite>, Deepfake Detectors <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib49\" title=\"\">49</a>]</cite>, and AudioLLMs <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib28\" title=\"\">28</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib53\" title=\"\">53</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib22\" title=\"\">22</a>]</cite>. Our evaluations reveal that even the most capable model&#8212;specifically, Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>]</cite> in our experiments&#8212;achieved less than 70% agreement with human preferences. This finding highlights a significant performance gap and underscores the substantial room for research and improvement in automated speech naturalness judgment.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "speech",
                    "model",
                    "detectors",
                    "models",
                    "mos",
                    "metrics",
                    "judgment",
                    "fad",
                    "audiollms",
                    "gemini25flash",
                    "wer",
                    "audio",
                    "deepfake",
                    "speechjudgeeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">3. A Generative Reward Model for Speech Naturalness: <span class=\"ltx_text ltx_font_italic\">SpeechJudge-GRM</span>.</span>\nTo develop a reward model that more effectively captures human preferences, we develop SpeechJudge-GRM, a generative reward model (GRM) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib59\" title=\"\">59</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib34\" title=\"\">34</a>]</cite> trained on the SpeechJudge-Data. Specifically, we base our model on Qwen2.5-Omni-7B <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>]</cite> and design a two-stage post-training process. During the first stage, we perform SFT as the &#8220;cold start&#8221; to improve the model&#8217;s instruction-following and rationale-based reasoning capabilities. To achieve this, we leverage Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>]</cite> to generate Chain-of-Thought (CoT) data for speech naturalness judgment task. In the second stage, we focus on more challenging cases of SpeechJudge-Data, which we define as instances where Gemini-2.5-Flash fails to make the correct judgment. Treating the human-annotated labels as the verifiable reward <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib34\" title=\"\">34</a>]</cite>, we apply the GRPO-based RL stage <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib42\" title=\"\">42</a>]</cite>. Our experiments demonstrate that when trained on the same data, SpeechJudge-GRM significantly outperformed the classic Bradley-Terry reward model (BTRM) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib39\" title=\"\">39</a>]</cite>, achieving a higher accuracy in predicting human preferences (77.2% for SpeechJudge-GRM vs. 72.7% for SpeechJudge-BTRM, Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Methodology &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Besides, SpeechJudge-GRM also supports inference-time scaling and offers explainability through its CoT outputs. Furthermore, SpeechJudge-GRM can also be employed as an objective naturalness metric for sample selection (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F5\" title=\"Figure 5 &#8227; 5.1 Methodology &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) or as a reward function in RL algorithms to enhance the quality of existing speech generation models (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6\" title=\"Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>).</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "speech",
                    "qwen25omni7b",
                    "model",
                    "quality",
                    "models",
                    "judgment",
                    "gemini25flash",
                    "accuracy",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human Alignment for Speech Generation</span>&#8195;Aligning generative models with human feedback has proven crucial, a process also known as RLHF in LLMs <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib37\" title=\"\">37</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib3\" title=\"\">3</a>]</cite>. In the vision domain, many similar human preference datasets exist, such as Pick-a-Pic <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib30\" title=\"\">30</a>]</cite>, ImageReward <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib54\" title=\"\">54</a>]</cite>, and VideoReward <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib33\" title=\"\">33</a>]</cite>. The speech synthesis field, pioneering efforts to construct human corpora involved MOS datasets <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib41\" title=\"\">41</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib21\" title=\"\">21</a>]</cite>. However, these datasets often did not use advanced TTS models for data generation, provided only the pointwise labels rather than the direct pairwise human preference, and were limited in scale. More recently, efforts have focused on building human feedback corpora centered on specific speech attributes, such as low-level acoustic quality <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib50\" title=\"\">50</a>]</cite>, intelligibility <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>]</cite>, or the instruction-following capabilities of spoken dialogue systems <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib17\" title=\"\">17</a>]</cite>. Despite this progress, a large-scale human feedback corpus built specifically around <span class=\"ltx_text ltx_font_italic\">naturalness</span>&#8212;one of the most general-purpose and fundamental metrics for speech synthesis <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib47\" title=\"\">47</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib45\" title=\"\">45</a>]</cite>&#8212;has remained a critical missing piece.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "quality",
                    "models",
                    "mos",
                    "metrics"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AudioLLM as a Judge</span>&#8195;Using LLMs as automated quality evaluators is a prominent topic in the textual LLM field, popularized by the &#8220;LLM-as-a-judge&#8221; paradigm <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib64\" title=\"\">64</a>]</cite>. This approach has recently been extended to the speech domain. A concurrent work, AudioJudge <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib36\" title=\"\">36</a>]</cite>, evaluates the capabilities and limitations of using AudioLLMs for speech quality assessment and paralinguistic understanding by prompt engineering. Furthermore, many studies have focused on fine-tuning AudioLLMs to elicit their understanding capabilities for specific tasks. Examples include discriminating the human-likeness of audio <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib51\" title=\"\">51</a>]</cite>, understanding low-level acoustic qualities <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib50\" title=\"\">50</a>]</cite>, and enhancing the assessment of instruction-following in spoken dialogue systems <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib17\" title=\"\">17</a>]</cite>. However, how to improve the ability of AudioLLMs to understand and judge speech naturalness, and how to use the quality assessment capabilities of AudioLLMs as a reward to improve the post-training of speech generation models themselves, remain significantly underexplored.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "speech",
                    "quality",
                    "models",
                    "audiollms",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work is grounded in <span class=\"ltx_text ltx_font_bold\">SpeechJudge-Data</span>, a large-scale human feedback corpus for assessing the <span class=\"ltx_text ltx_font_italic\">intelligibility</span> and <span class=\"ltx_text ltx_font_italic\">naturalness</span> of synthesized speech. Formally, we aim to construct a dataset <math alttext=\"\\mathcal{D}=\\{(t,a_{1},a_{2})\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{D}=\\{(t,a_{1},a_{2})\\}</annotation></semantics></math>, where each triplet comprises a pair of synthesized speech samples <math alttext=\"(a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{1},a_{2})</annotation></semantics></math> and the corresponding target text <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>. We instruct annotators to provide pointwise intelligibility and pairwise naturalness preference annotations based on <math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math> (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F1\" title=\"Figure 1 &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "speech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a diverse set of recent advanced zero-shot TTS models to prepare the dataset <math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math>. Formally, for each sample <math alttext=\"(t,a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(t,a_{1},a_{2})</annotation></semantics></math>, we denote the synthesized speech <math alttext=\"a_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>a</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">a_{i}</annotation></semantics></math> as being produced by the model <math alttext=\"\\mathcal{M}_{tts}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{M}_{tts}</annotation></semantics></math>, i.e., <math alttext=\"a_{i}\\sim\\mathcal{M}_{tts}(a_{ref},t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>a</mi><mi>i</mi></msub><mo>&#8764;</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">a_{i}\\sim\\mathcal{M}_{tts}(a_{ref},t)</annotation></semantics></math>, where <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math> is the reference speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Model Selection</span>&#8195;For <math alttext=\"\\mathcal{M}_{tts}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p2.m1\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{M}_{tts}</annotation></semantics></math>, we select the following six models of three architectures to enrich the distribution of the synthetic data (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F2.sf1\" title=\"Figure 2(a) &#8227; Figure 2 &#8227; 3.1 Dataset Construction &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2(a)</span></a>): (1) <span class=\"ltx_text ltx_font_bold\">AR-based</span>: ARS <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib52\" title=\"\">52</a>]</cite>, CosyVoice2 <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib14\" title=\"\">14</a>]</cite>, CosyVoice2-INTP <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>]</cite>, and Ints-INTP <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>]</cite>. The latter two are released by <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>]</cite> as intelligibility-enhanced models. (2) <span class=\"ltx_text ltx_font_bold\">FM-based</span>: F5-TTS. (3) <span class=\"ltx_text ltx_font_bold\">MGM-based</span>: MaskGCT <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib52\" title=\"\">52</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "models",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt Construction</span>&#8195;To build diverse prompts <math alttext=\"(a_{ref},t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{ref},t)</annotation></semantics></math> for TTS, for <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math>, we adopt both <span class=\"ltx_text ltx_font_bold\">regular</span> and <span class=\"ltx_text ltx_font_bold\">expressive</span> speech samples. The regular samples are randomly selected from the Emilia-Large dataset <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib19\" title=\"\">19</a>]</cite>. The expressive samples are sourced from corpora rich in paralinguistics, including the emotional corpora: ParaSpeechCaps <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib9\" title=\"\">9</a>]</cite>, the accented corpora: L2-Arctic <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib63\" title=\"\">63</a>]</cite> and KeSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib46\" title=\"\">46</a>]</cite>, the whisper samples from an in-house corpus, and the character voices from video games Genshin Impact <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib43\" title=\"\">43</a>]</cite>. We display the detailed distribution of speech references in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F2.sf2\" title=\"Figure 2(b) &#8227; Figure 2 &#8227; 3.1 Dataset Construction &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "regular",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The target text <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> paired with each <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m2\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math> is constructed as follows: For regular <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m3\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math> samples, we randomly sample transcriptions from the Emilia-Large dataset <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib19\" title=\"\">19</a>]</cite>. These are then refined using DeepSeek-V3 <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib10\" title=\"\">10</a>]</cite> to correct typos and normalize punctuations. For expressive <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m4\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math> samples, we instruct DeepSeek-V3 to generate several scripts in different writing styles, tailored to the topic of <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m5\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math> (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S7.SS1\" title=\"7.1 Details of Prompt Construction &#8227; 7 Details of SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">7.1</span></a> for more details). The languages of the target texts included Chinese (<span class=\"ltx_text ltx_font_italic\">zh</span>), English (<span class=\"ltx_text ltx_font_italic\">en</span>), and Chinese-English code-switching (<span class=\"ltx_text ltx_font_italic\">mixed</span>). For the combinations <math alttext=\"(a_{ref},t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{ref},t)</annotation></semantics></math>, we include both monolingual settings (<span class=\"ltx_text ltx_font_italic\">en2en</span> and <span class=\"ltx_text ltx_font_italic\">zh2zh</span>) and cross-lingual settings (<span class=\"ltx_text ltx_font_italic\">zh2en</span>, <span class=\"ltx_text ltx_font_italic\">en2zh</span>, <span class=\"ltx_text ltx_font_italic\">zh2mixed</span>, and <span class=\"ltx_text ltx_font_italic\">en2mixed</span>), where <span class=\"ltx_text ltx_font_italic\">zh2en</span> denotes Chinese <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m7\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math> with English <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m8\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, and similarly for others. The distribution of the language settings of <math alttext=\"(a_{ref},t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m9\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{ref},t)</annotation></semantics></math> is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F2.sf3\" title=\"Figure 2(c) &#8227; Figure 2 &#8227; 3.1 Dataset Construction &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2(c)</span></a>.</p>\n\n",
                "matched_terms": [
                    "regular",
                    "different",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Pair Construction</span>&#8195;To ensure the diversity of the <math alttext=\"(a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{1},a_{2})</annotation></semantics></math> pairs being compared, we follow <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>]</cite> and adopt both intra-model (i.e.,<math alttext=\"a_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m2\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">a_{1}</annotation></semantics></math> and <math alttext=\"a_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m3\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">a_{2}</annotation></semantics></math> being generated by the same model) and inter-model pairs (i.e., <math alttext=\"a_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m4\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">a_{1}</annotation></semantics></math> and <math alttext=\"a_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m5\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">a_{2}</annotation></semantics></math> being generated by the different models). The distribution of the speech pair is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S7.F7\" title=\"Figure 7 &#8227; 7.1 Details of Prompt Construction &#8227; 7 Details of SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "models",
                    "model",
                    "different"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a sample <math alttext=\"(t,a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(t,a_{1},a_{2})</annotation></semantics></math>, human annotators are instructed to perform both pointwise intelligibility and pairwise naturalness annotations (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F1\" title=\"Figure 1 &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). For intelligibility, annotators perform a binary classification to determine whether the speech (<math alttext=\"a_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">a_{1}</annotation></semantics></math> and <math alttext=\"a_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">a_{2}</annotation></semantics></math>) accurately reads the text <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> without any content insertion, omission, or mispronunciation. For naturalness, they perform a five-scale Comparative Mean Opinion Score (CMOS) annotation to determine which of the two audio clips (<math alttext=\"a_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">a_{1}</annotation></semantics></math> or <math alttext=\"a_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m6\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">a_{2}</annotation></semantics></math>) sounds more natural and human-like. We recruit human annotators and provide them with training. The detailed annotation guidelines are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S8\" title=\"8 Human Annotation Details &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "speech",
                    "content",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Statistics</span>&#8195;We recruit 69 annotators and conduct annotations over two months. The resulting constructed dataset <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math>, which we denote as SpeechJudge-Data (raw), contains 99K <math alttext=\"(t,a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(t,a_{1},a_{2})</annotation></semantics></math> samples, with each sample receiving an average of 2.49 annotations from different labelers. The market value of this annotation scale is estimated at over 500K RMB (about 70K USD). Based on the raw dataset, we also construct several subsets for analysis and reward model training. We provide detailed descriptions of each subset and its applications in the following sections and in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S7.SS2\" title=\"7.2 Subsets of SpeechJudge-Data &#8227; 7 Details of SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "different",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human Agreement Analysis</span>&#8195;We analyze the human annotations for naturalness in this section; discussions regarding intelligibility are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S8.SS2\" title=\"8.2 Intelligibility Annotation Analysis &#8227; 8 Human Annotation Details &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">8.2</span></a>. For naturalness annotations, we evaluate the inter-annotator agreement across our constructed dataset. To simplify the analysis, given the sample <math alttext=\"(t,a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(t,a_{1},a_{2})</annotation></semantics></math>, we transform the five-scale naturalness scale (CMOS) into a ternary classification system: either <math alttext=\"a_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">a_{1}</annotation></semantics></math> is better, <math alttext=\"a_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">a_{2}</annotation></semantics></math> is better, or their quality is a Tie. Based on this simplified classification, we categorize the annotation results into four distinct levels of agreement<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><span class=\"ltx_text ltx_font_bold\">Note</span>: Each sample of SpeechJudge-Data is independently annotated by a minimum of two and a maximum of three annotators (Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S8\" title=\"8 Human Annotation Details &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>).</span></span></span>: (1) <span class=\"ltx_text ltx_font_bold\">Full Agreement (FA)</span>: A consensus is reached among all annotators, with all ratings pointing to the same outcome (e.g., &#8220;2A&#8221;, &#8220;3A&#8221;, &#8220;2B&#8221;, &#8220;3B&#8221;). We use &#8220;2A&#8221; to indicate that two annotators both rated <math alttext=\"a_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m4\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">a_{1}</annotation></semantics></math> as better, while &#8220;3B&#8221; denotes three annotators all rating <math alttext=\"a_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m5\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">a_{2}</annotation></semantics></math> as better. (2) <span class=\"ltx_text ltx_font_bold\">Weak Agreement (WA)</span>: This level captures cases where two annotators agree on a specific polarity, while the third annotator marks a Tie (e.g., &#8220;2A+1T&#8221;, &#8220;2B+1T&#8221;). We also include the &#8220;2T+1A&#8221; and &#8220;2T+1B&#8221; cases in this level. (3) <span class=\"ltx_text ltx_font_bold\">Weak Disagreement (WD)</span>: This occurs when two annotators&#8217; ratings share the same polarity, but the third&#8217;s rating is the opposite (e.g., &#8220;2A+B&#8221;, &#8220;2B+A&#8221;). (4) <span class=\"ltx_text ltx_font_bold\">Full Disagreement (FD)</span>: This represents a complete lack of consensus, where all three annotators provide different classifications, denoted as &#8220;1A+1B+1T&#8221;.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "different",
                    "quality",
                    "across"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Human Annotation &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we demonstrate the distribution of these human agreement levels for the SpeechJudge-Data and its two subsets, <span class=\"ltx_text ltx_font_italic\">regular</span> and <span class=\"ltx_text ltx_font_italic\">expressive</span> (which are defined by their speech references). The figure shows that about 70% of the entire dataset falls into the Full Agreement (51.5%) or Weak Agreement (17.2%) levels. Furthermore, we observe that the <span class=\"ltx_text ltx_font_italic\">expressive</span> subset has a lower agreement level than the <span class=\"ltx_text ltx_font_italic\">regular</span> subset, which suggests that human evaluation of expressive speech generation is inherently a more challenging problem. Besides this sample-level agreement analysis, we also analyze the reliability of individual annotators, and we will discuss this in the Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S8.SS1\" title=\"8.1 Individual Annotator Reliability &#8227; 8 Human Annotation Details &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">8.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "regular",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate speech naturalness, existing studies typically organize their own listening tests, which often have inconsistent settings across different papers <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib28\" title=\"\">28</a>]</cite>. Alternatively, previous researchers use proxy MOS predictors, such as UTMOS <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib41\" title=\"\">41</a>]</cite>, as an objective metric. However, it remains an underexplored problem whether these metrics can accurately judge the naturalness of more advanced speech generation models <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib52\" title=\"\">52</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>]</cite> and align with human preferences. Motivated by this, we construct a benchmark, <span class=\"ltx_text ltx_font_bold\">SpeechJudge-Eval</span>, specifically for the speech naturalness judgment task.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "speech",
                    "across",
                    "models",
                    "mos",
                    "metrics",
                    "judgment",
                    "different",
                    "utmos",
                    "speechjudgeeval",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:80%;\">We use the protocols of Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T1\" style=\"font-size:80%;\" title=\"Table 1 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:80%;\"> to establish judgment rules for different models. The results of AudioLLMs here are obtained using the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">plain</span>\n  <span class=\"ltx_text\" style=\"font-size:80%;\"> prompt of Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T1\" style=\"font-size:80%;\" title=\"Table 1 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:80%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "models",
                    "judgment",
                    "different",
                    "audiollms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task Formulation</span>&#8195;We formulate the naturalness judgment task as a pairwise comparison, specifically a <span class=\"ltx_text ltx_font_italic\">win-or-lose</span> binary classification task: Given a target text <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> and a corresponding audio pair <math alttext=\"(a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{1},a_{2})</annotation></semantics></math>, a model needs to determine which audio has better naturalness. This results in a binary choice: either <math alttext=\"a_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">a_{1}</annotation></semantics></math> is better or <math alttext=\"a_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">a_{2}</annotation></semantics></math> is better. We use the human answer as the ground truth, and use <span class=\"ltx_text ltx_font_bold\">Accuracy</span> to measure the judgment performance of a model <math alttext=\"\\mathcal{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><annotation encoding=\"application/x-tex\">\\mathcal{M}</annotation></semantics></math> on the evaluation set <math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "model",
                    "judgment",
                    "accuracy",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"|\\mathcal{D}|\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|\\mathcal{D}|</annotation></semantics></math> is the total number of samples in the evaluation set, <math alttext=\"y_{\\mathcal{M}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m8\" intent=\":literal\"><semantics><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi></msub><annotation encoding=\"application/x-tex\">y_{\\mathcal{M}}</annotation></semantics></math> and <math alttext=\"y_{\\mathcal{H}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m9\" intent=\":literal\"><semantics><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></msub><annotation encoding=\"application/x-tex\">y_{\\mathcal{H}}</annotation></semantics></math> represent the answers of the model <math alttext=\"\\mathcal{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m10\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><annotation encoding=\"application/x-tex\">\\mathcal{M}</annotation></semantics></math> and human for the sample <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m11\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math>, respectively. <math alttext=\"\\mathbb{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m12\" intent=\":literal\"><semantics><mi>&#120128;</mi><annotation encoding=\"application/x-tex\">\\mathbb{I}</annotation></semantics></math> is the indicator function.</p>\n\n",
                "matched_terms": [
                    "total",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Data</span>&#8195;We sample a subset of the SpeechJudge-Data to create the evaluation set for SpeechJudge-Eval. Specifically, we first select a subset that contains only <span class=\"ltx_text ltx_font_italic\">preference data</span> (i.e., we filter out samples with the &#8220;Tie&#8221; annotation), and then choose only those with full-agreement-level (FA) samples to ensure a high-quality ground truth. We perform sampling from both the <span class=\"ltx_text ltx_font_italic\">regular</span> and <span class=\"ltx_text ltx_font_italic\">expressive</span> subsets of SpeechJudge-Data and proportionally cover the three target text languages (<span class=\"ltx_text ltx_font_italic\">zh</span>, <span class=\"ltx_text ltx_font_italic\">en</span>, and <span class=\"ltx_text ltx_font_italic\">mixed</span>) within each subset. The final SpeechJudge-Eval dataset consists of 1,000 samples. The construction details of SpeechJudge-Eval and its distribution can be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S7.SS2\" title=\"7.2 Subsets of SpeechJudge-Data &#8227; 7 Details of SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "regular",
                    "expressive",
                    "speechjudgeeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We test the naturalness judgment capability of various models based on SpeechJudge-Eval. We consider four different categories of models, whose evaluation protocols are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T1\" title=\"Table 1 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>:</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "models",
                    "judgment",
                    "different",
                    "speechjudgeeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Objective metrics</span>, such as WER <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib38\" title=\"\">38</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib16\" title=\"\">16</a>]</cite>, SIM <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib6\" title=\"\">6</a>]</cite>, and FAD <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib27\" title=\"\">27</a>]</cite> in audio generation tasks. We assume that a better value of these metrics (e.g., lower for WER and FAD; higher for SIM) indicates better naturalness.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "metrics",
                    "fad",
                    "sim",
                    "wer",
                    "audio",
                    "objective"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">MOS Predictors</span>, including DNSMOS <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib40\" title=\"\">40</a>]</cite>, UTMOS <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib41\" title=\"\">41</a>]</cite>, and predictors from audiobox-aesthetics (CE, CU, PC, and PQ) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib48\" title=\"\">48</a>]</cite>. We assume that a higher MOS score corresponds to better naturalness.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "mos",
                    "dnsmos",
                    "utmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Deepfake detectors</span>, which are typically pre-trained on a binary classification task to predict whether an audio is fake or not <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib49\" title=\"\">49</a>]</cite>. We assume that an audio with a lower fake probability should have better naturalness.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "detectors",
                    "audio",
                    "deepfake"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AudioLLMs</span>, which are employed to test their speech naturalness understanding capabilities in a <span class=\"ltx_text ltx_font_italic\">zero-shot</span> manner<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>We assume that the adopted AudioLLMs have not been directly trained on the speech naturalness judgment task. Their performance on this benchmark is therefore considered a <span class=\"ltx_text ltx_font_italic\">zero-shot</span> capability.</span></span></span>. We include the open-source Phi-4-Multimodal <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib1\" title=\"\">1</a>]</cite>, Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>]</cite>, Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib28\" title=\"\">28</a>]</cite>, Gemma-3n <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib26\" title=\"\">26</a>]</cite>, Voxtral <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib32\" title=\"\">32</a>]</cite>, MiDashengLM <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib12\" title=\"\">12</a>]</cite>, Mimo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib53\" title=\"\">53</a>]</cite>, and the closed-source Gemini-2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>]</cite> and GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib22\" title=\"\">22</a>]</cite>. We use the <span class=\"ltx_text ltx_font_italic\">plain</span> prompt of Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T1\" title=\"Table 1 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> to instruct the model to pairwise score the naturalness of two audios. We use their grading to determine the naturalness preference.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "speech",
                    "model",
                    "closedsource",
                    "judgment",
                    "opensource",
                    "midashenglm",
                    "phi4multimodal",
                    "audiollms",
                    "gpt4o"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the proposed SpeechJudge-Data, we further explore how to train a reward model capable of accurately capturing human preferences. Specifically, we propose <span class=\"ltx_text ltx_font_bold\">SpeechJudge-GRM</span>, where we leverage the inherent audio understanding capabilities of AudioLLMs (specifically, Qwen2.5-Omni-7B <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>]</cite>) to elicit their speech naturalness judgment capability. Compared to the classic BTRM <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib4\" title=\"\">4</a>]</cite>, the key strengths of GRM are its ability to enable Chain-of-Thought (CoT) reasoning and its support for test-time computation via majority voting, which ultimately leads to improved preference judgment performance <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib59\" title=\"\">59</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "speech",
                    "qwen25omni7b",
                    "model",
                    "judgment",
                    "audiollms",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RL Stage</span>&#8195;We treat the annotated human preference as a verifiable reward, and, building on the SFT model, we further trained it using the GRPO algorithm <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib42\" title=\"\">42</a>]</cite>. Specifically, for each sample <math alttext=\"d=(t,a_{1},a_{2},y_{\\mathcal{H}})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo>,</mo><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">d=(t,a_{1},a_{2},y_{\\mathcal{H}})</annotation></semantics></math> in the RL dataset, we adopt the CoT prompt to instruct the policy model to conduct multiple rollouts during each iteration. For the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th rollout, we parse the model&#8217;s preference for <math alttext=\"(a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{1},a_{2})</annotation></semantics></math>, denoted as <math alttext=\"y_{\\mathcal{M}}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m4\" intent=\":literal\"><semantics><msubsup><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">y_{\\mathcal{M}}^{i}</annotation></semantics></math>. Following <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib34\" title=\"\">34</a>]</cite>, we use an accuracy-based rule to calculate the reward: the reward is 1 if <math alttext=\"y_{\\mathcal{M}}^{i}=y_{\\mathcal{H}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m5\" intent=\":literal\"><semantics><mrow><msubsup><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mi>i</mi></msubsup><mo>=</mo><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></msub></mrow><annotation encoding=\"application/x-tex\">y_{\\mathcal{M}}^{i}=y_{\\mathcal{H}}</annotation></semantics></math>, and -1 otherwise. In other words, during the RL stage, we only constrain the model&#8217;s final naturalness judgment to align with human preferences, allowing the model to autonomously optimize its reasoning and rationale generation capabilities.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "judgment",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To verify the effectiveness of SpeechJudge-GRM for naturalness judgment, we evaluate it on the SpeechJudge-Eval benchmark. We develop SpeechJudge-BTRM as a baseline, which utilizes the BTRM paradigm <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib39\" title=\"\">39</a>]</cite> by adding a linear layer on Qwen2.5-Omni-7B (Thinker) to produce a single scalar reward prediction. SpeechJudge-BTRM also uses LoRA fine-tuning and uses the same training data as SpeechJudge-GRM.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "qwen25omni7b",
                    "judgment",
                    "speechjudgeeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the results of Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Methodology &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we can observe that: (1) The SpeechJudge-BTRM achieves a 72.7% agreement with human preferences on SpeechJudge-Eval, a level of performance comparable to the initial development of BTRMs in the textual LLM RLHF field <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib44\" title=\"\">44</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib37\" title=\"\">37</a>]</cite>. (2) After conducting SFT training with the CoT data, the accuracy of SpeechJudge-GRM (SFT) reaches 75.3%. Besides, further RLVR training improves the final model SpeechJudge-GRM (SFT+RL) to an accuracy of 77.2%. (3) Due to the generative nature of the GRM, we can further enhance the accuracy of SpeechJudge-GRM using inference-time scaling. For example, by using majority voting across 10 outputs instead of just one, the accuracy is improved by approximately 2 percentage points (75.3% <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> 77.6%; 77.2% <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> 79.4%). These results collectively verify the effectiveness of our proposed SpeechJudge-GRM for judging speech naturalness.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "speech",
                    "model",
                    "across",
                    "speechjudgeeval",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigate the effect of SpeechJudge-based reward models for high-quality sample selection. We use the hard cases from SeedTTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib2\" title=\"\">2</a>]</cite> and the code-switching cases from Amphion-TTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib60\" title=\"\">60</a>]</cite> as target texts. For each text, we instruct the Qwen2.5-Omni-7B (Talker) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib57\" title=\"\">57</a>]</cite> to generate 100 speeches. We then ask human subjects to compare the best-of-100 output&#8212;as selected by either SpeechJudge-BTRM or SpeechJudge-GRM&#8212;against a randomly sampled output. The evaluation measures the win/lose/tie ratios based on speech naturalness. From Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F5\" title=\"Figure 5 &#8227; 5.1 Methodology &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that the best-of-100 samples selected by both SpeechJudge-BTRM and SpeechJudge-GRM are more likely to outperform a randomly selected sample from the same set. This finding demonstrates the advantage of using the SpeechJudge-Data corpus for training human-aligned reward model. Furthermore, SpeechJudge-GRM exhibits better performance than SpeechJudge-BTRM, which highlights the superiority of the proposed GRM.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "speech",
                    "qwen25omni7b",
                    "model",
                    "models"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigate the effect of using SpeechJudge-GRM as a reward function for post-training of TTS model. Specifically, we develop a new zero-shot TTS model, <span class=\"ltx_text ltx_font_bold\">Qwen2.5-0.5B-TTS</span>, to serve as the base model, which was not involved in the construction of the SpeechJudge-Data. This model is based on Qwen2.5-0.5B <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib57\" title=\"\">57</a>]</cite>, adopts the classic two-stage &#8220;AR+Diffusion&#8221; architecture <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib13\" title=\"\">13</a>]</cite>, uses the speech tokenizer from DualCodec <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib31\" title=\"\">31</a>]</cite>, and is pre-trained on the Emilia dataset <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib19\" title=\"\">19</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on this pre-trained model, we design four comparative methods:\n(1) <span class=\"ltx_text ltx_font_bold\">w/ INTP</span>: We use the intelligibility preference dataset, INTP <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>]</cite>, to perform offline DPO alignment <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib39\" title=\"\">39</a>]</cite>.\n(2) <span class=\"ltx_text ltx_font_bold\">w/ SpeechJudge-Data</span>: We use the SpeechJudge-Data (train) to perform offline DPO alignment.\n(3) <span class=\"ltx_text ltx_font_bold\">w/ SpeechJudge-GRM (offline)</span>: We use SpeechJudge-GRM as an offline preference data annotator. We take all speech pairs from the INTP dataset and re-annotate their preference labels using SpeechJudge-GRM, then perform offline DPO alignment on the resulting data.\n(4) <span class=\"ltx_text ltx_font_bold\">w/ SpeechJudge-GRM (online)</span>: We use SpeechJudge-GRM as a reward function for the online DPO algorithm <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib18\" title=\"\">18</a>]</cite>. The training data consists of only the prompts from INTP (i.e., the target texts and speech references for zero-shot TTS).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use SeedTTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib2\" title=\"\">2</a>]</cite> and Amphion-TTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib60\" title=\"\">60</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib62\" title=\"\">62</a>]</cite> as evaluation sets. We present the objective results (WER and SIM) in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S12.T6\" title=\"Table 6 &#8227; 12.2 Objective Results &#8227; 12 Sample Selection and Post-Training based on SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and the subjective results in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6\" title=\"Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.\nWe observe that both intelligibility and naturalness are enhanced for all the four methods after post-training. Additionally, the post-training method based on SpeechJudge-GRM achieves a greater improvement in naturalness (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6.sf1\" title=\"Figure 6(a) &#8227; Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6(a)</span></a>). Besides, the SpeechJudge-based methods could match or lead to a slight improvement in speaker similarity (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6.sf2\" title=\"Figure 6(b) &#8227; Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6(b)</span></a>).</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "wer",
                    "objective",
                    "sim"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we address the challenge of aligning speech synthesis with human perception of naturalness by introducing SpeechJudge, a suite including a large-scale dataset (SpeechJudge-Data), a challenging benchmark (SpeechJudge-Eval), and a generative reward model (SpeechJudge-GRM). Our benchmark reveals that even top AudioLLMs struggle with naturalness judgment, achieving less than 70% human agreement. In contrast, our proposed SpeechJudge-GRM reaches 77.2% accuracy (up to 79.4% with inference-time scaling @10), outperforming the classic Bradley-Terry reward models (72.7%). We also demonstrate its practical utility as a reward function to effectively enhance TTS model naturalness via post-training. While our primary focus is on naturalness, future work could extend this framework to other subjective attributes like speaker similarity and emotional expressiveness, potentially through multi-objective reward modeling. By releasing our resources, we aim to catalyze research in building more human-aligned speech generation systems.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "speech",
                    "model",
                    "models",
                    "judgment",
                    "audiollms",
                    "accuracy",
                    "speechjudgeeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the target texts paired with the regular speech references, we use DeepSeek-V3 <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib10\" title=\"\">10</a>]</cite> to fix typos and normalize punctuations, the prompt used is listed below.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "regular"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the target texts paired with the expressive speech references, we use DeepSeek-V3 to generate several scripts in different writing styles based on the speech reference&#8217;s text, the prompt used is listed below.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "different",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During our preliminary analysis based on SpeechJudge-Data (pref), we observe that a significant disparity in intelligibility between two speech samples can overshadow the subtler quality of naturalness, biasing human preference toward the more comprehensible sample. To mitigate this confounding factor and create a more high-quality dataset focused specifically on naturalness, we further refine the data. Specifically, we retain only pairs where the absolute WER gap of those is below 12%. This process results in the 44K-pair high-quality <span class=\"ltx_text ltx_font_bold\">SpeechJudge-Data (hq)</span> subset, ensuring that its preference labels are more reflective of genuine differences in naturalness.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "speech",
                    "quality",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From SpeechJudge-Data (hq), we construct our benchmark, <span class=\"ltx_text ltx_font_bold\">SpeechJudge-Eval</span>, by applying stratified sampling to FA-level pairs, resulting in 1,000 pairs; its composition is detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S7.T4\" title=\"Table 4 &#8227; 7.2 Subsets of SpeechJudge-Data &#8227; 7 Details of SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. Similarly, we use the same strategy to construct a validation set of the same size, <span class=\"ltx_text ltx_font_bold\">SpeechJudge-Data (dev)</span>. The remaining 42K pairs, <span class=\"ltx_text ltx_font_bold\">SpeechJudge-Data (train)</span>, constitute the training set for our reward models.</p>\n\n",
                "matched_terms": [
                    "models",
                    "speechjudgeeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We provide a detailed analysis of the relationship between the mostly common used objective intelligibility metric, Word Error Rate (WER), and the subjective human judgments of intelligibility. Our goal is to determine the extent to which WER can serve as a reliable proxy for human perception.</p>\n\n",
                "matched_terms": [
                    "objective",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use all the speech samples from SpeechJudge-Data (raw) for this analysis. We visualize the relationship between WER and the subjective text accuracy in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S8.F10\" title=\"Figure 10 &#8227; 8.2 Intelligibility Annotation Analysis &#8227; 8 Human Annotation Details &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.\nFor the regular speeches (the orange curve), we observe a consistent negative correlation: as the WER increases, its perceived text accuracy steadily declines. For the expressive speeches (the green curve), the similar trend holds for expressive speech when WER is under about 12%. When WER is over the threshold, however, the correlation between WER and the subjective text accuracy weakens significantly. We think this divergence is sourced from that the greater stylistic variations in expressive speech pose a substantial challenge to the robustness of ASR systems compared to the regular samples.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "regular",
                    "expressive",
                    "wer",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AASIST</span> <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib25\" title=\"\">25</a>]</cite>: It is a common baseline model for audio deepfake detection, employs a heterogeneity-aware approach to integrate spectral and temporal sub-graphs. We use a large-scale in-house corpus to train the model.</p>\n\n",
                "matched_terms": [
                    "aasist",
                    "model",
                    "audio",
                    "deepfake"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">ADV</span> <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib49\" title=\"\">49</a>]</cite>: It is a state-of-the-art (SOTA) deepfake detection model built upon the pre-trained <span class=\"ltx_text ltx_font_typewriter\">w2v-bert-2.0<span class=\"ltx_note ltx_role_footnote\" id=\"footnote9\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">9</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">9</span></span><a class=\"ltx_ref ltx_href ltx_font_serif\" href=\"https://huggingface.co/facebook/w2v-bert-2.0\" title=\"\">https://huggingface.co/facebook/w2v-bert-2.0</a></span></span></span></span>. It utilizes a multi-task training approach involving deepfake source tracing to extract robust audio deepfake features. We use the same corpus of AASIST to train the model.</p>\n\n",
                "matched_terms": [
                    "model",
                    "aasist",
                    "adv",
                    "audio",
                    "deepfake"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AudioLLMs</span>: We use the plain prompt of Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T1\" title=\"Table 1 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> to instruct AudioLLMs to pairwise score the naturalness of two audios. For the closed-source models, we use the official API released by Google<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span><a class=\"ltx_ref ltx_href\" href=\"https://ai.google.dev/gemini-api/docs/models\" title=\"\">https://ai.google.dev/gemini-api/docs/models</a></span></span></span> for Gemini and OpenAI<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span><a class=\"ltx_ref ltx_href\" href=\"https://platform.openai.com/docs/models\" title=\"\">https://platform.openai.com/docs/models</a></span></span></span> for GPT. We use the model variants <span class=\"ltx_text ltx_font_typewriter\">gemini-2.5-flash</span>, <span class=\"ltx_text ltx_font_typewriter\">gemini-2.5-pro</span>, <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini-audio-preview-2024-12-17</span>, and <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-audio-preview-2025-06-03</span> for Gemini-2.5-Flash, Gemini-2.5-Pro, GPT-4o mini Audio, and GPT-4o Audio.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "model",
                    "mini",
                    "models",
                    "closedsource",
                    "gemini25pro",
                    "gemini25flash",
                    "audiollms",
                    "gpt4o",
                    "audio"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on our preliminary qualitative analysis, we believe the reason why the open-source AudioLLMs do not work well with the CoT prompt is that their foundational capabilities are relatively weak. These weaknesses include instruction-following (such as format-following), multiple-audio understanding, long-text generation, and reasoning abilities. This is also why, when we developed SpeechJudge-GRM, we did not directly apply RLVR on top of Qwen2.5-Omni-7B. Instead, we used an initial SFT stage as a cold start.</p>\n\n",
                "matched_terms": [
                    "opensource",
                    "qwen25omni7b",
                    "audiollms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SFT Stage</span>&#8195;We use Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>]</cite> to generate the CoT data for SpeechJudge-Data (train). For the total 42K samples, Gemini-2.5-Flash&#8217;s judgments agree with human feedback on 25K samples, while they disagree on 17K samples. During the SFT stage, we fine-tune Qwen2.5-Omni-7B (Thinker) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>]</cite> on the 25K CoT data using LoRA <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib20\" title=\"\">20</a>]</cite> with a rank of 128. We use Adam <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib29\" title=\"\">29</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib35\" title=\"\">35</a>]</cite> as the optimizer and set the learning rate to 5e-5. The maximum number of tokens per batch is 4000. We select the best checkpoint on SpeechJudge-Data (dev) as the SFT model, SpeechJudge-GRM (SFT).</p>\n\n",
                "matched_terms": [
                    "total",
                    "qwen25omni7b",
                    "model",
                    "gemini25flash"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the construction of SpeechJudge-Data, we hired human labelers from a data crowdsourcing company. To verify the effectiveness of our training for them and to ensure the high quality of both the dataset and the resulting SpeechJudge-GRM, the human subjects for the final sample selection and TTS post-training experiments (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.SS3\" title=\"5.3 High-Quality Sample Selection based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.SS4\" title=\"5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>) were all experienced speech generation researchers. All these researchers had extensive audio backgrounds, with a minimum of two years of experience in speech synthesis.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "audio",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We randomly selected the subjective evaluation samples from both SeedTTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib2\" title=\"\">2</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote13\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">13</sup><span class=\"ltx_tag ltx_tag_note\">13</span><a class=\"ltx_ref ltx_href\" href=\"https://github.com/BytedanceSpeech/seed-tts-eval\" title=\"\">https://github.com/BytedanceSpeech/seed-tts-eval</a></span></span></span> and Amphion-TTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib62\" title=\"\">62</a>]</cite><span class=\"ltx_note ltx_role_footnote\" id=\"footnote14\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">14</sup><span class=\"ltx_tag ltx_tag_note\">14</span><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/datasets/amphion/Amphion-TTS-Eval\" title=\"\">https://huggingface.co/datasets/amphion/Amphion-TTS-Eval</a></span></span></span>. The evaluation set for each system in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6\" title=\"Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> consists of 70 samples, while the set for each system in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F5\" title=\"Figure 5 &#8227; 5.1 Methodology &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> contains 100 samples. Each audio sample in these evaluations received at least three independent ratings. These subjective evaluation results show that the annotation quality of SpeechJudge-Data largely aligns with the judgments of professional researchers.</p>\n\n",
                "matched_terms": [
                    "audio",
                    "quality"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We present the objective results (WER and SIM) of the Qwen2.5-0.5B-TTS post-training in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S12.T6\" title=\"Table 6 &#8227; 12.2 Objective Results &#8227; 12 Sample Selection and Post-Training based on SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. The results show that all four post-training methods significantly improve the WER. This trend is similar to the subjective intelligibility results shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6.sf1\" title=\"Figure 6(a) &#8227; Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6(a)</span></a>.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "objective",
                    "sim"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding the SIM metric, both w/ INTP and w/ SpeechJudge-GRM (offline) either match or slightly outperform the baseline model, while the other two methods show a slight decline. However, the objective SIM results appear to be in slight conflict with the subjective speaker similarity results in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6.sf2\" title=\"Figure 6(b) &#8227; Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6(b)</span></a>. For instance, in the subjective evaluation, w/ INTP actually shows a decrease in speaker similarity (Win: 24.30%, Lose: 32.90%).</p>\n\n",
                "matched_terms": [
                    "model",
                    "objective",
                    "sim"
                ]
            }
        ]
    },
    "S5.T3": {
        "source_file": "SpeechJudge: Towards Human-Level Judgment for Speech Naturalness",
        "caption": "Table 3: Accuracy of speech naturalness judgment of SpeechJudge-GRM.",
        "body": "Model\nRegular\nExpressive\nTotal\n\n\nQwen2.5-Omni-7B\n62.0\n59.7\n60.6\n\n\nGemini-2.5-Flash\n73.5\n66.2\n69.1\n\n\nSpeechJudge-BTRM\n77.5\n69.5\n72.7\n\n\nSpeechJudge-GRM (SFT)\n77.8\n73.7\n75.3\n\n\nw/ Voting@10\n77.4\n77.6\n77.6\n\n\nSpeechJudge-GRM (SFT+RL)\n79.0\n76.0\n77.2\n\n\nw/ Voting@10\n80.5\n78.7\n79.4",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Regular</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Expressive</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Total</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Qwen2.5-Omni-7B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">62.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">59.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">60.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">Gemini-2.5-Flash</td>\n<td class=\"ltx_td ltx_align_center\">73.5</td>\n<td class=\"ltx_td ltx_align_center\">66.2</td>\n<td class=\"ltx_td ltx_align_center\">69.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">SpeechJudge-BTRM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">77.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">69.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">72.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">SpeechJudge-GRM (SFT)</td>\n<td class=\"ltx_td ltx_align_center\">77.8</td>\n<td class=\"ltx_td ltx_align_center\">73.7</td>\n<td class=\"ltx_td ltx_align_center\">75.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\">w/ Voting@10</td>\n<td class=\"ltx_td ltx_align_center\">77.4</td>\n<td class=\"ltx_td ltx_align_center\">77.6</td>\n<td class=\"ltx_td ltx_align_center\">77.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">SpeechJudge-GRM (SFT+RL)</td>\n<td class=\"ltx_td ltx_align_center\">79.0</td>\n<td class=\"ltx_td ltx_align_center\">76.0</td>\n<td class=\"ltx_td ltx_align_center\">77.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">w/ Voting@10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">80.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">78.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">79.4</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "speech",
            "naturalness",
            "regular",
            "qwen25omni7b",
            "model",
            "voting10",
            "sftrl",
            "judgment",
            "total",
            "sft",
            "expressive",
            "gemini25flash",
            "speechjudgegrm",
            "accuracy",
            "speechjudgebtrm"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">3. A Generative Reward Model for Speech Naturalness: <span class=\"ltx_text ltx_font_italic\">SpeechJudge-GRM</span>.</span>\nTo develop a reward model that more effectively captures human preferences, we develop SpeechJudge-GRM, a generative reward model (GRM) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib59\" title=\"\">59</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib34\" title=\"\">34</a>]</cite> trained on the SpeechJudge-Data. Specifically, we base our model on Qwen2.5-Omni-7B <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>]</cite> and design a two-stage post-training process. During the first stage, we perform SFT as the &#8220;cold start&#8221; to improve the model&#8217;s instruction-following and rationale-based reasoning capabilities. To achieve this, we leverage Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>]</cite> to generate Chain-of-Thought (CoT) data for speech naturalness judgment task. In the second stage, we focus on more challenging cases of SpeechJudge-Data, which we define as instances where Gemini-2.5-Flash fails to make the correct judgment. Treating the human-annotated labels as the verifiable reward <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib34\" title=\"\">34</a>]</cite>, we apply the GRPO-based RL stage <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib42\" title=\"\">42</a>]</cite>. Our experiments demonstrate that when trained on the same data, SpeechJudge-GRM significantly outperformed the classic Bradley-Terry reward model (BTRM) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib39\" title=\"\">39</a>]</cite>, achieving a higher accuracy in predicting human preferences (77.2% for SpeechJudge-GRM vs. 72.7% for SpeechJudge-BTRM, Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Methodology &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Besides, SpeechJudge-GRM also supports inference-time scaling and offers explainability through its CoT outputs. Furthermore, SpeechJudge-GRM can also be employed as an objective naturalness metric for sample selection (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F5\" title=\"Figure 5 &#8227; 5.1 Methodology &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) or as a reward function in RL algorithms to enhance the quality of existing speech generation models (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6\" title=\"Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>).</p>\n\n",
            "<p class=\"ltx_p\">From the results of Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Methodology &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we can observe that: (1) The SpeechJudge-BTRM achieves a 72.7% agreement with human preferences on SpeechJudge-Eval, a level of performance comparable to the initial development of BTRMs in the textual LLM RLHF field <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib44\" title=\"\">44</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib37\" title=\"\">37</a>]</cite>. (2) After conducting SFT training with the CoT data, the accuracy of SpeechJudge-GRM (SFT) reaches 75.3%. Besides, further RLVR training improves the final model SpeechJudge-GRM (SFT+RL) to an accuracy of 77.2%. (3) Due to the generative nature of the GRM, we can further enhance the accuracy of SpeechJudge-GRM using inference-time scaling. For example, by using majority voting across 10 outputs instead of just one, the accuracy is improved by approximately 2 percentage points (75.3% <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> 77.6%; 77.2% <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> 79.4%). These results collectively verify the effectiveness of our proposed SpeechJudge-GRM for judging speech naturalness.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Aligning large generative models with human feedback is a critical challenge. In speech synthesis, this is particularly pronounced due to the lack of a large-scale human preference dataset, which hinders the development of models that truly align with human perception. To address this, we introduce <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SpeechJudge</span>, a comprehensive suite comprising a dataset, a benchmark, and a reward model centered on <span class=\"ltx_text ltx_font_italic\">naturalness</span>&#8212;one of the most fundamental subjective metrics for speech synthesis. First, we present <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SpeechJudge-Data</span>, a large-scale human feedback corpus of 99K speech pairs. The dataset is constructed using a diverse set of advanced zero-shot text-to-speech (TTS) models across diverse speech styles and multiple languages, with human annotations for both intelligibility and naturalness preference. From this, we establish <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SpeechJudge-Eval</span>, a challenging benchmark for speech naturalness judgment. Our evaluation reveals that existing metrics and AudioLLMs struggle with this task; the leading model, Gemini-2.5-Flash, achieves less than 70% agreement with human judgment, highlighting a significant gap for improvement. To bridge this gap, we develop <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SpeechJudge-GRM</span>, a generative reward model (GRM) based on Qwen2.5-Omni-7B. It is trained on SpeechJudge-Data via a two-stage post-training process: Supervised Fine-Tuning (SFT) with Chain-of-Thought rationales followed by Reinforcement Learning (RL) with GRPO on challenging cases. On the SpeechJudge-Eval benchmark, the proposed SpeechJudge-GRM demonstrates superior performance, achieving 77.2% accuracy (and 79.4% after inference-time scaling @10) compared to a classic Bradley-Terry reward model (72.7%). Furthermore, SpeechJudge-GRM can be also employed as a reward function during the post-training of speech generation models to facilitate their alignment with human preferences.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "naturalness",
                    "qwen25omni7b",
                    "model",
                    "judgment",
                    "sft",
                    "gemini25flash",
                    "speechjudgegrm",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the field of speech synthesis, <span class=\"ltx_text ltx_font_bold ltx_font_italic\">naturalness</span> has long been a cornerstone subjective metric for quality assessment <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib28\" title=\"\">28</a>]</cite>, representing one of the most general-purpose indicators of performance <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib47\" title=\"\">47</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib45\" title=\"\">45</a>]</cite>. Prior research has explored automated speech assessment through MOS predictors <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib41\" title=\"\">41</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib21\" title=\"\">21</a>]</cite> and constructed the human feedback corpora for specific attributes like the low-level acoustic quality <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib50\" title=\"\">50</a>]</cite>. However, a large-scale human feedback corpus centered on the holistic quality of naturalness&#8212;and a corresponding reward model trained to capture these preferences&#8212;remains a notably underexplored area. To fill this void, this paper focuses on the dimension of speech naturalness and present a three-part contribution:</p>\n\n",
                "matched_terms": [
                    "speech",
                    "naturalness",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">1. A Large-scale Human Feedback Dataset: <span class=\"ltx_text ltx_font_italic\">SpeechJudge-Data</span>.</span>\nWe recruit human annotators to provide feedback on synthesized speeches, with a focus on assessing two fundamental speech aspects: <span class=\"ltx_text ltx_font_italic\">intelligibility</span> and <span class=\"ltx_text ltx_font_italic\">naturalness</span>. For data synthesis, we employ a diverse set of advanced, open-source zero-shot TTS models with varying architectures (such as CosyVoice2 <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib14\" title=\"\">14</a>]</cite>, Ints <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>]</cite>, F5-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib7\" title=\"\">7</a>]</cite>, and MaskGCT <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib52\" title=\"\">52</a>]</cite>) to produce the compared speech pairs. We prepare speech references in both regular and expressive styles, construct multilingual target texts, and cover both monolingual and cross-lingual synthesis scenarios to ensure data diversity (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.SS1\" title=\"3.1 Dataset Construction &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>). We instruct human annotators to perform two tasks based on a speech pair (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F1\" title=\"Figure 1 &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>): (a) pointwise annotation of text accuracy to assess intelligibility, and (b) pairwise preference annotation to judge relative speech naturalness. This extensive effort, involving 69 labelers over two months, results in 99K annotated pairs, with each pair receiving an average of 2.49 annotations from different labelers. We believe the SpeechJudge-Data can serve as a valuable corpus for alignment research in speech synthesis (e.g., DPO alignment <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib39\" title=\"\">39</a>]</cite> or reward modeling <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib44\" title=\"\">44</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib37\" title=\"\">37</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib3\" title=\"\">3</a>]</cite> in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5\" title=\"5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "naturalness",
                    "regular",
                    "expressive",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2. An Evaluation Benchmark for Speech Naturalness Judgment: <span class=\"ltx_text ltx_font_italic\">SpeechJudge-Eval</span>.</span>\nWe design a dedicated evaluation benchmark for the task of speech naturalness judgment. The task is structured as follows: given a target text and two corresponding speech samples, a model needs to judge which one is more natural. To construct the evaluation set, we select a subset from the SpeechJudge-Data where human annotators demonstrated high inter-annotator agreement, ensuring a high-quality ground truth. We assess the naturalness judgment capabilities of a wide range of metrics and models, including Word Error Rate (WER) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib38\" title=\"\">38</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib16\" title=\"\">16</a>]</cite>, Fr&#233;chet Audio Distance (FAD) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib27\" title=\"\">27</a>]</cite>, MOS predictors <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib41\" title=\"\">41</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib40\" title=\"\">40</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib48\" title=\"\">48</a>]</cite>, Deepfake Detectors <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib49\" title=\"\">49</a>]</cite>, and AudioLLMs <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib28\" title=\"\">28</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib53\" title=\"\">53</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib22\" title=\"\">22</a>]</cite>. Our evaluations reveal that even the most capable model&#8212;specifically, Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>]</cite> in our experiments&#8212;achieved less than 70% agreement with human preferences. This finding highlights a significant performance gap and underscores the substantial room for research and improvement in automated speech naturalness judgment.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "naturalness",
                    "model",
                    "judgment",
                    "gemini25flash"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AudioLLM as a Judge</span>&#8195;Using LLMs as automated quality evaluators is a prominent topic in the textual LLM field, popularized by the &#8220;LLM-as-a-judge&#8221; paradigm <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib64\" title=\"\">64</a>]</cite>. This approach has recently been extended to the speech domain. A concurrent work, AudioJudge <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib36\" title=\"\">36</a>]</cite>, evaluates the capabilities and limitations of using AudioLLMs for speech quality assessment and paralinguistic understanding by prompt engineering. Furthermore, many studies have focused on fine-tuning AudioLLMs to elicit their understanding capabilities for specific tasks. Examples include discriminating the human-likeness of audio <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib51\" title=\"\">51</a>]</cite>, understanding low-level acoustic qualities <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib50\" title=\"\">50</a>]</cite>, and enhancing the assessment of instruction-following in spoken dialogue systems <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib17\" title=\"\">17</a>]</cite>. However, how to improve the ability of AudioLLMs to understand and judge speech naturalness, and how to use the quality assessment capabilities of AudioLLMs as a reward to improve the post-training of speech generation models themselves, remain significantly underexplored.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "naturalness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work is grounded in <span class=\"ltx_text ltx_font_bold\">SpeechJudge-Data</span>, a large-scale human feedback corpus for assessing the <span class=\"ltx_text ltx_font_italic\">intelligibility</span> and <span class=\"ltx_text ltx_font_italic\">naturalness</span> of synthesized speech. Formally, we aim to construct a dataset <math alttext=\"\\mathcal{D}=\\{(t,a_{1},a_{2})\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{D}=\\{(t,a_{1},a_{2})\\}</annotation></semantics></math>, where each triplet comprises a pair of synthesized speech samples <math alttext=\"(a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{1},a_{2})</annotation></semantics></math> and the corresponding target text <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>. We instruct annotators to provide pointwise intelligibility and pairwise naturalness preference annotations based on <math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math> (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F1\" title=\"Figure 1 &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "naturalness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We employ a diverse set of recent advanced zero-shot TTS models to prepare the dataset <math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m1\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math>. Formally, for each sample <math alttext=\"(t,a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(t,a_{1},a_{2})</annotation></semantics></math>, we denote the synthesized speech <math alttext=\"a_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>a</mi><mi>i</mi></msub><annotation encoding=\"application/x-tex\">a_{i}</annotation></semantics></math> as being produced by the model <math alttext=\"\\mathcal{M}_{tts}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathcal{M}_{tts}</annotation></semantics></math>, i.e., <math alttext=\"a_{i}\\sim\\mathcal{M}_{tts}(a_{ref},t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m5\" intent=\":literal\"><semantics><mrow><msub><mi>a</mi><mi>i</mi></msub><mo>&#8764;</mo><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>s</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">a_{i}\\sim\\mathcal{M}_{tts}(a_{ref},t)</annotation></semantics></math>, where <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p1.m6\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math> is the reference speech.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt Construction</span>&#8195;To build diverse prompts <math alttext=\"(a_{ref},t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{ref},t)</annotation></semantics></math> for TTS, for <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math>, we adopt both <span class=\"ltx_text ltx_font_bold\">regular</span> and <span class=\"ltx_text ltx_font_bold\">expressive</span> speech samples. The regular samples are randomly selected from the Emilia-Large dataset <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib19\" title=\"\">19</a>]</cite>. The expressive samples are sourced from corpora rich in paralinguistics, including the emotional corpora: ParaSpeechCaps <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib9\" title=\"\">9</a>]</cite>, the accented corpora: L2-Arctic <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib63\" title=\"\">63</a>]</cite> and KeSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib46\" title=\"\">46</a>]</cite>, the whisper samples from an in-house corpus, and the character voices from video games Genshin Impact <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib43\" title=\"\">43</a>]</cite>. We display the detailed distribution of speech references in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F2.sf2\" title=\"Figure 2(b) &#8227; Figure 2 &#8227; 3.1 Dataset Construction &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "regular",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The target text <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> paired with each <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m2\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math> is constructed as follows: For regular <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m3\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math> samples, we randomly sample transcriptions from the Emilia-Large dataset <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib19\" title=\"\">19</a>]</cite>. These are then refined using DeepSeek-V3 <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib10\" title=\"\">10</a>]</cite> to correct typos and normalize punctuations. For expressive <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m4\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math> samples, we instruct DeepSeek-V3 to generate several scripts in different writing styles, tailored to the topic of <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m5\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math> (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S7.SS1\" title=\"7.1 Details of Prompt Construction &#8227; 7 Details of SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">7.1</span></a> for more details). The languages of the target texts included Chinese (<span class=\"ltx_text ltx_font_italic\">zh</span>), English (<span class=\"ltx_text ltx_font_italic\">en</span>), and Chinese-English code-switching (<span class=\"ltx_text ltx_font_italic\">mixed</span>). For the combinations <math alttext=\"(a_{ref},t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{ref},t)</annotation></semantics></math>, we include both monolingual settings (<span class=\"ltx_text ltx_font_italic\">en2en</span> and <span class=\"ltx_text ltx_font_italic\">zh2zh</span>) and cross-lingual settings (<span class=\"ltx_text ltx_font_italic\">zh2en</span>, <span class=\"ltx_text ltx_font_italic\">en2zh</span>, <span class=\"ltx_text ltx_font_italic\">zh2mixed</span>, and <span class=\"ltx_text ltx_font_italic\">en2mixed</span>), where <span class=\"ltx_text ltx_font_italic\">zh2en</span> denotes Chinese <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m7\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math> with English <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m8\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, and similarly for others. The distribution of the language settings of <math alttext=\"(a_{ref},t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m9\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{ref},t)</annotation></semantics></math> is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F2.sf3\" title=\"Figure 2(c) &#8227; Figure 2 &#8227; 3.1 Dataset Construction &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2(c)</span></a>.</p>\n\n",
                "matched_terms": [
                    "regular",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Speech Pair Construction</span>&#8195;To ensure the diversity of the <math alttext=\"(a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{1},a_{2})</annotation></semantics></math> pairs being compared, we follow <cite class=\"ltx_cite ltx_citemacro_citet\">Zhang et al. [<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>]</cite> and adopt both intra-model (i.e.,<math alttext=\"a_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m2\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">a_{1}</annotation></semantics></math> and <math alttext=\"a_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m3\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">a_{2}</annotation></semantics></math> being generated by the same model) and inter-model pairs (i.e., <math alttext=\"a_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m4\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">a_{1}</annotation></semantics></math> and <math alttext=\"a_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p5.m5\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">a_{2}</annotation></semantics></math> being generated by the different models). The distribution of the speech pair is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S7.F7\" title=\"Figure 7 &#8227; 7.1 Details of Prompt Construction &#8227; 7 Details of SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">7</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a sample <math alttext=\"(t,a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(t,a_{1},a_{2})</annotation></semantics></math>, human annotators are instructed to perform both pointwise intelligibility and pairwise naturalness annotations (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F1\" title=\"Figure 1 &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). For intelligibility, annotators perform a binary classification to determine whether the speech (<math alttext=\"a_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">a_{1}</annotation></semantics></math> and <math alttext=\"a_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">a_{2}</annotation></semantics></math>) accurately reads the text <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> without any content insertion, omission, or mispronunciation. For naturalness, they perform a five-scale Comparative Mean Opinion Score (CMOS) annotation to determine which of the two audio clips (<math alttext=\"a_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">a_{1}</annotation></semantics></math> or <math alttext=\"a_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m6\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">a_{2}</annotation></semantics></math>) sounds more natural and human-like. We recruit human annotators and provide them with training. The detailed annotation guidelines are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S8\" title=\"8 Human Annotation Details &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "naturalness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Human Annotation &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we demonstrate the distribution of these human agreement levels for the SpeechJudge-Data and its two subsets, <span class=\"ltx_text ltx_font_italic\">regular</span> and <span class=\"ltx_text ltx_font_italic\">expressive</span> (which are defined by their speech references). The figure shows that about 70% of the entire dataset falls into the Full Agreement (51.5%) or Weak Agreement (17.2%) levels. Furthermore, we observe that the <span class=\"ltx_text ltx_font_italic\">expressive</span> subset has a lower agreement level than the <span class=\"ltx_text ltx_font_italic\">regular</span> subset, which suggests that human evaluation of expressive speech generation is inherently a more challenging problem. Besides this sample-level agreement analysis, we also analyze the reliability of individual annotators, and we will discuss this in the Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S8.SS1\" title=\"8.1 Individual Annotator Reliability &#8227; 8 Human Annotation Details &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">8.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "regular",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate speech naturalness, existing studies typically organize their own listening tests, which often have inconsistent settings across different papers <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib28\" title=\"\">28</a>]</cite>. Alternatively, previous researchers use proxy MOS predictors, such as UTMOS <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib41\" title=\"\">41</a>]</cite>, as an objective metric. However, it remains an underexplored problem whether these metrics can accurately judge the naturalness of more advanced speech generation models <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib52\" title=\"\">52</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>]</cite> and align with human preferences. Motivated by this, we construct a benchmark, <span class=\"ltx_text ltx_font_bold\">SpeechJudge-Eval</span>, specifically for the speech naturalness judgment task.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "naturalness",
                    "judgment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task Formulation</span>&#8195;We formulate the naturalness judgment task as a pairwise comparison, specifically a <span class=\"ltx_text ltx_font_italic\">win-or-lose</span> binary classification task: Given a target text <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> and a corresponding audio pair <math alttext=\"(a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{1},a_{2})</annotation></semantics></math>, a model needs to determine which audio has better naturalness. This results in a binary choice: either <math alttext=\"a_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">a_{1}</annotation></semantics></math> is better or <math alttext=\"a_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">a_{2}</annotation></semantics></math> is better. We use the human answer as the ground truth, and use <span class=\"ltx_text ltx_font_bold\">Accuracy</span> to measure the judgment performance of a model <math alttext=\"\\mathcal{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><annotation encoding=\"application/x-tex\">\\mathcal{M}</annotation></semantics></math> on the evaluation set <math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "judgment",
                    "model",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"|\\mathcal{D}|\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|\\mathcal{D}|</annotation></semantics></math> is the total number of samples in the evaluation set, <math alttext=\"y_{\\mathcal{M}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m8\" intent=\":literal\"><semantics><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi></msub><annotation encoding=\"application/x-tex\">y_{\\mathcal{M}}</annotation></semantics></math> and <math alttext=\"y_{\\mathcal{H}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m9\" intent=\":literal\"><semantics><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></msub><annotation encoding=\"application/x-tex\">y_{\\mathcal{H}}</annotation></semantics></math> represent the answers of the model <math alttext=\"\\mathcal{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m10\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><annotation encoding=\"application/x-tex\">\\mathcal{M}</annotation></semantics></math> and human for the sample <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m11\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math>, respectively. <math alttext=\"\\mathbb{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m12\" intent=\":literal\"><semantics><mi>&#120128;</mi><annotation encoding=\"application/x-tex\">\\mathbb{I}</annotation></semantics></math> is the indicator function.</p>\n\n",
                "matched_terms": [
                    "total",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Data</span>&#8195;We sample a subset of the SpeechJudge-Data to create the evaluation set for SpeechJudge-Eval. Specifically, we first select a subset that contains only <span class=\"ltx_text ltx_font_italic\">preference data</span> (i.e., we filter out samples with the &#8220;Tie&#8221; annotation), and then choose only those with full-agreement-level (FA) samples to ensure a high-quality ground truth. We perform sampling from both the <span class=\"ltx_text ltx_font_italic\">regular</span> and <span class=\"ltx_text ltx_font_italic\">expressive</span> subsets of SpeechJudge-Data and proportionally cover the three target text languages (<span class=\"ltx_text ltx_font_italic\">zh</span>, <span class=\"ltx_text ltx_font_italic\">en</span>, and <span class=\"ltx_text ltx_font_italic\">mixed</span>) within each subset. The final SpeechJudge-Eval dataset consists of 1,000 samples. The construction details of SpeechJudge-Eval and its distribution can be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S7.SS2\" title=\"7.2 Subsets of SpeechJudge-Data &#8227; 7 Details of SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "regular",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We test the naturalness judgment capability of various models based on SpeechJudge-Eval. We consider four different categories of models, whose evaluation protocols are shown in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T1\" title=\"Table 1 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>:</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "judgment"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AudioLLMs</span>, which are employed to test their speech naturalness understanding capabilities in a <span class=\"ltx_text ltx_font_italic\">zero-shot</span> manner<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>We assume that the adopted AudioLLMs have not been directly trained on the speech naturalness judgment task. Their performance on this benchmark is therefore considered a <span class=\"ltx_text ltx_font_italic\">zero-shot</span> capability.</span></span></span>. We include the open-source Phi-4-Multimodal <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib1\" title=\"\">1</a>]</cite>, Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>]</cite>, Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib28\" title=\"\">28</a>]</cite>, Gemma-3n <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib26\" title=\"\">26</a>]</cite>, Voxtral <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib32\" title=\"\">32</a>]</cite>, MiDashengLM <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib12\" title=\"\">12</a>]</cite>, Mimo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib53\" title=\"\">53</a>]</cite>, and the closed-source Gemini-2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>]</cite> and GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib22\" title=\"\">22</a>]</cite>. We use the <span class=\"ltx_text ltx_font_italic\">plain</span> prompt of Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T1\" title=\"Table 1 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> to instruct the model to pairwise score the naturalness of two audios. We use their grading to determine the naturalness preference.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "naturalness",
                    "judgment",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance of different models on SpeechJudge-Eval is presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T2\" title=\"Table 2 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. A key observation is that speech naturalness judgment is a highly challenging task. The leading model, Gemini-2.5-Flash, still only achieves less than 70% agreement with human preferences. When comparing different models, we find that: (1) common objective metrics and MOS predictors show only a weak correlation with human preferences, often achieving less than 60% accuracy and sometimes performing at the level of a random guess (around 50%). (2) While deepfake detectors are highly effective at distinguishing between machine-generated and human-recorded speech <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib49\" title=\"\">49</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib25\" title=\"\">25</a>]</cite>, their ability to do so is not well-aligned with the naturalness objective when comparing two generated samples. (3) AudioLLMs demonstrate significant potential for this task. While some models, such as Gemma-3n and GPT-4o mini Audio, perform at a chance level, a number of others achieve an accuracy exceeding 60%. This promising performance motivates us to further leverage these AudioLLMs for the design of a reward model for speech naturalness.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "naturalness",
                    "model",
                    "judgment",
                    "gemini25flash",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the proposed SpeechJudge-Data, we further explore how to train a reward model capable of accurately capturing human preferences. Specifically, we propose <span class=\"ltx_text ltx_font_bold\">SpeechJudge-GRM</span>, where we leverage the inherent audio understanding capabilities of AudioLLMs (specifically, Qwen2.5-Omni-7B <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>]</cite>) to elicit their speech naturalness judgment capability. Compared to the classic BTRM <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib4\" title=\"\">4</a>]</cite>, the key strengths of GRM are its ability to enable Chain-of-Thought (CoT) reasoning and its support for test-time computation via majority voting, which ultimately leads to improved preference judgment performance <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib59\" title=\"\">59</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "naturalness",
                    "qwen25omni7b",
                    "model",
                    "judgment",
                    "speechjudgegrm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We develop SpeechJudge-GRM based on Qwen2.5-Omni-7B (Thinker) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>]</cite>. Inspired by the powerful capabilities of RL with the verifiable reward (RLVR) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib42\" title=\"\">42</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib11\" title=\"\">11</a>]</cite>, our natural initial approach is to treat the human preference <math alttext=\"y_{\\mathcal{H}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></msub><annotation encoding=\"application/x-tex\">y_{\\mathcal{H}}</annotation></semantics></math> for the pair <math alttext=\"(a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{1},a_{2})</annotation></semantics></math> as a verifiable reward, and launch a RLVR training based on Qwen2.5-Omni.\nHowever, in practice, we find that the instruction-following reasoning capabilities of Qwen2.5-Omni are very weak (more detailed discussions can be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S10\" title=\"10 More Evaluation Results of Existing AudioLLMs &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>). Therefore, we adopt a two-stage post-training process (&#8220;SFT + RL&#8221;) to develop SpeechJudge-GRM (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F4\" title=\"Figure 4 &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). We describe the details as follows.</p>\n\n",
                "matched_terms": [
                    "qwen25omni7b",
                    "speechjudgegrm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SFT Stage</span>&#8195;We consider SFT as a &#8220;cold start&#8221; stage to improve the Qwen2.5-Omni&#8217;s instruction-following, reasoning, and speech naturalness understanding capabilities. We select Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>]</cite>&#8212;one of the leading closed-source models on SpeechJudge-Eval (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T2\" title=\"Table 2 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>)&#8212;to serve as a teacher model, and instruct it to generate the CoT data. Specifically, for each sample <math alttext=\"d=(t,a_{1},a_{2},y_{\\mathcal{H}})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo>,</mo><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">d=(t,a_{1},a_{2},y_{\\mathcal{H}})</annotation></semantics></math> from SpeechJudge-Data, we use the CoT prompt from Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T1\" title=\"Table 1 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (denoted as <math alttext=\"\\mathbf{I}_{CoT}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><msub><mi>&#119816;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{I}_{CoT}</annotation></semantics></math>) to instruct Gemini-2.5-Flash to generate a rationale-based output (denoted as <math alttext=\"\\mathbf{O}_{teacher}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m3\" intent=\":literal\"><semantics><msub><mi>&#119822;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{O}_{teacher}</annotation></semantics></math>). We then extract the preference judgment (<math alttext=\"y_{\\mathcal{M}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m4\" intent=\":literal\"><semantics><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi></msub><annotation encoding=\"application/x-tex\">y_{\\mathcal{M}}</annotation></semantics></math>) from this output. For samples where Gemini-2.5-Flash&#8217;s preference is consistent with the human (i.e., <math alttext=\"y_{\\mathcal{M}}=y_{\\mathcal{H}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi></msub><mo>=</mo><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></msub></mrow><annotation encoding=\"application/x-tex\">y_{\\mathcal{M}}=y_{\\mathcal{H}}</annotation></semantics></math>), we concatenate the CoT prompt and the model&#8217;s output, <math alttext=\"[\\mathbf{I}_{CoT},\\mathbf{O}_{teacher}]\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119816;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><mo>,</mo><msub><mi>&#119822;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[\\mathbf{I}_{CoT},\\mathbf{O}_{teacher}]</annotation></semantics></math>, to create a data point for our SFT dataset. Conversely, we consider the sample <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m7\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> a challenging case and reserve the prompt <math alttext=\"\\mathbf{I}_{CoT}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m8\" intent=\":literal\"><semantics><msub><mi>&#119816;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{I}_{CoT}</annotation></semantics></math> for the second-stage RL dataset. During the SFT stage, for each training sample <math alttext=\"[\\mathbf{I}_{CoT},\\mathbf{O}_{teacher}]\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m9\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119816;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><mo>,</mo><msub><mi>&#119822;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[\\mathbf{I}_{CoT},\\mathbf{O}_{teacher}]</annotation></semantics></math>, we perform the next token prediction only on the segment <math alttext=\"\\mathbf{O}_{teacher}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m10\" intent=\":literal\"><semantics><msub><mi>&#119822;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{O}_{teacher}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "naturalness",
                    "model",
                    "judgment",
                    "sft",
                    "gemini25flash"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RL Stage</span>&#8195;We treat the annotated human preference as a verifiable reward, and, building on the SFT model, we further trained it using the GRPO algorithm <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib42\" title=\"\">42</a>]</cite>. Specifically, for each sample <math alttext=\"d=(t,a_{1},a_{2},y_{\\mathcal{H}})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo>,</mo><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">d=(t,a_{1},a_{2},y_{\\mathcal{H}})</annotation></semantics></math> in the RL dataset, we adopt the CoT prompt to instruct the policy model to conduct multiple rollouts during each iteration. For the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th rollout, we parse the model&#8217;s preference for <math alttext=\"(a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{1},a_{2})</annotation></semantics></math>, denoted as <math alttext=\"y_{\\mathcal{M}}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m4\" intent=\":literal\"><semantics><msubsup><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">y_{\\mathcal{M}}^{i}</annotation></semantics></math>. Following <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib34\" title=\"\">34</a>]</cite>, we use an accuracy-based rule to calculate the reward: the reward is 1 if <math alttext=\"y_{\\mathcal{M}}^{i}=y_{\\mathcal{H}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m5\" intent=\":literal\"><semantics><mrow><msubsup><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mi>i</mi></msubsup><mo>=</mo><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></msub></mrow><annotation encoding=\"application/x-tex\">y_{\\mathcal{M}}^{i}=y_{\\mathcal{H}}</annotation></semantics></math>, and -1 otherwise. In other words, during the RL stage, we only constrain the model&#8217;s final naturalness judgment to align with human preferences, allowing the model to autonomously optimize its reasoning and rationale generation capabilities.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "judgment",
                    "model",
                    "sft"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We denote the training dataset of SpeechJudge-GRM as SpeechJudge-Data (train). Its construction process is as follows (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S7.SS2\" title=\"7.2 Subsets of SpeechJudge-Data &#8227; 7 Details of SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a> for more details). Based on the raw SpeechJudge-Data, we first filter out all samples at the Full Disagreement (FD) level. For the other samples&#8212;at the FA, WA, and WD levels&#8212;we apply a majority voting principle among annotators to determine the final label for each. We then further exclude samples with a &#8220;Tie&#8221; label, using only the remaining preference data to form the SpeechJudge-Data (train).\nWe use LoRA <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib20\" title=\"\">20</a>]</cite> to fine-tune the GRM during both the SFT and RL stages. Other experimental setup details are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S11\" title=\"11 Training Details of SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "sft",
                    "speechjudgegrm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To verify the effectiveness of SpeechJudge-GRM for naturalness judgment, we evaluate it on the SpeechJudge-Eval benchmark. We develop SpeechJudge-BTRM as a baseline, which utilizes the BTRM paradigm <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib39\" title=\"\">39</a>]</cite> by adding a linear layer on Qwen2.5-Omni-7B (Thinker) to produce a single scalar reward prediction. SpeechJudge-BTRM also uses LoRA fine-tuning and uses the same training data as SpeechJudge-GRM.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "qwen25omni7b",
                    "judgment",
                    "speechjudgegrm",
                    "speechjudgebtrm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigate the effect of SpeechJudge-based reward models for high-quality sample selection. We use the hard cases from SeedTTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib2\" title=\"\">2</a>]</cite> and the code-switching cases from Amphion-TTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib60\" title=\"\">60</a>]</cite> as target texts. For each text, we instruct the Qwen2.5-Omni-7B (Talker) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib57\" title=\"\">57</a>]</cite> to generate 100 speeches. We then ask human subjects to compare the best-of-100 output&#8212;as selected by either SpeechJudge-BTRM or SpeechJudge-GRM&#8212;against a randomly sampled output. The evaluation measures the win/lose/tie ratios based on speech naturalness. From Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F5\" title=\"Figure 5 &#8227; 5.1 Methodology &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that the best-of-100 samples selected by both SpeechJudge-BTRM and SpeechJudge-GRM are more likely to outperform a randomly selected sample from the same set. This finding demonstrates the advantage of using the SpeechJudge-Data corpus for training human-aligned reward model. Furthermore, SpeechJudge-GRM exhibits better performance than SpeechJudge-BTRM, which highlights the superiority of the proposed GRM.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "naturalness",
                    "qwen25omni7b",
                    "model",
                    "speechjudgegrm",
                    "speechjudgebtrm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigate the effect of using SpeechJudge-GRM as a reward function for post-training of TTS model. Specifically, we develop a new zero-shot TTS model, <span class=\"ltx_text ltx_font_bold\">Qwen2.5-0.5B-TTS</span>, to serve as the base model, which was not involved in the construction of the SpeechJudge-Data. This model is based on Qwen2.5-0.5B <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib57\" title=\"\">57</a>]</cite>, adopts the classic two-stage &#8220;AR+Diffusion&#8221; architecture <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib13\" title=\"\">13</a>]</cite>, uses the speech tokenizer from DualCodec <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib31\" title=\"\">31</a>]</cite>, and is pre-trained on the Emilia dataset <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib19\" title=\"\">19</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speechjudgegrm",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on this pre-trained model, we design four comparative methods:\n(1) <span class=\"ltx_text ltx_font_bold\">w/ INTP</span>: We use the intelligibility preference dataset, INTP <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>]</cite>, to perform offline DPO alignment <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib39\" title=\"\">39</a>]</cite>.\n(2) <span class=\"ltx_text ltx_font_bold\">w/ SpeechJudge-Data</span>: We use the SpeechJudge-Data (train) to perform offline DPO alignment.\n(3) <span class=\"ltx_text ltx_font_bold\">w/ SpeechJudge-GRM (offline)</span>: We use SpeechJudge-GRM as an offline preference data annotator. We take all speech pairs from the INTP dataset and re-annotate their preference labels using SpeechJudge-GRM, then perform offline DPO alignment on the resulting data.\n(4) <span class=\"ltx_text ltx_font_bold\">w/ SpeechJudge-GRM (online)</span>: We use SpeechJudge-GRM as a reward function for the online DPO algorithm <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib18\" title=\"\">18</a>]</cite>. The training data consists of only the prompts from INTP (i.e., the target texts and speech references for zero-shot TTS).</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speechjudgegrm",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use SeedTTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib2\" title=\"\">2</a>]</cite> and Amphion-TTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib60\" title=\"\">60</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib62\" title=\"\">62</a>]</cite> as evaluation sets. We present the objective results (WER and SIM) in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S12.T6\" title=\"Table 6 &#8227; 12.2 Objective Results &#8227; 12 Sample Selection and Post-Training based on SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and the subjective results in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6\" title=\"Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.\nWe observe that both intelligibility and naturalness are enhanced for all the four methods after post-training. Additionally, the post-training method based on SpeechJudge-GRM achieves a greater improvement in naturalness (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6.sf1\" title=\"Figure 6(a) &#8227; Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6(a)</span></a>). Besides, the SpeechJudge-based methods could match or lead to a slight improvement in speaker similarity (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6.sf2\" title=\"Figure 6(b) &#8227; Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6(b)</span></a>).</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "speechjudgegrm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we address the challenge of aligning speech synthesis with human perception of naturalness by introducing SpeechJudge, a suite including a large-scale dataset (SpeechJudge-Data), a challenging benchmark (SpeechJudge-Eval), and a generative reward model (SpeechJudge-GRM). Our benchmark reveals that even top AudioLLMs struggle with naturalness judgment, achieving less than 70% human agreement. In contrast, our proposed SpeechJudge-GRM reaches 77.2% accuracy (up to 79.4% with inference-time scaling @10), outperforming the classic Bradley-Terry reward models (72.7%). We also demonstrate its practical utility as a reward function to effectively enhance TTS model naturalness via post-training. While our primary focus is on naturalness, future work could extend this framework to other subjective attributes like speaker similarity and emotional expressiveness, potentially through multi-objective reward modeling. By releasing our resources, we aim to catalyze research in building more human-aligned speech generation systems.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "naturalness",
                    "model",
                    "judgment",
                    "speechjudgegrm",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the target texts paired with the regular speech references, we use DeepSeek-V3 <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib10\" title=\"\">10</a>]</cite> to fix typos and normalize punctuations, the prompt used is listed below.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "regular"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the target texts paired with the expressive speech references, we use DeepSeek-V3 to generate several scripts in different writing styles based on the speech reference&#8217;s text, the prompt used is listed below.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During our preliminary analysis based on SpeechJudge-Data (pref), we observe that a significant disparity in intelligibility between two speech samples can overshadow the subtler quality of naturalness, biasing human preference toward the more comprehensible sample. To mitigate this confounding factor and create a more high-quality dataset focused specifically on naturalness, we further refine the data. Specifically, we retain only pairs where the absolute WER gap of those is below 12%. This process results in the 44K-pair high-quality <span class=\"ltx_text ltx_font_bold\">SpeechJudge-Data (hq)</span> subset, ensuring that its preference labels are more reflective of genuine differences in naturalness.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "naturalness"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use all the speech samples from SpeechJudge-Data (raw) for this analysis. We visualize the relationship between WER and the subjective text accuracy in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S8.F10\" title=\"Figure 10 &#8227; 8.2 Intelligibility Annotation Analysis &#8227; 8 Human Annotation Details &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.\nFor the regular speeches (the orange curve), we observe a consistent negative correlation: as the WER increases, its perceived text accuracy steadily declines. For the expressive speeches (the green curve), the similar trend holds for expressive speech when WER is under about 12%. When WER is over the threshold, however, the correlation between WER and the subjective text accuracy weakens significantly. We think this divergence is sourced from that the greater stylistic variations in expressive speech pose a substantial challenge to the robustness of ASR systems compared to the regular samples.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "regular",
                    "accuracy",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AudioLLMs</span>: We use the plain prompt of Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T1\" title=\"Table 1 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> to instruct AudioLLMs to pairwise score the naturalness of two audios. For the closed-source models, we use the official API released by Google<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span><a class=\"ltx_ref ltx_href\" href=\"https://ai.google.dev/gemini-api/docs/models\" title=\"\">https://ai.google.dev/gemini-api/docs/models</a></span></span></span> for Gemini and OpenAI<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span><a class=\"ltx_ref ltx_href\" href=\"https://platform.openai.com/docs/models\" title=\"\">https://platform.openai.com/docs/models</a></span></span></span> for GPT. We use the model variants <span class=\"ltx_text ltx_font_typewriter\">gemini-2.5-flash</span>, <span class=\"ltx_text ltx_font_typewriter\">gemini-2.5-pro</span>, <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini-audio-preview-2024-12-17</span>, and <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-audio-preview-2025-06-03</span> for Gemini-2.5-Flash, Gemini-2.5-Pro, GPT-4o mini Audio, and GPT-4o Audio.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "model",
                    "gemini25flash"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we investigate whether using the CoT from Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T1\" title=\"Table 1 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> helps AudioLLMs better judge speech naturalness. Interestingly, we find that some closed-source AudioLLMs, such as Gemini-2.5-Flash, improve their performance on SpeechJudge-Eval through this thinking and reasoning process. However, this strategy often does not work for existing open-source AudioLLMs. For example, the results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S10.T5\" title=\"Table 5 &#8227; 10 More Evaluation Results of Existing AudioLLMs &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> show that while Gemini-2.5-Flash consistently improves with the CoT prompt, Kimi-Audio-7B-Instruct, which is already the leading open-source model on SpeechJudge-Eval (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T2\" title=\"Table 2 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), actually sees a decline in performance when using the CoT prompt.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "naturalness",
                    "model",
                    "gemini25flash"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on our preliminary qualitative analysis, we believe the reason why the open-source AudioLLMs do not work well with the CoT prompt is that their foundational capabilities are relatively weak. These weaknesses include instruction-following (such as format-following), multiple-audio understanding, long-text generation, and reasoning abilities. This is also why, when we developed SpeechJudge-GRM, we did not directly apply RLVR on top of Qwen2.5-Omni-7B. Instead, we used an initial SFT stage as a cold start.</p>\n\n",
                "matched_terms": [
                    "sft",
                    "qwen25omni7b",
                    "speechjudgegrm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SFT Stage</span>&#8195;We use Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>]</cite> to generate the CoT data for SpeechJudge-Data (train). For the total 42K samples, Gemini-2.5-Flash&#8217;s judgments agree with human feedback on 25K samples, while they disagree on 17K samples. During the SFT stage, we fine-tune Qwen2.5-Omni-7B (Thinker) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>]</cite> on the 25K CoT data using LoRA <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib20\" title=\"\">20</a>]</cite> with a rank of 128. We use Adam <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib29\" title=\"\">29</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib35\" title=\"\">35</a>]</cite> as the optimizer and set the learning rate to 5e-5. The maximum number of tokens per batch is 4000. We select the best checkpoint on SpeechJudge-Data (dev) as the SFT model, SpeechJudge-GRM (SFT).</p>\n\n",
                "matched_terms": [
                    "qwen25omni7b",
                    "model",
                    "total",
                    "sft",
                    "gemini25flash",
                    "speechjudgegrm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RL Stage</span>&#8195;We use the 17K samples (as described above) to conduct DAPO <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib58\" title=\"\">58</a>]</cite>, which is an enhanced variant of GRPO <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib42\" title=\"\">42</a>]</cite>. We utilize the <span class=\"ltx_text ltx_font_typewriter\">ms-swift<span class=\"ltx_note ltx_role_footnote\" id=\"footnote12\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">12</span></span><a class=\"ltx_ref ltx_href ltx_font_serif\" href=\"https://github.com/modelscope/ms-swift\" title=\"\">https://github.com/modelscope/ms-swift</a></span></span></span></span> toolkit to launch the training process. We initialize the policy model with the SFT model and use LoRA training with a rank of 64. The number of rollouts for each prompt is set to 8, and the batch size is 32. The learning rate is 5e-6. We select the best checkpoint on SpeechJudge-Data (dev) as the final SpeechJudge-GRM model, i.e., SpeechJudge-GRM (SFT+RL).</p>\n\n",
                "matched_terms": [
                    "sft",
                    "sftrl",
                    "speechjudgegrm",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the construction of SpeechJudge-Data, we hired human labelers from a data crowdsourcing company. To verify the effectiveness of our training for them and to ensure the high quality of both the dataset and the resulting SpeechJudge-GRM, the human subjects for the final sample selection and TTS post-training experiments (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.SS3\" title=\"5.3 High-Quality Sample Selection based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.SS4\" title=\"5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>) were all experienced speech generation researchers. All these researchers had extensive audio backgrounds, with a minimum of two years of experience in speech synthesis.</p>\n\n",
                "matched_terms": [
                    "speech",
                    "speechjudgegrm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding the SIM metric, both w/ INTP and w/ SpeechJudge-GRM (offline) either match or slightly outperform the baseline model, while the other two methods show a slight decline. However, the objective SIM results appear to be in slight conflict with the subjective speaker similarity results in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6.sf2\" title=\"Figure 6(b) &#8227; Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6(b)</span></a>. For instance, in the subjective evaluation, w/ INTP actually shows a decrease in speaker similarity (Win: 24.30%, Lose: 32.90%).</p>\n\n",
                "matched_terms": [
                    "speechjudgegrm",
                    "model"
                ]
            }
        ]
    },
    "S5.F6.sf1": {
        "source_file": "SpeechJudge: Towards Human-Level Judgment for Speech Naturalness",
        "caption": "(a) Text Accuracy (T-ACC) and Naturalness CMOS (N-CMOS).",
        "body": "Model\nT-ACC\nN-CMOS\n\n\nQwen2.5-0.5B-TTS\n84.0%\n0.00 ±0.00{}_{\\scriptscriptstyle\\pm\\text{0.00}}\n\n\n\nw/ INTP\n87.0%\n0.18 ±0.07{}_{\\scriptscriptstyle\\pm\\text{0.07}}\n\n\n\nw/ SpeechJudge-Data\n91.0%\n0.16 ±0.08{}_{\\scriptscriptstyle\\pm\\text{0.08}}\n\n\n\nw/ SpeechJudge-GRM (offline)\n91.0%\n0.21 ±0.12{}_{\\scriptscriptstyle\\pm\\text{0.12}}\n\n\n\nw/ SpeechJudge-GRM (online)\n90.0%\n0.25 ±0.09{}_{\\scriptscriptstyle\\pm\\text{0.09}}",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">T-ACC</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">N-CMOS</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Qwen2.5-0.5B-TTS</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">84.0%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.00 <math alttext=\"{}_{\\scriptscriptstyle\\pm\\text{0.00}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.F6.sf1.m1\" intent=\":literal\"><semantics><msub><mi/><mrow><mo mathcolor=\"#FFFFFF\" mathsize=\"0.710em\" style=\"--ltx-fg-color:#FFFFFF;\">&#177;</mo><mtext mathcolor=\"#FFFFFF\" mathsize=\"0.710em\" style=\"--ltx-fg-color:#FFFFFF;\">0.00</mtext></mrow></msub><annotation encoding=\"application/x-tex\">{}_{\\scriptscriptstyle\\pm\\text{0.00}}</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right ltx_border_t\">w/ INTP</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">87.0%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.18 <math alttext=\"{}_{\\scriptscriptstyle\\pm\\text{0.07}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.F6.sf1.m2\" intent=\":literal\"><semantics><msub><mi/><mrow><mo mathsize=\"0.710em\">&#177;</mo><mtext mathsize=\"0.710em\">0.07</mtext></mrow></msub><annotation encoding=\"application/x-tex\">{}_{\\scriptscriptstyle\\pm\\text{0.07}}</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\">w/ SpeechJudge-Data</td>\n<td class=\"ltx_td ltx_align_right\">91.0%</td>\n<td class=\"ltx_td ltx_align_center\">0.16 <math alttext=\"{}_{\\scriptscriptstyle\\pm\\text{0.08}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.F6.sf1.m3\" intent=\":literal\"><semantics><msub><mi/><mrow><mo mathsize=\"0.710em\">&#177;</mo><mtext mathsize=\"0.710em\">0.08</mtext></mrow></msub><annotation encoding=\"application/x-tex\">{}_{\\scriptscriptstyle\\pm\\text{0.08}}</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\">w/ SpeechJudge-GRM (offline)</td>\n<td class=\"ltx_td ltx_align_right\">91.0%</td>\n<td class=\"ltx_td ltx_align_center\">0.21 <math alttext=\"{}_{\\scriptscriptstyle\\pm\\text{0.12}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.F6.sf1.m4\" intent=\":literal\"><semantics><msub><mi/><mrow><mo mathsize=\"0.710em\">&#177;</mo><mtext mathsize=\"0.710em\">0.12</mtext></mrow></msub><annotation encoding=\"application/x-tex\">{}_{\\scriptscriptstyle\\pm\\text{0.12}}</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">w/ SpeechJudge-GRM (online)</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">90.0%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.25 <math alttext=\"{}_{\\scriptscriptstyle\\pm\\text{0.09}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.F6.sf1.m5\" intent=\":literal\"><semantics><msub><mi/><mrow><mo mathsize=\"0.710em\">&#177;</mo><mtext mathsize=\"0.710em\">0.09</mtext></mrow></msub><annotation encoding=\"application/x-tex\">{}_{\\scriptscriptstyle\\pm\\text{0.09}}</annotation></semantics></math>\n</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "naturalness",
            "speechjudgedata",
            "ncmos",
            "offline",
            "model",
            "±008scriptscriptstylepmtext008",
            "qwen2505btts",
            "accuracy",
            "text",
            "±007scriptscriptstylepmtext007",
            "±012scriptscriptstylepmtext012",
            "±009scriptscriptstylepmtext009",
            "online",
            "±000scriptscriptstylepmtext000",
            "intp",
            "speechjudgegrm",
            "cmos",
            "tacc"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We use SeedTTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib2\" title=\"\">2</a>]</cite> and Amphion-TTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib60\" title=\"\">60</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib62\" title=\"\">62</a>]</cite> as evaluation sets. We present the objective results (WER and SIM) in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S12.T6\" title=\"Table 6 &#8227; 12.2 Objective Results &#8227; 12 Sample Selection and Post-Training based on SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and the subjective results in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6\" title=\"Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.\nWe observe that both intelligibility and naturalness are enhanced for all the four methods after post-training. Additionally, the post-training method based on SpeechJudge-GRM achieves a greater improvement in naturalness (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6.sf1\" title=\"Figure 6(a) &#8227; Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6(a)</span></a>). Besides, the SpeechJudge-based methods could match or lead to a slight improvement in speaker similarity (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6.sf2\" title=\"Figure 6(b) &#8227; Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6(b)</span></a>).</p>\n\n",
            "<p class=\"ltx_p\">We present the objective results (WER and SIM) of the Qwen2.5-0.5B-TTS post-training in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S12.T6\" title=\"Table 6 &#8227; 12.2 Objective Results &#8227; 12 Sample Selection and Post-Training based on SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. The results show that all four post-training methods significantly improve the WER. This trend is similar to the subjective intelligibility results shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6.sf1\" title=\"Figure 6(a) &#8227; Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6(a)</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Aligning large generative models with human feedback is a critical challenge. In speech synthesis, this is particularly pronounced due to the lack of a large-scale human preference dataset, which hinders the development of models that truly align with human perception. To address this, we introduce <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SpeechJudge</span>, a comprehensive suite comprising a dataset, a benchmark, and a reward model centered on <span class=\"ltx_text ltx_font_italic\">naturalness</span>&#8212;one of the most fundamental subjective metrics for speech synthesis. First, we present <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SpeechJudge-Data</span>, a large-scale human feedback corpus of 99K speech pairs. The dataset is constructed using a diverse set of advanced zero-shot text-to-speech (TTS) models across diverse speech styles and multiple languages, with human annotations for both intelligibility and naturalness preference. From this, we establish <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SpeechJudge-Eval</span>, a challenging benchmark for speech naturalness judgment. Our evaluation reveals that existing metrics and AudioLLMs struggle with this task; the leading model, Gemini-2.5-Flash, achieves less than 70% agreement with human judgment, highlighting a significant gap for improvement. To bridge this gap, we develop <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SpeechJudge-GRM</span>, a generative reward model (GRM) based on Qwen2.5-Omni-7B. It is trained on SpeechJudge-Data via a two-stage post-training process: Supervised Fine-Tuning (SFT) with Chain-of-Thought rationales followed by Reinforcement Learning (RL) with GRPO on challenging cases. On the SpeechJudge-Eval benchmark, the proposed SpeechJudge-GRM demonstrates superior performance, achieving 77.2% accuracy (and 79.4% after inference-time scaling @10) compared to a classic Bradley-Terry reward model (72.7%). Furthermore, SpeechJudge-GRM can be also employed as a reward function during the post-training of speech generation models to facilitate their alignment with human preferences.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "model",
                    "accuracy",
                    "speechjudgegrm",
                    "speechjudgedata"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The collection and integration of human feedback corpora for model alignment has become a critical stage in the development of modern large-scale generative models, proving indispensable in domains such as text <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib44\" title=\"\">44</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib37\" title=\"\">37</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib3\" title=\"\">3</a>]</cite>, image <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib54\" title=\"\">54</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib30\" title=\"\">30</a>]</cite>, and video generation <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib55\" title=\"\">55</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib33\" title=\"\">33</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the field of speech synthesis, <span class=\"ltx_text ltx_font_bold ltx_font_italic\">naturalness</span> has long been a cornerstone subjective metric for quality assessment <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib28\" title=\"\">28</a>]</cite>, representing one of the most general-purpose indicators of performance <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib47\" title=\"\">47</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib45\" title=\"\">45</a>]</cite>. Prior research has explored automated speech assessment through MOS predictors <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib41\" title=\"\">41</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib21\" title=\"\">21</a>]</cite> and constructed the human feedback corpora for specific attributes like the low-level acoustic quality <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib50\" title=\"\">50</a>]</cite>. However, a large-scale human feedback corpus centered on the holistic quality of naturalness&#8212;and a corresponding reward model trained to capture these preferences&#8212;remains a notably underexplored area. To fill this void, this paper focuses on the dimension of speech naturalness and present a three-part contribution:</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">1. A Large-scale Human Feedback Dataset: <span class=\"ltx_text ltx_font_italic\">SpeechJudge-Data</span>.</span>\nWe recruit human annotators to provide feedback on synthesized speeches, with a focus on assessing two fundamental speech aspects: <span class=\"ltx_text ltx_font_italic\">intelligibility</span> and <span class=\"ltx_text ltx_font_italic\">naturalness</span>. For data synthesis, we employ a diverse set of advanced, open-source zero-shot TTS models with varying architectures (such as CosyVoice2 <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib14\" title=\"\">14</a>]</cite>, Ints <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>]</cite>, F5-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib7\" title=\"\">7</a>]</cite>, and MaskGCT <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib52\" title=\"\">52</a>]</cite>) to produce the compared speech pairs. We prepare speech references in both regular and expressive styles, construct multilingual target texts, and cover both monolingual and cross-lingual synthesis scenarios to ensure data diversity (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.SS1\" title=\"3.1 Dataset Construction &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>). We instruct human annotators to perform two tasks based on a speech pair (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F1\" title=\"Figure 1 &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>): (a) pointwise annotation of text accuracy to assess intelligibility, and (b) pairwise preference annotation to judge relative speech naturalness. This extensive effort, involving 69 labelers over two months, results in 99K annotated pairs, with each pair receiving an average of 2.49 annotations from different labelers. We believe the SpeechJudge-Data can serve as a valuable corpus for alignment research in speech synthesis (e.g., DPO alignment <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib39\" title=\"\">39</a>]</cite> or reward modeling <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib44\" title=\"\">44</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib37\" title=\"\">37</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib3\" title=\"\">3</a>]</cite> in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5\" title=\"5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>).</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "text",
                    "speechjudgedata",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2. An Evaluation Benchmark for Speech Naturalness Judgment: <span class=\"ltx_text ltx_font_italic\">SpeechJudge-Eval</span>.</span>\nWe design a dedicated evaluation benchmark for the task of speech naturalness judgment. The task is structured as follows: given a target text and two corresponding speech samples, a model needs to judge which one is more natural. To construct the evaluation set, we select a subset from the SpeechJudge-Data where human annotators demonstrated high inter-annotator agreement, ensuring a high-quality ground truth. We assess the naturalness judgment capabilities of a wide range of metrics and models, including Word Error Rate (WER) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib38\" title=\"\">38</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib16\" title=\"\">16</a>]</cite>, Fr&#233;chet Audio Distance (FAD) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib27\" title=\"\">27</a>]</cite>, MOS predictors <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib41\" title=\"\">41</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib40\" title=\"\">40</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib48\" title=\"\">48</a>]</cite>, Deepfake Detectors <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib49\" title=\"\">49</a>]</cite>, and AudioLLMs <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib28\" title=\"\">28</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib53\" title=\"\">53</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib22\" title=\"\">22</a>]</cite>. Our evaluations reveal that even the most capable model&#8212;specifically, Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>]</cite> in our experiments&#8212;achieved less than 70% agreement with human preferences. This finding highlights a significant performance gap and underscores the substantial room for research and improvement in automated speech naturalness judgment.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "text",
                    "model",
                    "speechjudgedata"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">3. A Generative Reward Model for Speech Naturalness: <span class=\"ltx_text ltx_font_italic\">SpeechJudge-GRM</span>.</span>\nTo develop a reward model that more effectively captures human preferences, we develop SpeechJudge-GRM, a generative reward model (GRM) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib59\" title=\"\">59</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib34\" title=\"\">34</a>]</cite> trained on the SpeechJudge-Data. Specifically, we base our model on Qwen2.5-Omni-7B <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>]</cite> and design a two-stage post-training process. During the first stage, we perform SFT as the &#8220;cold start&#8221; to improve the model&#8217;s instruction-following and rationale-based reasoning capabilities. To achieve this, we leverage Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>]</cite> to generate Chain-of-Thought (CoT) data for speech naturalness judgment task. In the second stage, we focus on more challenging cases of SpeechJudge-Data, which we define as instances where Gemini-2.5-Flash fails to make the correct judgment. Treating the human-annotated labels as the verifiable reward <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib34\" title=\"\">34</a>]</cite>, we apply the GRPO-based RL stage <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib42\" title=\"\">42</a>]</cite>. Our experiments demonstrate that when trained on the same data, SpeechJudge-GRM significantly outperformed the classic Bradley-Terry reward model (BTRM) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib39\" title=\"\">39</a>]</cite>, achieving a higher accuracy in predicting human preferences (77.2% for SpeechJudge-GRM vs. 72.7% for SpeechJudge-BTRM, Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Methodology &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Besides, SpeechJudge-GRM also supports inference-time scaling and offers explainability through its CoT outputs. Furthermore, SpeechJudge-GRM can also be employed as an objective naturalness metric for sample selection (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F5\" title=\"Figure 5 &#8227; 5.1 Methodology &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) or as a reward function in RL algorithms to enhance the quality of existing speech generation models (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6\" title=\"Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>).</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "model",
                    "accuracy",
                    "speechjudgegrm",
                    "speechjudgedata"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work is grounded in <span class=\"ltx_text ltx_font_bold\">SpeechJudge-Data</span>, a large-scale human feedback corpus for assessing the <span class=\"ltx_text ltx_font_italic\">intelligibility</span> and <span class=\"ltx_text ltx_font_italic\">naturalness</span> of synthesized speech. Formally, we aim to construct a dataset <math alttext=\"\\mathcal{D}=\\{(t,a_{1},a_{2})\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{D}=\\{(t,a_{1},a_{2})\\}</annotation></semantics></math>, where each triplet comprises a pair of synthesized speech samples <math alttext=\"(a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{1},a_{2})</annotation></semantics></math> and the corresponding target text <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>. We instruct annotators to provide pointwise intelligibility and pairwise naturalness preference annotations based on <math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math> (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F1\" title=\"Figure 1 &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "text",
                    "speechjudgedata"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Given a sample <math alttext=\"(t,a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(t,a_{1},a_{2})</annotation></semantics></math>, human annotators are instructed to perform both pointwise intelligibility and pairwise naturalness annotations (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F1\" title=\"Figure 1 &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>). For intelligibility, annotators perform a binary classification to determine whether the speech (<math alttext=\"a_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m2\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">a_{1}</annotation></semantics></math> and <math alttext=\"a_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m3\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">a_{2}</annotation></semantics></math>) accurately reads the text <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m4\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> without any content insertion, omission, or mispronunciation. For naturalness, they perform a five-scale Comparative Mean Opinion Score (CMOS) annotation to determine which of the two audio clips (<math alttext=\"a_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m5\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">a_{1}</annotation></semantics></math> or <math alttext=\"a_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p1.m6\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">a_{2}</annotation></semantics></math>) sounds more natural and human-like. We recruit human annotators and provide them with training. The detailed annotation guidelines are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S8\" title=\"8 Human Annotation Details &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "text",
                    "cmos"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Statistics</span>&#8195;We recruit 69 annotators and conduct annotations over two months. The resulting constructed dataset <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math>, which we denote as SpeechJudge-Data (raw), contains 99K <math alttext=\"(t,a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(t,a_{1},a_{2})</annotation></semantics></math> samples, with each sample receiving an average of 2.49 annotations from different labelers. The market value of this annotation scale is estimated at over 500K RMB (about 70K USD). Based on the raw dataset, we also construct several subsets for analysis and reward model training. We provide detailed descriptions of each subset and its applications in the following sections and in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S7.SS2\" title=\"7.2 Subsets of SpeechJudge-Data &#8227; 7 Details of SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "speechjudgedata"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human Agreement Analysis</span>&#8195;We analyze the human annotations for naturalness in this section; discussions regarding intelligibility are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S8.SS2\" title=\"8.2 Intelligibility Annotation Analysis &#8227; 8 Human Annotation Details &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">8.2</span></a>. For naturalness annotations, we evaluate the inter-annotator agreement across our constructed dataset. To simplify the analysis, given the sample <math alttext=\"(t,a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(t,a_{1},a_{2})</annotation></semantics></math>, we transform the five-scale naturalness scale (CMOS) into a ternary classification system: either <math alttext=\"a_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">a_{1}</annotation></semantics></math> is better, <math alttext=\"a_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">a_{2}</annotation></semantics></math> is better, or their quality is a Tie. Based on this simplified classification, we categorize the annotation results into four distinct levels of agreement<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><span class=\"ltx_text ltx_font_bold\">Note</span>: Each sample of SpeechJudge-Data is independently annotated by a minimum of two and a maximum of three annotators (Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S8\" title=\"8 Human Annotation Details &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>).</span></span></span>: (1) <span class=\"ltx_text ltx_font_bold\">Full Agreement (FA)</span>: A consensus is reached among all annotators, with all ratings pointing to the same outcome (e.g., &#8220;2A&#8221;, &#8220;3A&#8221;, &#8220;2B&#8221;, &#8220;3B&#8221;). We use &#8220;2A&#8221; to indicate that two annotators both rated <math alttext=\"a_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m4\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">a_{1}</annotation></semantics></math> as better, while &#8220;3B&#8221; denotes three annotators all rating <math alttext=\"a_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m5\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">a_{2}</annotation></semantics></math> as better. (2) <span class=\"ltx_text ltx_font_bold\">Weak Agreement (WA)</span>: This level captures cases where two annotators agree on a specific polarity, while the third annotator marks a Tie (e.g., &#8220;2A+1T&#8221;, &#8220;2B+1T&#8221;). We also include the &#8220;2T+1A&#8221; and &#8220;2T+1B&#8221; cases in this level. (3) <span class=\"ltx_text ltx_font_bold\">Weak Disagreement (WD)</span>: This occurs when two annotators&#8217; ratings share the same polarity, but the third&#8217;s rating is the opposite (e.g., &#8220;2A+B&#8221;, &#8220;2B+A&#8221;). (4) <span class=\"ltx_text ltx_font_bold\">Full Disagreement (FD)</span>: This represents a complete lack of consensus, where all three annotators provide different classifications, denoted as &#8220;1A+1B+1T&#8221;.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "cmos",
                    "speechjudgedata"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task Formulation</span>&#8195;We formulate the naturalness judgment task as a pairwise comparison, specifically a <span class=\"ltx_text ltx_font_italic\">win-or-lose</span> binary classification task: Given a target text <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> and a corresponding audio pair <math alttext=\"(a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{1},a_{2})</annotation></semantics></math>, a model needs to determine which audio has better naturalness. This results in a binary choice: either <math alttext=\"a_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">a_{1}</annotation></semantics></math> is better or <math alttext=\"a_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">a_{2}</annotation></semantics></math> is better. We use the human answer as the ground truth, and use <span class=\"ltx_text ltx_font_bold\">Accuracy</span> to measure the judgment performance of a model <math alttext=\"\\mathcal{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><annotation encoding=\"application/x-tex\">\\mathcal{M}</annotation></semantics></math> on the evaluation set <math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "text",
                    "model",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Data</span>&#8195;We sample a subset of the SpeechJudge-Data to create the evaluation set for SpeechJudge-Eval. Specifically, we first select a subset that contains only <span class=\"ltx_text ltx_font_italic\">preference data</span> (i.e., we filter out samples with the &#8220;Tie&#8221; annotation), and then choose only those with full-agreement-level (FA) samples to ensure a high-quality ground truth. We perform sampling from both the <span class=\"ltx_text ltx_font_italic\">regular</span> and <span class=\"ltx_text ltx_font_italic\">expressive</span> subsets of SpeechJudge-Data and proportionally cover the three target text languages (<span class=\"ltx_text ltx_font_italic\">zh</span>, <span class=\"ltx_text ltx_font_italic\">en</span>, and <span class=\"ltx_text ltx_font_italic\">mixed</span>) within each subset. The final SpeechJudge-Eval dataset consists of 1,000 samples. The construction details of SpeechJudge-Eval and its distribution can be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S7.SS2\" title=\"7.2 Subsets of SpeechJudge-Data &#8227; 7 Details of SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speechjudgedata"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AudioLLMs</span>, which are employed to test their speech naturalness understanding capabilities in a <span class=\"ltx_text ltx_font_italic\">zero-shot</span> manner<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>We assume that the adopted AudioLLMs have not been directly trained on the speech naturalness judgment task. Their performance on this benchmark is therefore considered a <span class=\"ltx_text ltx_font_italic\">zero-shot</span> capability.</span></span></span>. We include the open-source Phi-4-Multimodal <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib1\" title=\"\">1</a>]</cite>, Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>]</cite>, Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib28\" title=\"\">28</a>]</cite>, Gemma-3n <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib26\" title=\"\">26</a>]</cite>, Voxtral <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib32\" title=\"\">32</a>]</cite>, MiDashengLM <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib12\" title=\"\">12</a>]</cite>, Mimo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib53\" title=\"\">53</a>]</cite>, and the closed-source Gemini-2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>]</cite> and GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib22\" title=\"\">22</a>]</cite>. We use the <span class=\"ltx_text ltx_font_italic\">plain</span> prompt of Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T1\" title=\"Table 1 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> to instruct the model to pairwise score the naturalness of two audios. We use their grading to determine the naturalness preference.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance of different models on SpeechJudge-Eval is presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T2\" title=\"Table 2 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. A key observation is that speech naturalness judgment is a highly challenging task. The leading model, Gemini-2.5-Flash, still only achieves less than 70% agreement with human preferences. When comparing different models, we find that: (1) common objective metrics and MOS predictors show only a weak correlation with human preferences, often achieving less than 60% accuracy and sometimes performing at the level of a random guess (around 50%). (2) While deepfake detectors are highly effective at distinguishing between machine-generated and human-recorded speech <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib49\" title=\"\">49</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib25\" title=\"\">25</a>]</cite>, their ability to do so is not well-aligned with the naturalness objective when comparing two generated samples. (3) AudioLLMs demonstrate significant potential for this task. While some models, such as Gemma-3n and GPT-4o mini Audio, perform at a chance level, a number of others achieve an accuracy exceeding 60%. This promising performance motivates us to further leverage these AudioLLMs for the design of a reward model for speech naturalness.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "model",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the proposed SpeechJudge-Data, we further explore how to train a reward model capable of accurately capturing human preferences. Specifically, we propose <span class=\"ltx_text ltx_font_bold\">SpeechJudge-GRM</span>, where we leverage the inherent audio understanding capabilities of AudioLLMs (specifically, Qwen2.5-Omni-7B <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>]</cite>) to elicit their speech naturalness judgment capability. Compared to the classic BTRM <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib4\" title=\"\">4</a>]</cite>, the key strengths of GRM are its ability to enable Chain-of-Thought (CoT) reasoning and its support for test-time computation via majority voting, which ultimately leads to improved preference judgment performance <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib59\" title=\"\">59</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "model",
                    "speechjudgedata",
                    "speechjudgegrm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SFT Stage</span>&#8195;We consider SFT as a &#8220;cold start&#8221; stage to improve the Qwen2.5-Omni&#8217;s instruction-following, reasoning, and speech naturalness understanding capabilities. We select Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>]</cite>&#8212;one of the leading closed-source models on SpeechJudge-Eval (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T2\" title=\"Table 2 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>)&#8212;to serve as a teacher model, and instruct it to generate the CoT data. Specifically, for each sample <math alttext=\"d=(t,a_{1},a_{2},y_{\\mathcal{H}})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo>,</mo><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">d=(t,a_{1},a_{2},y_{\\mathcal{H}})</annotation></semantics></math> from SpeechJudge-Data, we use the CoT prompt from Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T1\" title=\"Table 1 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (denoted as <math alttext=\"\\mathbf{I}_{CoT}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><msub><mi>&#119816;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{I}_{CoT}</annotation></semantics></math>) to instruct Gemini-2.5-Flash to generate a rationale-based output (denoted as <math alttext=\"\\mathbf{O}_{teacher}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m3\" intent=\":literal\"><semantics><msub><mi>&#119822;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{O}_{teacher}</annotation></semantics></math>). We then extract the preference judgment (<math alttext=\"y_{\\mathcal{M}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m4\" intent=\":literal\"><semantics><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi></msub><annotation encoding=\"application/x-tex\">y_{\\mathcal{M}}</annotation></semantics></math>) from this output. For samples where Gemini-2.5-Flash&#8217;s preference is consistent with the human (i.e., <math alttext=\"y_{\\mathcal{M}}=y_{\\mathcal{H}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi></msub><mo>=</mo><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></msub></mrow><annotation encoding=\"application/x-tex\">y_{\\mathcal{M}}=y_{\\mathcal{H}}</annotation></semantics></math>), we concatenate the CoT prompt and the model&#8217;s output, <math alttext=\"[\\mathbf{I}_{CoT},\\mathbf{O}_{teacher}]\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119816;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><mo>,</mo><msub><mi>&#119822;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[\\mathbf{I}_{CoT},\\mathbf{O}_{teacher}]</annotation></semantics></math>, to create a data point for our SFT dataset. Conversely, we consider the sample <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m7\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> a challenging case and reserve the prompt <math alttext=\"\\mathbf{I}_{CoT}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m8\" intent=\":literal\"><semantics><msub><mi>&#119816;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{I}_{CoT}</annotation></semantics></math> for the second-stage RL dataset. During the SFT stage, for each training sample <math alttext=\"[\\mathbf{I}_{CoT},\\mathbf{O}_{teacher}]\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m9\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119816;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><mo>,</mo><msub><mi>&#119822;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[\\mathbf{I}_{CoT},\\mathbf{O}_{teacher}]</annotation></semantics></math>, we perform the next token prediction only on the segment <math alttext=\"\\mathbf{O}_{teacher}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m10\" intent=\":literal\"><semantics><msub><mi>&#119822;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{O}_{teacher}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "model",
                    "speechjudgedata"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RL Stage</span>&#8195;We treat the annotated human preference as a verifiable reward, and, building on the SFT model, we further trained it using the GRPO algorithm <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib42\" title=\"\">42</a>]</cite>. Specifically, for each sample <math alttext=\"d=(t,a_{1},a_{2},y_{\\mathcal{H}})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo>,</mo><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">d=(t,a_{1},a_{2},y_{\\mathcal{H}})</annotation></semantics></math> in the RL dataset, we adopt the CoT prompt to instruct the policy model to conduct multiple rollouts during each iteration. For the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th rollout, we parse the model&#8217;s preference for <math alttext=\"(a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{1},a_{2})</annotation></semantics></math>, denoted as <math alttext=\"y_{\\mathcal{M}}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m4\" intent=\":literal\"><semantics><msubsup><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">y_{\\mathcal{M}}^{i}</annotation></semantics></math>. Following <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib34\" title=\"\">34</a>]</cite>, we use an accuracy-based rule to calculate the reward: the reward is 1 if <math alttext=\"y_{\\mathcal{M}}^{i}=y_{\\mathcal{H}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m5\" intent=\":literal\"><semantics><mrow><msubsup><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mi>i</mi></msubsup><mo>=</mo><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></msub></mrow><annotation encoding=\"application/x-tex\">y_{\\mathcal{M}}^{i}=y_{\\mathcal{H}}</annotation></semantics></math>, and -1 otherwise. In other words, during the RL stage, we only constrain the model&#8217;s final naturalness judgment to align with human preferences, allowing the model to autonomously optimize its reasoning and rationale generation capabilities.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We denote the training dataset of SpeechJudge-GRM as SpeechJudge-Data (train). Its construction process is as follows (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S7.SS2\" title=\"7.2 Subsets of SpeechJudge-Data &#8227; 7 Details of SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a> for more details). Based on the raw SpeechJudge-Data, we first filter out all samples at the Full Disagreement (FD) level. For the other samples&#8212;at the FA, WA, and WD levels&#8212;we apply a majority voting principle among annotators to determine the final label for each. We then further exclude samples with a &#8220;Tie&#8221; label, using only the remaining preference data to form the SpeechJudge-Data (train).\nWe use LoRA <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib20\" title=\"\">20</a>]</cite> to fine-tune the GRM during both the SFT and RL stages. Other experimental setup details are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S11\" title=\"11 Training Details of SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "speechjudgegrm",
                    "speechjudgedata"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To verify the effectiveness of SpeechJudge-GRM for naturalness judgment, we evaluate it on the SpeechJudge-Eval benchmark. We develop SpeechJudge-BTRM as a baseline, which utilizes the BTRM paradigm <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib39\" title=\"\">39</a>]</cite> by adding a linear layer on Qwen2.5-Omni-7B (Thinker) to produce a single scalar reward prediction. SpeechJudge-BTRM also uses LoRA fine-tuning and uses the same training data as SpeechJudge-GRM.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "speechjudgegrm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the results of Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Methodology &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we can observe that: (1) The SpeechJudge-BTRM achieves a 72.7% agreement with human preferences on SpeechJudge-Eval, a level of performance comparable to the initial development of BTRMs in the textual LLM RLHF field <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib44\" title=\"\">44</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib37\" title=\"\">37</a>]</cite>. (2) After conducting SFT training with the CoT data, the accuracy of SpeechJudge-GRM (SFT) reaches 75.3%. Besides, further RLVR training improves the final model SpeechJudge-GRM (SFT+RL) to an accuracy of 77.2%. (3) Due to the generative nature of the GRM, we can further enhance the accuracy of SpeechJudge-GRM using inference-time scaling. For example, by using majority voting across 10 outputs instead of just one, the accuracy is improved by approximately 2 percentage points (75.3% <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> 77.6%; 77.2% <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> 79.4%). These results collectively verify the effectiveness of our proposed SpeechJudge-GRM for judging speech naturalness.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "speechjudgegrm",
                    "model",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigate the effect of SpeechJudge-based reward models for high-quality sample selection. We use the hard cases from SeedTTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib2\" title=\"\">2</a>]</cite> and the code-switching cases from Amphion-TTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib60\" title=\"\">60</a>]</cite> as target texts. For each text, we instruct the Qwen2.5-Omni-7B (Talker) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib57\" title=\"\">57</a>]</cite> to generate 100 speeches. We then ask human subjects to compare the best-of-100 output&#8212;as selected by either SpeechJudge-BTRM or SpeechJudge-GRM&#8212;against a randomly sampled output. The evaluation measures the win/lose/tie ratios based on speech naturalness. From Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F5\" title=\"Figure 5 &#8227; 5.1 Methodology &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that the best-of-100 samples selected by both SpeechJudge-BTRM and SpeechJudge-GRM are more likely to outperform a randomly selected sample from the same set. This finding demonstrates the advantage of using the SpeechJudge-Data corpus for training human-aligned reward model. Furthermore, SpeechJudge-GRM exhibits better performance than SpeechJudge-BTRM, which highlights the superiority of the proposed GRM.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "model",
                    "text",
                    "speechjudgegrm",
                    "speechjudgedata"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigate the effect of using SpeechJudge-GRM as a reward function for post-training of TTS model. Specifically, we develop a new zero-shot TTS model, <span class=\"ltx_text ltx_font_bold\">Qwen2.5-0.5B-TTS</span>, to serve as the base model, which was not involved in the construction of the SpeechJudge-Data. This model is based on Qwen2.5-0.5B <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib57\" title=\"\">57</a>]</cite>, adopts the classic two-stage &#8220;AR+Diffusion&#8221; architecture <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib13\" title=\"\">13</a>]</cite>, uses the speech tokenizer from DualCodec <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib31\" title=\"\">31</a>]</cite>, and is pre-trained on the Emilia dataset <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib19\" title=\"\">19</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "speechjudgedata",
                    "speechjudgegrm",
                    "qwen2505btts"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on this pre-trained model, we design four comparative methods:\n(1) <span class=\"ltx_text ltx_font_bold\">w/ INTP</span>: We use the intelligibility preference dataset, INTP <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>]</cite>, to perform offline DPO alignment <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib39\" title=\"\">39</a>]</cite>.\n(2) <span class=\"ltx_text ltx_font_bold\">w/ SpeechJudge-Data</span>: We use the SpeechJudge-Data (train) to perform offline DPO alignment.\n(3) <span class=\"ltx_text ltx_font_bold\">w/ SpeechJudge-GRM (offline)</span>: We use SpeechJudge-GRM as an offline preference data annotator. We take all speech pairs from the INTP dataset and re-annotate their preference labels using SpeechJudge-GRM, then perform offline DPO alignment on the resulting data.\n(4) <span class=\"ltx_text ltx_font_bold\">w/ SpeechJudge-GRM (online)</span>: We use SpeechJudge-GRM as a reward function for the online DPO algorithm <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib18\" title=\"\">18</a>]</cite>. The training data consists of only the prompts from INTP (i.e., the target texts and speech references for zero-shot TTS).</p>\n\n",
                "matched_terms": [
                    "offline",
                    "model",
                    "online",
                    "intp",
                    "speechjudgegrm",
                    "speechjudgedata"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we address the challenge of aligning speech synthesis with human perception of naturalness by introducing SpeechJudge, a suite including a large-scale dataset (SpeechJudge-Data), a challenging benchmark (SpeechJudge-Eval), and a generative reward model (SpeechJudge-GRM). Our benchmark reveals that even top AudioLLMs struggle with naturalness judgment, achieving less than 70% human agreement. In contrast, our proposed SpeechJudge-GRM reaches 77.2% accuracy (up to 79.4% with inference-time scaling @10), outperforming the classic Bradley-Terry reward models (72.7%). We also demonstrate its practical utility as a reward function to effectively enhance TTS model naturalness via post-training. While our primary focus is on naturalness, future work could extend this framework to other subjective attributes like speaker similarity and emotional expressiveness, potentially through multi-objective reward modeling. By releasing our resources, we aim to catalyze research in building more human-aligned speech generation systems.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "model",
                    "accuracy",
                    "speechjudgegrm",
                    "speechjudgedata"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct several subsets based on SpeechJudge-Data (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S7.F8\" title=\"Figure 8 &#8227; 7.2 Subsets of SpeechJudge-Data &#8227; 7 Details of SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>). We begin with the <span class=\"ltx_text ltx_font_bold\">SpeechJudge-Data (raw)</span> corpus, containing 99K pairs, where each pair is annotated by multiple labelers as a five-scale naturalness CMOS. We aggregate these annotations via a majority vote for each pair, and subsequently discard all &#8220;Tie&#8221; pairs, yielding the 79K-pair human <span class=\"ltx_text ltx_font_italic\">preference</span> data, denoted as <span class=\"ltx_text ltx_font_bold\">SpeechJudge-Data (pref)</span>.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "cmos",
                    "speechjudgedata"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During our preliminary analysis based on SpeechJudge-Data (pref), we observe that a significant disparity in intelligibility between two speech samples can overshadow the subtler quality of naturalness, biasing human preference toward the more comprehensible sample. To mitigate this confounding factor and create a more high-quality dataset focused specifically on naturalness, we further refine the data. Specifically, we retain only pairs where the absolute WER gap of those is below 12%. This process results in the 44K-pair high-quality <span class=\"ltx_text ltx_font_bold\">SpeechJudge-Data (hq)</span> subset, ensuring that its preference labels are more reflective of genuine differences in naturalness.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "speechjudgedata"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use all the speech samples from SpeechJudge-Data (raw) for this analysis. We visualize the relationship between WER and the subjective text accuracy in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S8.F10\" title=\"Figure 10 &#8227; 8.2 Intelligibility Annotation Analysis &#8227; 8 Human Annotation Details &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.\nFor the regular speeches (the orange curve), we observe a consistent negative correlation: as the WER increases, its perceived text accuracy steadily declines. For the expressive speeches (the green curve), the similar trend holds for expressive speech when WER is under about 12%. When WER is over the threshold, however, the correlation between WER and the subjective text accuracy weakens significantly. We think this divergence is sourced from that the greater stylistic variations in expressive speech pose a substantial challenge to the robustness of ASR systems compared to the regular samples.</p>\n\n",
                "matched_terms": [
                    "text",
                    "speechjudgedata",
                    "accuracy"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AudioLLMs</span>: We use the plain prompt of Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T1\" title=\"Table 1 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> to instruct AudioLLMs to pairwise score the naturalness of two audios. For the closed-source models, we use the official API released by Google<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span><a class=\"ltx_ref ltx_href\" href=\"https://ai.google.dev/gemini-api/docs/models\" title=\"\">https://ai.google.dev/gemini-api/docs/models</a></span></span></span> for Gemini and OpenAI<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span><a class=\"ltx_ref ltx_href\" href=\"https://platform.openai.com/docs/models\" title=\"\">https://platform.openai.com/docs/models</a></span></span></span> for GPT. We use the model variants <span class=\"ltx_text ltx_font_typewriter\">gemini-2.5-flash</span>, <span class=\"ltx_text ltx_font_typewriter\">gemini-2.5-pro</span>, <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini-audio-preview-2024-12-17</span>, and <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-audio-preview-2025-06-03</span> for Gemini-2.5-Flash, Gemini-2.5-Pro, GPT-4o mini Audio, and GPT-4o Audio.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this study, we investigate whether using the CoT from Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T1\" title=\"Table 1 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> helps AudioLLMs better judge speech naturalness. Interestingly, we find that some closed-source AudioLLMs, such as Gemini-2.5-Flash, improve their performance on SpeechJudge-Eval through this thinking and reasoning process. However, this strategy often does not work for existing open-source AudioLLMs. For example, the results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S10.T5\" title=\"Table 5 &#8227; 10 More Evaluation Results of Existing AudioLLMs &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> show that while Gemini-2.5-Flash consistently improves with the CoT prompt, Kimi-Audio-7B-Instruct, which is already the leading open-source model on SpeechJudge-Eval (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T2\" title=\"Table 2 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), actually sees a decline in performance when using the CoT prompt.</p>\n\n",
                "matched_terms": [
                    "naturalness",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SFT Stage</span>&#8195;We use Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>]</cite> to generate the CoT data for SpeechJudge-Data (train). For the total 42K samples, Gemini-2.5-Flash&#8217;s judgments agree with human feedback on 25K samples, while they disagree on 17K samples. During the SFT stage, we fine-tune Qwen2.5-Omni-7B (Thinker) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>]</cite> on the 25K CoT data using LoRA <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib20\" title=\"\">20</a>]</cite> with a rank of 128. We use Adam <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib29\" title=\"\">29</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib35\" title=\"\">35</a>]</cite> as the optimizer and set the learning rate to 5e-5. The maximum number of tokens per batch is 4000. We select the best checkpoint on SpeechJudge-Data (dev) as the SFT model, SpeechJudge-GRM (SFT).</p>\n\n",
                "matched_terms": [
                    "model",
                    "speechjudgedata",
                    "speechjudgegrm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RL Stage</span>&#8195;We use the 17K samples (as described above) to conduct DAPO <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib58\" title=\"\">58</a>]</cite>, which is an enhanced variant of GRPO <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib42\" title=\"\">42</a>]</cite>. We utilize the <span class=\"ltx_text ltx_font_typewriter\">ms-swift<span class=\"ltx_note ltx_role_footnote\" id=\"footnote12\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">12</span></span><a class=\"ltx_ref ltx_href ltx_font_serif\" href=\"https://github.com/modelscope/ms-swift\" title=\"\">https://github.com/modelscope/ms-swift</a></span></span></span></span> toolkit to launch the training process. We initialize the policy model with the SFT model and use LoRA training with a rank of 64. The number of rollouts for each prompt is set to 8, and the batch size is 32. The learning rate is 5e-6. We select the best checkpoint on SpeechJudge-Data (dev) as the final SpeechJudge-GRM model, i.e., SpeechJudge-GRM (SFT+RL).</p>\n\n",
                "matched_terms": [
                    "model",
                    "speechjudgedata",
                    "speechjudgegrm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the construction of SpeechJudge-Data, we hired human labelers from a data crowdsourcing company. To verify the effectiveness of our training for them and to ensure the high quality of both the dataset and the resulting SpeechJudge-GRM, the human subjects for the final sample selection and TTS post-training experiments (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.SS3\" title=\"5.3 High-Quality Sample Selection based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.SS4\" title=\"5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>) were all experienced speech generation researchers. All these researchers had extensive audio backgrounds, with a minimum of two years of experience in speech synthesis.</p>\n\n",
                "matched_terms": [
                    "speechjudgegrm",
                    "speechjudgedata"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding the SIM metric, both w/ INTP and w/ SpeechJudge-GRM (offline) either match or slightly outperform the baseline model, while the other two methods show a slight decline. However, the objective SIM results appear to be in slight conflict with the subjective speaker similarity results in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6.sf2\" title=\"Figure 6(b) &#8227; Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6(b)</span></a>. For instance, in the subjective evaluation, w/ INTP actually shows a decrease in speaker similarity (Win: 24.30%, Lose: 32.90%).</p>\n\n",
                "matched_terms": [
                    "offline",
                    "speechjudgegrm",
                    "model",
                    "intp"
                ]
            }
        ]
    },
    "S7.T4": {
        "source_file": "SpeechJudge: Towards Human-Level Judgment for Speech Naturalness",
        "caption": "Table 4: Distribution of the SpeechJudge-Eval benchmark.",
        "body": "ParaSpeechCaps, L2-Arctic,\n\n\nKeSpeech, Genshin, etc.",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">ParaSpeechCaps, L2-Arctic,</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">KeSpeech, Genshin, etc.</td>\n</tr>\n</table> \n",
        "informative_terms_identified": [
            "genshin",
            "kespeech",
            "paraspeechcaps",
            "distribution",
            "l2arctic",
            "etc",
            "speechjudgeeval",
            "benchmark"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">From SpeechJudge-Data (hq), we construct our benchmark, <span class=\"ltx_text ltx_font_bold\">SpeechJudge-Eval</span>, by applying stratified sampling to FA-level pairs, resulting in 1,000 pairs; its composition is detailed in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S7.T4\" title=\"Table 4 &#8227; 7.2 Subsets of SpeechJudge-Data &#8227; 7 Details of SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>. Similarly, we use the same strategy to construct a validation set of the same size, <span class=\"ltx_text ltx_font_bold\">SpeechJudge-Data (dev)</span>. The remaining 42K pairs, <span class=\"ltx_text ltx_font_bold\">SpeechJudge-Data (train)</span>, constitute the training set for our reward models.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Aligning large generative models with human feedback is a critical challenge. In speech synthesis, this is particularly pronounced due to the lack of a large-scale human preference dataset, which hinders the development of models that truly align with human perception. To address this, we introduce <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SpeechJudge</span>, a comprehensive suite comprising a dataset, a benchmark, and a reward model centered on <span class=\"ltx_text ltx_font_italic\">naturalness</span>&#8212;one of the most fundamental subjective metrics for speech synthesis. First, we present <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SpeechJudge-Data</span>, a large-scale human feedback corpus of 99K speech pairs. The dataset is constructed using a diverse set of advanced zero-shot text-to-speech (TTS) models across diverse speech styles and multiple languages, with human annotations for both intelligibility and naturalness preference. From this, we establish <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SpeechJudge-Eval</span>, a challenging benchmark for speech naturalness judgment. Our evaluation reveals that existing metrics and AudioLLMs struggle with this task; the leading model, Gemini-2.5-Flash, achieves less than 70% agreement with human judgment, highlighting a significant gap for improvement. To bridge this gap, we develop <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SpeechJudge-GRM</span>, a generative reward model (GRM) based on Qwen2.5-Omni-7B. It is trained on SpeechJudge-Data via a two-stage post-training process: Supervised Fine-Tuning (SFT) with Chain-of-Thought rationales followed by Reinforcement Learning (RL) with GRPO on challenging cases. On the SpeechJudge-Eval benchmark, the proposed SpeechJudge-GRM demonstrates superior performance, achieving 77.2% accuracy (and 79.4% after inference-time scaling @10) compared to a classic Bradley-Terry reward model (72.7%). Furthermore, SpeechJudge-GRM can be also employed as a reward function during the post-training of speech generation models to facilitate their alignment with human preferences.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "speechjudgeeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2. An Evaluation Benchmark for Speech Naturalness Judgment: <span class=\"ltx_text ltx_font_italic\">SpeechJudge-Eval</span>.</span>\nWe design a dedicated evaluation benchmark for the task of speech naturalness judgment. The task is structured as follows: given a target text and two corresponding speech samples, a model needs to judge which one is more natural. To construct the evaluation set, we select a subset from the SpeechJudge-Data where human annotators demonstrated high inter-annotator agreement, ensuring a high-quality ground truth. We assess the naturalness judgment capabilities of a wide range of metrics and models, including Word Error Rate (WER) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib38\" title=\"\">38</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib16\" title=\"\">16</a>]</cite>, Fr&#233;chet Audio Distance (FAD) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib27\" title=\"\">27</a>]</cite>, MOS predictors <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib41\" title=\"\">41</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib40\" title=\"\">40</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib48\" title=\"\">48</a>]</cite>, Deepfake Detectors <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib49\" title=\"\">49</a>]</cite>, and AudioLLMs <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib28\" title=\"\">28</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib53\" title=\"\">53</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib22\" title=\"\">22</a>]</cite>. Our evaluations reveal that even the most capable model&#8212;specifically, Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>]</cite> in our experiments&#8212;achieved less than 70% agreement with human preferences. This finding highlights a significant performance gap and underscores the substantial room for research and improvement in automated speech naturalness judgment.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "speechjudgeeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt Construction</span>&#8195;To build diverse prompts <math alttext=\"(a_{ref},t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{ref},t)</annotation></semantics></math> for TTS, for <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math>, we adopt both <span class=\"ltx_text ltx_font_bold\">regular</span> and <span class=\"ltx_text ltx_font_bold\">expressive</span> speech samples. The regular samples are randomly selected from the Emilia-Large dataset <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib19\" title=\"\">19</a>]</cite>. The expressive samples are sourced from corpora rich in paralinguistics, including the emotional corpora: ParaSpeechCaps <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib9\" title=\"\">9</a>]</cite>, the accented corpora: L2-Arctic <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib63\" title=\"\">63</a>]</cite> and KeSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib46\" title=\"\">46</a>]</cite>, the whisper samples from an in-house corpus, and the character voices from video games Genshin Impact <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib43\" title=\"\">43</a>]</cite>. We display the detailed distribution of speech references in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F2.sf2\" title=\"Figure 2(b) &#8227; Figure 2 &#8227; 3.1 Dataset Construction &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a>.</p>\n\n",
                "matched_terms": [
                    "genshin",
                    "paraspeechcaps",
                    "distribution",
                    "l2arctic",
                    "kespeech"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To evaluate speech naturalness, existing studies typically organize their own listening tests, which often have inconsistent settings across different papers <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib28\" title=\"\">28</a>]</cite>. Alternatively, previous researchers use proxy MOS predictors, such as UTMOS <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib41\" title=\"\">41</a>]</cite>, as an objective metric. However, it remains an underexplored problem whether these metrics can accurately judge the naturalness of more advanced speech generation models <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib52\" title=\"\">52</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>]</cite> and align with human preferences. Motivated by this, we construct a benchmark, <span class=\"ltx_text ltx_font_bold\">SpeechJudge-Eval</span>, specifically for the speech naturalness judgment task.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "speechjudgeeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Data</span>&#8195;We sample a subset of the SpeechJudge-Data to create the evaluation set for SpeechJudge-Eval. Specifically, we first select a subset that contains only <span class=\"ltx_text ltx_font_italic\">preference data</span> (i.e., we filter out samples with the &#8220;Tie&#8221; annotation), and then choose only those with full-agreement-level (FA) samples to ensure a high-quality ground truth. We perform sampling from both the <span class=\"ltx_text ltx_font_italic\">regular</span> and <span class=\"ltx_text ltx_font_italic\">expressive</span> subsets of SpeechJudge-Data and proportionally cover the three target text languages (<span class=\"ltx_text ltx_font_italic\">zh</span>, <span class=\"ltx_text ltx_font_italic\">en</span>, and <span class=\"ltx_text ltx_font_italic\">mixed</span>) within each subset. The final SpeechJudge-Eval dataset consists of 1,000 samples. The construction details of SpeechJudge-Eval and its distribution can be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S7.SS2\" title=\"7.2 Subsets of SpeechJudge-Data &#8227; 7 Details of SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "distribution",
                    "speechjudgeeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">To verify the effectiveness of SpeechJudge-GRM for naturalness judgment, we evaluate it on the SpeechJudge-Eval benchmark. We develop SpeechJudge-BTRM as a baseline, which utilizes the BTRM paradigm <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib39\" title=\"\">39</a>]</cite> by adding a linear layer on Qwen2.5-Omni-7B (Thinker) to produce a single scalar reward prediction. SpeechJudge-BTRM also uses LoRA fine-tuning and uses the same training data as SpeechJudge-GRM.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "speechjudgeeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we address the challenge of aligning speech synthesis with human perception of naturalness by introducing SpeechJudge, a suite including a large-scale dataset (SpeechJudge-Data), a challenging benchmark (SpeechJudge-Eval), and a generative reward model (SpeechJudge-GRM). Our benchmark reveals that even top AudioLLMs struggle with naturalness judgment, achieving less than 70% human agreement. In contrast, our proposed SpeechJudge-GRM reaches 77.2% accuracy (up to 79.4% with inference-time scaling @10), outperforming the classic Bradley-Terry reward models (72.7%). We also demonstrate its practical utility as a reward function to effectively enhance TTS model naturalness via post-training. While our primary focus is on naturalness, future work could extend this framework to other subjective attributes like speaker similarity and emotional expressiveness, potentially through multi-objective reward modeling. By releasing our resources, we aim to catalyze research in building more human-aligned speech generation systems.</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "speechjudgeeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the evaluation on the SpeechJudge-Eval Benchmark of Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T2\" title=\"Table 2 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we adopt the following protocol for each model:</p>\n\n",
                "matched_terms": [
                    "benchmark",
                    "speechjudgeeval"
                ]
            }
        ]
    },
    "S10.T5": {
        "source_file": "SpeechJudge: Towards Human-Level Judgment for Speech Naturalness",
        "caption": "Table 5: Performance of AudioLLMs on SpeechJudge-Eval when using CoT prompt.",
        "body": "Model\nRegular\nExpressive\nTotal\n\n\nKimi-Audio-7B-Instruct\n65.5\n68.0\n67.0\n\n\nw/ CoT prompt\n67.4\n66.1\n66.5\n\n\nGemini-2.5-Flash\n73.5\n66.2\n69.1\n\n\nw/ CoT prompt\n75.0\n67.5\n70.5",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Regular</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Expressive</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Total</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Kimi-Audio-7B-Instruct</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">65.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">68.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">67.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right\">w/ CoT prompt</td>\n<td class=\"ltx_td ltx_align_center\">67.4</td>\n<td class=\"ltx_td ltx_align_center\">66.1</td>\n<td class=\"ltx_td ltx_align_center\">66.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Gemini-2.5-Flash</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">73.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">66.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">69.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">w/ CoT prompt</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">75.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">67.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">70.5</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "prompt",
            "regular",
            "cot",
            "model",
            "when",
            "total",
            "audiollms",
            "expressive",
            "speechjudgeeval",
            "gemini25flash",
            "performance",
            "kimiaudio7binstruct"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">In this study, we investigate whether using the CoT from Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T1\" title=\"Table 1 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> helps AudioLLMs better judge speech naturalness. Interestingly, we find that some closed-source AudioLLMs, such as Gemini-2.5-Flash, improve their performance on SpeechJudge-Eval through this thinking and reasoning process. However, this strategy often does not work for existing open-source AudioLLMs. For example, the results in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S10.T5\" title=\"Table 5 &#8227; 10 More Evaluation Results of Existing AudioLLMs &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5</span></a> show that while Gemini-2.5-Flash consistently improves with the CoT prompt, Kimi-Audio-7B-Instruct, which is already the leading open-source model on SpeechJudge-Eval (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T2\" title=\"Table 2 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>), actually sees a decline in performance when using the CoT prompt.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Aligning large generative models with human feedback is a critical challenge. In speech synthesis, this is particularly pronounced due to the lack of a large-scale human preference dataset, which hinders the development of models that truly align with human perception. To address this, we introduce <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SpeechJudge</span>, a comprehensive suite comprising a dataset, a benchmark, and a reward model centered on <span class=\"ltx_text ltx_font_italic\">naturalness</span>&#8212;one of the most fundamental subjective metrics for speech synthesis. First, we present <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SpeechJudge-Data</span>, a large-scale human feedback corpus of 99K speech pairs. The dataset is constructed using a diverse set of advanced zero-shot text-to-speech (TTS) models across diverse speech styles and multiple languages, with human annotations for both intelligibility and naturalness preference. From this, we establish <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SpeechJudge-Eval</span>, a challenging benchmark for speech naturalness judgment. Our evaluation reveals that existing metrics and AudioLLMs struggle with this task; the leading model, Gemini-2.5-Flash, achieves less than 70% agreement with human judgment, highlighting a significant gap for improvement. To bridge this gap, we develop <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SpeechJudge-GRM</span>, a generative reward model (GRM) based on Qwen2.5-Omni-7B. It is trained on SpeechJudge-Data via a two-stage post-training process: Supervised Fine-Tuning (SFT) with Chain-of-Thought rationales followed by Reinforcement Learning (RL) with GRPO on challenging cases. On the SpeechJudge-Eval benchmark, the proposed SpeechJudge-GRM demonstrates superior performance, achieving 77.2% accuracy (and 79.4% after inference-time scaling @10) compared to a classic Bradley-Terry reward model (72.7%). Furthermore, SpeechJudge-GRM can be also employed as a reward function during the post-training of speech generation models to facilitate their alignment with human preferences.</p>\n\n",
                "matched_terms": [
                    "model",
                    "audiollms",
                    "gemini25flash",
                    "speechjudgeeval",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In the field of speech synthesis, <span class=\"ltx_text ltx_font_bold ltx_font_italic\">naturalness</span> has long been a cornerstone subjective metric for quality assessment <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib24\" title=\"\">24</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib13\" title=\"\">13</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib28\" title=\"\">28</a>]</cite>, representing one of the most general-purpose indicators of performance <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib47\" title=\"\">47</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib45\" title=\"\">45</a>]</cite>. Prior research has explored automated speech assessment through MOS predictors <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib41\" title=\"\">41</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib21\" title=\"\">21</a>]</cite> and constructed the human feedback corpora for specific attributes like the low-level acoustic quality <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib50\" title=\"\">50</a>]</cite>. However, a large-scale human feedback corpus centered on the holistic quality of naturalness&#8212;and a corresponding reward model trained to capture these preferences&#8212;remains a notably underexplored area. To fill this void, this paper focuses on the dimension of speech naturalness and present a three-part contribution:</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">1. A Large-scale Human Feedback Dataset: <span class=\"ltx_text ltx_font_italic\">SpeechJudge-Data</span>.</span>\nWe recruit human annotators to provide feedback on synthesized speeches, with a focus on assessing two fundamental speech aspects: <span class=\"ltx_text ltx_font_italic\">intelligibility</span> and <span class=\"ltx_text ltx_font_italic\">naturalness</span>. For data synthesis, we employ a diverse set of advanced, open-source zero-shot TTS models with varying architectures (such as CosyVoice2 <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib14\" title=\"\">14</a>]</cite>, Ints <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>]</cite>, F5-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib7\" title=\"\">7</a>]</cite>, and MaskGCT <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib52\" title=\"\">52</a>]</cite>) to produce the compared speech pairs. We prepare speech references in both regular and expressive styles, construct multilingual target texts, and cover both monolingual and cross-lingual synthesis scenarios to ensure data diversity (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.SS1\" title=\"3.1 Dataset Construction &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>). We instruct human annotators to perform two tasks based on a speech pair (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F1\" title=\"Figure 1 &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>): (a) pointwise annotation of text accuracy to assess intelligibility, and (b) pairwise preference annotation to judge relative speech naturalness. This extensive effort, involving 69 labelers over two months, results in 99K annotated pairs, with each pair receiving an average of 2.49 annotations from different labelers. We believe the SpeechJudge-Data can serve as a valuable corpus for alignment research in speech synthesis (e.g., DPO alignment <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib39\" title=\"\">39</a>]</cite> or reward modeling <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib44\" title=\"\">44</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib37\" title=\"\">37</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib3\" title=\"\">3</a>]</cite> in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5\" title=\"5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>).</p>\n\n",
                "matched_terms": [
                    "regular",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2. An Evaluation Benchmark for Speech Naturalness Judgment: <span class=\"ltx_text ltx_font_italic\">SpeechJudge-Eval</span>.</span>\nWe design a dedicated evaluation benchmark for the task of speech naturalness judgment. The task is structured as follows: given a target text and two corresponding speech samples, a model needs to judge which one is more natural. To construct the evaluation set, we select a subset from the SpeechJudge-Data where human annotators demonstrated high inter-annotator agreement, ensuring a high-quality ground truth. We assess the naturalness judgment capabilities of a wide range of metrics and models, including Word Error Rate (WER) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib38\" title=\"\">38</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib16\" title=\"\">16</a>]</cite>, Fr&#233;chet Audio Distance (FAD) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib27\" title=\"\">27</a>]</cite>, MOS predictors <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib41\" title=\"\">41</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib40\" title=\"\">40</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib48\" title=\"\">48</a>]</cite>, Deepfake Detectors <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib49\" title=\"\">49</a>]</cite>, and AudioLLMs <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib28\" title=\"\">28</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib53\" title=\"\">53</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib22\" title=\"\">22</a>]</cite>. Our evaluations reveal that even the most capable model&#8212;specifically, Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>]</cite> in our experiments&#8212;achieved less than 70% agreement with human preferences. This finding highlights a significant performance gap and underscores the substantial room for research and improvement in automated speech naturalness judgment.</p>\n\n",
                "matched_terms": [
                    "model",
                    "audiollms",
                    "gemini25flash",
                    "speechjudgeeval",
                    "performance"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">3. A Generative Reward Model for Speech Naturalness: <span class=\"ltx_text ltx_font_italic\">SpeechJudge-GRM</span>.</span>\nTo develop a reward model that more effectively captures human preferences, we develop SpeechJudge-GRM, a generative reward model (GRM) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib59\" title=\"\">59</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib34\" title=\"\">34</a>]</cite> trained on the SpeechJudge-Data. Specifically, we base our model on Qwen2.5-Omni-7B <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>]</cite> and design a two-stage post-training process. During the first stage, we perform SFT as the &#8220;cold start&#8221; to improve the model&#8217;s instruction-following and rationale-based reasoning capabilities. To achieve this, we leverage Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>]</cite> to generate Chain-of-Thought (CoT) data for speech naturalness judgment task. In the second stage, we focus on more challenging cases of SpeechJudge-Data, which we define as instances where Gemini-2.5-Flash fails to make the correct judgment. Treating the human-annotated labels as the verifiable reward <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib34\" title=\"\">34</a>]</cite>, we apply the GRPO-based RL stage <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib42\" title=\"\">42</a>]</cite>. Our experiments demonstrate that when trained on the same data, SpeechJudge-GRM significantly outperformed the classic Bradley-Terry reward model (BTRM) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib39\" title=\"\">39</a>]</cite>, achieving a higher accuracy in predicting human preferences (77.2% for SpeechJudge-GRM vs. 72.7% for SpeechJudge-BTRM, Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Methodology &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Besides, SpeechJudge-GRM also supports inference-time scaling and offers explainability through its CoT outputs. Furthermore, SpeechJudge-GRM can also be employed as an objective naturalness metric for sample selection (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F5\" title=\"Figure 5 &#8227; 5.1 Methodology &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) or as a reward function in RL algorithms to enhance the quality of existing speech generation models (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6\" title=\"Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>).</p>\n\n",
                "matched_terms": [
                    "when",
                    "cot",
                    "model",
                    "gemini25flash"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AudioLLM as a Judge</span>&#8195;Using LLMs as automated quality evaluators is a prominent topic in the textual LLM field, popularized by the &#8220;LLM-as-a-judge&#8221; paradigm <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib64\" title=\"\">64</a>]</cite>. This approach has recently been extended to the speech domain. A concurrent work, AudioJudge <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib36\" title=\"\">36</a>]</cite>, evaluates the capabilities and limitations of using AudioLLMs for speech quality assessment and paralinguistic understanding by prompt engineering. Furthermore, many studies have focused on fine-tuning AudioLLMs to elicit their understanding capabilities for specific tasks. Examples include discriminating the human-likeness of audio <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib51\" title=\"\">51</a>]</cite>, understanding low-level acoustic qualities <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib50\" title=\"\">50</a>]</cite>, and enhancing the assessment of instruction-following in spoken dialogue systems <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib23\" title=\"\">23</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib17\" title=\"\">17</a>]</cite>. However, how to improve the ability of AudioLLMs to understand and judge speech naturalness, and how to use the quality assessment capabilities of AudioLLMs as a reward to improve the post-training of speech generation models themselves, remain significantly underexplored.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "audiollms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt Construction</span>&#8195;To build diverse prompts <math alttext=\"(a_{ref},t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{ref},t)</annotation></semantics></math> for TTS, for <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math>, we adopt both <span class=\"ltx_text ltx_font_bold\">regular</span> and <span class=\"ltx_text ltx_font_bold\">expressive</span> speech samples. The regular samples are randomly selected from the Emilia-Large dataset <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib19\" title=\"\">19</a>]</cite>. The expressive samples are sourced from corpora rich in paralinguistics, including the emotional corpora: ParaSpeechCaps <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib9\" title=\"\">9</a>]</cite>, the accented corpora: L2-Arctic <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib63\" title=\"\">63</a>]</cite> and KeSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib46\" title=\"\">46</a>]</cite>, the whisper samples from an in-house corpus, and the character voices from video games Genshin Impact <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib43\" title=\"\">43</a>]</cite>. We display the detailed distribution of speech references in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F2.sf2\" title=\"Figure 2(b) &#8227; Figure 2 &#8227; 3.1 Dataset Construction &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a>.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "regular",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The target text <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> paired with each <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m2\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math> is constructed as follows: For regular <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m3\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math> samples, we randomly sample transcriptions from the Emilia-Large dataset <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib19\" title=\"\">19</a>]</cite>. These are then refined using DeepSeek-V3 <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib10\" title=\"\">10</a>]</cite> to correct typos and normalize punctuations. For expressive <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m4\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math> samples, we instruct DeepSeek-V3 to generate several scripts in different writing styles, tailored to the topic of <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m5\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math> (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S7.SS1\" title=\"7.1 Details of Prompt Construction &#8227; 7 Details of SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">7.1</span></a> for more details). The languages of the target texts included Chinese (<span class=\"ltx_text ltx_font_italic\">zh</span>), English (<span class=\"ltx_text ltx_font_italic\">en</span>), and Chinese-English code-switching (<span class=\"ltx_text ltx_font_italic\">mixed</span>). For the combinations <math alttext=\"(a_{ref},t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{ref},t)</annotation></semantics></math>, we include both monolingual settings (<span class=\"ltx_text ltx_font_italic\">en2en</span> and <span class=\"ltx_text ltx_font_italic\">zh2zh</span>) and cross-lingual settings (<span class=\"ltx_text ltx_font_italic\">zh2en</span>, <span class=\"ltx_text ltx_font_italic\">en2zh</span>, <span class=\"ltx_text ltx_font_italic\">zh2mixed</span>, and <span class=\"ltx_text ltx_font_italic\">en2mixed</span>), where <span class=\"ltx_text ltx_font_italic\">zh2en</span> denotes Chinese <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m7\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math> with English <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m8\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, and similarly for others. The distribution of the language settings of <math alttext=\"(a_{ref},t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m9\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{ref},t)</annotation></semantics></math> is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F2.sf3\" title=\"Figure 2(c) &#8227; Figure 2 &#8227; 3.1 Dataset Construction &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2(c)</span></a>.</p>\n\n",
                "matched_terms": [
                    "regular",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Human Annotation &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we demonstrate the distribution of these human agreement levels for the SpeechJudge-Data and its two subsets, <span class=\"ltx_text ltx_font_italic\">regular</span> and <span class=\"ltx_text ltx_font_italic\">expressive</span> (which are defined by their speech references). The figure shows that about 70% of the entire dataset falls into the Full Agreement (51.5%) or Weak Agreement (17.2%) levels. Furthermore, we observe that the <span class=\"ltx_text ltx_font_italic\">expressive</span> subset has a lower agreement level than the <span class=\"ltx_text ltx_font_italic\">regular</span> subset, which suggests that human evaluation of expressive speech generation is inherently a more challenging problem. Besides this sample-level agreement analysis, we also analyze the reliability of individual annotators, and we will discuss this in the Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S8.SS1\" title=\"8.1 Individual Annotator Reliability &#8227; 8 Human Annotation Details &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">8.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "regular",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:80%;\">We instruct AudioLLMs using two modes of prompt: </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">plain</span>\n  <span class=\"ltx_text\" style=\"font-size:80%;\"> and </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">CoT</span>\n  <span class=\"ltx_text\" style=\"font-size:80%;\">. The text in </span>\n  <span class=\"ltx_text\" style=\"font-size:80%;--ltx-fg-color:#2954AD;\">blue</span>\n  <span class=\"ltx_text\" style=\"font-size:80%;\"> is only employed during the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">CoT</span>\n  <span class=\"ltx_text\" style=\"font-size:80%;\"> mode.</span>\n</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "cot",
                    "audiollms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:80%;\">We use the protocols of Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T1\" style=\"font-size:80%;\" title=\"Table 1 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:80%;\"> to establish judgment rules for different models. The results of AudioLLMs here are obtained using the </span>\n  <span class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">plain</span>\n  <span class=\"ltx_text\" style=\"font-size:80%;\"> prompt of Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T1\" style=\"font-size:80%;\" title=\"Table 1 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\">\n    <span class=\"ltx_text ltx_ref_tag\">1</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:80%;\">.</span>\n</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "audiollms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Task Formulation</span>&#8195;We formulate the naturalness judgment task as a pairwise comparison, specifically a <span class=\"ltx_text ltx_font_italic\">win-or-lose</span> binary classification task: Given a target text <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> and a corresponding audio pair <math alttext=\"(a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{1},a_{2})</annotation></semantics></math>, a model needs to determine which audio has better naturalness. This results in a binary choice: either <math alttext=\"a_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m3\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">a_{1}</annotation></semantics></math> is better or <math alttext=\"a_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m4\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">a_{2}</annotation></semantics></math> is better. We use the human answer as the ground truth, and use <span class=\"ltx_text ltx_font_bold\">Accuracy</span> to measure the judgment performance of a model <math alttext=\"\\mathcal{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m5\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><annotation encoding=\"application/x-tex\">\\mathcal{M}</annotation></semantics></math> on the evaluation set <math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m6\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math>:</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">where <math alttext=\"|\\mathcal{D}|\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m7\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">|</mo><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mo stretchy=\"false\">|</mo></mrow><annotation encoding=\"application/x-tex\">|\\mathcal{D}|</annotation></semantics></math> is the total number of samples in the evaluation set, <math alttext=\"y_{\\mathcal{M}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m8\" intent=\":literal\"><semantics><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi></msub><annotation encoding=\"application/x-tex\">y_{\\mathcal{M}}</annotation></semantics></math> and <math alttext=\"y_{\\mathcal{H}}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m9\" intent=\":literal\"><semantics><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></msub><annotation encoding=\"application/x-tex\">y_{\\mathcal{H}}</annotation></semantics></math> represent the answers of the model <math alttext=\"\\mathcal{M}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m10\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><annotation encoding=\"application/x-tex\">\\mathcal{M}</annotation></semantics></math> and human for the sample <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m11\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math>, respectively. <math alttext=\"\\mathbb{I}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.SS1.p1.m12\" intent=\":literal\"><semantics><mi>&#120128;</mi><annotation encoding=\"application/x-tex\">\\mathbb{I}</annotation></semantics></math> is the indicator function.</p>\n\n",
                "matched_terms": [
                    "total",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Data</span>&#8195;We sample a subset of the SpeechJudge-Data to create the evaluation set for SpeechJudge-Eval. Specifically, we first select a subset that contains only <span class=\"ltx_text ltx_font_italic\">preference data</span> (i.e., we filter out samples with the &#8220;Tie&#8221; annotation), and then choose only those with full-agreement-level (FA) samples to ensure a high-quality ground truth. We perform sampling from both the <span class=\"ltx_text ltx_font_italic\">regular</span> and <span class=\"ltx_text ltx_font_italic\">expressive</span> subsets of SpeechJudge-Data and proportionally cover the three target text languages (<span class=\"ltx_text ltx_font_italic\">zh</span>, <span class=\"ltx_text ltx_font_italic\">en</span>, and <span class=\"ltx_text ltx_font_italic\">mixed</span>) within each subset. The final SpeechJudge-Eval dataset consists of 1,000 samples. The construction details of SpeechJudge-Eval and its distribution can be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S7.SS2\" title=\"7.2 Subsets of SpeechJudge-Data &#8227; 7 Details of SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "regular",
                    "expressive",
                    "speechjudgeeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AudioLLMs</span>, which are employed to test their speech naturalness understanding capabilities in a <span class=\"ltx_text ltx_font_italic\">zero-shot</span> manner<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>We assume that the adopted AudioLLMs have not been directly trained on the speech naturalness judgment task. Their performance on this benchmark is therefore considered a <span class=\"ltx_text ltx_font_italic\">zero-shot</span> capability.</span></span></span>. We include the open-source Phi-4-Multimodal <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib1\" title=\"\">1</a>]</cite>, Qwen2.5-Omni <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>]</cite>, Kimi-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib28\" title=\"\">28</a>]</cite>, Gemma-3n <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib26\" title=\"\">26</a>]</cite>, Voxtral <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib32\" title=\"\">32</a>]</cite>, MiDashengLM <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib12\" title=\"\">12</a>]</cite>, Mimo-Audio <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib53\" title=\"\">53</a>]</cite>, and the closed-source Gemini-2.5 <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>]</cite> and GPT-4o <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib22\" title=\"\">22</a>]</cite>. We use the <span class=\"ltx_text ltx_font_italic\">plain</span> prompt of Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T1\" title=\"Table 1 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> to instruct the model to pairwise score the naturalness of two audios. We use their grading to determine the naturalness preference.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "performance",
                    "model",
                    "audiollms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The performance of different models on SpeechJudge-Eval is presented in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T2\" title=\"Table 2 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>. A key observation is that speech naturalness judgment is a highly challenging task. The leading model, Gemini-2.5-Flash, still only achieves less than 70% agreement with human preferences. When comparing different models, we find that: (1) common objective metrics and MOS predictors show only a weak correlation with human preferences, often achieving less than 60% accuracy and sometimes performing at the level of a random guess (around 50%). (2) While deepfake detectors are highly effective at distinguishing between machine-generated and human-recorded speech <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib49\" title=\"\">49</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib25\" title=\"\">25</a>]</cite>, their ability to do so is not well-aligned with the naturalness objective when comparing two generated samples. (3) AudioLLMs demonstrate significant potential for this task. While some models, such as Gemma-3n and GPT-4o mini Audio, perform at a chance level, a number of others achieve an accuracy exceeding 60%. This promising performance motivates us to further leverage these AudioLLMs for the design of a reward model for speech naturalness.</p>\n\n",
                "matched_terms": [
                    "model",
                    "performance",
                    "audiollms",
                    "gemini25flash",
                    "speechjudgeeval",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the proposed SpeechJudge-Data, we further explore how to train a reward model capable of accurately capturing human preferences. Specifically, we propose <span class=\"ltx_text ltx_font_bold\">SpeechJudge-GRM</span>, where we leverage the inherent audio understanding capabilities of AudioLLMs (specifically, Qwen2.5-Omni-7B <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>]</cite>) to elicit their speech naturalness judgment capability. Compared to the classic BTRM <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib4\" title=\"\">4</a>]</cite>, the key strengths of GRM are its ability to enable Chain-of-Thought (CoT) reasoning and its support for test-time computation via majority voting, which ultimately leads to improved preference judgment performance <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib59\" title=\"\">59</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "cot",
                    "model",
                    "audiollms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">\n  <span class=\"ltx_text\" style=\"font-size:80%;\">Our evaluation is conducted on SpeechJudge-Eval (like Table </span>\n  <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T2\" style=\"font-size:80%;\" title=\"Table 2 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\">\n    <span class=\"ltx_text ltx_ref_tag\">2</span>\n  </a>\n  <span class=\"ltx_text\" style=\"font-size:80%;\">). </span>\n  <span class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">w/ Voting@10</span>\n  <span class=\"ltx_text\" style=\"font-size:80%;\">: For each prompt, the GRM generates 10 outputs, and we use the majority voting from these 10 outputs as the final result.</span>\n</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "speechjudgeeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SFT Stage</span>&#8195;We consider SFT as a &#8220;cold start&#8221; stage to improve the Qwen2.5-Omni&#8217;s instruction-following, reasoning, and speech naturalness understanding capabilities. We select Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>]</cite>&#8212;one of the leading closed-source models on SpeechJudge-Eval (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T2\" title=\"Table 2 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>)&#8212;to serve as a teacher model, and instruct it to generate the CoT data. Specifically, for each sample <math alttext=\"d=(t,a_{1},a_{2},y_{\\mathcal{H}})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo>,</mo><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">d=(t,a_{1},a_{2},y_{\\mathcal{H}})</annotation></semantics></math> from SpeechJudge-Data, we use the CoT prompt from Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T1\" title=\"Table 1 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (denoted as <math alttext=\"\\mathbf{I}_{CoT}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><msub><mi>&#119816;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{I}_{CoT}</annotation></semantics></math>) to instruct Gemini-2.5-Flash to generate a rationale-based output (denoted as <math alttext=\"\\mathbf{O}_{teacher}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m3\" intent=\":literal\"><semantics><msub><mi>&#119822;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{O}_{teacher}</annotation></semantics></math>). We then extract the preference judgment (<math alttext=\"y_{\\mathcal{M}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m4\" intent=\":literal\"><semantics><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi></msub><annotation encoding=\"application/x-tex\">y_{\\mathcal{M}}</annotation></semantics></math>) from this output. For samples where Gemini-2.5-Flash&#8217;s preference is consistent with the human (i.e., <math alttext=\"y_{\\mathcal{M}}=y_{\\mathcal{H}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi></msub><mo>=</mo><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></msub></mrow><annotation encoding=\"application/x-tex\">y_{\\mathcal{M}}=y_{\\mathcal{H}}</annotation></semantics></math>), we concatenate the CoT prompt and the model&#8217;s output, <math alttext=\"[\\mathbf{I}_{CoT},\\mathbf{O}_{teacher}]\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119816;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><mo>,</mo><msub><mi>&#119822;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[\\mathbf{I}_{CoT},\\mathbf{O}_{teacher}]</annotation></semantics></math>, to create a data point for our SFT dataset. Conversely, we consider the sample <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m7\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> a challenging case and reserve the prompt <math alttext=\"\\mathbf{I}_{CoT}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m8\" intent=\":literal\"><semantics><msub><mi>&#119816;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{I}_{CoT}</annotation></semantics></math> for the second-stage RL dataset. During the SFT stage, for each training sample <math alttext=\"[\\mathbf{I}_{CoT},\\mathbf{O}_{teacher}]\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m9\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119816;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><mo>,</mo><msub><mi>&#119822;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[\\mathbf{I}_{CoT},\\mathbf{O}_{teacher}]</annotation></semantics></math>, we perform the next token prediction only on the segment <math alttext=\"\\mathbf{O}_{teacher}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m10\" intent=\":literal\"><semantics><msub><mi>&#119822;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{O}_{teacher}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "cot",
                    "model",
                    "gemini25flash",
                    "speechjudgeeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RL Stage</span>&#8195;We treat the annotated human preference as a verifiable reward, and, building on the SFT model, we further trained it using the GRPO algorithm <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib42\" title=\"\">42</a>]</cite>. Specifically, for each sample <math alttext=\"d=(t,a_{1},a_{2},y_{\\mathcal{H}})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo>,</mo><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">d=(t,a_{1},a_{2},y_{\\mathcal{H}})</annotation></semantics></math> in the RL dataset, we adopt the CoT prompt to instruct the policy model to conduct multiple rollouts during each iteration. For the <math alttext=\"i\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m2\" intent=\":literal\"><semantics><mi>i</mi><annotation encoding=\"application/x-tex\">i</annotation></semantics></math>-th rollout, we parse the model&#8217;s preference for <math alttext=\"(a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{1},a_{2})</annotation></semantics></math>, denoted as <math alttext=\"y_{\\mathcal{M}}^{i}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m4\" intent=\":literal\"><semantics><msubsup><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mi>i</mi></msubsup><annotation encoding=\"application/x-tex\">y_{\\mathcal{M}}^{i}</annotation></semantics></math>. Following <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib34\" title=\"\">34</a>]</cite>, we use an accuracy-based rule to calculate the reward: the reward is 1 if <math alttext=\"y_{\\mathcal{M}}^{i}=y_{\\mathcal{H}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p3.m5\" intent=\":literal\"><semantics><mrow><msubsup><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi><mi>i</mi></msubsup><mo>=</mo><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></msub></mrow><annotation encoding=\"application/x-tex\">y_{\\mathcal{M}}^{i}=y_{\\mathcal{H}}</annotation></semantics></math>, and -1 otherwise. In other words, during the RL stage, we only constrain the model&#8217;s final naturalness judgment to align with human preferences, allowing the model to autonomously optimize its reasoning and rationale generation capabilities.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "cot",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the results of Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Methodology &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we can observe that: (1) The SpeechJudge-BTRM achieves a 72.7% agreement with human preferences on SpeechJudge-Eval, a level of performance comparable to the initial development of BTRMs in the textual LLM RLHF field <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib44\" title=\"\">44</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib37\" title=\"\">37</a>]</cite>. (2) After conducting SFT training with the CoT data, the accuracy of SpeechJudge-GRM (SFT) reaches 75.3%. Besides, further RLVR training improves the final model SpeechJudge-GRM (SFT+RL) to an accuracy of 77.2%. (3) Due to the generative nature of the GRM, we can further enhance the accuracy of SpeechJudge-GRM using inference-time scaling. For example, by using majority voting across 10 outputs instead of just one, the accuracy is improved by approximately 2 percentage points (75.3% <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> 77.6%; 77.2% <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> 79.4%). These results collectively verify the effectiveness of our proposed SpeechJudge-GRM for judging speech naturalness.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "cot",
                    "model",
                    "speechjudgeeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigate the effect of SpeechJudge-based reward models for high-quality sample selection. We use the hard cases from SeedTTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib2\" title=\"\">2</a>]</cite> and the code-switching cases from Amphion-TTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib60\" title=\"\">60</a>]</cite> as target texts. For each text, we instruct the Qwen2.5-Omni-7B (Talker) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib57\" title=\"\">57</a>]</cite> to generate 100 speeches. We then ask human subjects to compare the best-of-100 output&#8212;as selected by either SpeechJudge-BTRM or SpeechJudge-GRM&#8212;against a randomly sampled output. The evaluation measures the win/lose/tie ratios based on speech naturalness. From Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F5\" title=\"Figure 5 &#8227; 5.1 Methodology &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that the best-of-100 samples selected by both SpeechJudge-BTRM and SpeechJudge-GRM are more likely to outperform a randomly selected sample from the same set. This finding demonstrates the advantage of using the SpeechJudge-Data corpus for training human-aligned reward model. Furthermore, SpeechJudge-GRM exhibits better performance than SpeechJudge-BTRM, which highlights the superiority of the proposed GRM.</p>\n\n",
                "matched_terms": [
                    "performance",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we address the challenge of aligning speech synthesis with human perception of naturalness by introducing SpeechJudge, a suite including a large-scale dataset (SpeechJudge-Data), a challenging benchmark (SpeechJudge-Eval), and a generative reward model (SpeechJudge-GRM). Our benchmark reveals that even top AudioLLMs struggle with naturalness judgment, achieving less than 70% human agreement. In contrast, our proposed SpeechJudge-GRM reaches 77.2% accuracy (up to 79.4% with inference-time scaling @10), outperforming the classic Bradley-Terry reward models (72.7%). We also demonstrate its practical utility as a reward function to effectively enhance TTS model naturalness via post-training. While our primary focus is on naturalness, future work could extend this framework to other subjective attributes like speaker similarity and emotional expressiveness, potentially through multi-objective reward modeling. By releasing our resources, we aim to catalyze research in building more human-aligned speech generation systems.</p>\n\n",
                "matched_terms": [
                    "model",
                    "speechjudgeeval",
                    "audiollms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the target texts paired with the regular speech references, we use DeepSeek-V3 <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib10\" title=\"\">10</a>]</cite> to fix typos and normalize punctuations, the prompt used is listed below.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "regular"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the target texts paired with the expressive speech references, we use DeepSeek-V3 to generate several scripts in different writing styles based on the speech reference&#8217;s text, the prompt used is listed below.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use all the speech samples from SpeechJudge-Data (raw) for this analysis. We visualize the relationship between WER and the subjective text accuracy in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S8.F10\" title=\"Figure 10 &#8227; 8.2 Intelligibility Annotation Analysis &#8227; 8 Human Annotation Details &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.\nFor the regular speeches (the orange curve), we observe a consistent negative correlation: as the WER increases, its perceived text accuracy steadily declines. For the expressive speeches (the green curve), the similar trend holds for expressive speech when WER is under about 12%. When WER is over the threshold, however, the correlation between WER and the subjective text accuracy weakens significantly. We think this divergence is sourced from that the greater stylistic variations in expressive speech pose a substantial challenge to the robustness of ASR systems compared to the regular samples.</p>\n\n",
                "matched_terms": [
                    "regular",
                    "expressive",
                    "when"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the evaluation on the SpeechJudge-Eval Benchmark of Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T2\" title=\"Table 2 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>, we adopt the following protocol for each model:</p>\n\n",
                "matched_terms": [
                    "model",
                    "speechjudgeeval"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">AudioLLMs</span>: We use the plain prompt of Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T1\" title=\"Table 1 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> to instruct AudioLLMs to pairwise score the naturalness of two audios. For the closed-source models, we use the official API released by Google<span class=\"ltx_note ltx_role_footnote\" id=\"footnote10\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">10</sup><span class=\"ltx_tag ltx_tag_note\">10</span><a class=\"ltx_ref ltx_href\" href=\"https://ai.google.dev/gemini-api/docs/models\" title=\"\">https://ai.google.dev/gemini-api/docs/models</a></span></span></span> for Gemini and OpenAI<span class=\"ltx_note ltx_role_footnote\" id=\"footnote11\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">11</sup><span class=\"ltx_tag ltx_tag_note\">11</span><a class=\"ltx_ref ltx_href\" href=\"https://platform.openai.com/docs/models\" title=\"\">https://platform.openai.com/docs/models</a></span></span></span> for GPT. We use the model variants <span class=\"ltx_text ltx_font_typewriter\">gemini-2.5-flash</span>, <span class=\"ltx_text ltx_font_typewriter\">gemini-2.5-pro</span>, <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-mini-audio-preview-2024-12-17</span>, and <span class=\"ltx_text ltx_font_typewriter\">gpt-4o-audio-preview-2025-06-03</span> for Gemini-2.5-Flash, Gemini-2.5-Pro, GPT-4o mini Audio, and GPT-4o Audio.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "model",
                    "gemini25flash",
                    "audiollms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Using AudioLLM as a judge models, prompt engineering strategies are usually believed crucial for improving the performance <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib64\" title=\"\">64</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib36\" title=\"\">36</a>]</cite>. Some common prompt engineering strategies include using the CoT prompts to activate the model&#8217;s thinking and reasoning abilities <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib59\" title=\"\">59</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib34\" title=\"\">34</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib36\" title=\"\">36</a>]</cite>, or employing few-shot evaluation formats <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib64\" title=\"\">64</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib53\" title=\"\">53</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "performance",
                    "cot"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on our preliminary qualitative analysis, we believe the reason why the open-source AudioLLMs do not work well with the CoT prompt is that their foundational capabilities are relatively weak. These weaknesses include instruction-following (such as format-following), multiple-audio understanding, long-text generation, and reasoning abilities. This is also why, when we developed SpeechJudge-GRM, we did not directly apply RLVR on top of Qwen2.5-Omni-7B. Instead, we used an initial SFT stage as a cold start.</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "when",
                    "cot",
                    "audiollms"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SFT Stage</span>&#8195;We use Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>]</cite> to generate the CoT data for SpeechJudge-Data (train). For the total 42K samples, Gemini-2.5-Flash&#8217;s judgments agree with human feedback on 25K samples, while they disagree on 17K samples. During the SFT stage, we fine-tune Qwen2.5-Omni-7B (Thinker) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>]</cite> on the 25K CoT data using LoRA <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib20\" title=\"\">20</a>]</cite> with a rank of 128. We use Adam <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib29\" title=\"\">29</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib35\" title=\"\">35</a>]</cite> as the optimizer and set the learning rate to 5e-5. The maximum number of tokens per batch is 4000. We select the best checkpoint on SpeechJudge-Data (dev) as the SFT model, SpeechJudge-GRM (SFT).</p>\n\n",
                "matched_terms": [
                    "total",
                    "cot",
                    "model",
                    "gemini25flash"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RL Stage</span>&#8195;We use the 17K samples (as described above) to conduct DAPO <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib58\" title=\"\">58</a>]</cite>, which is an enhanced variant of GRPO <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib42\" title=\"\">42</a>]</cite>. We utilize the <span class=\"ltx_text ltx_font_typewriter\">ms-swift<span class=\"ltx_note ltx_role_footnote\" id=\"footnote12\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">12</span></span><a class=\"ltx_ref ltx_href ltx_font_serif\" href=\"https://github.com/modelscope/ms-swift\" title=\"\">https://github.com/modelscope/ms-swift</a></span></span></span></span> toolkit to launch the training process. We initialize the policy model with the SFT model and use LoRA training with a rank of 64. The number of rollouts for each prompt is set to 8, and the batch size is 32. The learning rate is 5e-6. We select the best checkpoint on SpeechJudge-Data (dev) as the final SpeechJudge-GRM model, i.e., SpeechJudge-GRM (SFT+RL).</p>\n\n",
                "matched_terms": [
                    "prompt",
                    "model"
                ]
            }
        ]
    },
    "S12.T6": {
        "source_file": "SpeechJudge: Towards Human-Level Judgment for Speech Naturalness",
        "caption": "Table 6: Post-training of Qwen2.5-0.5B-TTS based on SpeechJudge.",
        "body": "Model\nRegular\nArticulatory\nCode-switching\nCross-lingual\nExpressive\nAvg\n\n\nWER\nSIM\nWER\nSIM\nWER\nSIM\nWER\nSIM\nWER\nSIM\nWER\nSIM\n\n\nQwen2.5-0.5B-TTS\n2.63\n0.698\n10.53\n0.679\n23.87\n0.666\n10.51\n0.593\n11.10\n0.706\n11.73\n0.668\n\n\nw/ INTP\n2.06\n0.697\n8.62\n0.694\n18.37\n0.663\n7.12\n0.588\n9.80\n0.708\n9.19\n0.670\n\n\nw/ SpeechJudge-Data\n2.12\n0.698\n8.92\n0.678\n19.01\n0.657\n7.72\n0.583\n9.97\n0.707\n9.55\n0.664\n\n\nw/ SpeechJudge-GRM (offline)\n2.31\n0.698\n7.83\n0.681\n15.36\n0.662\n7.84\n0.593\n9.72\n0.709\n8.51\n0.668\n\n\nw/ SpeechJudge-GRM (online)\n2.35\n0.696\n8.45\n0.674\n15.87\n0.653\n7.82\n0.580\n9.79\n0.702\n8.85\n0.661",
        "html_code": "<table class=\"ltx_tabular ltx_align_middle\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_rr ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Regular</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Articulatory</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Code-switching</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Cross-lingual</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Expressive</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Avg</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">WER</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">SIM</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">WER</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">SIM</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">WER</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">SIM</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">WER</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">SIM</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">WER</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">SIM</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">WER</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">SIM</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_rr ltx_border_t\">Qwen2.5-0.5B-TTS</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">2.63</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">0.698</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">10.53</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">0.679</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">23.87</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">0.666</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">10.51</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr ltx_border_t\">0.593</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">11.10</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr ltx_border_t\">0.706</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">11.73</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.668</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right ltx_border_rr ltx_border_t\">w/ INTP</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">2.06</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">0.697</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">8.62</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">0.694</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">18.37</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">0.663</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">7.12</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr ltx_border_t\">0.588</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">9.80</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr ltx_border_t\">0.708</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">9.19</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\">0.670</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right ltx_border_rr\">w/ SpeechJudge-Data</td>\n<td class=\"ltx_td ltx_align_right\">2.12</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">0.698</td>\n<td class=\"ltx_td ltx_align_right\">8.92</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">0.678</td>\n<td class=\"ltx_td ltx_align_right\">19.01</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">0.657</td>\n<td class=\"ltx_td ltx_align_right\">7.72</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\">0.583</td>\n<td class=\"ltx_td ltx_align_right\">9.97</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\">0.707</td>\n<td class=\"ltx_td ltx_align_right\">9.55</td>\n<td class=\"ltx_td ltx_align_right\">0.664</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right ltx_border_rr\">w/ SpeechJudge-GRM (offline)</td>\n<td class=\"ltx_td ltx_align_right\">2.31</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">0.698</td>\n<td class=\"ltx_td ltx_align_right\">7.83</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">0.681</td>\n<td class=\"ltx_td ltx_align_right\">15.36</td>\n<td class=\"ltx_td ltx_align_right ltx_border_r\">0.662</td>\n<td class=\"ltx_td ltx_align_right\">7.84</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\">0.593</td>\n<td class=\"ltx_td ltx_align_right\">9.72</td>\n<td class=\"ltx_td ltx_align_right ltx_border_rr\">0.709</td>\n<td class=\"ltx_td ltx_align_right\">8.51</td>\n<td class=\"ltx_td ltx_align_right\">0.668</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_rr\">w/ SpeechJudge-GRM (online)</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">2.35</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\">0.696</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">8.45</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\">0.674</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">15.87</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\">0.653</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">7.82</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_rr\">0.580</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">9.79</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_rr\">0.702</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">8.85</td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb\">0.661</td>\n</tr>\n</table>\n\n",
        "informative_terms_identified": [
            "speechjudgedata",
            "codeswitching",
            "regular",
            "offline",
            "model",
            "speechjudge",
            "qwen2505btts",
            "articulatory",
            "sim",
            "online",
            "avg",
            "expressive",
            "wer",
            "intp",
            "speechjudgegrm",
            "crosslingual",
            "based",
            "posttraining"
        ],
        "citing_paragraphs": [
            "<p class=\"ltx_p\">We use SeedTTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib2\" title=\"\">2</a>]</cite> and Amphion-TTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib60\" title=\"\">60</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib62\" title=\"\">62</a>]</cite> as evaluation sets. We present the objective results (WER and SIM) in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S12.T6\" title=\"Table 6 &#8227; 12.2 Objective Results &#8227; 12 Sample Selection and Post-Training based on SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> and the subjective results in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6\" title=\"Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>.\nWe observe that both intelligibility and naturalness are enhanced for all the four methods after post-training. Additionally, the post-training method based on SpeechJudge-GRM achieves a greater improvement in naturalness (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6.sf1\" title=\"Figure 6(a) &#8227; Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6(a)</span></a>). Besides, the SpeechJudge-based methods could match or lead to a slight improvement in speaker similarity (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6.sf2\" title=\"Figure 6(b) &#8227; Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6(b)</span></a>).</p>\n\n",
            "<p class=\"ltx_p\">We present the objective results (WER and SIM) of the Qwen2.5-0.5B-TTS post-training in Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S12.T6\" title=\"Table 6 &#8227; 12.2 Objective Results &#8227; 12 Sample Selection and Post-Training based on SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>. The results show that all four post-training methods significantly improve the WER. This trend is similar to the subjective intelligibility results shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6.sf1\" title=\"Figure 6(a) &#8227; Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6(a)</span></a>.</p>\n\n"
        ],
        "contextual_paragraphs": [
            {
                "html": "<p class=\"ltx_p\">Aligning large generative models with human feedback is a critical challenge. In speech synthesis, this is particularly pronounced due to the lack of a large-scale human preference dataset, which hinders the development of models that truly align with human perception. To address this, we introduce <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SpeechJudge</span>, a comprehensive suite comprising a dataset, a benchmark, and a reward model centered on <span class=\"ltx_text ltx_font_italic\">naturalness</span>&#8212;one of the most fundamental subjective metrics for speech synthesis. First, we present <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SpeechJudge-Data</span>, a large-scale human feedback corpus of 99K speech pairs. The dataset is constructed using a diverse set of advanced zero-shot text-to-speech (TTS) models across diverse speech styles and multiple languages, with human annotations for both intelligibility and naturalness preference. From this, we establish <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SpeechJudge-Eval</span>, a challenging benchmark for speech naturalness judgment. Our evaluation reveals that existing metrics and AudioLLMs struggle with this task; the leading model, Gemini-2.5-Flash, achieves less than 70% agreement with human judgment, highlighting a significant gap for improvement. To bridge this gap, we develop <span class=\"ltx_text ltx_font_bold ltx_font_italic\">SpeechJudge-GRM</span>, a generative reward model (GRM) based on Qwen2.5-Omni-7B. It is trained on SpeechJudge-Data via a two-stage post-training process: Supervised Fine-Tuning (SFT) with Chain-of-Thought rationales followed by Reinforcement Learning (RL) with GRPO on challenging cases. On the SpeechJudge-Eval benchmark, the proposed SpeechJudge-GRM demonstrates superior performance, achieving 77.2% accuracy (and 79.4% after inference-time scaling @10) compared to a classic Bradley-Terry reward model (72.7%). Furthermore, SpeechJudge-GRM can be also employed as a reward function during the post-training of speech generation models to facilitate their alignment with human preferences.</p>\n\n",
                "matched_terms": [
                    "model",
                    "speechjudge",
                    "speechjudgegrm",
                    "speechjudgedata",
                    "based",
                    "posttraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">1. A Large-scale Human Feedback Dataset: <span class=\"ltx_text ltx_font_italic\">SpeechJudge-Data</span>.</span>\nWe recruit human annotators to provide feedback on synthesized speeches, with a focus on assessing two fundamental speech aspects: <span class=\"ltx_text ltx_font_italic\">intelligibility</span> and <span class=\"ltx_text ltx_font_italic\">naturalness</span>. For data synthesis, we employ a diverse set of advanced, open-source zero-shot TTS models with varying architectures (such as CosyVoice2 <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib14\" title=\"\">14</a>]</cite>, Ints <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>]</cite>, F5-TTS <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib7\" title=\"\">7</a>]</cite>, and MaskGCT <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib52\" title=\"\">52</a>]</cite>) to produce the compared speech pairs. We prepare speech references in both regular and expressive styles, construct multilingual target texts, and cover both monolingual and cross-lingual synthesis scenarios to ensure data diversity (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.SS1\" title=\"3.1 Dataset Construction &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">3.1</span></a>). We instruct human annotators to perform two tasks based on a speech pair (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F1\" title=\"Figure 1 &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>): (a) pointwise annotation of text accuracy to assess intelligibility, and (b) pairwise preference annotation to judge relative speech naturalness. This extensive effort, involving 69 labelers over two months, results in 99K annotated pairs, with each pair receiving an average of 2.49 annotations from different labelers. We believe the SpeechJudge-Data can serve as a valuable corpus for alignment research in speech synthesis (e.g., DPO alignment <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib39\" title=\"\">39</a>]</cite> or reward modeling <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib44\" title=\"\">44</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib37\" title=\"\">37</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib3\" title=\"\">3</a>]</cite> in Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5\" title=\"5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>).</p>\n\n",
                "matched_terms": [
                    "speechjudgedata",
                    "regular",
                    "expressive",
                    "crosslingual",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">2. An Evaluation Benchmark for Speech Naturalness Judgment: <span class=\"ltx_text ltx_font_italic\">SpeechJudge-Eval</span>.</span>\nWe design a dedicated evaluation benchmark for the task of speech naturalness judgment. The task is structured as follows: given a target text and two corresponding speech samples, a model needs to judge which one is more natural. To construct the evaluation set, we select a subset from the SpeechJudge-Data where human annotators demonstrated high inter-annotator agreement, ensuring a high-quality ground truth. We assess the naturalness judgment capabilities of a wide range of metrics and models, including Word Error Rate (WER) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib38\" title=\"\">38</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib16\" title=\"\">16</a>]</cite>, Fr&#233;chet Audio Distance (FAD) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib27\" title=\"\">27</a>]</cite>, MOS predictors <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib41\" title=\"\">41</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib40\" title=\"\">40</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib48\" title=\"\">48</a>]</cite>, Deepfake Detectors <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib25\" title=\"\">25</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib49\" title=\"\">49</a>]</cite>, and AudioLLMs <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib28\" title=\"\">28</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib53\" title=\"\">53</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib22\" title=\"\">22</a>]</cite>. Our evaluations reveal that even the most capable model&#8212;specifically, Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>]</cite> in our experiments&#8212;achieved less than 70% agreement with human preferences. This finding highlights a significant performance gap and underscores the substantial room for research and improvement in automated speech naturalness judgment.</p>\n\n",
                "matched_terms": [
                    "model",
                    "speechjudgedata",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">3. A Generative Reward Model for Speech Naturalness: <span class=\"ltx_text ltx_font_italic\">SpeechJudge-GRM</span>.</span>\nTo develop a reward model that more effectively captures human preferences, we develop SpeechJudge-GRM, a generative reward model (GRM) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib59\" title=\"\">59</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib34\" title=\"\">34</a>]</cite> trained on the SpeechJudge-Data. Specifically, we base our model on Qwen2.5-Omni-7B <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>]</cite> and design a two-stage post-training process. During the first stage, we perform SFT as the &#8220;cold start&#8221; to improve the model&#8217;s instruction-following and rationale-based reasoning capabilities. To achieve this, we leverage Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>]</cite> to generate Chain-of-Thought (CoT) data for speech naturalness judgment task. In the second stage, we focus on more challenging cases of SpeechJudge-Data, which we define as instances where Gemini-2.5-Flash fails to make the correct judgment. Treating the human-annotated labels as the verifiable reward <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib11\" title=\"\">11</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib34\" title=\"\">34</a>]</cite>, we apply the GRPO-based RL stage <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib42\" title=\"\">42</a>]</cite>. Our experiments demonstrate that when trained on the same data, SpeechJudge-GRM significantly outperformed the classic Bradley-Terry reward model (BTRM) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib4\" title=\"\">4</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib39\" title=\"\">39</a>]</cite>, achieving a higher accuracy in predicting human preferences (77.2% for SpeechJudge-GRM vs. 72.7% for SpeechJudge-BTRM, Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Methodology &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>). Besides, SpeechJudge-GRM also supports inference-time scaling and offers explainability through its CoT outputs. Furthermore, SpeechJudge-GRM can also be employed as an objective naturalness metric for sample selection (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F5\" title=\"Figure 5 &#8227; 5.1 Methodology &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>) or as a reward function in RL algorithms to enhance the quality of existing speech generation models (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6\" title=\"Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6</span></a>).</p>\n\n",
                "matched_terms": [
                    "model",
                    "speechjudgedata",
                    "speechjudgegrm",
                    "posttraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Our work is grounded in <span class=\"ltx_text ltx_font_bold\">SpeechJudge-Data</span>, a large-scale human feedback corpus for assessing the <span class=\"ltx_text ltx_font_italic\">intelligibility</span> and <span class=\"ltx_text ltx_font_italic\">naturalness</span> of synthesized speech. Formally, we aim to construct a dataset <math alttext=\"\\mathcal{D}=\\{(t,a_{1},a_{2})\\}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m1\" intent=\":literal\"><semantics><mrow><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\mathcal{D}=\\{(t,a_{1},a_{2})\\}</annotation></semantics></math>, where each triplet comprises a pair of synthesized speech samples <math alttext=\"(a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{1},a_{2})</annotation></semantics></math> and the corresponding target text <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m3\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>. We instruct annotators to provide pointwise intelligibility and pairwise naturalness preference annotations based on <math alttext=\"\\mathcal{D}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.p1.m4\" intent=\":literal\"><semantics><mi class=\"ltx_font_mathcaligraphic\">&#119967;</mi><annotation encoding=\"application/x-tex\">\\mathcal{D}</annotation></semantics></math> (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F1\" title=\"Figure 1 &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>).</p>\n\n",
                "matched_terms": [
                    "speechjudgedata",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt Construction</span>&#8195;To build diverse prompts <math alttext=\"(a_{ref},t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{ref},t)</annotation></semantics></math> for TTS, for <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p3.m2\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math>, we adopt both <span class=\"ltx_text ltx_font_bold\">regular</span> and <span class=\"ltx_text ltx_font_bold\">expressive</span> speech samples. The regular samples are randomly selected from the Emilia-Large dataset <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib19\" title=\"\">19</a>]</cite>. The expressive samples are sourced from corpora rich in paralinguistics, including the emotional corpora: ParaSpeechCaps <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib9\" title=\"\">9</a>]</cite>, the accented corpora: L2-Arctic <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib63\" title=\"\">63</a>]</cite> and KeSpeech <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib46\" title=\"\">46</a>]</cite>, the whisper samples from an in-house corpus, and the character voices from video games Genshin Impact <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib43\" title=\"\">43</a>]</cite>. We display the detailed distribution of speech references in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F2.sf2\" title=\"Figure 2(b) &#8227; Figure 2 &#8227; 3.1 Dataset Construction &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2(b)</span></a>.</p>\n\n",
                "matched_terms": [
                    "regular",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">The target text <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m1\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math> paired with each <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m2\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math> is constructed as follows: For regular <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m3\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math> samples, we randomly sample transcriptions from the Emilia-Large dataset <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib19\" title=\"\">19</a>]</cite>. These are then refined using DeepSeek-V3 <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib10\" title=\"\">10</a>]</cite> to correct typos and normalize punctuations. For expressive <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m4\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math> samples, we instruct DeepSeek-V3 to generate several scripts in different writing styles, tailored to the topic of <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m5\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math> (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S7.SS1\" title=\"7.1 Details of Prompt Construction &#8227; 7 Details of SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">7.1</span></a> for more details). The languages of the target texts included Chinese (<span class=\"ltx_text ltx_font_italic\">zh</span>), English (<span class=\"ltx_text ltx_font_italic\">en</span>), and Chinese-English code-switching (<span class=\"ltx_text ltx_font_italic\">mixed</span>). For the combinations <math alttext=\"(a_{ref},t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{ref},t)</annotation></semantics></math>, we include both monolingual settings (<span class=\"ltx_text ltx_font_italic\">en2en</span> and <span class=\"ltx_text ltx_font_italic\">zh2zh</span>) and cross-lingual settings (<span class=\"ltx_text ltx_font_italic\">zh2en</span>, <span class=\"ltx_text ltx_font_italic\">en2zh</span>, <span class=\"ltx_text ltx_font_italic\">zh2mixed</span>, and <span class=\"ltx_text ltx_font_italic\">en2mixed</span>), where <span class=\"ltx_text ltx_font_italic\">zh2en</span> denotes Chinese <math alttext=\"a_{ref}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m7\" intent=\":literal\"><semantics><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><annotation encoding=\"application/x-tex\">a_{ref}</annotation></semantics></math> with English <math alttext=\"t\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m8\" intent=\":literal\"><semantics><mi>t</mi><annotation encoding=\"application/x-tex\">t</annotation></semantics></math>, and similarly for others. The distribution of the language settings of <math alttext=\"(a_{ref},t)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS1.p4.m9\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mrow><mi>r</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>f</mi></mrow></msub><mo>,</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{ref},t)</annotation></semantics></math> is shown in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F2.sf3\" title=\"Figure 2(c) &#8227; Figure 2 &#8227; 3.1 Dataset Construction &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2(c)</span></a>.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "regular",
                    "crosslingual",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Statistics</span>&#8195;We recruit 69 annotators and conduct annotations over two months. The resulting constructed dataset <math alttext=\"D\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m1\" intent=\":literal\"><semantics><mi>D</mi><annotation encoding=\"application/x-tex\">D</annotation></semantics></math>, which we denote as SpeechJudge-Data (raw), contains 99K <math alttext=\"(t,a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p2.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(t,a_{1},a_{2})</annotation></semantics></math> samples, with each sample receiving an average of 2.49 annotations from different labelers. The market value of this annotation scale is estimated at over 500K RMB (about 70K USD). Based on the raw dataset, we also construct several subsets for analysis and reward model training. We provide detailed descriptions of each subset and its applications in the following sections and in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S7.SS2\" title=\"7.2 Subsets of SpeechJudge-Data &#8227; 7 Details of SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "speechjudgedata",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Human Agreement Analysis</span>&#8195;We analyze the human annotations for naturalness in this section; discussions regarding intelligibility are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S8.SS2\" title=\"8.2 Intelligibility Annotation Analysis &#8227; 8 Human Annotation Details &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">8.2</span></a>. For naturalness annotations, we evaluate the inter-annotator agreement across our constructed dataset. To simplify the analysis, given the sample <math alttext=\"(t,a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m1\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(t,a_{1},a_{2})</annotation></semantics></math>, we transform the five-scale naturalness scale (CMOS) into a ternary classification system: either <math alttext=\"a_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m2\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">a_{1}</annotation></semantics></math> is better, <math alttext=\"a_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m3\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">a_{2}</annotation></semantics></math> is better, or their quality is a Tie. Based on this simplified classification, we categorize the annotation results into four distinct levels of agreement<span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><span class=\"ltx_text ltx_font_bold\">Note</span>: Each sample of SpeechJudge-Data is independently annotated by a minimum of two and a maximum of three annotators (Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S8\" title=\"8 Human Annotation Details &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>).</span></span></span>: (1) <span class=\"ltx_text ltx_font_bold\">Full Agreement (FA)</span>: A consensus is reached among all annotators, with all ratings pointing to the same outcome (e.g., &#8220;2A&#8221;, &#8220;3A&#8221;, &#8220;2B&#8221;, &#8220;3B&#8221;). We use &#8220;2A&#8221; to indicate that two annotators both rated <math alttext=\"a_{1}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m4\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>1</mn></msub><annotation encoding=\"application/x-tex\">a_{1}</annotation></semantics></math> as better, while &#8220;3B&#8221; denotes three annotators all rating <math alttext=\"a_{2}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS2.p3.m5\" intent=\":literal\"><semantics><msub><mi>a</mi><mn>2</mn></msub><annotation encoding=\"application/x-tex\">a_{2}</annotation></semantics></math> as better. (2) <span class=\"ltx_text ltx_font_bold\">Weak Agreement (WA)</span>: This level captures cases where two annotators agree on a specific polarity, while the third annotator marks a Tie (e.g., &#8220;2A+1T&#8221;, &#8220;2B+1T&#8221;). We also include the &#8220;2T+1A&#8221; and &#8220;2T+1B&#8221; cases in this level. (3) <span class=\"ltx_text ltx_font_bold\">Weak Disagreement (WD)</span>: This occurs when two annotators&#8217; ratings share the same polarity, but the third&#8217;s rating is the opposite (e.g., &#8220;2A+B&#8221;, &#8220;2B+A&#8221;). (4) <span class=\"ltx_text ltx_font_bold\">Full Disagreement (FD)</span>: This represents a complete lack of consensus, where all three annotators provide different classifications, denoted as &#8220;1A+1B+1T&#8221;.</p>\n\n",
                "matched_terms": [
                    "speechjudgedata",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S3.F3\" title=\"Figure 3 &#8227; 3.2 Human Annotation &#8227; 3 SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we demonstrate the distribution of these human agreement levels for the SpeechJudge-Data and its two subsets, <span class=\"ltx_text ltx_font_italic\">regular</span> and <span class=\"ltx_text ltx_font_italic\">expressive</span> (which are defined by their speech references). The figure shows that about 70% of the entire dataset falls into the Full Agreement (51.5%) or Weak Agreement (17.2%) levels. Furthermore, we observe that the <span class=\"ltx_text ltx_font_italic\">expressive</span> subset has a lower agreement level than the <span class=\"ltx_text ltx_font_italic\">regular</span> subset, which suggests that human evaluation of expressive speech generation is inherently a more challenging problem. Besides this sample-level agreement analysis, we also analyze the reliability of individual annotators, and we will discuss this in the Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S8.SS1\" title=\"8.1 Individual Annotator Reliability &#8227; 8 Human Annotation Details &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">8.1</span></a>.</p>\n\n",
                "matched_terms": [
                    "regular",
                    "speechjudgedata",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Evaluation Data</span>&#8195;We sample a subset of the SpeechJudge-Data to create the evaluation set for SpeechJudge-Eval. Specifically, we first select a subset that contains only <span class=\"ltx_text ltx_font_italic\">preference data</span> (i.e., we filter out samples with the &#8220;Tie&#8221; annotation), and then choose only those with full-agreement-level (FA) samples to ensure a high-quality ground truth. We perform sampling from both the <span class=\"ltx_text ltx_font_italic\">regular</span> and <span class=\"ltx_text ltx_font_italic\">expressive</span> subsets of SpeechJudge-Data and proportionally cover the three target text languages (<span class=\"ltx_text ltx_font_italic\">zh</span>, <span class=\"ltx_text ltx_font_italic\">en</span>, and <span class=\"ltx_text ltx_font_italic\">mixed</span>) within each subset. The final SpeechJudge-Eval dataset consists of 1,000 samples. The construction details of SpeechJudge-Eval and its distribution can be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S7.SS2\" title=\"7.2 Subsets of SpeechJudge-Data &#8227; 7 Details of SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a>.</p>\n\n",
                "matched_terms": [
                    "regular",
                    "speechjudgedata",
                    "expressive"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Objective metrics</span>, such as WER <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib38\" title=\"\">38</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib16\" title=\"\">16</a>]</cite>, SIM <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib6\" title=\"\">6</a>]</cite>, and FAD <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib27\" title=\"\">27</a>]</cite> in audio generation tasks. We assume that a better value of these metrics (e.g., lower for WER and FAD; higher for SIM) indicates better naturalness.</p>\n\n",
                "matched_terms": [
                    "wer",
                    "sim"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on the proposed SpeechJudge-Data, we further explore how to train a reward model capable of accurately capturing human preferences. Specifically, we propose <span class=\"ltx_text ltx_font_bold\">SpeechJudge-GRM</span>, where we leverage the inherent audio understanding capabilities of AudioLLMs (specifically, Qwen2.5-Omni-7B <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>]</cite>) to elicit their speech naturalness judgment capability. Compared to the classic BTRM <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib4\" title=\"\">4</a>]</cite>, the key strengths of GRM are its ability to enable Chain-of-Thought (CoT) reasoning and its support for test-time computation via majority voting, which ultimately leads to improved preference judgment performance <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib59\" title=\"\">59</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "speechjudgedata",
                    "speechjudgegrm",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We develop SpeechJudge-GRM based on Qwen2.5-Omni-7B (Thinker) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>]</cite>. Inspired by the powerful capabilities of RL with the verifiable reward (RLVR) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib42\" title=\"\">42</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib11\" title=\"\">11</a>]</cite>, our natural initial approach is to treat the human preference <math alttext=\"y_{\\mathcal{H}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m1\" intent=\":literal\"><semantics><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></msub><annotation encoding=\"application/x-tex\">y_{\\mathcal{H}}</annotation></semantics></math> for the pair <math alttext=\"(a_{1},a_{2})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.m2\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(a_{1},a_{2})</annotation></semantics></math> as a verifiable reward, and launch a RLVR training based on Qwen2.5-Omni.\nHowever, in practice, we find that the instruction-following reasoning capabilities of Qwen2.5-Omni are very weak (more detailed discussions can be found in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S10\" title=\"10 More Evaluation Results of Existing AudioLLMs &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>). Therefore, we adopt a two-stage post-training process (&#8220;SFT + RL&#8221;) to develop SpeechJudge-GRM (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F4\" title=\"Figure 4 &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>). We describe the details as follows.</p>\n\n",
                "matched_terms": [
                    "speechjudgegrm",
                    "based",
                    "posttraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SFT Stage</span>&#8195;We consider SFT as a &#8220;cold start&#8221; stage to improve the Qwen2.5-Omni&#8217;s instruction-following, reasoning, and speech naturalness understanding capabilities. We select Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>]</cite>&#8212;one of the leading closed-source models on SpeechJudge-Eval (Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T2\" title=\"Table 2 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">2</span></a>)&#8212;to serve as a teacher model, and instruct it to generate the CoT data. Specifically, for each sample <math alttext=\"d=(t,a_{1},a_{2},y_{\\mathcal{H}})\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m1\" intent=\":literal\"><semantics><mrow><mi>d</mi><mo>=</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo>,</mo><msub><mi>a</mi><mn>1</mn></msub><mo>,</mo><msub><mi>a</mi><mn>2</mn></msub><mo>,</mo><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">d=(t,a_{1},a_{2},y_{\\mathcal{H}})</annotation></semantics></math> from SpeechJudge-Data, we use the CoT prompt from Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S4.T1\" title=\"Table 1 &#8227; 4 SpeechJudge-Eval &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">1</span></a> (denoted as <math alttext=\"\\mathbf{I}_{CoT}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m2\" intent=\":literal\"><semantics><msub><mi>&#119816;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{I}_{CoT}</annotation></semantics></math>) to instruct Gemini-2.5-Flash to generate a rationale-based output (denoted as <math alttext=\"\\mathbf{O}_{teacher}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m3\" intent=\":literal\"><semantics><msub><mi>&#119822;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{O}_{teacher}</annotation></semantics></math>). We then extract the preference judgment (<math alttext=\"y_{\\mathcal{M}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m4\" intent=\":literal\"><semantics><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi></msub><annotation encoding=\"application/x-tex\">y_{\\mathcal{M}}</annotation></semantics></math>) from this output. For samples where Gemini-2.5-Flash&#8217;s preference is consistent with the human (i.e., <math alttext=\"y_{\\mathcal{M}}=y_{\\mathcal{H}}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m5\" intent=\":literal\"><semantics><mrow><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8499;</mi></msub><mo>=</mo><msub><mi>y</mi><mi class=\"ltx_font_mathcaligraphic\">&#8459;</mi></msub></mrow><annotation encoding=\"application/x-tex\">y_{\\mathcal{M}}=y_{\\mathcal{H}}</annotation></semantics></math>), we concatenate the CoT prompt and the model&#8217;s output, <math alttext=\"[\\mathbf{I}_{CoT},\\mathbf{O}_{teacher}]\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m6\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119816;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><mo>,</mo><msub><mi>&#119822;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[\\mathbf{I}_{CoT},\\mathbf{O}_{teacher}]</annotation></semantics></math>, to create a data point for our SFT dataset. Conversely, we consider the sample <math alttext=\"d\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m7\" intent=\":literal\"><semantics><mi>d</mi><annotation encoding=\"application/x-tex\">d</annotation></semantics></math> a challenging case and reserve the prompt <math alttext=\"\\mathbf{I}_{CoT}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m8\" intent=\":literal\"><semantics><msub><mi>&#119816;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{I}_{CoT}</annotation></semantics></math> for the second-stage RL dataset. During the SFT stage, for each training sample <math alttext=\"[\\mathbf{I}_{CoT},\\mathbf{O}_{teacher}]\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m9\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>&#119816;</mi><mrow><mi>C</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>o</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>T</mi></mrow></msub><mo>,</mo><msub><mi>&#119822;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[\\mathbf{I}_{CoT},\\mathbf{O}_{teacher}]</annotation></semantics></math>, we perform the next token prediction only on the segment <math alttext=\"\\mathbf{O}_{teacher}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p2.m10\" intent=\":literal\"><semantics><msub><mi>&#119822;</mi><mrow><mi>t</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>a</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>c</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>h</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>e</mi><mo lspace=\"0em\" rspace=\"0em\">&#8203;</mo><mi>r</mi></mrow></msub><annotation encoding=\"application/x-tex\">\\mathbf{O}_{teacher}</annotation></semantics></math>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "speechjudgedata"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We denote the training dataset of SpeechJudge-GRM as SpeechJudge-Data (train). Its construction process is as follows (see Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S7.SS2\" title=\"7.2 Subsets of SpeechJudge-Data &#8227; 7 Details of SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">7.2</span></a> for more details). Based on the raw SpeechJudge-Data, we first filter out all samples at the Full Disagreement (FD) level. For the other samples&#8212;at the FA, WA, and WD levels&#8212;we apply a majority voting principle among annotators to determine the final label for each. We then further exclude samples with a &#8220;Tie&#8221; label, using only the remaining preference data to form the SpeechJudge-Data (train).\nWe use LoRA <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib20\" title=\"\">20</a>]</cite> to fine-tune the GRM during both the SFT and RL stages. Other experimental setup details are provided in Appendix <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S11\" title=\"11 Training Details of SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">11</span></a>.</p>\n\n",
                "matched_terms": [
                    "speechjudgegrm",
                    "speechjudgedata",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">From the results of Table <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.T3\" title=\"Table 3 &#8227; 5.1 Methodology &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>, we can observe that: (1) The SpeechJudge-BTRM achieves a 72.7% agreement with human preferences on SpeechJudge-Eval, a level of performance comparable to the initial development of BTRMs in the textual LLM RLHF field <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib44\" title=\"\">44</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib37\" title=\"\">37</a>]</cite>. (2) After conducting SFT training with the CoT data, the accuracy of SpeechJudge-GRM (SFT) reaches 75.3%. Besides, further RLVR training improves the final model SpeechJudge-GRM (SFT+RL) to an accuracy of 77.2%. (3) Due to the generative nature of the GRM, we can further enhance the accuracy of SpeechJudge-GRM using inference-time scaling. For example, by using majority voting across 10 outputs instead of just one, the accuracy is improved by approximately 2 percentage points (75.3% <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> 77.6%; 77.2% <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS2.p2.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">&#8594;</mo><annotation encoding=\"application/x-tex\">\\rightarrow</annotation></semantics></math> 79.4%). These results collectively verify the effectiveness of our proposed SpeechJudge-GRM for judging speech naturalness.</p>\n\n",
                "matched_terms": [
                    "speechjudgegrm",
                    "model"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigate the effect of SpeechJudge-based reward models for high-quality sample selection. We use the hard cases from SeedTTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib2\" title=\"\">2</a>]</cite> and the code-switching cases from Amphion-TTS-Eval <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib60\" title=\"\">60</a>]</cite> as target texts. For each text, we instruct the Qwen2.5-Omni-7B (Talker) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib57\" title=\"\">57</a>]</cite> to generate 100 speeches. We then ask human subjects to compare the best-of-100 output&#8212;as selected by either SpeechJudge-BTRM or SpeechJudge-GRM&#8212;against a randomly sampled output. The evaluation measures the win/lose/tie ratios based on speech naturalness. From Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F5\" title=\"Figure 5 &#8227; 5.1 Methodology &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5</span></a>, we observe that the best-of-100 samples selected by both SpeechJudge-BTRM and SpeechJudge-GRM are more likely to outperform a randomly selected sample from the same set. This finding demonstrates the advantage of using the SpeechJudge-Data corpus for training human-aligned reward model. Furthermore, SpeechJudge-GRM exhibits better performance than SpeechJudge-BTRM, which highlights the superiority of the proposed GRM.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "model",
                    "speechjudgegrm",
                    "speechjudgedata",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We investigate the effect of using SpeechJudge-GRM as a reward function for post-training of TTS model. Specifically, we develop a new zero-shot TTS model, <span class=\"ltx_text ltx_font_bold\">Qwen2.5-0.5B-TTS</span>, to serve as the base model, which was not involved in the construction of the SpeechJudge-Data. This model is based on Qwen2.5-0.5B <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib57\" title=\"\">57</a>]</cite>, adopts the classic two-stage &#8220;AR+Diffusion&#8221; architecture <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib13\" title=\"\">13</a>]</cite>, uses the speech tokenizer from DualCodec <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib31\" title=\"\">31</a>]</cite>, and is pre-trained on the Emilia dataset <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib19\" title=\"\">19</a>]</cite>.</p>\n\n",
                "matched_terms": [
                    "model",
                    "qwen2505btts",
                    "speechjudgegrm",
                    "speechjudgedata",
                    "based",
                    "posttraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on this pre-trained model, we design four comparative methods:\n(1) <span class=\"ltx_text ltx_font_bold\">w/ INTP</span>: We use the intelligibility preference dataset, INTP <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib61\" title=\"\">61</a>]</cite>, to perform offline DPO alignment <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib39\" title=\"\">39</a>]</cite>.\n(2) <span class=\"ltx_text ltx_font_bold\">w/ SpeechJudge-Data</span>: We use the SpeechJudge-Data (train) to perform offline DPO alignment.\n(3) <span class=\"ltx_text ltx_font_bold\">w/ SpeechJudge-GRM (offline)</span>: We use SpeechJudge-GRM as an offline preference data annotator. We take all speech pairs from the INTP dataset and re-annotate their preference labels using SpeechJudge-GRM, then perform offline DPO alignment on the resulting data.\n(4) <span class=\"ltx_text ltx_font_bold\">w/ SpeechJudge-GRM (online)</span>: We use SpeechJudge-GRM as a reward function for the online DPO algorithm <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib18\" title=\"\">18</a>]</cite>. The training data consists of only the prompts from INTP (i.e., the target texts and speech references for zero-shot TTS).</p>\n\n",
                "matched_terms": [
                    "offline",
                    "model",
                    "online",
                    "intp",
                    "speechjudgegrm",
                    "speechjudgedata",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">In this work, we address the challenge of aligning speech synthesis with human perception of naturalness by introducing SpeechJudge, a suite including a large-scale dataset (SpeechJudge-Data), a challenging benchmark (SpeechJudge-Eval), and a generative reward model (SpeechJudge-GRM). Our benchmark reveals that even top AudioLLMs struggle with naturalness judgment, achieving less than 70% human agreement. In contrast, our proposed SpeechJudge-GRM reaches 77.2% accuracy (up to 79.4% with inference-time scaling @10), outperforming the classic Bradley-Terry reward models (72.7%). We also demonstrate its practical utility as a reward function to effectively enhance TTS model naturalness via post-training. While our primary focus is on naturalness, future work could extend this framework to other subjective attributes like speaker similarity and emotional expressiveness, potentially through multi-objective reward modeling. By releasing our resources, we aim to catalyze research in building more human-aligned speech generation systems.</p>\n\n",
                "matched_terms": [
                    "model",
                    "speechjudge",
                    "speechjudgegrm",
                    "speechjudgedata",
                    "posttraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">For the target texts paired with the expressive speech references, we use DeepSeek-V3 to generate several scripts in different writing styles based on the speech reference&#8217;s text, the prompt used is listed below.</p>\n\n",
                "matched_terms": [
                    "expressive",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We construct several subsets based on SpeechJudge-Data (Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S7.F8\" title=\"Figure 8 &#8227; 7.2 Subsets of SpeechJudge-Data &#8227; 7 Details of SpeechJudge-Data &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">8</span></a>). We begin with the <span class=\"ltx_text ltx_font_bold\">SpeechJudge-Data (raw)</span> corpus, containing 99K pairs, where each pair is annotated by multiple labelers as a five-scale naturalness CMOS. We aggregate these annotations via a majority vote for each pair, and subsequently discard all &#8220;Tie&#8221; pairs, yielding the 79K-pair human <span class=\"ltx_text ltx_font_italic\">preference</span> data, denoted as <span class=\"ltx_text ltx_font_bold\">SpeechJudge-Data (pref)</span>.</p>\n\n",
                "matched_terms": [
                    "speechjudgedata",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During our preliminary analysis based on SpeechJudge-Data (pref), we observe that a significant disparity in intelligibility between two speech samples can overshadow the subtler quality of naturalness, biasing human preference toward the more comprehensible sample. To mitigate this confounding factor and create a more high-quality dataset focused specifically on naturalness, we further refine the data. Specifically, we retain only pairs where the absolute WER gap of those is below 12%. This process results in the 44K-pair high-quality <span class=\"ltx_text ltx_font_bold\">SpeechJudge-Data (hq)</span> subset, ensuring that its preference labels are more reflective of genuine differences in naturalness.</p>\n\n",
                "matched_terms": [
                    "speechjudgedata",
                    "based",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">We use all the speech samples from SpeechJudge-Data (raw) for this analysis. We visualize the relationship between WER and the subjective text accuracy in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S8.F10\" title=\"Figure 10 &#8227; 8.2 Intelligibility Annotation Analysis &#8227; 8 Human Annotation Details &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">10</span></a>.\nFor the regular speeches (the orange curve), we observe a consistent negative correlation: as the WER increases, its perceived text accuracy steadily declines. For the expressive speeches (the green curve), the similar trend holds for expressive speech when WER is under about 12%. When WER is over the threshold, however, the correlation between WER and the subjective text accuracy weakens significantly. We think this divergence is sourced from that the greater stylistic variations in expressive speech pose a substantial challenge to the robustness of ASR systems compared to the regular samples.</p>\n\n",
                "matched_terms": [
                    "regular",
                    "speechjudgedata",
                    "expressive",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">WER</span> <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib38\" title=\"\">38</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib16\" title=\"\">16</a>]</cite>: We employ <span class=\"ltx_text ltx_font_typewriter\">Whisper-large-v3<span class=\"ltx_note ltx_role_footnote\" id=\"footnote4\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">4</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">4</span></span><a class=\"ltx_ref ltx_href ltx_font_serif\" href=\"https://huggingface.co/openai/whisper-large-v3\" title=\"\">https://huggingface.co/openai/whisper-large-v3</a></span></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib38\" title=\"\">38</a>]</cite> for English texts, and <span class=\"ltx_text ltx_font_typewriter\">Paraformer-zh<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">5</span></span><a class=\"ltx_ref ltx_href ltx_font_serif\" href=\"https://huggingface.co/funasr/paraformer-zh\" title=\"\">https://huggingface.co/funasr/paraformer-zh</a></span></span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib15\" title=\"\">15</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib16\" title=\"\">16</a>]</cite> for Chinese and code-switching texts.</p>\n\n",
                "matched_terms": [
                    "codeswitching",
                    "wer"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Based on our preliminary qualitative analysis, we believe the reason why the open-source AudioLLMs do not work well with the CoT prompt is that their foundational capabilities are relatively weak. These weaknesses include instruction-following (such as format-following), multiple-audio understanding, long-text generation, and reasoning abilities. This is also why, when we developed SpeechJudge-GRM, we did not directly apply RLVR on top of Qwen2.5-Omni-7B. Instead, we used an initial SFT stage as a cold start.</p>\n\n",
                "matched_terms": [
                    "speechjudgegrm",
                    "based"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">SFT Stage</span>&#8195;We use Gemini-2.5-Flash <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib8\" title=\"\">8</a>]</cite> to generate the CoT data for SpeechJudge-Data (train). For the total 42K samples, Gemini-2.5-Flash&#8217;s judgments agree with human feedback on 25K samples, while they disagree on 17K samples. During the SFT stage, we fine-tune Qwen2.5-Omni-7B (Thinker) <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib56\" title=\"\">56</a>]</cite> on the 25K CoT data using LoRA <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib20\" title=\"\">20</a>]</cite> with a rank of 128. We use Adam <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib29\" title=\"\">29</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib35\" title=\"\">35</a>]</cite> as the optimizer and set the learning rate to 5e-5. The maximum number of tokens per batch is 4000. We select the best checkpoint on SpeechJudge-Data (dev) as the SFT model, SpeechJudge-GRM (SFT).</p>\n\n",
                "matched_terms": [
                    "model",
                    "speechjudgedata",
                    "speechjudgegrm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">RL Stage</span>&#8195;We use the 17K samples (as described above) to conduct DAPO <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib58\" title=\"\">58</a>]</cite>, which is an enhanced variant of GRPO <cite class=\"ltx_cite ltx_citemacro_citep\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#bib.bib42\" title=\"\">42</a>]</cite>. We utilize the <span class=\"ltx_text ltx_font_typewriter\">ms-swift<span class=\"ltx_note ltx_role_footnote\" id=\"footnote12\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">12</sup><span class=\"ltx_tag ltx_tag_note\"><span class=\"ltx_text ltx_font_serif\">12</span></span><a class=\"ltx_ref ltx_href ltx_font_serif\" href=\"https://github.com/modelscope/ms-swift\" title=\"\">https://github.com/modelscope/ms-swift</a></span></span></span></span> toolkit to launch the training process. We initialize the policy model with the SFT model and use LoRA training with a rank of 64. The number of rollouts for each prompt is set to 8, and the batch size is 32. The learning rate is 5e-6. We select the best checkpoint on SpeechJudge-Data (dev) as the final SpeechJudge-GRM model, i.e., SpeechJudge-GRM (SFT+RL).</p>\n\n",
                "matched_terms": [
                    "model",
                    "speechjudgedata",
                    "speechjudgegrm"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">During the construction of SpeechJudge-Data, we hired human labelers from a data crowdsourcing company. To verify the effectiveness of our training for them and to ensure the high quality of both the dataset and the resulting SpeechJudge-GRM, the human subjects for the final sample selection and TTS post-training experiments (Section <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.SS3\" title=\"5.3 High-Quality Sample Selection based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5.3</span></a> and <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.SS4\" title=\"5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">5.4</span></a>) were all experienced speech generation researchers. All these researchers had extensive audio backgrounds, with a minimum of two years of experience in speech synthesis.</p>\n\n",
                "matched_terms": [
                    "speechjudgegrm",
                    "speechjudgedata",
                    "posttraining"
                ]
            },
            {
                "html": "<p class=\"ltx_p\">Regarding the SIM metric, both w/ INTP and w/ SpeechJudge-GRM (offline) either match or slightly outperform the baseline model, while the other two methods show a slight decline. However, the objective SIM results appear to be in slight conflict with the subjective speaker similarity results in Figure <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2511.07931v1#S5.F6.sf2\" title=\"Figure 6(b) &#8227; Figure 6 &#8227; 5.4 Post-Training of Zero-Shot TTS based on SpeechJudge-GRM &#8227; 5 SpeechJudge-GRM &#8227; SpeechJudge: Towards Human-Level Judgment for Speech Naturalness\"><span class=\"ltx_text ltx_ref_tag\">6(b)</span></a>. For instance, in the subjective evaluation, w/ INTP actually shows a decrease in speaker similarity (Win: 24.30%, Lose: 32.90%).</p>\n\n",
                "matched_terms": [
                    "offline",
                    "model",
                    "sim",
                    "intp",
                    "speechjudgegrm"
                ]
            }
        ]
    }
}